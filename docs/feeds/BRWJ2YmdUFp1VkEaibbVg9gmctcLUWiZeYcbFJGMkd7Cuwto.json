{"id":"BRWJ2YmdUFp1VkEaibbVg9gmctcLUWiZeYcbFJGMkd7Cuwto","title":"Stories by Kyodo Tech on Medium","displayTitle":"Dev - Kyodo-Tech","url":"https://medium.com/@kyodo-tech/feed","feedLink":"https://medium.com/@kyodo-tech?source=rss-ac02ab142942------2","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":1,"items":[{"title":"Local Diffusion Inference with Stable-Diffusion.cpp and Flux.1","url":"https://medium.com/@kyodo-tech/local-diffusion-inference-with-stable-diffusion-cpp-and-flux-1-5d1841fff9cf?source=rss-ac02ab142942------2","date":1738238284,"author":"Kyodo Tech","guid":290,"unread":true,"content":"<p><a href=\"https://github.com/leejet/stable-diffusion.cpp\">Stable-Diffusion.cpp</a> is designed for efficient, high-performance inference of Stable Diffusion models, especially on low-resource environments where traditional deep learning pipelines struggle. We provide an examination of SDcpp, its components, and the technical concepts that enable efficient inference of Stable Diffusion models on lower-end hardware such as CPUs and lightweight GPUs, such as Apple M-series chips. In particular, we’re looking at <a href=\"https://github.com/black-forest-labs/flux\">Flux.1</a>, a text-to-image model developed by <a href=\"https://blackforestlabs.ai/\">Black Forest&nbsp;Labs</a>.</p><h3>Introduction to Diffusion Models and Stable Diffusion</h3><p>Diffusion models, specifically Denoising Diffusion Probabilistic Models (DDPMs), have emerged as powerful generative frameworks capable of producing high-fidelity images from random noise through iterative denoising processes. Stable Diffusion extends the DDPM framework by incorporating text conditioning, allowing for the generation of images based on descriptive prompts. However, the computational demands of such models typically necessitate high-memory, high-bandwidth GPU resources, limiting their accessibility and scalability.</p><p>SDcpp addresses these limitations by providing an optimized inference framework that leverages techniques such as quantization, memory-efficient sampling, and optimized latent space representations to facilitate the deployment of Stable Diffusion models on devices with constrained computational resources. This paper delves into the architectural and algorithmic components of SDcpp, highlighting the methodologies that underpin its efficiency and performance.</p><p>The architecture is composed of several interdependent modules, each contributing to the inference process. First, it is <strong>loading the required models</strong> (1), including text encoders, the diffusion model, and the VAE decoder. It then performs  (2), tokenizing the input prompt and generating text embeddings using a CLIP-based model and an additional T5XXL text conditioning model. Next, it <strong>computes the conditioning graph</strong> (3), transforming embeddings to create a guidance vector for the diffusion process. The system then  (4) by iteratively refining a randomly initialized latent tensor using a noise schedule and a chosen sampling method, progressively steering it towards a structured latent representation in a number of . This step is the most compute-intensive, consuming roughly 90% of the total processing time. Once sampling completes, the pipeline <strong>decodes the latent representation</strong> (5) using a VAE model to reconstruct pixel-space information. Finally, it  (6) as an image file, completing the text-to-image generation process.</p><h4>Diffusion Model Core Components</h4><p>The diffusion model within SDcpp is the principal generative engine, responsible for transforming Gaussian noise into coherent images through a series of denoising . More steps allow finer details and smoother outputs but increase inference time, as each step requires additional computation. This process is governed by the DDPM framework, wherein the model iteratively refines the image by removing noise in a controlled manner. The diffusion model encompasses three primary components:</p><p><strong>UNet (Denoising Network):</strong> the UNet architecture employs an encoder-decoder structure with skip connections, enabling multi-scale feature extraction and reconstruction. The network predicts and removes noise at each denoising step, progressively enhancing image&nbsp;quality.</p><p><strong>Text Encoder (Conditioning Module):</strong> The text encoder converts textual prompts into latent embeddings that guide the denoising process. SDcpp supports multiple text encoders, including CLIP-L, CLIP-G, and T5XXL, each tailored for different model versions and memory considerations.</p><p><strong>Scheduler (Noise Controller):</strong> The scheduler orchestrates the noise removal strategy, dictating the temporal progression of denoising steps. It determines the rate and manner in which noise is reduced, balancing computational efficiency and image fidelity.</p><p>A recent model, <a href=\"https://huggingface.co/black-forest-labs/FLUX.1-schnell\"></a>&nbsp;, is optimized for low-resource environments, featuring a compact UNet architecture that accelerates inference by reducing the number of denoising steps. Flux.1 is designed for memory-efficient quantization and compatibility with the GGUF format, making it suitable for deployment on lightweight hardware. It reduces inference steps to only 4 steps and can achieve fast inference.</p><h4>Variational Autoencoder (VAE)</h4><p>VAEs facilitating the encoding of images into a lower-dimensional latent space. This reduction significantly diminishes computational and memory overhead while retaining high-resolution image details. Latent space is a <strong>compact mathematical representation</strong> of an image. Think of it as a highly efficient shorthand where only the most essential details are stored. Instead of storing every pixel, the image is encoded as numbers that describe its features. This makes it possible to work with images at a much lower computational cost. Stable Diffusion doesn’t generate images pixel by pixel. Instead, it creates images in latent space, where information is stored in a more compact and manageable way. Once the model finishes generating an image in this space, the VAE  it back into a high-resolution format that we can&nbsp;see.</p><p>Different VAEs can influence how well the final image looks. A generic VAE works for most images, but a model trained for a specific style, like anime or realistic photography, may use a custom VAE to better match its training data. A poorly matched VAE can cause color shifts, blurriness, or unwanted artifacts.</p><p>Technically, any model can use any VAE, but results vary. A VAE trained on cartoon-style images won’t reconstruct a photorealistic image correctly. This is why some models provide their own VAEs — they have been optimized to maintain details that matter most for a specific&nbsp;style.</p><h4>LoRA (Low-Rank Adaptation)</h4><p>LoRA facilitates model customization without the need to adjust the entire diffusion model. By modifying only a small subset of parameters (~10 million), LoRA significantly reduces memory usage compared to full model fine-tuning. Multiple LoRAs can be combined with weighted influences on the final image, enabling diverse stylistic and feature modifications. For instance, combining two LoRAs with different weights can be executed using the syntax &lt;lora:styleA:0.6&gt; &lt;lora:styleB:0.8&gt;. To optimize performance, it is advisable to minimize the number and strength of LoRAs, especially in memory-limited environments.</p><h4>Training and Inference File&nbsp;Formats</h4><p>The  is the standard PyTorch checkpoint format, that stores full precision weights and optimizer states. Supports training and fine-tuning but can execute arbitrary Python code, posing security risks. <strong>SafeTensors (.safetensors)</strong> is a safer alternative to CKPT, with structured, immutable storage and no executable code. Loads faster but still used mainly for training.</p><p><strong>GGUF (GPT-Generated Unified Format)</strong> is a binary model format introduced by llama.cpp, succeeding GGML. Initially designed for language models (like LLaMA models), GGUF has been adopted in diffusion models to enable efficient, quantized inference without reformatting model files into proprietary formats (e.g.,&nbsp;.ggml or&nbsp;.gguf conversions required previously). Traditional formats (.safetensors,&nbsp;.ckpt,&nbsp;.bin) store full-precision FP16/FP32 model weights, consuming significant VRAM. GGUF introduces quantization at multiple levels, storing models in 2-bit to 16-bit precision while maintaining compatibility with&nbsp;SDcpp.</p><p>Use GGUF for efficient inference on low-resource devices, prefer&nbsp;.safetensors for training.</p><p>In our experiments,  demonstrates an extremely fast denoising process, with images at 1 and 2 steps appearing nearly identical, suggesting that most structural information is determined in the very first iterations. By 4 steps, images diverge significantly, and by 6 steps, they change even more, indicating that additional steps introduce large variations rather than just refining details. Interestingly, <strong>Euler and Euler_a produce the best results</strong>, likely because they follow a balanced noise reduction trajectory, allowing structure to form naturally without over-smoothing or excessive deviations. Euler introduces noticeable changes between 3, 4, and 6 steps, including shifts in pose, face, and hair color, implying that it follows a latent space trajectory where small step changes can drastically impact the final result. Euler_a, being an ancestral sampler, adds controlled randomness at each step, further amplifying these changes. In contrast, <strong>LCM performs poorly with Flux 1</strong>, excessively blurring images, likely due to its tuning for models that require more iterative refinement, making it unsuitable for Flux 1’s fast-converging architecture. We observed that <strong>4 steps often produce better images</strong> than 5 or 6 steps, as extra iterations introduce unnecessary alterations instead of improving details, reinforcing that Flux 1 is optimized for extremely low-step inference. This is a stark contrast to Stable Diffusion models like SD 1.5 or SDXL, which typically require 20–50 steps for high-quality outputs. These findings highlight that Flux 1’s unique training favors fewer steps with the right sampler, making Euler and Euler_a better choices at around 4 steps, while models like LCM fail due to excessive denoising.</p><h4>Denoising Schedules and Sampling Integration</h4><p>Denoising schedules define a series of noise levels that guide reverse diffusion. These schedules set discrete or continuous noise decay steps, essentially parameterizing the stochastic differential equation used to reverse the diffusion process. Discrete schedules apply abrupt noise transitions, forcing the model to rapidly shift between latent states. In contrast, smooth schedules such as karras or exponential distribute noise reduction evenly, supporting more accurate latent&nbsp;updates.</p><p>Sampling methods numerically integrate these latent trajectories. Euler-type samplers update the latent state with fixed time step approximations, benefiting from stable transitions when paired with smooth noise decay. Latent Consistency Models (LCM) depend on gradual noise reduction; abrupt schedules can induce over-smoothing and detail loss. Our observations indicate that matching a sampling method’s sensitivity to noise decay with an appropriate denoising schedule is key to preserving image structure while ensuring efficient inference.</p><p>Flux.1’s behavior indicates that its training regime emphasizes rapid structure formation in early steps. Euler’s robustness across discrete, exponential, and karras schedules suggests that its integration of abrupt noise transitions still aligns well with Flux.1’s latent dynamics. LCM, being more sensitive to the noise decay, only works well with smoother schedules (exponential and karras) that provide gradual latent&nbsp;updates.</p><p>The failures with AYS and GITS imply that these schedules impose noise trajectories incompatible with Flux.1’s fast-convergence design. Their latent paths likely diverge from the model’s learned reverse diffusion trajectory, causing the state to collapse into uniform or noisy outputs rather than structured images. This sensitivity reinforces that Flux.1 is optimized for few-step denoising with specific noise decay characteristics, and departures from that — especially with schedules that enforce alternative trajectory regularity — can disrupt the image generation process.</p><p>Flux.1 departs from prior diffusion models by leveraging a T5-style text encoder instead of CLIP, allowing it to process full sentences fluently rather than relying on discrete token hierarchies. Unlike Stable Diffusion, where Booru-style tags and keyword emphasis are required for precision, <strong>Flux.1 understands natural language</strong> natively, reducing reliance on rigid token structures. Long, descriptive prompts yield better scene coherence, material accuracy, and relational awareness than fragmented keyword&nbsp;lists.</p><p>Trigger words are optional rather than mandatory, as the model activates concepts contextually rather than requiring explicit token matching. However, structured elements — short modifiers or category cues — can refine specificity without overpowering Flux.1’s broader language comprehension. The model is particularly strong at multi-concept blending, which traditionally caused token interference in CLIP-based architectures. Negative prompting is also more effective, reducing the unintended blending of attributes.</p><p>Flux.1 performs best with contextual and relational descriptions, leveraging semantic depth over isolated tokens. While Booru tags still function, hybrid prompting — mixing structured elements with full sentences — offers the best balance between control and generative adaptability.</p><p>LoRA functions differently in Flux.1 compared to Stable Diffusion. Since Flux.1 uses a T5-based encoder, LoRA activation does not require fixed trigger words but instead responds dynamically to natural descriptions. This allows LoRAs to integrate seamlessly into prompts without rigid dependencies.</p><p>Key parameters such as network dimension, alpha scaling, and captioning density influence performance. Higher network dimension strengthens adaptation but risks overfitting, while alpha scaling balances LoRA integration with the base model. <strong>Flux.1’s LoRAs benefit from mixed captioning strategies — varying between short tags and natural descriptions — ensuring activation without over-constraining the generative process.</strong> Unlike SDXL, which often requires explicit weight scaling (1.2–1.5x) to activate LoRAs, Flux.1 adapts LoRA strength more organically within context, reducing the need for manual weight adjustments.</p><p>To maximize LoRA performance in Flux.1, descriptive phrasing should replace rigid token triggers, using context-driven prompts to activate learned modifications naturally. Captions should balance specificity with generalization, ensuring LoRAs enhance rather than dominate the generative output.</p><h3>Optimization Strategies for Low-Resource Inference</h3><p>Quantization is a pivotal technique in reducing model size and memory usage by lowering the precision of model weights. SDcpp supports various quantization levels, each offering a different balance between memory efficiency and image&nbsp;quality:</p><ul><li> Reduces memory usage by approximately 75% compared to full precision (FP16) while maintaining good image quality. It introduces a slight computational overhead due to dequantization but is optimal for low-end hardware.</li><li> Offers slightly better precision than Q4 at the cost of requiring twice the memory. It is suitable for systems with more VRAM where higher image fidelity is&nbsp;desired.</li></ul><p>For lower-end hardware, Q4_K provides a good balance of speed and quality, while  is better for systems with more VRAM&nbsp;(&gt;16GB).</p><h4>Quantization Impact of Model&nbsp;Parts</h4><p>We’re loading a model, vae, clip, and t5, and can choose quantized parts for each. The model itself must be quantized due to memory constraints, but the VAE, CLIP, and T5 can remain in higher precision. We tested a q4_K model with safetensors in higher precision, and a fully quantized version where all parts were&nbsp;q4_0.</p><p>We observed <strong>a 2.5x slowdown with the fully quantized run</strong>, despite reduced memory usage. Likely due to increased dequantization overhead, precision mismatch-induced stalls, and reduced SIMD efficiency on the Apple M2. While background processes may have contributed to higher involuntary context switches (3.3M vs. 1.48M), they do not fully explain the slowdown. The quantization format (q4_K vs. q4_0) influences computational efficiency rather than raw inference speed, with q4_K employing grouped quantization, which improves data locality and reduces dequantization frequency, while q4_0 likely applies simpler per-tensor quantization, increasing compute overhead. The hybrid approach—quantizing only the diffusion model while keeping VAE, CLIP, and T5 in higher precision—seems to better utilize hardware by avoiding unnecessary precision conversion costs and leveraging FP computation optimizations. These results suggest that full quantization may degrade performance when not optimized for architectural strengths, particularly on FP-optimized hardware like the&nbsp;M2.</p><p>The sampling method determines the strategy by which noise is removed during the denoising process. Ancestral samplers introduce random variations at each step, leading to more diverse outputs, while fixed-step samplers follow a deterministic trajectory, yielding consistent results.</p><p>SDcpp integrates several sampling algorithms optimized for different performance criteria:</p><ul><li> A simple and fast method effective with a lower number of denoising steps, suitable for rapid inference.</li><li> Prioritizes image quality, requiring more steps and memory. It is ideal for applications where high detail is essential.</li><li><strong>Latent Consistency Models (LCM):</strong> Optimizes speed by drastically reducing the number of required denoising steps (e.g., 4–8 instead of 20–30), making it highly suitable for low-end systems. Users can specify the sampling method using the --sampling-method flag, such as --sampling-method lcm.</li></ul><p>Memory management is critical in ensuring that diffusion models run efficiently on constrained hardware. SDcpp employs several techniques to optimize memory&nbsp;usage:</p><ul><li> By processing the VAE decoder in smaller, manageable segments, VAE tiling minimizes peak memory&nbsp;usage.</li><li> Flash Attention optimizes memory allocation during UNet computations. On CUDA-enabled GPUs, it offers significant memory savings and speed improvements. Flash Attention can be activated via the --diffusion-fa flag.</li></ul><p>In our experiments on an Apple M2, enabling --vae-tiling reduced memory usage but increased execution time, suggesting a trade-off between efficiency and speed. When --diffusion-fa (Flash Attention) was enabled, execution time did not significantly improve. The combination of both options (vae-tiling and diffusion-fa) resulted in the  but also the , confirming that while these settings help on low-VRAM devices, they introduce performance overhead. The fastest run occurred with both options disabled, though it had the highest memory footprint.</p><p>Image resolution directly impacts both memory consumption and inference speed. Generating images at a resolution of 1280×640 requires approximately 2.5 times the memory of generating images at 512×512. To accommodate low-resource environments, users can opt for intermediate resolutions such as 768×384 or 1024×512, balancing image quality and computational efficiency.</p><h4>LoRA Delay and Conditioning Graph Recalculation</h4><p>When LoRA modules are applied, the conditioning graph must be recalculated due to LoRA’s modification of internal activations, which can reintroduce dependencies on structured tagging if the LoRA was trained on datasets requiring them. The conditioner parses and reweights prompt segments dynamically, adjusting token influence based on LoRA-induced shifts in the embedding space, ensuring compatibility between the altered diffusion model state and the input text. This recalibration process accounts for the delay seen post-LoRA application and before sampling, as the system must harmonize LoRA-induced parameter shifts with Flux.1’s inherently context-aware text encoding.</p><h4>Classifier-Free Guidance&nbsp;Scale</h4><p>CFG scale (Classifier-Free Guidance Scale) can be set with the --cfg-scale argument. It defaults to 7.0 and controls how strictly the model follows the text prompt, balancing adherence and creativity. Higher values (e.g., 10-15) force the model to generate images that closely match the prompt but can introduce artifacts, while lower values (e.g., 1-5) allow more randomness and artistic interpretation. It impacts performance slightly because higher CFG values increase the weight difference between conditional (prompted) and unconditional (random) predictions, requiring additional computation to balance them. However, the effect on inference speed is minor compared to factors like resolution, model architecture, and sampling&nbsp;steps.</p><p>Flux.1 seem to have been trained with  as target setting, meaning its internal weighting and contrast are optimized for that value. Lowering CFG to  reduces guidance too much, allowing excessive noise influence, resulting in darker, underexposed images. Raising it to 2.0 strengthens prompt adherence but overemphasizes highlights and contrast, making images appear overly bright. Unlike standard Stable Diffusion, where 7.0 is the default for balanced outputs, Flux.1’s tuning makes , with deviations causing unintended brightness shifts.</p><p>To analyze the impact of CFG scale on image generation, we generated eight images using CFG 1.0 and 2.0 across the Euler, LCM, iPNDM, and Euler_a samplers with 4 steps and a fixed seed of 42, using the prompt “a beautiful girl in futuristic Tokyo, neon lights,” comparing how different guidance levels influence brightness, contrast, and prompt adherence.</p><h4>Memory-Efficient Inference</h4><p>Running Flux.1 (schnell) on an Apple M2 (16GB RAM), we achieve inference without a GPU, completing a 256×256 image in 216.3s with 9.28GB RAM usage. The UNet required just 96.59MB, and VAE decoding used 416MB, demonstrating the efficiency of quantized GGUF&nbsp;models.</p><p>Our analysis of Stable-Diffusion.cpp and Flux.1 demonstrates that efficient local diffusion inference is feasible even on low-resource hardware, provided that model selection, quantization, and sampling strategies are optimized. Flux.1’s extremely low-step denoising makes it uniquely suited for fast inference, especially when paired with Euler and Euler_a samplers, which balance structure formation and controlled randomness. In contrast, models like LCM introduce excessive smoothing, making them unsuitable for Flux.1’s fast convergence.</p><p>We also observed that 4 steps yield optimal results with Flux.1, as additional iterations introduce unnecessary alterations rather than improving image fidelity. While our results indicate that full quantization introduces dequantization overhead and computational stalls, <em>external factors such as background system load may have influenced the extent of the observed slowdown</em>. Memory-efficient techniques such as VAE tiling and Flash Attention can reduce hardware requirements but may introduce performance trade-offs. Users should select Q4_K or similar quantization for lower-end hardware, as grouped quantization schemes appear to mitigate some of the performance penalties associated with fully quantized inference.</p><p>These findings highlight that with the right model architecture, quantization, and sampler choice, high-quality image generation is possible on constrained devices without relying on high-end GPUs. Future improvements may focus on further refining model architectures like Flux.1 and optimizing quantization methods for even faster, memory-efficient inference.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d1841fff9cf\" width=\"1\" height=\"1\" alt=\"\">","contentLength":22810,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}