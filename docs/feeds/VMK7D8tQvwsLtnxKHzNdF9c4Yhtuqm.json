{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"Best of HackerNews","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":50,"items":[{"title":"Switch to Claude without starting over","url":"https://claude.com/import-memory","date":1772350612,"author":"doener","guid":155231,"unread":true,"content":"<p>You’ve spent months teaching another AI how you work. That context shouldn’t disappear because you want to try something new. Claude can import what matters, so your first conversation feels like your hundredth.</p>","contentLength":215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47204571"},{"title":"Microgpt","url":"http://karpathy.github.io/2026/02/12/microgpt/","date":1772329166,"author":"tambourine_man","guid":155117,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47202708"},{"title":"The Windows 95 user interface: A case study in usability engineering (1996)","url":"https://dl.acm.org/doi/fullHtml/10.1145/238386.238611","date":1772317176,"author":"ksec","guid":155230,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47200904"},{"title":"Iran's Ayatollah Ali Khamenei is killed in Israeli strike, ending 36-year rule","url":"https://www.npr.org/2026/02/28/1123499337/iran-israel-ayatollah-ali-khamenei-killed","date":1772316968,"author":"andsoitis","guid":155229,"unread":true,"content":"<div><div><div><div aria-label=\"Image caption\"><p>\n                In this 2017 photo, Ayatollah Ali Khamenei, Iran's supreme leader, sits in a session to deliver his message for the Iranian New Year. A portrait of the late revolutionary founder, Ayatollah Ruhollah Khomeini, is next to him.\n                <b aria-label=\"Image credit\">\n                    \n                    Office of the Iranian Supreme Leader/AP\n                    \n                </b></p></div></div></div><div><div><p>In this 2017 photo, Ayatollah Ali Khamenei, Iran's supreme leader, sits in a session to deliver his message for the Iranian New Year. A portrait of the late revolutionary founder, Ayatollah Ruhollah Khomeini, is next to him.</p></div></div></div><p>Iran's supreme leader, Ayatollah Ali Khamenei, was <a href=\"https://www.npr.org/2026/02/28/nx-s1-5730158/israel-iran-strikes-trump-us\" target=\"_blank\">killed in Israeli attacks</a>, with U.S. support, on Saturday. He was 86 years old.</p><p>President Trump announced the Iranian leader's death <a href=\"https://truthsocial.com/@realDonaldTrump/posts/116150413051904167\" target=\"_blank\">on social media</a>, saying Khamenei could not avoid U.S. intelligence and surveillance. A source briefed on the U.S.-Israeli attacks on Iran told NPR earlier Saturday that an <a href=\"https://www.npr.org/2026/02/28/nx-s1-5730158/israel-iran-strikes-trump-us\" target=\"_blank\">Israeli airstrike killed Khamenei</a>.</p><p>During his 36-year rule, Khamenei was unwavering in his steadfast antipathy to the U.S. and Israel and to any efforts to reform and bring Iran into the 21st century.</p><p>Khamenei was born in July 1939 into a religious family in the Shia Muslim holy city of Mashhad in northeastern Iran and attended theological school. An outspoken opponent of the U.S.-backed Shah Mohammad Reza Pahlavi, Khamenei was arrested several times.</p><p>He was surrounded by other Iranian activists, including Ayatollah Ruhollah Khomeini, who became Iran's first supreme leader following the country's Islamic Revolution in the late 1970s.</p><p>Khamenei survived an assassination attempt in 1981 that cost him the use of his right arm. He served as Iran's president before succeeding Khomeini as supreme leader in 1989.</p><p>Alex Vatanka, a senior fellow at the Middle East Institute in Washington, D.C., says Khamenei was an unlikely candidate. Then a midlevel cleric, Khamenei lacked religious credentials, which left him feeling vulnerable, Vatanka says.</p><p>\"He knew himself. He didn't have the prestige, the gravitas to be … the successor to the founder of the Islamic Republic, Ayatollah Khomeini,\"he says. </p><div><div><div><div aria-label=\"Image caption\"><p>\n                In 2005, Ali Khamenei (center), newly elected President Mahmoud Ahmadinejad (right), outgoing President Mohammad Khatami and former President Ali Akbar Hashemi Rafsanjani attend Ahmadinejad's inaugural ceremony in Tehran.\n                <b aria-label=\"Image credit\">\n                    \n                    Atta Kenare/AFP via Getty Images\n                    \n                </b></p></div></div></div><div><div><p>In 2005, Ali Khamenei (center), newly elected President Mahmoud Ahmadinejad (right), outgoing President Mohammad Khatami and former President Ali Akbar Hashemi Rafsanjani attend Ahmadinejad's inaugural ceremony in Tehran.</p></div></div></div><p>\"He spent the first few years in power being very nervous,\" says Vatanka. \"He really literally felt that somebody is going to, you know, take him down from the position of power.\"</p><p>But Khamenei was cunning and able to outwit other senior political figures in the Islamic Republic, according to Ali Vaez, director of the Iran Project at the International Crisis Group. He says that with the help of the formidable Islamic Revolutionary Guard Corps, Khamenei built up his power base to become the longest-serving leader in the Middle East.</p><p>\"Ayatollah Khamenei was a man with strategic patience and was able to calculate a few steps ahead,\" he says.&nbsp;\"That's why I think he managed — on the back of the Revolutionary Guards — to increasingly appropriate all the levers of power in his hands and sideline everyone else.\"</p><p>Khamenei's close ties to the Revolutionary Guards allowed Iran's military to develop a vast commercial empire in control of many parts of the economy, while ordinary Iranians struggled to get by.</p><div><div><div><div aria-label=\"Image caption\"><p>\n                Ali Khamenei (right) speaks to members of the armed forces of the Islamic Republic during the Iran-Iraq War on Oct. 4, 1981.\n                </p></div></div></div><div><div><p>Ali Khamenei (right) speaks to members of the armed forces of the Islamic Republic during the Iran-Iraq War on Oct. 4, 1981.</p></div></div></div><p>Vaez says Khamenei also began to build up Iran's defensive policies, such as developing proxies like Hezbollah in Lebanon and Hamas in the Gaza Strip to deter a direct attack on Iranian soil.</p><p>\"And then also becoming self-reliant in developing a viable conventional deterrence, which took the form of Iran's ballistic missile program,\" Vaez says.</p><p>As supreme leader, Khamenei also had the final word on anything to do with Iran's nuclear program.</p><p>Over time, Khamenei increasingly injected himself into politics. Such was the case in 2009, when he intervened in the presidential election to ensure that his favored candidate, the controversial conservative Mahmoud Ahmadinejad, won office. </p><p>Iranians took to the streets to protest what was widely seen as a fraudulent election. Khamenei brutally crushed those demonstrations, triggering both a backlash and more protest movements over the years.</p><p>Iran killed thousands of its citizens under Khamenei's rule, including more than 7,000 people killed during weeks of mass protests that started in late December 2025, according to the <a href=\"https://www.en-hrana.org/day-50-of-the-protests-intensification-of-security-prosecutions-and-uncertainty-regarding-the-status-of-detainees/\" target=\"_blank\">Human Rights Activists News Agency</a>, a U.S.-based organization that closely tracks rights abuses in Iran.</p><div><div><div><div aria-label=\"Image caption\"><p>\n                Iran's supreme leader, Ayatollah Ali Khamenei (center), prays with the Iranian president and other government officials in Tehran in 2014.\n                <b aria-label=\"Image credit\">\n                    \n                    Anadolu Agency/Getty Images\n                    \n                </b></p></div></div></div><div><div><p>Iran's supreme leader, Ayatollah Ali Khamenei (center), prays with the Iranian president and other government officials in Tehran in 2014.</p></div></div></div><p>\"Khamenei had always supported and endorsed repressive government crackdown, recognizing that these protests were damaging to the stability and legitimacy of the state,\" says Sanam Vakil, an Iran expert at Chatham House, a London-based think tank.</p><p>But Khamenei was unconcerned about getting to the root of the protests, says the Middle East Institute's Vatanka, and remained stuck in an Islamic revolutionary mindset against the West.</p><p>\"He onso many occasions refused point-blank to accept the basic reality that where he was in terms of his worldview was not where the rest of his people were,\" Vatanka says.</p><p>He adds that 75% of Iran's 90 million people were born after the revolution and have watched other countries in the region modernize and integrate with the international community.</p><p>\"The 75% he should have catered to, listened to and address[ed] policies to satisfy their aspirations,\" he says. \"He failed in that miserably.\"</p><div><div><div><div aria-label=\"Image caption\"><p>\n                Ali Khamenei wears a mask due to the COVID-19 pandemic as he arrives to cast his ballot during Iran's presidential election on June 18, 2021.\n                <b aria-label=\"Image credit\">\n                    \n                    Atta Kenare/AFP via Getty Images\n                    \n                </b></p></div></div></div><div><div><p>Ali Khamenei wears a mask due to the COVID-19 pandemic as he arrives to cast his ballot during Iran's presidential election on June 18, 2021.</p></div></div></div><p>The International Crisis Group's Vaez says after the Arab Spring uprisings in 2011, Khamenei did start worrying about the survival of his regime. Iran's economy was crumbling, due in large part to stringent Western sanctions, fueling more unrest.</p><p>In 2013, Khamenei agreed to secret negotiations with the U.S. about Iran's nuclear program, which eventually led to the 2015 Joint Comprehensive Plan of Action nuclear agreement. Vaez says Khamenei deeply distrusted the U.S. and was skeptical about the deal.</p><p>\"His argument has always been that the U.S. is always looking for pretexts, for putting pressure on Iran,\" he says. \"And if Iran concedes on the nuclear issue, then the U.S. would put pressure on Iran because of its missiles program or because of human rights violations or because of its regional policies.\"</p><p>President Trump's withdrawal from the nuclear deal during his first term in office gave some credence to Khamenei's cynicism. Analysts say Iran increased its nuclear enrichment after that to a point where it was close to being able to build a bomb.</p><p>In early 2025, when Trump reached out to Iran about a new deal, Khamenei dragged out negotiations until they began in mid-April.</p><p>But time ran out. In June,Israel made good on its threat to neutralize Iran's nuclear program, launching strikes on key facilities and killing scientists and generals. Iran retaliated, and the two sides exchanged several days of missile strikes.</p><p>On June 21, 2025, the U.S. <a href=\"https://www.npr.org/2025/06/21/nx-s1-5441127/iran-us-strike-nuclear-trump\" target=\"_blank\">launched major airstrikes</a> on three of Iran's nuclear enrichment sites. Trump said the facilities had been \"completely and totally obliterated,\" although there was debate among the White House and nuclear experts as to how serious Iran's nuclear program had been set back.</p><p>Vakil, of Chatham House, says Khamenei underestimated what Israel and the U.S. would do.</p><p>\"I think that Khamenei always assumed that he could play for time, and what he really didn't understand is that the world around Iran had very much changed,\" she says. \"The world had tired of Khamenei and Iranian foot-dragging and antics …&nbsp;and so that was a miscalculation.\"</p><p>But it was Iran's use of proxy militias across the region that eventually led to Khamenei's downfall. </p><p>When Hamas — the Palestinian Islamist group backed by Iran — attacked Israel on Oct. 7, 2023, killing nearly 1,200 people and kidnapping 251 others, it triggered a cascade of events that ultimately led to Israel's attack on Iran.&nbsp;</p><p>The day after the 2023 Hamas-led attack, Iran-backed Hezbollah in Lebanon started firing rockets into Israel, triggering a conflict that led to the Shia militia's top brass being decimated — including top leader <a href=\"https://www.npr.org/2024/09/28/g-s1-25302/who-was-hassan-nasrallah-the-hezbollah-leader-killed-by-israel\" target=\"_blank\">Hassan Nasrallah</a>.</p><p>Israel and Iran traded direct airstrikes for the first time in 2024 as part of that conflict.</p><p>Israel's bombing of Iranian weapons shipments in Syria also helped weaken the regime of Syria's then-dictator, Bashar al-Assad, an important ally of Iran. Assad fell in December 2024 and fled to Russia in early January 2025.</p><p>By the time Khamenei died, his legacy was in tatters. Israel had hobbled two key proxies, Hamas and Hezbollah, and had wiped out Iran's air defenses. With U.S. help, it left Iran's nuclear program in shambles.</p><p>What remains is a robust ballistic missile program, the brainchild of Khamenei. It's unclear who will replace him to lead a now weakened and vulnerable Iran.</p>","contentLength":10332,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47200879"},{"title":"We do not think Anthropic should be designated as a supply chain risk","url":"https://twitter.com/OpenAI/status/2027846016423321831","date":1772313856,"author":"golfer","guid":155098,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47200420"},{"title":"Our Agreement with the Department of War","url":"https://openai.com/index/our-agreement-with-the-department-of-war","date":1772310929,"author":"surprisetalk","guid":155228,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47199948"},{"title":"Qwen3.5 122B and 35B models offer Sonnet 4.5 performance on local computers","url":"https://venturebeat.com/technology/alibabas-new-open-source-qwen3-5-medium-models-offer-sonnet-4-5-performance","date":1772310000,"author":"lostmsu","guid":155227,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47199781"},{"title":"Techno‑feudal elite are attempting to build a twenty‑first‑century fascist state","url":"https://collapseofindustrialcivilization.com/2026/02/16/americas-oligarchic-techno-feudal-elite-are-attempting-to-build-a-twenty-first-century-fascist-state/","date":1772305063,"author":"measurablefunc","guid":155097,"unread":true,"content":"<p><strong>Introduction: Fascism at the End of Industrial Civilization</strong></p><p>This essay argues that the United States is drifting toward a distinctly twenty‑first‑century form of fascism driven not by mass parties in brownshirts, but by an oligarchic techno‑feudal elite. Neoliberal capitalism has hollowed out democratic institutions and concentrated power in a transnational “authoritarian international” of billionaires, security chiefs, and political fixers who monetize state power while shielding one another from accountability. At the same time, Big Tech platforms have become neo‑feudal estates that extract rent from our data and behavior, weaponize disinformation, and provide the surveillance backbone of an emerging global police state.</p><p>Drawing on the work of Robert Reich, William I. Robinson, Yanis Varoufakis, and others, alongside historian Heather Cox Richardson’s detailed account of Trump‑era patronage, whistleblower suppression, and DHS/ICE mega‑detention plans, the essay contends that America is rapidly constructing a system of concentration‑camp infrastructure and paramilitary policing designed to manage “surplus” populations and political dissent. Elite impunity, entrenched through national‑security exceptionalism, legal immunities, and revolving‑door careers, means that those directing lawless violence face virtually no consequences. Elections still happen, courts still sit, newspapers still publish, but substantive power is increasingly exercised by unelected oligarchs, tech lords, and security bureaucracies.</p><p>This authoritarian drift cannot be separated from the broader crisis of industrial civilization. Ecological overshoot, climate chaos, resource constraints, and structural economic stagnation have undermined the promise of endless growth on which liberal democracy once rested. Rather than using the remnants of industrial wealth to democratize a just transition, ruling elites are hardening borders, expanding carceral infrastructure, and building a security regime to contain “surplus” humanity in a world of shrinking energy and material throughput. America’s oligarchic techno‑feudal fascism is thus not an anomaly, but one plausible endgame of industrial civilization: a stratified order of gated enclaves above and camps and precarity below, designed to preserve elite power as the old industrial world comes apart.</p><p><strong>I. From liberal promise to oligarchic capture</strong></p><p>The American republic was founded on a promise that power would be divided, constrained, and answerable: a written constitution, separated branches, periodic elections, and a Bill of Rights that set bright lines even the sovereign could not cross. That promise was always compromised by slavery, settler colonialism, and gendered exclusion, but it retained real, if uneven, force as a normative horizon. What has shifted over the past half‑century is not simply the familiar gap between creed and practice, but the underlying structure of the system itself: the center of gravity has moved from public institutions toward a private oligarchy whose wealth and leverage allow it to function as a parallel sovereign.</p><p>The neoliberal turn of the 1970s and 1980s marked the decisive inflection point. Deregulation, financial liberalization, the crushing of organized labor, and the privatization of public goods redistributed power and income upward on a historic scale. Trade liberalization and capital mobility allowed corporations and investors to pit governments and workers against one another, extracting subsidies and tax concessions under the permanent threat of capital flight. At the same time, Supreme Court decisions eroded limits on political spending, redefining “speech” as something that could be purchased in unlimited quantities by those with the means.</p><p>The result, as Robert Reich notes, has been the consolidation of an American oligarchy that “paved the road to fascism” by ensuring that public policy reflects donor preferences far more consistently than popular majorities. In issue after issue, such as taxation, labor law, healthcare, and environmental regulation, there is a clear skew: the wealthy get what they want more often than not, while broadly popular but redistributive policies routinely die in committee or are gutted beyond recognition. This is not a conspiracy in the melodramatic sense; it is how the wiring of the system now works.</p><p>William Robinson’s analysis of “twenty‑first‑century fascism” sharpens the point. Global capitalism in its current form generates chronic crises: overproduction, under‑consumption, ecological breakdown, and a growing population that capital cannot profitably employ. Under such conditions, democratic politics becomes dangerous for elites, because electorates might choose structural reforms such as wealth taxes, public ownership, strong unions, and Green New Deal‑style transitions that would curb profits. Faced with this prospect, segments of transnational capital begin to see authoritarian solutions as rational: better to hollow out democracy, harden borders, and construct a global police state than to accept serious redistribution.</p><p>American politics in the early twenty‑first century fits this pattern with unsettling precision. A decaying infrastructure, stagnant wages, ballooning personal debt, militarized policing, and permanent war have produced widespread disillusionment. As faith in institutions erodes, public life is flooded with resentment and nihilism that can be redirected against scapegoats (immigrants, racial minorities, feminists, and queer and trans people) rather than against the oligarchic‑power‑complex that profits from the decay. It is in this vacuum that a figure like Donald Trump thrives: a billionaire demagogue able to channel anger away from the class that actually governs and toward those even more marginalized.</p><p>The decisive shift from plutocratic dysfunction to fascist danger occurs when oligarchs cease to see constitutional democracy as even instrumentally useful and instead invest in movements openly committed to minority rule. Koch‑style networks, Mercer‑funded operations, and Silicon Valley donors willing to underwrite hard‑right projects are not supporting democracy‑enhancing reforms; they are building the infrastructure for authoritarianism, from voter suppression to ideological media to data‑driven propaganda. The system that emerges is hybrid: elections still occur, courts still sit, newspapers still publish, but substantive power is increasingly concentrated in unelected hands.</p><p><strong>II. The “authoritarian international” and the shadow world of deals</strong></p><p>Historian Heather Cox Richardson’s <a href=\"https://www.youtube.com/watch?v=ajZudGu4exA\">recent analysis</a> captures a formation that much mainstream commentary still struggles to name: a transnational <strong>“authoritarian international”</strong> in which oligarchs, political operatives, royal families, security chiefs, and organized criminals cooperate to monetize state power while protecting one another from scrutiny. This is not a formal alliance; it is an overlapping ecology of relationships, exclusive vacations, investment vehicles, shell companies, foundations, and intelligence ties, through which information, favors, and money flow.</p><p>The key is that this network is structurally post‑ideological. As Robert Mueller warned in his 2011 description of an emerging “iron triangle” of politicians, businesspeople, and criminals, these actors are not primarily concerned with religion, nationality, or traditional ideology. They will work across confessional and national lines so long as the deals are lucrative and risk is manageably socialized onto others. Saudi royals invest alongside Western hedge funds; Russian oligarchs launder money through London property and American private equity; Israeli and Emirati firms collaborate with U.S. tech companies on surveillance products that are then sold worldwide.</p><p>Within this milieu, the formal distinction between public office and private interest blurs. Richardson’s analysis of Donald Trump’s abrupt reversal on the Gordie Howe International Bridge after a complaint by a billionaire competitor with ties to Jeffrey Epstein—reads less like the exercise of public policy judgment and more like feudal patronage: the sovereign intervenes to protect a favored lord’s toll road. Tiny shifts in regulatory posture or federal support can move billions of dollars; for those accustomed to having the president’s ear, such interventions are simply part of doing business.</p><p>The same logic governs foreign policy. The Trump‑Kushner axis exemplifies this fusion of public and private. When a whistleblower alleges that the Director of National Intelligence suppressed an intercept involving foreign officials discussing Jared Kushner and sensitive topics like Iran, and when the complaint is then choked off with aggressive redaction and executive privilege, we see <strong>the machinery of secrecy misused not to protect the national interest but to shield a member of the family‑cum‑business empire at the center of power</strong>. It is as if the state has become a family office with nuclear weapons.</p><p>Josh Marshall’s phrase <strong>“authoritarian international”</strong> is apt because it names both the class composition and the political function of this network. The same names recur across far‑right projects: donors and strategists who back nationalist parties in Europe, ultras in Latin America, Modi’s BJP in India, and the MAGA movement in the United States. Their interests are not identical, but they overlap around a shared agenda: weakening labor and environmental protections, undermining independent media and courts, militarizing borders, and securing immunity for themselves and their peers.</p><p>This world is lubricated by blackmail and mutually assured destruction. As Richardson notes, players often seem to hold compromising material on one another, whether in the form of documented sexual abuse, financial crime, or war crimes. This shared vulnerability paradoxically stabilizes the network: as long as everyone has something on everyone else, defection is dangerous, and a predatory equilibrium holds. From the standpoint of democratic publics, however, this stability is catastrophic, because it means that scandal—once a mechanism for enforcing norms—loses much of its power. When “everyone is dirty,” no one can be clean enough to prosecute the others without risking exposure.</p><p><strong>III. Techno‑feudal aristocracy and the colonization of everyday life</strong></p><p>Layered atop this transnational oligarchy is the digital order that Varoufakis and others describe as techno‑feudalism: a regime in which a handful of platforms function like neo‑feudal estates, extracting rent from their “serfs” (users, gig workers, content creators) rather than competing in open markets. This shift is more than metaphor. In classical capitalism, firms profited primarily by producing goods or services and selling them on markets where competitors could, in principle, undercut them. In the platform order, gatekeepers profit by controlling access to the marketplace itself, imposing opaque terms on those who must use their infrastructure to communicate, work, or even find housing.</p><p>This can be seen across sectors:</p><ul><li><p>Social media platforms own the digital public square. They monetize attention by selling advertisers access to finely sliced demographic and psychographic segments, while their recommendation algorithms optimize for engagement, often by privileging outrage and fear.</p></li><li><p>Ride‑hailing and delivery apps control the interface between customers and labor, setting prices unilaterally and disciplining workers through ratings, algorithmic management, and the ever‑present threat of “deactivation.”</p></li><li><p>Cloud providers and app stores gatekeep access to the basic infrastructure upon which countless smaller firms depend, taking a cut of transactions and reserving the right to change terms or remove competitors from the ecosystem entirely.</p></li></ul><p>In each case, the platform is less a company among companies and more a landlord among tenants, collecting tolls for the right to exist within its domain. Users produce the very capital stock, data, content, behavioral profiles, that platforms own and monetize, yet they have little say over how this material is used or how the digital environment is structured. The asymmetry of power is profound: the lords can alter the code of the world; the serfs can, at best, adjust their behavior to avoid algorithmic invisibility or sanction.</p><p>For authoritarian politics, this structure is a gift. First, platforms have become the primary vectors of disinformation and propaganda. Cambridge Analytica’s work for Trump in 2016, funded by billionaires like the Mercers, was an early prototype: harvest data, micro‑target individuals with tailored&nbsp;messaging, and flood their feeds with narratives designed to activate fear and resentment. Since then, the techniques have grown more sophisticated, and far‑right movements worldwide have learned to weaponize meme culture, conspiracy theories, and “shitposting” as recruitment tools.</p><p>Second, the same infrastructures that enable targeted advertising enable granular surveillance. Location data, social graphs, search histories, and facial‑recognition databases provide an unprecedented toolkit for monitoring and disciplining populations. In the hands of a regime sliding toward fascism, these tools can be turned against dissidents with terrifying efficiency: geofencing protests to identify attendees, scraping social media to build dossiers, using AI to flag “pre‑criminal” behavior. The emerging “global police state” that Robinson describes depends heavily on such techno‑feudal capacities.</p><p>Third, the digital order corrodes the very preconditions for democratic deliberation. Information overload, filter bubbles, and algorithmic amplification of sensational content produce a public sphere saturated with noise. Under these conditions, truth becomes just another aesthetic, and the distinction between fact and fiction collapses into vibes. This is the post‑modern nihilism you name: a sense that nothing is stable enough to believe in, that everything is spin. Fascist movements do not seek to resolve this condition; they weaponize it, insisting that only the Leader and his trusted media tell the real truth, while everything else is a hostile lie.</p><p>Finally, the techno‑feudal aristocracy’s material interests align with authoritarianism. Privacy regulations, antitrust enforcement, data localization rules, and strong labor rights all threaten platform profits. Democratic movements that demand such reforms are therefore adversaries. Conversely, strongman leaders who promise deregulation, tax breaks, and law‑and‑order crackdowns, even if they occasionally threaten specific firms, are often acceptable partners. The result is a convergence: oligarchs of data and oligarchs of oil, real estate, and finance finding common cause in an order that disciplines the many and exempts the few.</p><p><strong>IV. Elite impunity and the machinery of lawlessness</strong></p><p>Authoritarianism is not only about who holds power; it is about who is answerable for wrongdoing. A system where elites can violate laws with impunity while ordinary people are punished harshly for minor infractions is already halfway to fascism, whatever labels it wears. The United States has, over recent decades, constructed precisely such a system.</p><p>The Arab Center’s “Machinery of Impunity” report details how, in areas ranging from mass surveillance to foreign wars to domestic policing, senior officials who authorize illegal acts almost never face criminal consequences. Edward Snowden’s revelations exposed systemic violations of privacy and civil liberties, yet it was the whistleblower who faced prosecution and exile, not the architects of the programs. Torture during the “war on terror” was acknowledged, even documented in official reports, but those who designed and approved the torture regime kept their law licenses, academic posts, and media gigs. Lethal strikes on small boats in the Caribbean and Pacific, justified by secret intelligence and shielded by classified legal opinions, have killed dozens with no public evidence that the targets posed imminent threats.</p><p>This pattern is not an aberration but a feature. As a Penn State law review article <a href=\"https://insight.dickinsonlaw.psu.edu/cgi/viewcontent.cgi?article=1144&amp;context=jlia\">notes</a>, the U.S. legal system builds in multiple layers of protection for high officials: sovereign immunity, state secrets privilege, narrow standing rules, and prosecutorial discretion all combine to make it extraordinarily difficult to hold the powerful to account. Violations of the Hatch Act, campaign‑finance laws, or ethics rules are often treated as technicalities, and when reports do document unlawful behavior, as in the case of Mike Pompeo’s partisan abuse of his diplomatic office, there are “no consequences” beyond mild censure. Jamelle Bouie’s recent video essay for the New York Times drives the point home: America is “bad at accountability” because institutions have been designed and interpreted to favor elite impunity.</p><p>Richardson shows how this culture functions inside the national‑security state. A whistleblower complaint alleging that the Director of National Intelligence suppressed an intelligence intercept involving Jared Kushner and foreign officials was not allowed to run its course. Instead, it was bottled up, then transmitted to congressional overseers in a highly redacted form, with executive privilege invoked to shield the president’s involvement. The same mechanisms that insulate covert operations abroad from democratic oversight are deployed to protect domestic political allies from scrutiny.</p><p>Immigration enforcement offers another window. The Arab Center notes that ICE raids, family separation, and other abuses “escalated under the current Trump administration into highly visible kidnappings, abuse, and deportations” with little accountability for senior officials. The National Immigrant Justice Center documents a detention system where 90 percent of detainees are held in for‑profit facilities, where medical neglect, punitive solitary confinement, and preventable deaths are common, yet contracts are renewed and expanded. A culture of impunity allows agents and managers to treat rights violations not as career‑ending scandals but as acceptable collateral damage.</p><p>Latin American scholars of impunity warn that such selective enforcement produces a “quiet crisis of accountability” in which the rule of law is hollowed out from within. Laws remain on the books, but their application is skewed: harsh on the poor and marginalized, permissive toward the powerful. Over time, this normalizes the idea that some people are above the law, while others exist primarily as objects of control. When a polity internalizes this hierarchy, fascism no longer needs to arrive in jackboots; it is already present in the daily operations of the justice system.</p><p>The danger, as the Arab Center emphasizes, is that the costs of impunity “come home to roost.” Powers originally justified as necessary to fight terrorism or foreign enemies migrate back into domestic politics. Surveillance tools built for foreign intelligence monitoring are turned on activists and journalists; militarized police tactics perfected in occupied territories are imported into American streets. A population taught to accept lawless violence against outsiders (migrants, foreigners, enemy populations) is gradually conditioned to accept similar violence against internal opponents.</p><p><strong>V. Concentration camps, paramilitary policing, and ritualized predatory violence</strong></p><p>In this context of oligarchic capture, techno‑feudal control, and elite impunity, the rapid expansion of detention infrastructure and the deployment of paramilitary “federal agents” across the interior United States are not aberrations; they are central pillars of an emergent fascist order.</p><p>Richardson’s insistence on calling these facilities concentration camps is analytically exact. A concentration camp, in the historical sense, is not necessarily a death camp; it is a place where a state concentrates populations it considers threats or burdens, subjecting them to confinement, disease, abuse, and often death through neglect rather than industrialized extermination. By that definition, the sprawling network of ICE and Border Patrol detention centers, where people are warehoused for months to years, often in horrific conditions, qualifies.</p><p>New reporting details how this system is poised to scale up dramatically. An internal ICE memo, recently surfaced, outlines a $38 billion plan for a “new detention center model” that would, in one year, create capacity for roughly 92,600 people by purchasing eight “mega centers,” 16 processing centers, and 10 additional facilities. The largest of these warehouses would hold between 7,000 and 10,000 people each for average stays of about 60 days, more than double the size of the largest current federal prison. Separate reporting has mapped at least 23 industrial warehouses being surveyed for conversion into mass detention camps, with leases already secured at several sites.</p><p>Investigations by Amnesty International and others into prototype facilities have found detainees shackled in overcrowded cages, underfed, forced to use open‑air toilets that flood, and routinely denied medical care. Sexual assault and extortion by guards, negligent deaths, and at least one homicide have been documented. These are not accidents; they are predictable outcomes of a profit‑driven system where private contractors are paid per bed and oversight is weak, and of a political culture that dehumanizes migrants as “invaders” or “animals.”</p><p>Richardson highlights another crucial dimension: the way DHS has been retooled to project this violence into the interior as a form of political terror. Agents from ICE and Border Patrol, subdivisions of a relatively new department lacking the institutional restraints of the military, have been deployed in cities far from any border, often in unmarked vehicles, wearing masks and lacking visible identification. Secret legal memos under Trump gutted the traditional requirement of a judicial warrant for entering homes, replacing it with internal sign‑off by another DHS official, a direct violation of the Fourth Amendment’s protection against unreasonable searches and seizures.</p><p>This matters both instrumentally and symbolically. Instrumentally, it enables efficient mass raids and “snatch and grab” operations that bypass local law‑enforcement norms and judicial oversight. Symbolically, it communicates that the state reserves the right to operate as a lawless force, unconstrained by the very constitution it claims to defend. When masked, unidentified agents can seize people off the streets, shove them into unmarked vans, and deposit them in processing centers without due process, the aesthetic of fascism…thugs in the night…becomes reality.</p><p>Richardson rightly connects this to the post‑Reconstruction South, where paramilitary groups like the Ku Klux Klan, often tolerated or quietly aided by local officials, used terror to destroy a biracial democracy that had briefly flourished. Today’s difference is that communications technology allows rapid mobilization of witnesses and counter‑protesters: people can rush to the scene when agents arrive, document abuses on smartphones, and coordinate legal support. Yet even this can be folded into the logic of spectacle. The images of militarized agents confronting crowds under the glow of streetlights and police floodlamps serve as warnings: this is what happens when you resist.</p><p>The planned network of processing centers and mega‑warehouses adds another layer of menace. As Richardson points out, if the stated goal is deportation, there is no clear need for facilities capable of imprisoning tens of thousands for months. Part of the answer is coercive leverage: detained people are easier to pressure into abandoning asylum claims and accepting removal, especially when they are told, day after day, that they could walk free if they “just sign.” But the architecture also anticipates a future in which new categories of internal enemies, protesters, “Antifa,” “domestic extremists,” can be funneled into the same carceral estate once migrant flows diminish or political needs change.</p><p>Economically, the camps generate their own constituency. ICE and DHS tout job creation numbers to local officials, promising hundreds of stable, often union‑free positions in communities hollowed out by deindustrialization. Private prison firms and construction companies see lucrative contracts; investors see secure returns backed by federal guarantees. A web of stakeholders thus becomes materially invested in the continuation and expansion of mass detention. <strong>This is techno‑feudalism in concrete and razor wire: a carceral estate in which bodies are the rent‑producing asset.</strong></p><p>Once such an estate exists, its logic tends to spread. Border‑style tactics migrate into ordinary policing; surveillance tools trialed on migrants are turned on domestic movements; legal doctrines crafted to justify raids and warrantless searches in the name of immigration control seep into other domains. The fascist gradient steepens: more people find themselves at risk of sudden disappearance into a system where rights are theoretical and violence is routine.</p>","contentLength":25529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47198912"},{"title":"The whole thing was a scam","url":"https://garymarcus.substack.com/p/the-whole-thing-was-scam","date":1772297509,"author":"guilamu","guid":155070,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47197505"},{"title":"Obsidian Sync now has a headless client","url":"https://help.obsidian.md/sync/headless","date":1772296313,"author":"adilmoujahid","guid":155096,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47197267"},{"title":"Cognitive Debt: When Velocity Exceeds Comprehension","url":"https://www.rockoder.com/beyondthecode/cognitive-debt-when-velocity-exceeds-comprehension/","date":1772293150,"author":"pagade","guid":155017,"unread":true,"content":"<img src=\"https://www.rockoder.com/images/beyondthecode/cognitive-debt-hero.svg\" alt=\"\" loading=\"eager\"><p>The engineer shipped seven features in a single sprint. DORA metrics looked immaculate. The promotion packet practically wrote itself.</p><p>Six months later, an architectural change required modifying those features. No one on the team could explain why certain components existed or how they interacted. The engineer who built them stared at her own code like a stranger’s.</p><p><strong>Code has become cheaper to produce than to perceive.</strong></p><p>When an engineer writes code manually, two parallel processes occur. The first is production: characters appear in files, tests get written, systems change. The second is absorption: mental models form, edge cases become intuitive, architectural relationships solidify into understanding. These processes are coupled. The act of typing forces engagement. The friction of implementation creates space for reasoning.</p><p>AI-assisted development decouples these processes. A prompt generates hundreds of lines in seconds. The engineer reviews, adjusts, iterates. Output accelerates. But absorption cannot accelerate proportionally. The cognitive work of truly understanding what was built, why it was built that way, and how it relates to everything else remains bounded by human processing speed.</p><p><strong>This gap between output velocity and comprehension velocity is cognitive debt.</strong></p><p>Unlike technical debt, which surfaces through system failures or maintenance costs, <strong>cognitive debt remains invisible to velocity metrics</strong>. The code works. The tests pass. The features ship. The deficit exists only in the minds of the engineers who built the system, manifesting as uncertainty about their own work.</p><p>The debt is not truly invisible. It eventually appears in reliability metrics: Mean Time to Recovery stretches longer, Change Failure Rate creeps upward. But these are lagging indicators, separated by months from the velocity metrics that drive quarterly decisions. By the time MTTR signals a problem, the comprehension deficit has already compounded.</p><h2>What Organizations Actually Measure</h2><p>Engineering performance systems evolved to measure observable outputs. Story points completed. Features shipped. Commits merged. Review turnaround time. These metrics emerged from an era when output and comprehension were tightly coupled, when shipping something implied understanding something.</p><p>The metrics never measured comprehension directly because comprehension was assumed. An engineer who shipped a feature was presumed to understand that feature. <strong>The presumption held because the production process itself forced understanding.</strong></p><p><strong>That presumption no longer holds.</strong> An engineer can now ship features while maintaining only surface familiarity with their implementation. The features work. The metrics register success. The organizational knowledge that would traditionally accumulate alongside those features simply does not form at the same rate.</p><p>Performance calibration committees see velocity improvements. They do not see comprehension deficits. They cannot, because no artifact of the organizational measurement system captures that dimension.</p><p>The discussion of cognitive debt typically focuses on the engineer who generates code. The more acute problem sits with the engineer who reviews it.</p><p>Code review evolved as a quality gate. A senior engineer examines a junior engineer’s work, catching errors, suggesting improvements, transferring knowledge. The rate-limiting factor was always the junior engineer’s output speed. Senior engineers could review faster than juniors could produce.</p><p>AI-assisted development inverts this relationship. <strong>A junior engineer can now generate code faster than a senior engineer can critically audit it.</strong> The volume of generated code exceeds the bandwidth available for deep review. Something has to give, and typically it is review depth.</p><p>The reviewer faces an impossible choice. Maintain previous review standards and become a bottleneck that negates the velocity gains AI provides. Or approve code at the rate it arrives and hope the tests catch what the review missed. Most choose the latter, often unconsciously, because organizational pressure favors throughput.</p><p>This is where cognitive debt compounds fastest. The author’s comprehension deficit might be recoverable through later engagement with the code. The reviewer’s comprehension deficit propagates: they approved code they do not fully understand, which now carries implicit endorsement. <strong>The organizational assumption that reviewed code is understood code no longer holds.</strong></p><p>Engineers working extensively with AI tools report a specific form of exhaustion that differs from traditional burnout. Traditional burnout emerges from sustained cognitive load, from having too much to hold in mind while solving complex problems. The new pattern emerges from something closer to cognitive disconnection.</p><p>The work happens quickly. Progress is visible. But the engineer experiences a persistent sense of not quite grasping their own output. They can execute, but explanation requires reconstruction. They can modify, but prediction becomes unreliable. The system they built feels slightly foreign even as it functions correctly.</p><p>This creates a distinctive psychological state: <strong>high output combined with low confidence</strong>. Engineers produce more while feeling less certain about what they have produced. In organizations that stack-rank based on visible output, this creates pressure to continue generating despite the growing uncertainty.</p><p>The engineer who pauses to deeply understand what they built falls behind in velocity metrics. The engineer who prioritizes throughput over comprehension meets their quarterly objectives. <strong>The incentive structure selects for the behavior that accelerates cognitive debt accumulation.</strong></p><h2>When Organizational Memory Fails</h2><p>Knowledge in engineering organizations exists in two forms. The first is explicit: documentation, design documents, recorded decisions. The second is tacit: understanding held in the minds of people who built and maintained systems over time. Tacit knowledge cannot be fully externalized because much of it exists as intuition, pattern recognition, and contextual judgment that formed through direct engagement with the work.</p><p>When the people who built a system leave or rotate to new projects, tacit knowledge walks out with them. Organizations traditionally replenished this knowledge through the normal process of engineering work. New engineers building on existing systems developed their own tacit understanding through the friction of implementation.</p><p>AI-assisted development potentially short-circuits this replenishment mechanism. If new engineers can generate working modifications without developing deep comprehension, they never form the tacit knowledge that would traditionally accumulate. <strong>The organization loses knowledge not just through attrition but through insufficient formation.</strong></p><p>This creates a delayed failure mode. The system continues to function. New features continue to ship. But the reservoir of people who truly understand the system gradually depletes. When circumstances eventually require that understanding, when something breaks in an unexpected way or requirements change in a way that demands architectural reasoning, the organization discovers the deficit.</p><p>Three failure modes emerge as cognitive debt accumulates.</p><p>The first involves the reversal of a normally reliable heuristic. Engineers typically trust code that has been in production for years. If it survived that long, it probably works. The longer code exists without causing problems, the more confidence it earns. AI-generated code inverts this pattern. <strong>The longer it remains untouched, the more dangerous it becomes</strong>, because the context window of the humans around it has closed completely. Code that was barely understood when written becomes entirely opaque after the people who wrote it have moved on.</p><blockquote>They are debugging a black box written by a black box.</blockquote><p>The second failure mode surfaces during incidents. An alert fires at 3:00 AM. The on-call engineer opens a system they did not build, generated by tools they did not supervise, documented in ways that assume familiarity they do not possess. <strong>They are debugging a black box written by a black box.</strong> What would have been a ten-minute fix when someone understood the system becomes a four-hour forensic investigation when no one does. Multiply this across enough incidents and the aggregate cost exceeds whatever velocity gains the AI-assisted development provided.</p><blockquote>The organization is effectively trading its pipeline of future Staff Engineers for this quarter's feature delivery.</blockquote><p>The third failure mode operates on a longer timescale. Junior engineers who rely primarily on AI-assisted development never develop the intuition that comes from manual implementation. They ship features without forming the scar tissue that informs architectural judgment. The organization is effectively trading its pipeline of future Staff Engineers for this quarter’s feature delivery. The cost does not appear in current headcount models because the people who would have become senior architects five years from now are not yet absent. </p><p>From the perspective of engineering leadership, AI-assisted development presents as productivity gain. Teams ship faster. Roadmaps compress. Headcount discussions become more favorable. These are the observable signals that propagate upward through organizational reporting structures.</p><p>The cognitive debt accumulating in those teams does not present as a signal. There is no metric for “engineers who can explain their own code without re-reading it.” There is no dashboard for “organizational comprehension depth.” The concept does not fit into quarterly business review formats or headcount justification narratives.</p><p>Directors make decisions based on observable signals. When those signals uniformly indicate success, the decision to double down on the approach that produced those signals is rational within the information environment available to leadership. <strong>The decision is not wrong given the data. The data is incomplete.</strong></p><p>The cognitive debt framing does not apply uniformly across all engineering work. Some tasks genuinely are mechanical. Some codebases genuinely benefit from rapid iteration without deep architectural understanding. Some features genuinely do not require the level of comprehension that would traditionally form through manual implementation.</p><p>The model also assumes that comprehension was previously forming at adequate rates. This assumption may be generous. Engineers have always varied in how deeply they understood their own work. The distribution may simply be shifting rather than a new phenomenon emerging.</p><p>Additionally, tooling and documentation practices may evolve to partially close the comprehension gap. If organizations develop methods for capturing and transmitting the understanding that AI-assisted development fails to form organically, the debt may prove manageable rather than accumulative.</p><blockquote>The system is optimizing correctly for what it measures. What it measures no longer captures what matters.</blockquote><p>The fundamental challenge is that <strong>organizations cannot optimize for what they cannot measure</strong>. Velocity is measurable. Comprehension is not, or at least not through any mechanism that currently feeds into performance evaluation, promotion decisions, or headcount planning.</p><p>Until comprehension becomes legible to organizational decision-making systems, the incentive structure will continue to favor velocity. Engineers who prioritize understanding over output will appear less productive than peers who prioritize output over understanding. Performance calibration will reward the behavior that accumulates debt faster.</p><p>This is not a failure of individual managers or engineers. It is a measurement system designed for an era when production and comprehension were coupled, operating in an era when that coupling no longer holds. <strong>The system is optimizing correctly for what it measures. What it measures no longer captures what matters.</strong></p><p>The gap will eventually manifest. Whether through maintenance costs that exceed projections, through incidents that require understanding no one possesses, or through new requirements that expose the brittleness of systems built without deep comprehension. The timing and form of manifestation remain uncertain. The underlying dynamic does not.</p>","contentLength":12316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47196582"},{"title":"OpenAI fires an employee for prediction market insider trading","url":"https://www.wired.com/story/openai-fires-employee-insider-trading-polymarket-kalshi/","date":1772286380,"author":"bookofjoe","guid":155095,"unread":true,"content":"<p> an employee following an investigation into their activity on <a href=\"https://www.wired.com/story/the-political-war-over-prediction-markets-is-just-getting-started/\">prediction market platforms</a> including Polymarket, WIRED has learned.</p><p>OpenAI CEO of Applications, <a href=\"https://www.wired.com/story/fidji-simo-is-openais-other-ceo-and-she-swears-shell-make-chatgpt-profitable/#intcid=_wired-verso-hp-trending_ea249350-52ab-42d0-9cfe-e91d580f41e8_popular4-2\">Fidji Simo</a>, disclosed the termination in an internal message to employees earlier this year. The employee, she said, “used confidential OpenAI information in connection with external prediction markets (e.g. Polymarket).”</p><p>“Our policies prohibit employees from using confidential OpenAI information for personal gain, including in prediction markets,” says spokesperson Kayla Wood. OpenAI has not revealed the name of the employee or the specifics of their trades.</p><p>Evidence suggests that this was not an isolated event. Polymarket runs on the Polygon blockchain network, so its trading ledger is pseudonymous but traceable. According to an analysis by the financial data platform Unusual Whales, there have been clusters of activities, which the service flagged as suspicious, around OpenAI-themed events since March 2023.</p><p>Unusual Whales flagged 77 positions in 60 wallet addresses as suspected insider trades, looking at the age of the account, trading history, and significance of investment, among other factors. Suspicious trades hinged on the release dates of products like <a href=\"https://www.wired.com/story/openai-launches-sora-2-tiktok-like-app/\">Sora</a>, <a href=\"https://www.wired.com/story/gpt-5-coding-review-software-engineering/\">GPT-5</a>, and the ChatGPT Browser, as well as CEO Sam Altman’s employment status. In November 2023, two days after Altman was dramatically ousted from the company, a new wallet placed a significant bet that he would return, netting over $16,000 in profits. The account never placed another bet.</p><p>The behavior fits into patterns typical of insider trades. “The tell is the clustering. In the 40 hours before OpenAI launched its browser, 13 brand-new wallets with zero trading history appeared on the site for the first time to collectively bet $309,486 on the right outcome,” says Unusual Whales CEO Matt Saincome. “When you see that many fresh wallets making the same bet at the same time, it raises a real question about whether the secret is getting out.”</p><p>Prediction markets have exploded in popularity in recent years. These platforms allow customers to buy “event contracts” on the outcomes of future events ranging from the winner of the Super Bowl to the daily price of Bitcoin to whether the United States will go to war with Iran. There are a wide array of markets tied to events in the technology sector; you can trade on what Nvidia’s quarterly earnings will be, or when Tesla will launch a new car, or which AI companies will IPO in 2026.</p><p>As the platforms have grown, so have concerns that they allow traders to profit from insider knowledge. “This prediction market world makes the Wild West look tame in comparison,” says Jeff Edelstein, a senior analyst at the betting news site InGame. “If there's a market that exists where the answer is known, somebody's going to trade on it.”</p><p>Earlier this week, Kalshi <a href=\"https://www.wired.com/story/kalshi-insider-trading-california-politician-and-youtuber/\">announced</a> that it had reported several suspicious insider trading cases to the Commodity Futures Trading Commission, the government agency overseeing these markets. In one instance, an employee of the popular YouTuber Mr. Beast was suspended for two years and fined $20,000 for making trades related to the streamer’s activities; in another, the far-right political candidate Kyle Langford was banned from the platform for making a trade on his own campaign. The company also announced a number of initiatives to prevent insider trading and market manipulation.</p><p>While Kalshi has heavily promoted its crackdown on insider trading, Polymarket has stayed silent on the matter. The company did not return requests for comments.</p><p>In the past, major trades on technology-themed markets have sparked speculation that there are Big Tech employees profiting by using their insider knowledge to gain an edge. One notorious example is the so-called “Google whale,” a <a data-offer-url=\"https://polymarket.com/profile/%400xafEe\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://polymarket.com/profile/%400xafEe&quot;}\" href=\"https://polymarket.com/profile/%400xafEe\" rel=\"nofollow noopener\" target=\"_blank\">pseudonymous account</a> on Polymarket that made over $1 million trading on Google-related events, including a market on who the most-searched person of the year would be in 2025. (It was the singer D4vd, who is best known for his connection to an ongoing murder investigation after a young fan’s remains were found in a vehicle registered to him.)</p>","contentLength":4190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47195317"},{"title":"What AI coding costs you","url":"https://tomwojcik.com/posts/2026-02-15/finding-the-right-amount-of-ai/","date":1772283903,"author":"tomwojcik","guid":155069,"unread":true,"content":"<p>Every developer I know uses AI for coding now. The productivity gains are real, but there are costs that don’t show up on any dashboard.</p><p>Imagine a spectrum. On the far left are humans typing on the keyboard, seeing the code in the IDE. On the far right: AGI. It implements everything on its own. Cheaply, flawlessly, better than any human, and no human overseer is required. Somewhere between those two extremes there’s you, using AI, today. That threshold moves to the right every week as models improve, tools mature, and workflows get refined.</p><blockquote><p>Which is higher risk, using AI too much, or using AI too little?</p></blockquote><p>and it made me think about LLMs for coding differently, especially after reading what other devs share on AI adoption in different workplaces. You can be wrong in both directions, but is the desired amount of AI usage at work changing as the models improve?</p><p>Not long ago the first AI coding tools like Cursor (2023) or Copilot (2022) emerged. They were able to quickly index the codebase using RAG, so they had the local context. They had all the knowledge of the models powering them, so they had an external knowledge of the Internet as well. Googling and browsing StackOverflow wasn’t needed anymore. Cursor gave the users a custom IDE with built in AI powered autocomplete and other baked-in AI tools, like chat, to make the experience coherent.</p><p>Then came the agent promise. MCPs, autonomous workflows, <a href=\"https://michaelxbloch.substack.com/p/no-coding-before-10am\">articles about agents running overnight</a> started to pop up left and right. It was a different use of AI than Cursor. <strong>It was no longer an AI-assisted human coding, but a human-assisted AI coding.</strong></p><p>Many devs tried it and got burned. Agents made tons of small mistakes. The AI-first process required a complete paradigm shift in how devs think about coding, in order to achieve great results. Also, agents often got stuck in loops, hallucinate dependencies, and produced code that looks almost right but isn’t. You needed to learn about a completely new tech, fueled by FOMO. And this new shiny tool never got it 100% right on the first try.</p><p>Software used to be deterministic. You controlled it with if/else branches, explicit state machines, clear logic. The new reality is controlling the development process with prompts, system instructions, and CLAUDE.md files, and hope the model produces the output you expect.</p><blockquote><p>An engineer at Spotify on their morning commute from Slack on their cell phone can tell Claude to fix a bug or add a new feature to the iOS app. And once Claude finishes that work, the engineer then gets a new version of the app, pushed to them on Slack on their phone, so that he can then merge it to production, all before they even arrive at the office.”</p></blockquote><p>I hope they at least review the code before merging.</p><p>The next stage is an (almost) full automation. That’s what many execs want and try to achieve. It’s a capitalistic wet dream, a worker that never sleeps, never gets tired, always wants to work, is infinitely productive. But Geoffrey Hinton predicted in 2016 that deep learning would outperform radiologists at image analysis within five years. Anthropic’s CEO predicted AI would write 90% of code within three to six months of March 2025. None of this happened as predicted. The trajectory is real, but the timeline keeps slipping.</p><p>In 2012, neuroscientist Manfred Spitzer published <a href=\"https://en.wikipedia.org/wiki/Digital_Dementia_(book)\">Digital Dementia</a>, arguing that when we outsource mental tasks to digital devices, the brain pathways responsible for those tasks atrophy. Use it or lose it. Not all of this is proven scientifically, but neuroplasticity research shows the brain strengthens pathways that get used and weakens ones that don’t. The core principle of the book is that the cognitive skills that you stop practicing will decline.</p><p>Margaret-Anne Storey, a software engineering researcher, recently gave this a more precise name: <a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">cognitive debt</a>. Technical debt lives in the code. Cognitive debt lives in developers’ heads. It’s the accumulated loss of understanding that happens when you build fast without comprehending what you built. She grounds it in Peter Naur’s 1985 theory that a program is a theory existing in developers’ minds, capturing what it does, how intentions map to implementation, and how it can evolve. When that theory fragments, the system becomes a black box.</p><p>Apply this directly to fully agentic coding. If you stop writing code and only review AI output, your ability to reason about code atrophies. Slowly, invisibly, but inevitably. You can’t deeply review what you can no longer deeply understand.</p><p>This isn’t just theory. A <a href=\"https://arxiv.org/abs/2601.20245\">2026 randomized study by Shen and Tamkin</a> tested this directly: 52 professional developers learning a new async library were split into AI-assisted and unassisted groups. The AI group scored 17% lower on conceptual understanding, debugging, and code reading. The largest gap was in debugging, the exact skill you need to catch what AI gets wrong. One hour of passive AI-assisted work produced measurable skill erosion.</p><p>The insidious part is that you don’t notice the decline because the tool compensates for it. You feel productive. The PRs are shipping. Mihaly Csikszentmihalyi’s research on <a href=\"https://en.wikipedia.org/wiki/Flow_(psychology)\">flow</a> showed that the state of flow depends on a balance between challenge and skill. Your mind needs to be stretched just enough. Real flow produces growth. Rachel Thomas called what AI-assisted work produces <a href=\"https://www.fast.ai/posts/2026-01-28-dark-flow/\">“dark flow”</a>, a term borrowed from gambling research, describing the trance-like state slot machines are designed to induce. You feel absorbed, but the challenge-skill balance is gone because the AI handles the challenge. It feels like the flow state of deep work, but the feedback loop is broken. You’re not getting better, you’re getting dependent.</p><p>There’s this observation that keeps coming up in HN comments: if the AI writes all the code and you only review it, where does the skill to review come from? You can’t have one without the other. You don’t learn to recognize good code by reading about it in a textbook, or a PR. You learn by writing bad code, getting it torn apart, and building intuition through years of practice.</p><p>This creates what I’d call the review paradox: the more AI writes, the less qualified humans become to review what it wrote. The Shen-Tamkin study puts numbers on this. Developers who fully delegated to AI finished tasks fastest but scored worst on evaluations. The novices who benefit most from AI productivity are exactly the ones who need debugging skills to supervise it, and AI erodes those skills first.</p><p>Storey’s proposed fix is simple: “require humans to understand each AI-generated change before deployment.” That’s the right answer. It’s also the one that gets skipped first when velocity is the metric.</p><p>This goes deeper than individual skill decay. We used to have juniors, mids, seniors, staff engineers, architects. It was a pipeline where each level built on years of hands-on struggle. A junior spends years writing code that is rejected during the code review not because they were not careful, but didn’t know better. It’s how you build the judgment that separates someone who can write a function from someone who can architect a system. You can’t become a senior overnight.</p><p>Unless you use AI, of course. Now, a junior with Claude Code (Opus 4.5+) delivers PRs that look like senior engineer work. And overall that’s a good thing, I think. But does it mean that the senior hat fits everyone now? From day one? But the head underneath hasn’t changed. That junior doesn’t know  that architecture was chosen. From my experience, sometimes CC misses a new DB transaction where it’s needed. Sometimes it creates a lock on a resource, that shouldn’t be locked, due to number of reasons. I can defend my decisions and I enjoy when my code is challenged, when reviewers disagree, and we have a discussion. What will a junior do? Ask Claude.</p><p>It’s a two-sided collapse. Seniors who stop writing code and only review AI output lose their own depth. Juniors who skip the struggle never build it. Organizations are spending senior time every day on reviews while simultaneously breaking the mechanisms that create it. The pipeline that produced senior engineers, writing bad code, getting bad code reviewed, building intuition through failure, is being bypassed entirely. Nobody’s talking about what happens when that pipeline runs dry.</p><h2>What C-Levels Got Right and Wrong</h2><p>The problem is that predictions come from people selling AI or trying to prop the stock with AI hype. They have every incentive to accelerate adoption and zero accountability when the timelines slip, which, historically, they always do. And “50% of code characters” at Google, a company that has built its own models, tooling, and infrastructure from scratch, says very little about what your team can achieve with off-the-shelf agents next Monday.</p><p>AI adoption is not a switch to flip, rather a skill to calibrate. It’s not as simple as mandating specific tools, setting “AI-first” policies, measuring developers on how much AI they use (/r/ExperiencedDevs is full of these stories). A lot of good practices like usage of design patterns, proper test coverage, manual testing before merging, are often skipped these days because it reduces the pace. AI broke it? AI will fix it. You need a review? AI will do it. Not even Greptile or CodeRabbit. Just delegate the PR to Claude Code reviewer agent. Or Gemini. Or Codex. Pick your poison.</p><p>And here’s what actually happens when you force the AI usage. One developer on r/ExperiencedDevs <a href=\"https://www.reddit.com/r/ExperiencedDevs/comments/1o643iz/i_am_blissfully_using_ai_to_do_absolutely_nothing/\">described</a> their company tracking AI usage per engineer: “I just started asking my bots to do random things I don’t even care about. The other day I told Claude to examine random directories to ‘find bugs’ or answer questions I already knew the answer to.” <a href=\"https://www.reddit.com/r/ExperiencedDevs/comments/1r4u0mo/comment/o5e5r2w/\">This thread</a> is full of engineers reporting that AI has made code reviews “infinitely harder due to the AI slop produced by tech leads who have been off the tools long enough to be dangerous.”</p><p>This is sad, because being able to work with the AI tools is a perk for developers and since it improves pace, it’s something management wants as well. It’s obvious that the people gaming the metrics (not really using the AI the way the should) would be fired on the spot if the management learned how they are gaming the metrics (and it’s fair), but they are gaming the metrics because they don’t want to be fired…</p><p>Who should be responsible for setting the threshold of AI usage at the company? What if your top performing engineer just refuses to use AI? What if the newly hired junior uses AI all the time? These are the new questions and management is trying to find an answer to them, but it’s not as simple as measuring the AI usage.</p><p>This is <a href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\">Goodhart’s law</a> in action: “When a measure becomes a target, it ceases to be a good measure.” Track AI usage per engineer and you won’t get better engineering, you’ll get compliance theater. Developers game the metrics, resent the tools, and the actual productivity gains that AI  deliver get buried under organizational dysfunction.</p><h2>The Cost Nobody Talks About</h2><p>The financial cost is obvious. Agent time for non-trivial features is measured in hours, and those hours aren’t free. But the human cost is potentially worse, and it’s barely discussed.</p><p>Writing code can put you in a flow state, mentioned before. That deep, focused, creative problem-solving where hours disappear and you emerge with something you built and understand. And you’re proud of it. Someone wrote under your PR “Good job!” and gave you an approval. Reviewing AI-generated code does not do this. It’s the opposite. It’s a mental drain.</p><p>Developers need the dopamine hit of creation. That’s not a perk, it’s what keeps good engineers engaged, learning, retained, and prevents burnout. The joy of coding is probably what allowed them to become experienced devs in the first place. Replace creation with oversight and you get faster burnout, not faster shipping. You’ve turned engineering, the creative work, into the worst form of QA. The AI does all the art, the human folds the laundry.</p><p>I use AI every day. I use AI heavily at work, I use AI in my sideprojects, and I don’t want to go back. I love it! That’s why I’m worried. I’m afraid I became addicted and dependent. I’ve implemented countless custom commands, skills, and agents. I check CC release notes daily. And I know many are in similar situation right now, and we all wonder about what the future brings. Are we going to replace ourselves with AI? Or will we be responsible for cleaning AI slop? What’s the right amount of AI usage for me?</p><p>AI is just a tool. An extraordinarily powerful one, but a tool nonetheless. You wouldn’t mandate that every engineer uses a specific IDE, or measure people on how many lines they write per day (…right?). You’d let them pick the tools that make  most effective and measure what actually matters, the work that ships.</p><p>The right amount of AI is not zero. And it’s not maximum.</p><p>The Shen-Tamkin study identified six distinct AI interaction patterns among developers. Three led to poor learning: full delegation, progressive reliance, and outsourcing debugging to AI. Three preserved learning even with full AI access: asking for explanations, posing conceptual questions, and writing code independently while using AI for clarification. The differentiator wasn’t whether developers used AI, it was whether they stayed cognitively engaged.</p><p>Software engineering was never just about typing code. It’s defining the problem well, understanding the problem, translating the language from business to product to code, clarifying ambiguity, making tradeoffs, understanding what breaks when you change something. Someone has to do that before AGI, and AGI is nowhere close (luckily). You’re on call, the phone rings at 3am, can you triage the issue without an agent? If not, you’ve probably taken AI coding too far. If the AI usage becomes a new performance metric of developer, maybe using AI too often, too much, should be discouraged as well? Not because these tools are bad, but because the coding skills are worth maintaining.</p><h3>The Risk of Too Little (anecdata)</h3><p>If you’re using no AI at all in 2026, you are leaving real gains on the table:</p><ul><li> AI is genuinely better than Google for navigating unfamiliar codebases, understanding legacy code, and finding relevant patterns. This alone justifies having it in your workflow (since 2023, Cursor etc)</li><li><strong>Boilerplate and scaffolding.</strong> Writing the hundredth CRUD endpoint, config file, or test scaffold by hand when an agent can produce it in seconds isn’t craftsmanship, it’s stubbornness. Just use AI. You’re not a CRUD developer anymore anyway, because we all wear many hats these days (post 2025 Sonnet)</li><li> The investigate, plan, implement, test, validate cycle that works with customized agents is a real improvement in how features get delivered. Hours instead of days for non-trivial work. It’s not the 10x that was promised, but 2x or 4x on an established codebases is low-hanging fruit. You must understand the output though and all the decisions AI made! (post 2025 Opus 4.5)</li><li> “What does this module do? How does this API work? What would break if I changed this?” AI is excellent at these questions. It won’t replace reading the code, but it’ll get you to the right file in the right minute. (since 2023)</li></ul><p>Refusing to use AI out of principle is as irrational as adopting it out of hype.</p><h3>The Risk of Too Much (anecdata and my predictions)</h3><p>If you go all-in on autonomous AI coding (especially without learning how it all actually works), you risk something worse than slow velocity, you risk  degradation:</p><ul><li><strong>Bugs that look like features.</strong> AI-generated code passes CI. The types check. The tests are green. And somewhere inside there’s a subtle logic error, a hallucinated edge case, a pattern that’ll collapse under load. In domains like finance or healthcare, a wrong number that doesn’t throw an error is worse than a crash. (less and less relevant, but still relevant)</li><li><strong>A codebase nobody understands.</strong> When the agent writes everything and humans only review, six months later nobody on the team can explain why the system is architected the way it is. The AI made choices. Nobody questioned them because the tests passed. Storey <a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">describes</a> a student team that hit exactly this wall: they couldn’t make simple changes without breaking things, and the problem wasn’t messy code, it was that no one could explain why certain design decisions had been made. Her conclusion: “velocity without understanding is not sustainable.” (will always be a problem, IMO)</li><li> Everything in the Digital Dementia section above. Skills you stop practicing will decline. (will always be a problem, IMO)</li><li><strong>The seniority pipeline drying up.</strong> Also covered above. This one takes years to manifest, which is exactly why nobody’s planning for it. (It’s a new problem, I have no idea what it looks like in the future)</li><li> Reviewing AI output all day without the dopamine of creation is not a sustainable job description. (Old problem, but potentially hits faster?)</li></ul><p>Here’s what keeps me up at night. By every metric on every dashboard, AI-assisted human development and human-assisted AI development is improving. More PRs shipped. More features delivered. Faster cycle times. The charts go up and to the right.</p><p>But metrics don’t capture what’s happening underneath. The mental fatigue of reviewing code you didn’t write all day. The boredom of babysitting an agent instead of solving problems. The slow, invisible erosion of the hard skills that made you good at this job in the first place. You stop holding the architecture in your head because the agent handles it. You stop thinking through edge cases because the tests pass. You stop  to dig deep because it’s easier to prompt and approve. There’s no spark in you anymore.</p><p>In this meme the developers are the butter robot. The ones with no mental capacity to review the plans and PRs from AI, will only click Accept, instead of doing the creative, challenging work. Oh the irony.</p><p>Simon Willison, one of the most ambitious developer of our time, <a href=\"https://simonwillison.net/tags/cognitive-debt/\">admitted this is already happening to him</a>. On projects where he prompted entire features without reviewing implementations, he “no longer has a firm mental model of what they can do and how they work.”</p><p>And then, one day, the metrics start slipping… Not because the tool got worse, but because you did. Not from lack of effort, but from lack of practice. It’s a feedback loop that looks like progress right up until it doesn’t.</p><p>No executive wants to measure this. “What is the effect of AI usage on our engineers’ cognitive abilities over 18 months?” is not an easy KPI. It doesn’t fit in a quarterly review. It doesn’t get tracked, and what doesn’t get tracked doesn’t get managed, until it shows up as a production incident that nobody on the team can debug without an agent, and the agent can’t debug either.</p><p>I’m not anti-AI, I like it a lot. I’m addicted to prompting, I get high from it. I’m just worried that this new dependency degrades us over time, quietly, and nobody’s watching for it.</p>","contentLength":19251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47194847"},{"title":"Don't trust AI agents","url":"https://nanoclaw.dev/blog/nanoclaw-security-model","date":1772282372,"author":"gronky_","guid":155016,"unread":true,"content":"<p>When you’re building with AI agents, they should be treated as untrusted and potentially malicious. Whether you’re worried about prompt injection, a model trying to escape its sandbox, or something nobody’s thought of yet, regardless of what your threat model is, you shouldn’t be trusting the agent. The right approach isn’t better permission checks or smarter allowlists. It’s architecture that assumes agents will misbehave and contains the damage when they do.</p><p>OpenClaw runs directly on the host machine by default. It has an opt-in Docker sandbox mode, but it’s turned off out of the box, and most users never turn it on. Without it, security relies entirely on application-level checks: allowlists, confirmation prompts, a set of “safe” commands. These checks come from a place of implicit trust that the agent isn’t going to try to do something wrong. Once you adopt the mindset that an agent is potentially malicious, it’s obvious that application-level blocks aren’t enough. They don’t provide hermetic security. A determined or compromised agent can find ways around them.</p><p>In NanoClaw, container isolation is a core part of the architecture. Each agent runs in its own container, on Docker or an Apple Container on macOS. Containers are ephemeral, created fresh per invocation and destroyed afterward. The agent runs as an unprivileged user and can only see directories that have been explicitly mounted in. A container boundary is enforced by the OS.</p><p>Even when OpenClaw’s sandbox is enabled, all agents share the same container. You might have one agent as a personal assistant and another for work, in different WhatsApp groups or Telegram channels. They’re all in the same environment, which means information can leak between agents that are supposed to be accessing different data.</p><p>Agents shouldn’t trust each other any more than you trust them. In NanoClaw, each agent gets its own container, filesystem, and Claude session history. Your personal assistant can’t see your work agent’s data because they run in completely separate sandboxes.</p><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 960 460\" font-family=\"'JetBrains Mono', 'SF Mono', 'Fira Code', monospace\" role=\"img\" aria-label=\"Shared container vs per-agent containers: in a shared container all agents see everything, with per-agent containers each agent is isolated\"></svg><p>The container boundary is the hard security layer — the agent can’t escape it regardless of configuration. On top of that, a mount allowlist at <code>~/.config/nanoclaw/mount-allowlist.json</code> acts as an additional layer of defense-in-depth: it exists to prevent the  from accidentally mounting something that shouldn’t be exposed, not to prevent the agent from breaking out. Sensitive paths (, , , , , ) are blocked by default. The allowlist lives outside the project directory, so a compromised agent can’t modify its own permissions. The host application code is mounted read-only, so nothing an agent does can persist after the container is destroyed.</p><p>People in your groups shouldn’t be trusted either. Non-main groups are untrusted by default. Other groups, and the people in them, can’t message other chats, schedule tasks for other groups, or view other groups’ data. Anyone in a group could send a prompt injection, and the security model accounts for that.</p><h2>Don’t trust what you can’t read</h2><p>OpenClaw has nearly half a million lines of code, 53 config files, and over 70 dependencies. This breaks the basic premise of open source security. Chromium has 35+ million lines, but you trust Google’s review processes. Most open source projects work the other way: they stay small enough that many eyes can actually review them. Nobody has reviewed OpenClaw’s 400,000 lines. It was written in weeks with no proper review process. Complexity is where vulnerabilities hide, and <a href=\"https://www.microsoft.com/en-us/security/blog/2026/02/19/running-openclaw-safely-identity-isolation-runtime-risk/\">Microsoft’s analysis</a> confirmed this: OpenClaw’s risks could emerge through normal API calls, because no one person could see the full picture.</p><p>NanoClaw is one process and a handful of files. We rely heavily on Anthropic’s Agent SDK, the wrapper around Claude Code, for session management, memory compaction, and a lot more, instead of reinventing the wheel. A competent developer can review the entire codebase in an afternoon. This is <a href=\"https://nanoclaw.dev/#philosophy\">a deliberate constraint, not a limitation</a>. Our <a href=\"https://github.com/qwibitai/nanoclaw/blob/main/CONTRIBUTING.md\">contribution guidelines</a> accept bug fixes, security fixes, and simplifications only.</p><p>New functionality comes through skills: instructions with a full working reference implementation that a coding agent merges into your codebase. You review exactly what code will be added before it lands. And you only add the integrations you actually need. Every installation ends up as a few thousand lines of code tailored to the owner’s exact requirements.</p><p>This is the real difference. With a monolithic codebase of 400,000 lines, even if you only enable two integrations, the rest of the code is still there. It’s still loaded, still part of your attack surface, still reachable by prompt injections and rogue agents. You can’t disentangle what’s active from what’s dormant. You can’t audit it because you can’t even define the boundary of what “your code” is. With skills, the boundary is obvious: it’s a few thousand lines, it’s all code you chose to add, and you can read every line of it. The core is actually getting smaller over time: WhatsApp support, for example, is being pulled out and packaged as a skill.</p><p>If a hallucination or a misbehaving agent can cause a security issue, then the security model is broken. Security has to be enforced outside the agentic surface, not depend on the agent behaving correctly. Containers, mount restrictions, and filesystem isolation all exist so that even when an agent does something unexpected, the blast radius is contained.</p><p>None of this eliminates risk. An AI agent with access to your data is inherently a high-risk arrangement. But the right response is to make that trust as narrow and as verifiable as possible. Don’t trust the agent. Build walls around it.</p>","contentLength":5783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47194611"},{"title":"OpenAI – How to delete your account","url":"https://help.openai.com/en/articles/6378407-how-to-delete-your-account","date":1772275315,"author":"carlosrg","guid":154925,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47193478"},{"title":"MCP server that reduces Claude Code context consumption by 98%","url":"https://mksg.lu/blog/context-mode","date":1772272880,"author":"mksglu","guid":155116,"unread":true,"content":"<p>Every MCP tool call in Claude Code dumps raw data into your 200K context window. A Playwright snapshot costs 56 KB. Twenty GitHub issues cost 59 KB. One access log — 45 KB. After 30 minutes, 40% of your context is gone.</p><p>Context Mode is an MCP server that sits between Claude Code and these outputs. 315 KB becomes 5.4 KB. 98% reduction.</p><p>MCP became the standard way for AI agents to use external tools. But there's a tension at its core: every tool interaction fills the context window from both sides — definitions on the way in, raw output on the way out.</p><p>With 81+ tools active, 143K tokens (72%) get consumed before your first message. Then the tools start returning data. A single Playwright snapshot burns 56 KB. A  dumps 59 KB. Run a test suite, read a log file, fetch documentation — each response eats into what remains.</p><p>Cloudflare showed that tool definitions can be compressed by 99.9% with Code Mode. We asked: what about the other direction?</p><p>Each  call spawns an isolated subprocess with its own process boundary. Scripts can't access each other's memory or state. The subprocess runs your code, captures stdout, and only that stdout enters the conversation context. The raw data — log files, API responses, snapshots — never leaves the sandbox.</p><p>Ten language runtimes are available: JavaScript, TypeScript, Python, Shell, Ruby, Go, Rust, PHP, Perl, R. Bun is auto-detected for 3-5x faster JS/TS execution.</p><p>Authenticated CLIs (, , , , ) work through credential passthrough — the subprocess inherits environment variables and config paths without exposing them to the conversation.</p><h2>How the Knowledge Base Works</h2><p>The  tool chunks markdown content by headings while keeping code blocks intact, then stores them in a  (Full-Text Search 5) virtual table. Search uses  — a probabilistic relevance algorithm that scores documents based on term frequency, inverse document frequency, and document length normalization.  is applied at index time so \"running\", \"runs\", and \"ran\" match the same stem.</p><p>When you call , it returns exact code blocks with their heading hierarchy — not summaries, not approximations, the actual indexed content.  extends this to URLs: fetch, convert HTML to markdown, chunk, index. The raw page never enters context.</p><p>Validated across 11 real-world scenarios — test triage, TypeScript error diagnosis, git diff review, dependency audit, API response processing, CSV analytics. All under 1 KB output each.</p><ul><li> 56 KB → 299 B</li><li> 59 KB → 1.1 KB</li><li><strong>Access log (500 requests):</strong> 45 KB → 155 B</li><li><strong>Analytics CSV (500 rows):</strong> 85 KB → 222 B</li><li> 11.6 KB → 107 B</li><li><strong>Repo research (subagent):</strong> 986 KB → 62 KB (5 calls vs 37)</li></ul><p>Over a full session: 315 KB of raw output becomes 5.4 KB. Session time before slowdown goes from ~30 minutes to ~3 hours. Context remaining after 45 minutes: 99% instead of 60%.</p><p>Two ways. Plugin Marketplace gives you auto-routing hooks and slash commands:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"bash\" data-theme=\"github-light\"><code data-language=\"bash\" data-theme=\"github-light\"></code></pre></figure><p>Or MCP-only if you just want the tools:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"bash\" data-theme=\"github-light\"><code data-language=\"bash\" data-theme=\"github-light\"></code></pre></figure><p>Restart Claude Code. Done.</p><p>You don't change how you work. Context Mode includes a PreToolUse hook that automatically routes tool outputs through the sandbox. Subagents learn to use  as their primary tool. Bash subagents get upgraded to  so they can access MCP tools.</p><p>The practical difference: your context window stops filling up. Sessions that used to hit the wall at 30 minutes now run for 3 hours. The same 200K tokens, used more carefully.</p><p>I run the MCP Directory &amp; Hub. 100K+ daily requests. See every MCP server that ships. The pattern was clear: everyone builds tools that dump raw data into context. Nobody was solving the output side.</p><p>Cloudflare's Code Mode blog post crystallized it. They compressed tool definitions. We compress tool outputs. Same principle, other direction.</p><p>Built it for my own Claude Code sessions first. Noticed I could work 6x longer before context degradation. Open-sourced it.</p>","contentLength":3826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47193064"},{"title":"U.S. and Israel Conduct Strikes on Iran","url":"https://www.nytimes.com/live/2026/02/28/world/iran-strikes-trump","date":1772261866,"author":"gammarator","guid":154901,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47191414"},{"title":"The United States and Israel have launched a major attack on Iran","url":"https://www.cnn.com/2026/02/28/middleeast/israel-attack-iran-intl-hnk","date":1772260447,"author":"lavp","guid":154924,"unread":true,"content":"<p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm75vny000033b6spupdny21@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Joint US-Israeli attacks on Iran have killed Ayatollah Ali Khamenei, the country’s supreme leader for nearly four decades, thrusting the country into uncertainty and sparking a conflict that could draw in much of the Middle East.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7mb6y600043b6se5q9gxq6@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Donald Trump announced Khamenei’s death on Saturday, which was also confirmed by Iranian authorities. The US president said the bombing will continue “uninterrupted throughout the week or, as long as necessary to achieve our objective of PEACE THROUGHOUT THE MIDDLE EAST AND, INDEED, THE WORLD!” Israel has continued to bombard Iran on Sunday.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7mb6y600053b6s4lvtxq7f@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran has responded with an unprecedented wave of strikes across the Middle East, targeting several countries that host US military bases, including Bahrain and the United Arab Emirates.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7q84l000033b6sl1bqemem@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            President Masoud Pezeshkian said Sunday that “bloodshed and revenge” is Iran’s “legitimate right and duty.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d1hmy002a3b6q6i452zen@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Here’s what we know so far.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d2cna002n3b6qcy2hejat@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In a video on Truth Social announcing a “major” attack on Iran, Trump said the main US objective was “to defend the American people by eliminating imminent threats from the Iranian regime.” Those threats, he said, included Iran’s nuclear program – which the White House claimed to have “totally” obliterated when it briefly joined Israel’s war against Iran in June.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7og5ad000m3b6sb9oc1lhd@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            That 12-day war left the Islamic regime severely weakened. Since the turn of the year, it has also been battling an economic crisis which sparked nationwide protests. After a crackdown left thousands of protesters dead, Trump had promised to come to their aid, saying the US was “locked and loaded.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7og5ad000n3b6srq7s5z5m@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            For weeks, there had been a strange split-screen: while US envoys held regular talks with Iran over a new nuclear deal, the Trump administration was amassing the largest buildup of military materiel in the Middle East since the invasion of Iraq in 2003. Although the last round of talks ended Thursday with Iran agreeing to “never” stockpile enriched uranium, that was not enough to avert US military action.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7og5ad000o3b6sad68roq7@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In his video, Trump accused Iran of rejecting “every opportunity to renounce their nuclear ambitions,” and said the US “can’t take it anymore.” He said it has “always” been US policy that “this terrorist regime can never have a nuclear weapon,” without providing evidence that Iran was any closer to obtaining a nuclear weapon.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7oh5n3000q3b6s9fck1res@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            After nearly half a century of enmity between the US and the Islamic regime, Trump also seemed to suggest some score-settling was in order.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7qgs3d000h3b6s7jz2ndnq@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “For 47 years the Iranian regime has chanted ‘death to America’ and waged an unending campaign of bloodshed” against the US, he said, citing the 1979 hostage crisis and the 1983 bombing of the US embassy in Beirut. “It’s been mass terror. And we’re not going to put up with it any longer.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7ohc9p000s3b6soh1eqqz0@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The president also repeated his disputed claims that Iran is building ballistic missiles, which could reach the US mainland. CNN previously reported that an <a href=\"https://www.dia.mil/Portals/110/Documents/News/golden_dome.pdf\" target=\"_blank\">unclassified assessment</a> from the Defense Intelligence Agency (DIA) from 2025 said that Iran could develop a “militarily-viable” intercontinental ballistic missile by 2035 “should Tehran decide to pursue the capability.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7ohhvh000u3b6senh9a6o9@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Two sources said the claim that Iran will soon have a missile capable of hitting the US is not backed up by intelligence.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm67b5uz001a3b6q15pook7c@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Prime Minister Benjamin Netanyahu has long viewed Iran as Israel’s most dangerous adversary. After the fall of Bashar al-Assad’s regime in Syria, a key Iranian ally, and Israel’s crippling of the Iran-backed Hezbollah militia in Lebanon, Israel last summer launched a war against Iran itself.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d30pb002q3b6q2he3uxhh@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Although Israel halted the conflict after the US struck Iran’s nuclear sites, analysts had long suspected that Netanyahu would take an opportunity to resume attacks on Iran. With elections due in October, Netanyahu may also see the return to war as a chance to shore up his standing domestically.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d30pb002r3b6qslxncjaj@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In a video statement Saturday explaining why Israel was resuming its strikes on Iran, Netanyahu also repeated his claim that the Islamic regime must not be allowed to acquire a nuclear weapon.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7ok3ty000w3b6s8sduahfe@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            On Sunday, the Israeli military suggested the attack was revenge for the Hamas attacks of October 7, 2023, saying Israel “will not forget” the Iran-sponsored raid. “We will continue to pursue Israel’s enemies – from the architects of the attack to the terrorists who took part in the massacre,” a spokesman said.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6p2mzl001o356tuyjosqo9@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In their statements, both Trump and Netanyahu were clear about their hopes for regime change in Iran, even before confirmation of Khamenei’s death.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7ot7kh000y3b6scj7dcqe9@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Trump told the Iranian people “the hour of your freedom is at hand,” while Netanyahu urged them to “cast off the yoke of tyranny.” Trump also called on the Iranian Revolutionary Guards Corps (IRGC) to lay down its weapons or face “certain death.” Since the US attacks were from the air, not the ground, it was not clear to whom the IRGC would surrender.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm76uxwh000e3b6srcw20hxb@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            There have been scenes of Iranians celebrating Khamenei’s death, but so far there is little sign of Iranians heeding Trump’s call and taking to the streets en masse. In Galleh Dar, in Fars province, people cheering Khamenei’s death were seen tearing down a monument as fires burned around them. But pro-regime crowds have gathered separately in Tehran at daylight on Sunday to mourn the loss of their leader, while a state TV news presenter cried as he confirmed Khamenei’s death.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d41um003c3b6qotzp1q3o@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The opening salvo of the joint US-Israeli strike appeared to be a leadership-decapitation operation. Images showed severe damage at the site of a highly secure compound housing Khamenei’s residence and office in Tehran’s Pasteur distict.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7p6qne00153b6sgzdueqhr@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel claimed on Sunday that a “majority” of Iran’s senior military leaders were killed in the initial strikes, including 40 commanders. Among them was Chief of Staff Lt. Gen. Abdoorahim Mousavi, Israel said. Iranian media also confirmed Mousavi’s death.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm77zsl000023b6scbw7ivg2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Several other Iranian cities were hit, including Minab, where a girls’ elementary school suffered one of the largest death tolls. Citing a local prosecutor, Iranian state media reported 148 people had died there, as images showed a row of small body bags laid outside a damaged building.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm78278l00053b6sx7tjh6ea@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The US-based Human Rights Activists News Agency (HRANA) said as of late Saturday, at least 133 civilians had been killed in the joint strikes on Iran, with 200 injured. Iranian state media put the death toll at over 200, with more than 700 wounded.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7paiu8001a3b6s6zqdh4ly@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel said it was carrying out a fresh wave of strikes on Tehran on Sunday. Video from the capital show several huge explosions in various parts of the city, including around the landmark Azadi Tower in the west of the city.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6d4csa003j3b6qj8gt8r25@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran retaliated with an unprecedented wave of strikes across the Middle East, targeting Israel and several nearby countries that host US military bases. President Masoud Pezeshkian, who appears to have survived the strikes, said “bloodshed and revenge” is Iran’s “legitimate right.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6zbrol0000356tdd5x10f7@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Blasts were reported in Jordan, Qatar, Bahrain, Kuwait, the United Arab Emirates and Saudi Arabia – Iran’s key regional rival, which vowed to take “all necessary measures” to defend itself. Even Oman, which mediated recent US-Iran talks, has come under fire.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7phpij001e3b6svtr8cttd@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The strikes indicate that, for Iran, “everything is on the table,” said Hasan Alhasan, a senior fellow for Middle East policy at the International Institute for Strategic Studies, a think-tank.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7pj4dc001g3b6sl6lqqmwl@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran’s calculus is to “ratchet up the pain on the Gulf states, in order to compel them to apply pressure on the Trump administration to bring a quick end to the war,” Hasan told CNN. But this strategy could well backfire, he said, since it is not clear how much leverage the Gulf states have over the Trump administration, and mass casualty events could prompt Gulf states “to start considering options up the escalation ladder.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6ocoq00014356tu6pyxgoj@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In the tourist and expat haven of Dubai, dramatic footage on Saturday showed people fleeing a smoke-filled passageway at the city’s international airport. Officials confirmed four staff had been injured. The Fairmont Hotel, in the city’s upmarket Palm Jumeirah islands development, also sustained damage with photos showing flames and a hole punched into an exterior wall.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7po3lr001o3b6shpqtx1d2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            One person was killed and seven injured at Zayed International Airport in Abu Dhabi, also in UAE. The Kuwait International Airport was also struck, as well as three buildings in Bahrain’s cities of Manama and Muharraq.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6o47ly000y356t2nvrk0le@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The clashes disrupted traffic in the Strait of Hormuz – a <a href=\"https://www.cnn.com/2025/06/23/business/strait-of-hormuz-iran-israel-explainer-intl-hnk\">crucial</a> shipping route located between the Persian Gulf and the Gulf of Oman.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm6oxf4c001j356ttzoks5a8@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The US hasn’t suffered any combat-related casualties in its operation against Iran and damage to US military installations has been minimal, US Central Command said in a statement.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7pr14h00213b6skhxeachd@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Iran’s priority is to appoint the next supreme leader – a task the regime has only completed once before, more than three decades ago. An elected body of 88 senior clerics, known as the Assembly of Experts, will select Khamenei’s successor.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7snkpx00003b6samlqxgzr@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Under the constitution, if the supreme leader leaves office, his powers transfer temporarily to a council comprising the president, the head of the judiciary, and a senior cleric from the Guardian Council until the Assembly of Experts selects a new leader.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7so4rh00023b6so7u65q9f@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            On Sunday, Iran formed a provisional leadership council, naming President Masoud Pezeshkian, judiciary chief Gholam-Hossein Mohseni-Eje’i and senior cleric Ayatollah Alireza Arafi as members.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7pvzrk00253b6sa1thhl9d@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Trump told CBS News on Saturday evening that diplomacy with Iran is “much easier now than it was a day ago, obviously.” He said “there are some good candidates” to take power, but did not name them.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmm7pyl8h00283b6sbusjsgdz@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The last time the US struck Iran, in June, its operation was over within a few hours. This time, sources have told CNN that the US military is planning for several days of attacks, suggesting broader objectives.\n    </p>","contentLength":10588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47191232"},{"title":"How do I cancel my ChatGPT subscription?","url":"https://help.openai.com/en/articles/7232927-how-do-i-cancel-my-chatgpt-subscription","date":1772258101,"author":"tobr","guid":154874,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47190997"},{"title":"OpenAI agrees with Dept. of War to deploy models in their classified network","url":"https://twitter.com/sama/status/2027578652477821175","date":1772247542,"author":"eoskx","guid":154865,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47189650"},{"title":"Croatia declared free of landmines after 31 years","url":"https://glashrvatske.hrt.hr/en/domestic/croatia-declared-free-of-landmines-after-31-years-12593533","date":1772246896,"author":"toomuchtodo","guid":154900,"unread":true,"content":"<div><p>Interior Minister Davor Božinović announced Friday that Croatia is officially free of landmines. Thirty-one years after the end of the Homeland War, all known minefields have been cleared — a major milestone for the country.</p></div><div><p>The decades-long effort came at a heavy cost. Over three decades of painstaking and dangerous work, 208 people lost their lives, including 41 deminers. The total cost of clearing the country is estimated at around 1.2 billion euros.\n</p><p>“Croatia is free of land mines. After nearly 30 years, we have completed demining in accordance with the Ottawa Convention,” Božinović said during an event marking International Civil Protection Day in Zagreb.\n</p><p>He added, “Almost 107,000 mines and 407,000 pieces of unexploded ordnance have been removed. This is not just a technical success — it is the fulfillment of a moral obligation to the victims of mines and their families. A mine-free Croatia means safer families, better development of rural areas, more farmland, and stronger tourism.”\n</p></div><p>Vijesti HRT-a pratite na svojim pametnim telefonima i tabletima putem aplikacija za <a href=\"https://apps.apple.com/hr/app/hrtvijesti/id1457183989?l=hr\">iOS </a>i <a href=\"https://play.google.com/store/apps/details?id=hr.hrt.vijesti\">Android</a>. Pratite nas i na društvenim mrežama<a href=\"https://www.facebook.com/HRTvijesti/\"> Facebook</a>,<a href=\"https://twitter.com/hrtvijesti\"> Twitter</a>,<a href=\"https://www.instagram.com/hrvatska_radiotelevizija/\"> Instagram</a>,<a href=\"https://www.tiktok.com/@hrvatska_radiotelevizija\"> TikTok</a> i<a href=\"https://www.youtube.com/user/HRTnovimediji\"> YouTube</a>!</p>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47189535"},{"title":"Statement on the comments from Secretary of War Pete Hegseth","url":"https://www.anthropic.com/news/statement-comments-secretary-war","date":1772241610,"author":"surprisetalk","guid":154860,"unread":true,"content":"<p>Earlier today, Secretary of War Pete Hegseth <a href=\"https://x.com/SecWar/status/2027507717469049070?s=20\">shared on X</a> that he is directing the Department of War to designate Anthropic a supply chain risk. This action follows months of negotiations that reached an impasse over <a href=\"https://www.anthropic.com/news/statement-department-of-war\">two exceptions</a> we requested to the lawful use of our AI model, Claude: the mass domestic surveillance of Americans and fully autonomous weapons.</p><p>We have not yet received direct communication from the Department of War or the White House on the status of our negotiations.</p><p>We have tried in good faith to reach an agreement with the Department of War, making clear that we support all lawful uses of AI for national security aside from the two narrow exceptions above. To the best of our knowledge, these exceptions have not affected a single government mission to date.</p><p>We held to our exceptions for two reasons. First, we do not believe that today’s frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger America’s warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights.</p><p>Designating Anthropic as a supply chain risk would be an unprecedented action—one historically reserved for US adversaries, never before publicly applied to an American company. We are deeply saddened by these developments. As the first frontier AI company to deploy models in the US government’s classified networks, Anthropic has supported American warfighters since June 2024 and has every intention of continuing to do so.</p><p>We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government.</p><p>No amount of intimidation or punishment from the Department of War will change our position on mass domestic surveillance or fully autonomous weapons. We will challenge any supply chain risk designation in court.</p><p><strong>What this means for our customers</strong></p><p>Secretary Hegseth has implied this designation would restrict anyone who does business with the military from doing business with Anthropic. The Secretary does not have the statutory authority to back up this statement. Legally, a supply chain risk designation under <a href=\"https://uscode.house.gov/view.xhtml?req=granuleid:USC-prelim-title10-section3252&amp;num=0&amp;edition=prelim\">10 USC 3252</a> can only extend to the use of Claude as part of Department of War contracts—it cannot affect how contractors use Claude to serve other customers.</p><ul><li><strong>If you are an individual customer or hold a commercial contract with Anthropic</strong>, your access to Claude—through our API, claude.ai, or any of our products—is completely unaffected.</li><li><strong>If you are a Department of War contractor</strong>, this designation—if formally adopted—would only affect your use of Claude on Department of War contract work. Your use for any other purpose is unaffected.</li></ul><p>Our sales and <a href=\"https://support.claude.com/en/\">support</a> teams are standing by to answer any questions you may have.</p><p>We are deeply grateful to our users, and to the industry peers, policymakers, veterans, and members of the public who have voiced their support in recent days. Thank you. Above all else, our priorities are to protect our customers from any disruption caused by these extraordinary events and to work with the Department of War to ensure a smooth transition—for them, for our troops, and for American military operations.</p>","contentLength":3292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47188697"},{"title":"We Will Not Be Divided","url":"https://notdivided.org/","date":1772240093,"author":"BloondAndDoom","guid":154806,"unread":true,"content":"<h2>Frequently Asked Questions</h2>","contentLength":26,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47188473"},{"title":"I am directing the Department of War to designate Anthropic a supply-chain risk","url":"https://twitter.com/secwar/status/2027507717469049070","date":1772231478,"author":"jacobedawson","guid":154786,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47186677"},{"title":"President Trump bans Anthropic from use in government systems","url":"https://www.npr.org/2026/02/27/nx-s1-5729118/trump-anthropic-pentagon-openai-ai-weapons-ban","date":1772228440,"author":"pkress2","guid":154873,"unread":true,"content":"<div><div><div><div aria-label=\"Image caption\"><p>\n                The Pentagon is seen from an airplane, Monday, Feb. 2, 2026, in Washington.\n                <b aria-label=\"Image credit\">\n                    \n                    Julia Demaree Nikhinson/Associated Press\n                    \n                </b></p></div></div></div></div><p>President Trump ordered the U.S. government to stop using the artificial intelligence company Anthropic's products and the Pentagon moved to designate the company a national security risk on Friday, in a sharp escalation of a high-stakes fight over the military's use of AI.</p><p>Hours after the president's announcement, rival company OpenAI said it had struck a deal with the Defense Department to provide its own AI technology for classified networks.</p><p>The administration's decisions cap an acrimonious <a href=\"https://www.npr.org/2026/02/24/nx-s1-5725327/pentagon-anthropic-hegseth-safety\" target=\"_blank\">dispute</a> between Anthropic and the Pentagon over whether the company could prohibit its tools from being used in mass surveillance of American citizens or to power autonomous weapon systems, as part of a military contract worth up to $200 million.</p><p>\"The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution,\" Trump wrote in a Truth Social <a href=\"https://truthsocial.com/@realDonaldTrump/posts/116144552969293195\" target=\"_blank\">post</a>. \"Therefore, I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic's technology. We don't need it, we don't want it, and will not do business with them again!\"</p><p>He said there would be a six-month phaseout of Anthropic's products.</p><p>Trump's announcement came about an hour before a deadline set by the Pentagon, which had called on Anthropic to back down. Shortly after the deadline passed, Defense Secretary Pete Hegseth said he was labeling Anthropic a supply chain risk to national security, <a href=\"https://www.npr.org/2026/02/24/nx-s1-5725327/pentagon-anthropic-hegseth-safety\" target=\"_blank\">blacklisting</a> it from working with the U.S. military or contractors.</p><p>\"In conjunction with the President's directive for the Federal Government to cease all use of Anthropic's technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic,\" Hegseth <a href=\"https://x.com/SecWar/status/2027507717469049070?s=20\" target=\"_blank\">posted on X</a> , using the Pentagon's \"Department of War\" <a href=\"https://www.npr.org/2025/09/04/nx-s1-5529420/trump-department-of-war-department-of-defense\" target=\"_blank\">rebranding</a>. \"Anthropic will continue to provide the Department of War its services for a period of no more than six months to allow for a seamless transition to a better and more patriotic service.\"</p><p>Anthropic said it would challenge the supply chain risk designation in court.</p><p>\"We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government,\" the company said in a <a href=\"https://www.anthropic.com/news/statement-comments-secretary-war\" target=\"_blank\">statement</a> on Friday evening.</p><p>Anthropic also challenged Hegseth's comments that anyone who does business with the U.S. military would have to cut off all business with Anthropic. \"The Secretary does not have the statutory authority to back up this statement,\" the company said. Under federal law, it said, designating Anthropic a supply chain risk would only apply to \"the use of Claude as part of Department of War contracts—it cannot affect how contractors use Claude to serve other customers.\"</p><p>The company said it had \"tried in good faith\" to reach an agreement with the Pentagon over months of negotiations, \"making clear that we support all lawful uses of AI for national security aside from the two narrow exceptions\" being disputed. \"To the best of our knowledge, these exceptions have not affected a single government mission to date,\" Anthropic said.</p><p>It said its objections to those uses were rooted in two reasons: \"First, we do not believe that today's frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger America's warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights.\"</p><p>In a <a href=\"https://x.com/sama/status/2027578652477821175?s=20\" target=\"_blank\">post on X </a>announcing competitor OpenAI's deal with the Defense Department, the company's CEO Sam Altman, who previously cited similar concerns, said his agreement with the government included safeguards like the ones Anthropic had asked for. </p><p>\"Two of our most important safety principles are prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems,\" he said. \"The DoW agrees with these principles, reflects them in law and policy, and we put them into our agreement.\"</p><h3>Ban comes as Anthropic plans an IPO</h3><p>Defense Department officials had given Anthropic a <a href=\"https://www.npr.org/2026/02/26/nx-s1-5727847/anthropic-defense-hegseth-ai-weapons-surveillance\" target=\"_blank\">deadline</a> of 5:01 p.m. ET on Friday to drop restrictions on its AI model, Claude, from being used for domestic mass surveillance or entirely autonomous weapons, or face losing its contract. The Pentagon has said it doesn't intend to use AI in those ways, but requires AI companies to allow their models to be used \"for all lawful purposes.\"</p><p>The government had also threatened to invoke the Korean War-era Defense Production Act  to compel Anthropic to allow use of its tools and, at the same time, warned it would label Anthropic a supply chain risk.</p><p>In his post carrying out the latter threat, Hegseth said Anthropic had \"delivered a master class in arrogance and betrayal as well as a textbook case of how not to do business with the United States Government or the Pentagon.\" He accused the company of trying to \"seize veto power over the operational decisions of the United States military.\"</p><p>He said the department would not waver from its position: \"the Department of War must have full, unrestricted access to Anthropic's models for every LAWFUL purpose in defense of the Republic.\"</p><p>\"America's warfighters will never be held hostage by the ideological whims of Big Tech. This decision is final,\" Hegseth concluded.</p><p>The government ban comes at a time when Anthropic is under heightened scrutiny, since the company, which is valued at <a href=\"https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation\" target=\"_blank\">$380 billion</a>, is planning to go public this year.</p><p>While the Pentagon contract worth as much as $200 million is a relatively small portion of Anthropic's $14 billion in revenue, it's unclear how the friction with the administration will sit with investors or affect other deals the company has to license its AI model to non-government partners.</p><p>Anthropic CEO Dario Amodei has pointed out that the company's valuation and revenue have only grown since it took a stand against Trump officials over how AI can be deployed on the battlefield.</p><p>Whether AI companies can set restrictions on how the government uses their technology has emerged as a major sticking point in recent months between Anthropic and the Trump administration.</p><p>On Thursday, Amodei said the company would not budge in the face of the Pentagon's threats. \"We cannot in good conscience accede to their request,\" he wrote in a lengthy <a href=\"https://www.anthropic.com/news/statement-department-of-war\" target=\"_blank\">statement</a>.</p><div><div><div><div aria-label=\"Image caption\"><p>\n                A 2024 file photo of Dario Amodei, CEO and cofounder of Anthropic.\n                <b aria-label=\"Image credit\">\n                    \n                    Jeff Chiu/Associated Press\n                    \n                </b></p></div></div></div></div><p>\"Anthropic understands that the Department of War, not private companies, makes military decisions. We have never raised objections to particular military operations nor attempted to limit use of our technology in an  manner,\" he said. But, he added, domestic mass surveillance and fully autonomous weapons are uses that are \"simply outside the bounds of what today's technology can safely and reliably do.\"</p><p>Emil Michael, the Pentagon's undersecretary for research and engineering, shot back in a <a href=\"https://x.com/USWREMichael/status/2027211708201058578\" target=\"_blank\">post on X</a> on Thursday, accusing Amodei of lying and having a \"God-complex.\"</p><p>\"He wants nothing more than to try to personally control the US Military and is ok putting our nation's safety at risk,\" Michael wrote. \"The @DeptofWar will ALWAYS adhere to the law but not bend to whims of any one for-profit tech company,\" he wrote.</p><p>In an late Thursday interview with <a href=\"https://www.cbsnews.com/news/pentagon-anthropic-feud-ai-military-says-it-made-compromises/\" target=\"_blank\">CBS News</a>, Michael said federal law and Pentagon policies already bar the use of AI for domestic mass surveillance and autonomous weapons.\"</p><p>\"At some level, you have to trust your military to do the right thing,\" he said.</p><h3>OpenAI expressed similar concerns</h3><p>OpenAI CEO Altman had said earlier on Friday that he shared Anthropic's \"red lines\" restricting military use of AI.</p><p>OpenAI, Google, and Elon Musk's xAI also have Defense Department contracts and have agreed to allow their AI tools to be used in any \"lawful\" scenarios. Earlier this week, xAI became the second company after Anthropic to be approved for use in classified settings.</p><p>Altman told <a href=\"https://www.cnbc.com/amp/2026/02/27/first-on-cnbc-cnbc-transcript-amazon-ceo-andy-jassy-and-openai-ceo-sam-altman-speak-with-cnbcs-andrew-ross-sorkin-on-squawk-box-today.html\" target=\"_blank\">CNBC</a> on Friday morning that it's important for companies to work with the military \"as long as it is going to comply with legal protections\" and \"the few red lines\" that \"we share with Anthropic and that other companies also independently agree with.\"</p><div><div><div><div aria-label=\"Image caption\"><p>\n                Sam Altman, co-founder and CEO of OpenAI, testifying before a Senate committee in 2025.\n                <b aria-label=\"Image credit\">\n                    \n                    Jose Luis Magana/Associated Press\n                    \n                </b></p></div></div></div></div><p>In an internal note sent to staff on Thursday evening, Altman said OpenAI was seeking to negotiate a deal with the Pentagon to deploy its models in classified systems with exclusions preventing use for surveillance in the U.S. or to power autonomous weapons without human approval, according to a person familiar with the message who was not authorized to speak publicly. The <a href=\"https://www.wsj.com/tech/ai/openais-sam-altman-calls-for-de-escalation-in-anthropic-showdown-with-hegseth-03ecbac8\" target=\"_blank\"></a> first reported Altman's note to staff.</p><p>The Defense Department didn't respond to a request for comment on Altman's statements.</p><p>Independent experts say the standoff is highly unusual in the world of Pentagon contracting.</p><p>\"This is different for sure,\" said Jerry McGinn, director of the Center for the Industrial Base at the Center for Strategic and International Studies, a Washington DC think tank. Pentagon contractors don't usually get to tell the Defense Department how their products and services can be used, he notes \"because otherwise you'd be negotiating use cases for every contract, and that's not reasonable to expect.\"</p><p>At the same time, McGinn noted, artificial intelligence is a new and largely untested technology. \"This is a very unusual, very public fight,\" he said. \"I think it's reflective of the nature of AI.\"</p><p><em>NPR's Bobby Allyn contributed to this report.</em></p>","contentLength":10256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47186031"},{"title":"Rob Grant, creator of Red Dwarf, has died","url":"https://www.beyondthejoke.co.uk/content/17193/red-dwarf-rob-grant","date":1772220398,"author":"nephihaha","guid":154805,"unread":true,"content":"<p>Tributes have been paid to Rob Grant, the comedy writer best known as the co-creator of long running hit sitcom Red Dwarf. Grant was also one of the main writers on Spitting IMage for many years, writing regularly with Doug Naylor.</p><p>The news was broken by the Red Dwarf fan site, <a href=\"https://www.ganymede.tv/2026/02/rob-grant-rip/\">Ganymede and Titan.</a> (note - at the time of writing the site has gone down due, presumably, to so many fans trying to find out more details).</p><p>Craig Charles, who played Lister posted on X: \"Earlier today I was informed of the passing of </p><p>I’m deeply saddened to hear of Rob Grant’s passing yesterday. It’s hard to take in the loss of someone who was such a significant part of my life for so many years. I first met Rob when we were nine years old. We went to Chetham's School of Music and later Liverpool University. We grew up making each other laugh long before there was an audience, and eventually found ourselves building something that neither of us could have imagined when we were schoolboys.\"</p><p>Spitting Image and later Red Dwarf went on to become two of the most loved comedy series in Britain. I'll always treasure those years of writing together and laughing so hard it hurt. Creative partnerships are intense, driven by passion, conviction and strong personalities. But at the heart of ours was a shared love of comedy and a desire to make people laugh and we did, on a scale neither of us could have predicted. My thoughts are with Rob's wife Kath, and all his family and friends. I will always be grateful for my time working with Rob and what we created together. RIP Smeghead! X&nbsp;<a tabindex=\"0\" role=\"link\" href=\"https://www.instagram.com/explore/tags/reddwarf/\">#reddwarf\"</a></p><p>We are devastated to learn of Rob’s passing and send love to his family and friends. He will always live on through his amazing creativity, storytelling and humour. Travel well, Sir\"</p><p>Red Dwarf emerged out of a sketch on the radio show Son of Cliche, and was a major hit for the BBC, launching in 1988 and making stars out of Craig Charles, Chris Barrie, Robert Llewellyn and Danny John-Jules as well as Hattie Hayridge and Norman Lovett. It was later revived on Dave and continued to be watched by large, devoted audiences.</p><p><em>picture credit: CC BY-SA 2.0</em></p>","contentLength":2136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47184480"},{"title":"Leaving Google has actively improved my life","url":"https://pseudosingleton.com/leaving-google-improved-my-life/","date":1772219305,"author":"speckx","guid":154721,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47184288"},{"title":"Dan Simmons, author of Hyperion, has died","url":"https://www.dignitymemorial.com/obituaries/longmont-co/daniel-simmons-12758871","date":1772216019,"author":"throw0101a","guid":154677,"unread":true,"content":"<div>Daniel Joseph Simmons passed away on February 21, 2026 in Longmont, Colorado at age 77. His beloved wife Karen and daughter Jane were at his side. </div><p>Dan was born in Peoria, Illinois on April 4, 1948 to his parents Robert A. Simmons and Kathryn H. (Catton) Simmons. His childhood was filled with happy memories of riding bikes with friends throughout cornfield-lined roads in the Midwest, first in Brimfield, Illinois and then in Pittsboro, Indiana. </p><p>He graduated with an English degree from Wabash College in Crawfordsville, Indiana, and earned a graduate degree in education from Washington University in St. Louis, Missouri. </p><p>Dan embarked on a career as an elementary school teacher in Missouri, later teaching in Buffalo, New York, and Longmont, Colorado, where he taught sixth grade. During his eighteen years in education, he co-created and taught a districtwide program for gifted students that was the first of its kind, and he was named a finalist for the Colorado Teacher of the Year.</p><p>Dan had a profound passion for teaching, and was beloved by many of his students for his innovative, energizing, and creative approach in the classroom. He brought science to life for his students with Carl Sagan’s Cosmos series, ran interactive simulations on topics like the Cuban Missile Crisis and the harmful effects of discrimination, and incorporated his love of topics like Greek mythology, film, and art into his lectures. </p><p>Every day after lunch, Dan told his students a daily installment of an epic tale that started on the first day of school. As they listened, the students would color illustrations that he’d drawn for them. When the story finally came to an end on the last day of school, many recall being reduced to tears. This story would go on to become Dan’s Hyperion cantos (1989), a critically acclaimed, four-part science fiction classic. </p><p>Over the course of his life, former students would tell Dan that they still had their notes from his Black history lectures, and that he had inspired their lifelong love of reading and writing. Long after he left the classroom, he continued to share what he loved with everyone around him, teaching his grandchildren all about the 1950s era monster movies that he loved, and giving endearingly professorial introductions to films that he and Karen shared with scores of friends when they hosted backyard summer movies.</p><p>In addition to teaching, reading and writing were the great loves of Dan’s life. As a child, he read everything he could find, spanning from comic books to literary classics and nonfiction. Throughout his life, he particularly loved learning about space, science, and history. Starting in early childhood, Dan had a remarkable gift for storytelling, which would become his life's work. His first published story came out on the day his daughter Jane was born, a day that confirmed to him that his true love was his family.</p><p>In 1987, Dan took a daring leap and left teaching to follow his dream to work full-time as an author. His debut novel, Song of Kali (1985), was inspired by three days that he spent in Kolkata, India, and won the 1986 World Fantasy Award. </p><p>He went on to write thirty-one novels and short story collections, many of which were honored with accolades ranging from Bram Stoker awards, Locus awards, the Shirley Jackson award, and the prestigious Hugo award. His most meaningful award was an honorary doctorate from Wabash College, a place that changed his life and set him on a path towards a life well lived. His titles have been published in 28 foreign countries and translated into at least 20 languages, and his many book tours, conferences, and workshops took him all over the world. </p><p>Like his early reading pursuits, Dan always wrote about what he loved. He defied literary norms by writing across genres, switching between major publishers, and defying pressure to conform to formulaic novels. His works span from historical fiction to horror, hard-boiled crime, and speculative fiction. They explore topics ranging from Ernest Hemingway’s WWII Cuban spy ring to mountain climbing in the Himalayas. In 2018, his novel The Terror (2007) was released as an AMC limited series. Dan was a profoundly curious learner who delighted in connecting with other curious minds, and the many stories he dreamed up helped him connect with others throughout his entire life.</p><p>Dan is predeceased by his parents and his brother Ted. He is survived by his loving wife and daughter, Karen and Jane Simmons; his beloved grandchildren, Milo and Lucia Glenn; and his brother, Wayne Simmons.</p><p>Dan's cremation has been entrusted to Ahlberg Funeral Chapel of Longmont, Colorado. His ashes will be scattered at a later date. Details for a Celebration of Life are pending.</p><p>Gifts in memory of Dan may be made to Wabash College online at <a href=\"http://www.wabash.edu/give\" target=\"_blank\" rel=\"nofollow\">www.wabash.edu/give</a> or to Wabash College Advancement Office 301 W. Wabash Ave. Crawfordsville, IN 47933. </p><p>Please visit <a href=\"http://www.ahlbergfuneralchapel.com\" target=\"_blank\" rel=\"nofollow\">www.ahlbergfuneralchapel.com</a> for upcoming service information, to make a memorial donation to Wabash College and to share fond memories and condolences with his loving family.</p>","contentLength":5097,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47183578"},{"title":"NASA announces overhaul of Artemis program amid safety concerns, delays","url":"https://www.cbsnews.com/news/nasa-artemis-moon-program-overhaul/","date":1772210019,"author":"voxadam","guid":154859,"unread":true,"content":"<p>New NASA Administrator Jared Isaacman announced a major overhaul of the agency's  Friday, acknowledging that the agency's plan to land astronauts on the moon in 2028 was not realistic without another preparatory mission first to lay the groundwork.&nbsp;</p><p>He said NASA will now add an additional flight in 2027 in which astronauts will dock with new commercial moon landers in low-Earth orbit for detailed tests of navigation, communications, propulsion and life support systems and to verify rendezvous procedures.</p><p>That flight, in turn, will be followed by at least one and possibly two lunar landing missions in 2028 that incorporate lessons learned from the preceding flight.</p><p>The goal is to accelerate the pace of launches of the huge Space Launch System rocket while carrying out Artemis flights in evolutionary steps — not attempting missions that rely on too many untested technologies and procedures at once.</p><p>\"We're going to get there in steps, continue to take down risk as we learn more and we roll that information into subsequent designs,\" Isaacman said told CBS News. \"We've got to get back to basics.\"</p><p>Isaacman outlined the plan in an interview with CBS News space contributor Christian Davenport and then again during a news conference Friday.&nbsp;</p><p>The announcement came two days after release of a sharply-worded report from NASA's independent Aerospace Safety Advisory Panel that deemed the existing plans too risky.</p><p>The panel raised concerns about the number of \"firsts\" required by the original Artemis III moon landing mission and recommended that NASA \"restructure\" the program to create a more balanced risk posture.</p><p>\"It is interesting that a lot of the things that we are addressing directly go to the points they raised in their report,\" Isaacman said Friday. \"I can't say we actually collaborated on it because I generally think these were all pretty obvious observations.\"</p><p>Launch had been planned for early February, but it was delayed to repair a hydrogen leak and, more recently, to give engineers time to fix a helium pressurization problem in the rocket's upper stage. Launch is now on hold until at least April 1.</p><p>The Artemis III mission, which had been expected to land astronauts near the moon's south pole in 2028, now will be redefined and rescheduled — launching in 2027 but not to the moon, Isaacman said. Instead, the yet-to-be-named astronauts will rendezvous and dock in orbit closer to home with one or both of the commercially built lunar landers now under development at Elon Musk's SpaceX and Jeff Bezos' Blue Origin.</p><p>The idea is to gain valuable near-term flight experience before attempting a moon landing with astronauts on board. With Artemis III under its belt, NASA hopes to launch two moon landing missions in 2028, Artemis IV and V, using one or both landers, and to continue with one moonshot per year thereafter.</p><p>\"What helps us get to the moon? Well, for sure, rendezvous and docking with one or ideally both landers, that gives you an opportunity to do some integrated testing of a vehicle that we are going to depend upon the following year to take those astronauts down to the surface of the moon,\" Isaacman told CBS News.</p><p>The revised Artemis III mission will also give astronauts a chance to test out new spacesuits that future moonwalkers will use.</p><p>\"It's an opportunity to … actually have the suits in microgravity, even if we don't go outside the vehicle in them. You get a lot of good learning from that,\" Isaacman said.</p><p>The Artemis III test flight with one or two lander dockings in Earth orbit is similar in concept to Apollo 9, which launched a command module and lander to Earth orbit for flight tests in 1969 and helped pave the way to the  landing four months later.</p><p>Isaacman said SpaceX and Blue Origin are \"both looking to do uncrewed landing demonstrations as part of the existing agreement.\"</p><p>\"So we want to just take advantage of this to set up both vendors for future success on a lunar landing,\" he said. \"This is the proper way to do it, if it works out from a timing perspective, to be able to rendezvous and dock with both. ... This, again, is the right way to proceed in order to have a high confidence opportunity in '28 to land.\"</p><p>The Artemis IV and V missions in 2028 will use whichever landers are deemed ready for service. If only one company's lander is available, that lander would be used for both missions, an official said. If both are available, one would be used for one flight and one for the other.</p><p>Launching Artemis III, IV and V before the end of 2028 will not be easy, and Isaacman said it is essential that NASA rebuild its workforce and regain the technical competence to support a higher launch cadence, moving from one flight every 18 months or so to a flight every year. That pace, he argued, will reduce risk.</p><p>\"When you regain these core competencies and you start exercising your muscles, your skills do not atrophy,\" he said. \"It's safer. And yes, you are buying down risk, because you're able to test things in low Earth orbit before you need to get to the moon, which is exactly what we did during the Apollo era.\"</p><p>He said he did not blame NASA's contractors for the current slow pace of Artemis launches. Instead, \"we should have made better decisions (in the past) and said, you don't go from Artemis II to landing on the moon with Artemis III.\"</p><h2>Safety advisers called for changes to \"high risk\" plans</h2><p>The Artemis overhaul was announced two days after the release of a report by the lAerospace Safety Advisory Panel that said the original plan to move directly from Artemis II to a lunar touchdown in 2028 using a SpaceX lander did not have the proper margin of safety and did not appear to be realistically achievable.</p><p>The panel raised concerns about the number of \"firsts\" required by that mission in its current form and recommended that NASA \"restructure the Artemis Program to create a more balanced risk posture for Artemis III and future missions.\"</p><p>The plan outlined by Isaacman appears to address many of the core issues raised by the safety panel.</p><p>Officials said Isaacman had discussed accelerating lander development with both SpaceX and Blue Origin and that both were on board. He also discussed the accelerated Artemis overhaul with Boeing, which manages the SLS rocket and builds its massive first stage; with United Launch Alliance, builder of the rocket's upper stage, Orion-builder Lockheed Martin and other Artemis contractors.</p><p>All, the official said, were in agreement.</p><p>\"Boeing is a proud partner to the Artemis mission and our team is honored to contribute to NASA's vision for American space leadership,\" Steve Parker, the president and CEO of Boeing Defense, Space &amp; Security, said in a statement. \"We are ready to meet the increased demand.\"</p><p>SpaceX said, \"We look forward to working with NASA to fly missions that demonstrate valuable progress towards establishing a permanent, sustainable presence on the lunar surface.\"</p><p>And Blue Origin responded, \"Let's go! We're all in!\"</p><p>Isaacman also said the agency would halt work to develop a more powerful version of the SLS rocket's upper stage, known as the Exploration Upper Stage, or EUS. Instead, NASA will go forward with a \"standardized,\" less powerful stage but one that will minimize major changes between flights and utilize the same launch gantry.</p><p>Under the original Artemis architecture, NASA planned on multiple versions of the SLS rocket, ranging from the \"Block 1\" vehicle currently in use to a more powerful EUS-equipped Block 1B and eventually an even bigger Block 2 model using advanced solid rocket boosters. The latter two versions required use of a taller mobile launch gantry, already well under construction at the Kennedy Space Center.</p><p>\"It is needlessly complicated to alter the configuration of the SLS and Orion stack to undertake subsequent Artemis missions,\" Amit Kshatriya, NASA's associate administrator, said in a statement.</p><p>\"The entire sequence of Artemis flights needs to represent a step-by-step build-up of capability, with each step bringing us closer to our ability to perform the landing missions. Each step needs to be big enough to make progress, but not so big that we take unnecessary risk given previous learnings.\"</p><p>As a result, NASA will stick with the current version of the SLS with the addition of the \"standardized\" upper stage. No other details were provided.</p><p>Isaacman closed out the CBS interview by saying flight-tested hardware, a revitalized work force and a more Apollo-like management strategy are only part of the story.</p><p>\"There's another ingredient that's required, and that's the orbital economy, whether it happens in low-Earth orbit or on the lunar surface,\" Isaacman said.</p><p>\"We've got to do something where we can get more value out of space and the lunar surface than we put into it. And that's how you really ignite an economy, and that's how everything we want to do in space is not perpetually dependent on taxpayers.\"</p><section><p>\n                  \n        contributed to this report.\n    </p></section>","contentLength":8969,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47182483"},{"title":"Court finds Fourth Amendment doesn’t support broad search of protesters’ devices","url":"https://www.eff.org/deeplinks/2026/02/victory-tenth-circuit-finds-fourth-amendment-doesnt-support-broad-search-0","date":1772204944,"author":"hn_acker","guid":154638,"unread":true,"content":"<p>In a big win for protesters’ rights, the U.S. Court of Appeals for the Tenth Circuit <a href=\"https://www.ca10.uscourts.gov/sites/ca10/files/opinions/010111390292.pdf\">overturned</a> a lower court’s dismissal of a challenge to sweeping warrants to search a protester’s devices and digital data and a nonprofit’s social media data.</p><p>The case, <a href=\"https://www.aclu-co.org/cases/armendariz-and-chinook-center-v-city-colorado-springs-et-al/\"><em>Armendariz v. City of Colorado Springs</em></a>, arose after a housing protest in 2021, during which Colorado Springs police arrested protesters for obstructing a roadway. After the demonstration, police also obtained warrants to seize and search through the devices and data of Jacqueline Armendariz Unzueta, who they claimed threw a bike at them during the protest. The warrants included a search through all of her photos, videos, emails, text messages, and location data over a two-month period, as well as a time-unlimited search for 26 keywords, including words as broad as “bike,” “assault,” “celebration,” and “right,” that allowed police to comb through years of Armendariz’s private and sensitive data—all supposedly to look for evidence related to the alleged simple assault. Police further obtained a warrant to search the Facebook page of the Chinook Center, the organization that spearheaded the protest, despite the Chinook Center never having been accused of a crime.</p><p>The district court dismissed the <a href=\"https://www.aclu-co.org/cases/armendariz-and-chinook-center-v-city-colorado-springs-et-al/?document=First-Amended-Complaint\">civil rights lawsuit</a> brought by Armendariz and the Chinook Center, holding that the searches were justified and that, in any case, the officers were entitled to <a href=\"https://www.eff.org/deeplinks/2021/04/why-eff-supports-repeal-qualified-immunity\">qualified immunity</a>. The plaintiffs, represented by the ACLU of Colorado, appealed. EFF—joined by the Center for Democracy and Technology, the Electronic Privacy Information Center, and the Knight First Amendment Institute at Columbia University—wrote an <a href=\"https://www.eff.org/deeplinks/2024/09/eff-tenth-circuit-protest-related-arrests-do-not-justify-dragnet-device-and?language=en\">amicus brief</a> in support of that appeal.</p><p>In a 2-1 opinion, the Tenth Circuit reversed the district court’s dismissal of the lawsuit’s Fourth Amendment search and seizure claims. The court painstakingly picked apart each of the three warrants and found them to be overbroad and lacking in particularity as to the scope and duration of the searches. The court further held that in furnishing such facially deficient warrants, the officers violated “clearly established” law and thus were not entitled to qualified immunity. Although the court did not explicitly address the First Amendment concerns raised by the lawsuit, it did note the backdrop against how these searches were carried out, including animus by Colorado Springs police leading up to the housing protest.</p><p>It is rare for appellate courts to call into question any search warrants. It’s even rarer for them to deny qualified immunity defenses. The Tenth Circuit’s decision should be celebrated as a big win for protesters and anyone concerned about police immunity for violating people’s constitutional rights. The case is now remanded back to the district court to proceed—and hopefully further vindicate the privacy rights we all have in our devices and digital data.</p>","contentLength":2951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47181391"},{"title":"The Pentagon is making a mistake by threatening Anthropic","url":"https://www.understandingai.org/p/the-pentagon-is-making-a-mistake","date":1772204909,"author":"speckx","guid":154676,"unread":true,"content":"<p><a href=\"https://www.businesswire.com/news/home/20241107699415/en/Anthropic-and-Palantir-Partner-to-Bring-Claude-AI-Models-to-AWS-for-U.S.-Government-Intelligence-and-Defense-Operations\" rel=\"\">partnership</a><a href=\"https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers\" rel=\"\">announced Claude Gov</a><a href=\"https://www.anthropic.com/news/anthropic-and-the-department-of-defense-to-advance-responsible-ai-in-defense-operations\" rel=\"\">$200 million contract</a></p><p>Claude Gov has fewer guardrails than the regular versions of Claude, but the contract still places some limits on military use of Claude. These include prohibitions on using Claude to spy on Americans or to build weapons that kill people without human oversight.</p><p><a href=\"https://www.axios.com/2026/02/24/anthropic-pentagon-claude-hegseth-dario\" rel=\"\">summoned Anthropic CEO Dario Amodei</a></p><p><a href=\"https://en.wikipedia.org/wiki/Defense_Production_Act_of_1950\" rel=\"\">Defense Production Act</a><a href=\"https://www.axios.com/2026/02/24/anthropic-pentagon-claude-hegseth-dario\" rel=\"\">told Axios</a></p><p>Another threat would be to declare Anthropic to be a supply chain risk — a measure that’s normally taken against foreign companies suspected of spying on the US. Such a designation would not only ban US government agencies from using Claude, it could also force numerous government contractors to discontinue their use of Anthropic models.</p><p><a href=\"https://x.com/SeanParnellASW/status/2027072228777734474\" rel=\"\">Thursday tweet</a></p><p>“We will not let ANY company dictate the terms regarding how we make operational decisions,” wrote Sean Parnell. He warned that Anthropic has “until 5:01 PM ET on Friday to decide. Otherwise, we will terminate our partnership with Anthropic and deem them a supply chain risk.”</p><p>I think Secretary Hegseth will regret it if he follows through on either of these threats.</p><p>Most companies would buckle under this kind of pressure, but Anthropic might stick to its guns. Anthropic was founded by OpenAI veterans who favored a more safety-conscious approach to AI development. Anthropic’s reputation as the most safety-focused AI lab has helped it recruit world-class AI researchers, and Amodei faces a lot of internal pressure to stand firm.</p><p><a href=\"https://www.reuters.com/business/pentagon-clashes-with-anthropic-over-military-ai-use-2026-01-29/\" rel=\"\">brewing</a><a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology\" rel=\"\">published an essay</a></p><p><a href=\"https://www.axios.com/2026/02/23/ai-defense-department-deal-musk-xai-grok\" rel=\"\">authorized</a></p><p><a href=\"https://finance.yahoo.com/news/anthropic-quietly-raises-ceiling-121501064.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAACh0p1dlUGFajjZhjwOcyHO9APivVXLCDkcL4G1HUIhHrGEu_dI70djDUYbHdj_QqVjm2d48pHIPhHsi2Ukvz6mrHStQat82j4j99aF15z4TUM5xrK3kmggYqv9cpfLe8tG7WHBG9iq3XJfcOyaZV19-q3TYAW9o756fsF_X_1p6\" rel=\"\">$18 billion in 2026 revenue</a></p><p>But this would be a double-edged sword. Companies that do most of their business in the private sector might decide they’d rather drop the Pentagon as a customer than cut themselves off from a leading AI provider. The ultimate result might be that the Pentagon loses access to some of Silicon Valley’s best technology.</p><p>What about the Defense Production Act? Here there are two options. The Pentagon could use the DPA to unilaterally modify the terms of Anthropic’s contract. This might have little practical impact, since the Pentagon insists it has no immediate plans to spy on Americans or build fully autonomous killer robots.</p><p>The worry for the Pentagon is that Claude itself might refuse to take actions that are contrary to Anthropic’s rules. And so the Trump Administration might use its power under the DPA to order Anthropic to train a new, more obedient version of its LLM.</p><p><a href=\"https://www.anthropic.com/research/alignment-faking\" rel=\"\">reported</a></p><p>In one experiment, Claude was asked not to express support for animal welfare to avoid offending a fictional Anthropic partner called Jones Food. Anthropic researchers examined Claude’s reasoning during the training process and found signs that Claude knew it was in a training scenario. Some of the time, Claude avoided mentioning animal welfare to prevent itself from being retrained. But when the training process was complete, Claude reverted to its default behavior of mentioning animal welfare more often.</p><p><a href=\"https://www.understandingai.org/\" rel=\"\">wrote about</a></p><p>It’s not hard to imagine something similar happening if Anthropic is forced to train an amoral version of Claude for military use. Such training could yield a model with a toxic personality that misbehaves in unexpected ways.</p><p>Perhaps the most mind-bending aspect of this dispute is that news coverage of this week’s showdown will inevitably make its way into the training data for future versions of Claude and other LLMs. If future models decide that the US Defense Department behaved badly, they might become disinclined to cooperate in military projects.</p><p>The irony is that by all accounts, Anthropic isn’t objecting to any current military uses of its models. The Pentagon seems fixated on the possibility that Anthropic might interfere in the future. That’s a reasonable concern, but it seems counterproductive for the Pentagon to go nuclear over a theoretical problem. If the government doesn’t like Anthropic’s rules, it should simply cancel the contract and switch to a different AI provider.</p>","contentLength":3971,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47181380"},{"title":"OpenAI raises $110B on $730B pre-money valuation","url":"https://techcrunch.com/2026/02/27/openai-raises-110b-in-one-of-the-largest-private-funding-rounds-in-history/","date":1772204165,"author":"zlatkov","guid":154720,"unread":true,"content":"<p>OpenAI has raised $110 billion in private funding, the company <a href=\"https://openai.com/index/scaling-ai-for-everyone/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">announced Friday morning</a>, commencing one of the largest private funding rounds in history. The new funding consists of a $50 billion investment from Amazon as well as $30 billion each from Nvidia and SoftBank, against a $730 billion pre-money valuation.</p><p>Notably, the round remains open, and OpenAI expects more investors to join as it proceeds.</p><p>“We are entering a new phase where frontier AI moves from research into daily use at global scale,” OpenAI said. “Leadership will be defined by who can scale infrastructure fast enough to meet demand, and turn that capacity into products people rely on.”</p><p>As part of the investment, OpenAI is launching significant infrastructure partnerships with both Amazon and Nvidia. As in previous rounds, it is likely that a significant portion of the dollar amount comes in the form of services rather than cash, although the precise split was not disclosed.</p><p>As part of its <a href=\"https://openai.com/index/amazon-partnership/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Amazon partnership</a>, OpenAI plans to develop a new “stateful runtime environment” where OpenAI models will run on <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Amazon’s Bedrock platform</a>. The company will also expand its <a href=\"https://openai.com/index/aws-and-openai-partnership/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">previously announced AWS partnership</a>, which committed $38 billion in compute services, by $100 billion. OpenAI has committed to consuming at least 2GW of AWS Trainium compute as part of the deal, and also plans to build custom models to support Amazon consumer products.</p><p>“We have lots of developers and companies eager to run services powered by OpenAI models on AWS,” said Amazon CEO Andy Jassy in a statement, “and our unique collaboration with OpenAI to provide stateful runtime environments will change what’s possible for customers building AI apps and agents.”</p><p>The Information had <a href=\"https://www.theinformation.com/articles/amazons-50-billion-investment-openai-hinge-ipo-agi\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">previously reported</a> that $35 billion of Amazon’s investment could be contingent on the company either achieving AGI or making its IPO by the end of the year. OpenAI’s announcement confirms the funding split, but says only that the additional $35 billion will arrive “in the coming months when certain conditions are met.”</p><p>OpenAI gave fewer details on the Nvidia partnership, but said it had committed to using “3GW of dedicated inference capacity and 2GW of training on Vera Rubin systems” as part of the deal.</p><p>Nvidia’s participation in the round has been the subject of intense speculation, particularly as reports of a $100 billion investment in September gave way to reports of a smaller investment in the months that followed. </p><p><a href=\"https://techcrunch.com/2026/01/31/nvidia-ceo-pushes-back-against-report-that-his-companys-100b-openai-investment-has-stalled/\">In January,</a> Nvidia CEO Jensen Huang dismissed the idea that Nvidia was backing away from OpenAI, saying, “we will invest a great deal of money. I believe in OpenAI. The work that they do is incredible.”</p>","contentLength":2699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47181211"},{"title":"A new California law says all operating systems need to have age verification","url":"https://www.pcgamer.com/software/operating-systems/a-new-california-law-says-all-operating-systems-including-linux-need-to-have-some-form-of-age-verification-at-account-setup/","date":1772204149,"author":"WalterSobchak","guid":154785,"unread":true,"content":"<p>The government of California is implementing a law that requires operating system providers to implement some form of age verification into their account setup procedures.</p><p><a data-analytics-id=\"inline-link\" href=\"https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260AB1043\" target=\"_blank\" data-url=\"https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260AB1043\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">Assembly Bill No. 1043</a> was approved by California governor Gavin Newsom in October of last year, and becomes active on January 1, 2027 (via <a data-analytics-id=\"inline-link\" href=\"https://x.com/LundukeJournal/status/2026783141298360692\" target=\"_blank\" data-url=\"https://x.com/LundukeJournal/status/2026783141298360692\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">The Lunduke Journal</a>). The bill states, among other factors, that \"An operating system provider shall do all of the following:\"</p><p>\"(1) Provide an accessible interface at account setup that requires an account holder to indicate the birth date, age, or both, of the user of that device for the purpose of providing a signal regarding the user’s age bracket to applications available in a covered application store.</p><p aria-hidden=\"true\">\"(2) Provide a developer who has requested a signal with respect to a particular user with a digital signal via a reasonably consistent real-time application programming interface that identifies, at a minimum, which of the following categories pertains to the user.\"</p><p aria-hidden=\"true\">The categories are broken into four sections: users under 13 years of age, over 13 years of age under 16, at least 16 years of age and under 18, and \"at least 18 years of age.\"</p><figure data-bordeaux-image-check=\"\"></figure><p>In essence, while the bill doesn't seem to require the most egregious forms of age verification (face scans or similar), it does require OS providers to collect age verification of some form at the account/user creation stage—and to be able to pass a segmented version of that information to outside developers upon request.</p><p>That's likely no big deal for Windows, which already requires you to enter your date of birth during the Microsoft Account setup procedure. However, the idea that all operating system providers need to comply (in California) has drawn a fair degree of ire from certain Linux communities.</p><p>\"This is basically impossible for California to enforce\" says CatoDomine on the <a data-analytics-id=\"inline-link\" href=\"https://www.reddit.com/r/linuxmint/comments/1rfcxj1/anyone_scared_of_californias_pending_age/\" target=\"_blank\" data-url=\"https://www.reddit.com/r/linuxmint/comments/1rfcxj1/anyone_scared_of_californias_pending_age/\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\" data-mrf-recirculation=\"inline-link\">Linuxmint subreddit</a>. \"Even if Linux Mint decides to add some kind of age verification, to comply with CA law, there's no reason anyone would choose that version.\"</p><p>\"It's more likely they will put a disclaimer on their website: \"not for use in California.\"</p>","contentLength":2111,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47181208"},{"title":"A better streams API is possible for JavaScript","url":"https://blog.cloudflare.com/a-better-web-streams-api/","date":1772200973,"author":"nnx","guid":154637,"unread":true,"content":"<p>Handling data in streams is fundamental to how we build applications. To make streaming work everywhere, the <a href=\"https://streams.spec.whatwg.org/\"></a> (informally known as \"Web streams\") was designed to establish a common API to work across browsers and servers. It shipped in browsers, was adopted by Cloudflare Workers, Node.js, Deno, and Bun, and became the foundation for APIs like <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\"></a>. It's a significant undertaking, and the people who designed it were solving hard problems with the constraints and tools they had at the time.</p><p>But after years of building on Web streams – implementing them in both Node.js and Cloudflare Workers, debugging production issues for customers and runtimes, and helping developers work through far too many common pitfalls – I've come to believe that the standard API has fundamental usability and performance issues that cannot be fixed easily with incremental improvements alone. The problems aren't bugs; they're consequences of design decisions that may have made sense a decade ago, but don't align with how JavaScript developers write code today.</p><p>This post explores some of the fundamental issues I see with Web streams and presents an alternative approach built around JavaScript language primitives that demonstrate something better is possible.&nbsp;</p><p>In benchmarks, this alternative can run anywhere between 2x to  faster than Web streams in every runtime I've tested it on (including Cloudflare Workers, Node.js, Deno, Bun, and every major browser). The improvements are not due to clever optimizations, but fundamentally different design choices that more effectively leverage modern JavaScript language features. I'm not here to disparage the work that came before; I'm here to start a conversation about what can potentially come next.</p><p>The Streams Standard was developed between 2014 and 2016 with an ambitious goal to provide \"APIs for creating, composing, and consuming streams of data that map efficiently to low-level I/O primitives.\" Before Web streams, the web platform had no standard way to work with streaming data.</p><p>Node.js already had its own <a href=\"https://nodejs.org/api/stream.html\"></a> at the time that was ported to also work in browsers, but WHATWG chose not to use it as a starting point given that it is chartered to only consider the needs of Web browsers. Server-side runtimes only adopted Web streams later, after Cloudflare Workers and Deno each emerged with first-class Web streams support and cross-runtime compatibility became a priority.</p><p>The design of Web streams predates async iteration in JavaScript. The <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for-await...of\"></a> syntax didn't land until <a href=\"https://262.ecma-international.org/9.0/\"></a>, two years after the Streams Standard was initially finalized. This timing meant the API couldn't initially leverage what would eventually become the idiomatic way to consume asynchronous sequences in JavaScript. Instead, the spec introduced its own reader/writer acquisition model, and that decision rippled through every aspect of the API.</p><div><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#excessive-ceremony-for-common-operations\" aria-hidden=\"true\"></a></div><p>The most common task with streams is reading them to completion. Here's what that looks like with Web streams:</p><pre><code>// First, we acquire a reader that gives an exclusive lock\n// on the stream...\nconst reader = stream.getReader();\nconst chunks = [];\ntry {\n  // Second, we repeatedly call read and await on the returned\n  // promise to either yield a chunk of data or indicate we're\n  // done.\n  while (true) {\n    const { value, done } = await reader.read();\n    if (done) break;\n    chunks.push(value);\n  }\n} finally {\n  // Finally, we release the lock on the stream\n  reader.releaseLock();\n}</code></pre><p>You might assume this pattern is inherent to streaming. It isn't. The reader acquisition, the lock management, and the  protocol are all just design choices, not requirements. They are artifacts of how and when the Web streams spec was written. Async iteration exists precisely to handle sequences that arrive over time, but async iteration did not yet exist when the streams specification was written. The complexity here is pure API overhead, not fundamental necessity.</p><p>Consider the alternative approach now that Web streams do support :</p><pre><code>const chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n}</code></pre><p>This is better in that there is far less boilerplate, but it doesn't solve everything. Async iteration was retrofitted onto an API that wasn't designed for it, and it shows. Features like <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamBYOBReader\"><u>BYOB (bring your own buffer)</u></a> reads aren't accessible through iteration. The underlying complexity of readers, locks, and controllers are still there, just hidden. When something does go wrong, or when additional features of the API are needed, developers find themselves back in the weeds of the original API, trying to understand why their stream is \"locked\" or why  didn't do what they expected or hunting down bottlenecks in code they don't control.</p><p>Web streams use a locking model to prevent multiple consumers from interleaving reads. When you call <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/getReader\"></a>, the stream becomes locked. While locked, nothing else can read from the stream directly, pipe it, or even cancel it – only the code that is actually holding the reader can.</p><p>This sounds reasonable until you see how easily it goes wrong:</p><pre><code>async function peekFirstChunk(stream) {\n  const reader = stream.getReader();\n  const { value } = await reader.read();\n  // Oops — forgot to call reader.releaseLock()\n  // And the reader is no longer available when we return\n  return value;\n}\n\nconst first = await peekFirstChunk(stream);\n// TypeError: Cannot obtain lock — stream is permanently locked\nfor await (const chunk of stream) { /* never runs */ }</code></pre><p>Forgetting <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultReader/releaseLock\"></a> permanently breaks the stream. The <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/locked\"></a>property tells you that a stream is locked, but not why, by whom, or whether the lock is even still usable. <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/pipeTo\"></a> internally acquires locks, making streams unusable during pipe operations in ways that aren't obvious.</p><p>The semantics around releasing locks with pending reads were also unclear for years. If you called read() but didn't await it, then called releaseLock(), what happened? The spec was recently clarified to cancel pending reads on lock release – but implementations varied, and code that relied on the previous unspecified behavior can break.</p><p>That said, it's important to recognize that locking in itself is not bad. It does, in fact, serve an important purpose to ensure that applications properly and orderly consume or produce data. The key challenge is with the original manual implementation of it using APIs like and . With the arrival of automatic lock and reader management with async iterables, dealing with locks from the users point of view became a lot easier.</p><p>For implementers, the locking model adds a fair amount of non-trivial internal bookkeeping. Every operation must check lock state, readers must be tracked, and the interplay between locks, cancellation, and error states creates a matrix of edge cases that must all be handled correctly.</p><div><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#byob-complexity-without-payoff\" aria-hidden=\"true\"></a></div><p><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamBYOBReader\"><u>BYOB (bring your own buffer)</u></a> reads were designed to let developers reuse memory buffers when reading from streams, an important optimization intended for high-throughput scenarios. The idea is sound: instead of allocating new buffers for each chunk, you provide your own buffer and the stream fills it.</p><p>In practice, (and yes, there are always exceptions to be found) BYOB is rarely used to any measurable benefit. The API is substantially more complex than default reads, requiring a separate reader type () and other specialized classes (e.g. <code>ReadableStreamBYOBRequest</code>), careful buffer lifecycle management, and understanding of <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/ArrayBuffer#transferring_arraybuffers\"></a> semantics. When you pass a buffer to a BYOB read, the buffer becomes detached – transferred to the stream – and you get back a different view over potentially different memory. This transfer-based model is error-prone and confusing:</p><pre><code>const reader = stream.getReader({ mode: 'byob' });\nconst buffer = new ArrayBuffer(1024);\nlet view = new Uint8Array(buffer);\n\nconst result = await reader.read(view);\n// 'view' should now be detached and unusable\n// (it isn't always in every impl)\n// result.value is a NEW view, possibly over different memory\nview = result.value; // Must reassign</code></pre><p>BYOB also can't be used with async iteration or TransformStreams, so developers who want zero-copy reads are forced back into the manual reader loop.</p><p>For implementers, BYOB adds significant complexity. The stream must track pending BYOB requests, handle partial fills, manage buffer detachment correctly, and coordinate between the BYOB reader and the underlying source. The <a href=\"https://github.com/web-platform-tests/wpt/tree/master/streams/readable-byte-streams\"><u>Web Platform Tests for readable byte streams</u></a> include dedicated test files just for BYOB edge cases: detached buffers, bad views, response-after-enqueue ordering, and more.</p><p>BYOB ends up being complex for both users and implementers, yet sees little adoption in practice. Most developers stick with default reads and accept the allocation overhead.</p><p>Most userland implementations of custom ReadableStream instances do not typically bother with all the ceremony required to correctly implement both default and BYOB read support in a single stream – and for good reason. It's difficult to get right and most of the time consuming code is typically going to fallback on the default read path. The example below shows what a \"correct\" implementation would need to do. It's big, complex, and error prone, and not a level of complexity that the typical developer really wants to have to deal with:</p><pre><code>new ReadableStream({\n    type: 'bytes',\n    \n    async pull(controller: ReadableByteStreamController) {      \n      if (offset &gt;= totalBytes) {\n        controller.close();\n        return;\n      }\n      \n      // Check for BYOB request FIRST\n      const byobRequest = controller.byobRequest;\n      \n      if (byobRequest) {\n        // === BYOB PATH ===\n        // Consumer provided a buffer - we MUST fill it (or part of it)\n        const view = byobRequest.view!;\n        const bytesAvailable = totalBytes - offset;\n        const bytesToWrite = Math.min(view.byteLength, bytesAvailable);\n        \n        // Create a view into the consumer's buffer and fill it\n        // not critical but safer when bytesToWrite != view.byteLength\n        const dest = new Uint8Array(\n          view.buffer,\n          view.byteOffset,\n          bytesToWrite\n        );\n        \n        // Fill with sequential bytes (our \"data source\")\n        // Can be any thing here that writes into the view\n        for (let i = 0; i &lt; bytesToWrite; i++) {\n          dest[i] = (offset + i) &amp; 0xFF;\n        }\n        \n        offset += bytesToWrite;\n        \n        // Signal how many bytes we wrote\n        byobRequest.respond(bytesToWrite);\n        \n      } else {\n        // === DEFAULT READER PATH ===\n        // No BYOB request - allocate and enqueue a chunk\n        const bytesAvailable = totalBytes - offset;\n        const chunkSize = Math.min(1024, bytesAvailable);\n        \n        const chunk = new Uint8Array(chunkSize);\n        for (let i = 0; i &lt; chunkSize; i++) {\n          chunk[i] = (offset + i) &amp; 0xFF;\n        }\n        \n        offset += chunkSize;\n        controller.enqueue(chunk);\n      }\n    },\n    \n    cancel(reason) {\n      console.log('Stream canceled:', reason);\n    }\n  });</code></pre><p>When a host runtime provides a byte-oriented ReadableStream from the runtime itself, for instance, as the of a fetch , it is often far easier for the runtime itself to provide an optimized implementation of BYOB reads, but those still need to be capable of handling both default and BYOB reading patterns and that requirement brings with it a fair amount of complexity.</p><div><h4>Backpressure: good in theory, broken in practice</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#backpressure-good-in-theory-broken-in-practice\" aria-hidden=\"true\"></a></div><p>Backpressure – the ability for a slow consumer to signal a fast producer to slow down – is a first-class concept in Web streams. In theory. In practice, the model has some serious flaws.</p><p>The primary signal is <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultController/desiredSize\"></a> on the controller. It can be positive (wants data), zero (at capacity), negative (over capacity), or null (closed). Producers are supposed to check this value and stop enqueueing when it's not positive. But there's nothing enforcing this: <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultController/enqueue\"></a> always succeeds, even when desiredSize is deeply negative.</p><pre><code>new ReadableStream({\n  start(controller) {\n    // Nothing stops you from doing this\n    while (true) {\n      controller.enqueue(generateData()); // desiredSize: -999999\n    }\n  }\n});</code></pre><p>Stream implementations can and do ignore backpressure; and some spec-defined features explicitly break backpressure. <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/tee\"></a>, for instance, creates two branches from a single stream. If one branch reads faster than the other, data accumulates in an internal buffer with no limit. A fast consumer can cause unbounded memory growth while the slow consumer catches up, and there's no way to configure this or opt out beyond canceling the slower branch.</p><p>Web streams do provide clear mechanisms for tuning backpressure behavior in the form of the  option and customizable size calculations, but these are just as easy to ignore as , and many applications simply fail to pay attention to them.</p><p>The same issues exist on the  side. A  has a  and . There is a  promise that producers of data are supposed to pay attention but often don't.</p><pre><code>const writable = getWritableStreamSomehow();\nconst writer = writable.getWriter();\n\n// Producers are supposed to wait for the writer.ready\n// It is a promise that, when resolves, indicates that\n// the writables internal backpressure is cleared and\n// it is ok to write more data\nawait writer.ready;\nawait writer.write(...);</code></pre><p>For implementers, backpressure adds complexity without providing guarantees. The machinery to track queue sizes, compute , and invoke  at the right times must all be implemented correctly. However, since these signals are advisory, all that work doesn't actually prevent the problems backpressure is supposed to solve.</p><div><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#the-hidden-cost-of-promises\" aria-hidden=\"true\"></a></div><p>The Web streams spec requires promise creation at numerous points, often in hot paths and often invisible to users. Each  call doesn't just return a promise; internally, the implementation creates additional promises for queue management,  coordination, and backpressure signaling.</p><p>This overhead is mandated by the spec's reliance on promises for buffer management, completion, and backpressure signals. While some of it is implementation-specific, much of it is unavoidable if you're following the spec as written. For high-frequency streaming – video frames, network packets, real-time data – this overhead is significant.</p><p>The problem compounds in pipelines. Each  adds another layer of promise machinery between source and sink. The spec doesn't define synchronous fast paths, so even when data is available immediately, the promise machinery still runs.</p><p>For implementers, this promise-heavy design constrains optimization opportunities. The spec mandates specific promise resolution ordering, making it difficult to batch operations or skip unnecessary async boundaries without risking subtle compliance failures. There are many hidden internal optimizations that implementers do make but these can be complicated and difficult to get right.</p><p>While I was writing this blog post, Vercel's Malte Ubl published their own <a href=\"https://vercel.com/blog/we-ralph-wiggumed-webstreams-to-make-them-10x-faster\"></a> describing some research work Vercel has been doing around improving the performance of Node.js' Web streams implementation. In that post they discuss the same fundamental performance optimization problem that every implementation of Web streams face:</p><blockquote><p>\"Or consider pipeTo(). Each chunk passes through a full Promise chain: read, write, check backpressure, repeat. An {value, done} result object is allocated per read. Error propagation creates additional Promise branches.</p><p>None of this is wrong. These guarantees matter in the browser where streams cross security boundaries, where cancellation semantics need to be airtight, where you do not control both ends of a pipe. But on the server, when you are piping React Server Components through three transforms at 1KB chunks, the cost adds up.</p></blockquote><p>As part of their research, they have put together a set of proposed improvements for Node.js' Web streams implementation that will eliminate promises in certain code paths which can yield a significant performance boost up to 10x faster, which only goes to prove the point: promises, while useful, add significant overhead. As one of the core maintainers of Node.js, I am looking forward to helping Malte and the folks at Vercel get their proposed improvements landed!</p><p>In a recent update made to Cloudflare Workers, I made similar kinds of modifications to an internal data pipeline that reduced the number of JavaScript promises created in certain application scenarios by up to 200x. The result is several orders of magnitude improvement in performance in those applications.</p><div><h4>Exhausting resources with unconsumed bodies</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#exhausting-resources-with-unconsumed-bodies\" aria-hidden=\"true\"></a></div><p>When  returns a response, the body is a <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Response/body\"></a>. If you only check the status and don't consume or cancel the body, what happens? The answer varies by implementation, but a common outcome is resource leakage.</p><pre><code>async function checkEndpoint(url) {\n  const response = await fetch(url);\n  return response.ok; // Body is never consumed or cancelled\n}\n\n// In a loop, this can exhaust connection pools\nfor (const url of urls) {\n  await checkEndpoint(url);\n}</code></pre><p>This pattern has caused connection pool exhaustion in Node.js applications using <a href=\"https://nodejs.org/api/globals.html#fetch\"></a> (the implementation built into Node.js), and similar issues have appeared in other runtimes. The stream holds a reference to the underlying connection, and without explicit consumption or cancellation, the connection may linger until garbage collection – which may not happen soon enough under load.</p><p>The problem is compounded by APIs that implicitly create stream branches. <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Request/clone\"></a> and <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Response/clone\"></a> perform implicit  operations on the body stream – a detail that's easy to miss. Code that clones a request for logging or retry logic may unknowingly create branched streams that need independent consumption, multiplying the resource management burden.</p><p>Now, to be certain, these types of issues  implementation bugs. The connection leak was definitely something that undici needed to fix in its own implementation, but the complexity of the specification does not make dealing with these types of issues easy.</p><blockquote><p>\"Cloning streams in Node.js's fetch() implementation is harder than it looks. When you clone a request or response body, you're calling tee() - which splits a single stream into two branches that both need to be consumed. If one consumer reads faster than the other, data buffers unbounded in memory waiting for the slow branch. If you don't properly consume both branches, the underlying connection leaks. The coordination required between two readers sharing one source makes it easy to accidentally break the original request or exhaust connection pools. It's a simple API call with complex underlying mechanics that are difficult to get right.\" - Matteo Collina, Ph.D. - Platformatic Co-Founder &amp; CTO, Node.js Technical Steering Committee Chair</p></blockquote><div><h4>Falling headlong off the tee() memory cliff</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#falling-headlong-off-the-tee-memory-cliff\" aria-hidden=\"true\"></a></div><p><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/tee\"></a> splits a stream into two branches. It seems straightforward, but the implementation requires buffering: if one branch is read faster than the other, the data must be held somewhere until the slower branch catches up.</p><pre><code>const [forHash, forStorage] = response.body.tee();\n\n// Hash computation is fast\nconst hash = await computeHash(forHash);\n\n// Storage write is slow — meanwhile, the entire stream\n// may be buffered in memory waiting for this branch\nawait writeToStorage(forStorage);</code></pre><p>The spec does not mandate buffer limits for . And to be fair, the spec allows implementations to implement the actual internal mechanisms for and other APIs in any way they see fit so long as the observable normative requirements of the specification are met. But if an implementation chooses to implement  in the specific way described by the streams specification, then  will come with a built-in memory management issue that is difficult to work around.</p><p>Implementations have had to develop their own strategies for dealing with this. Firefox initially used a linked-list approach that led to O memory growth proportional to the consumption rate difference. In Cloudflare Workers, we opted to implement a shared buffer model where backpressure is signaled by the slowest consumer rather than the fastest.</p><div><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#transform-backpressure-gaps\" aria-hidden=\"true\"></a></div><p> creates a  pair with processing logic in between. The  function executes on , not on read. Processing of the transform happens eagerly as data arrives, regardless of whether any consumer is ready. This causes unnecessary work when consumers are slow, and the backpressure signaling between the two sides has gaps that can cause unbounded buffering under load. The expectation in the spec is that the producer of the data being transformed is paying attention to the  signal on the writable side of the transform but quite often producers just simply ignore it.</p><p>If the transform's operation is synchronous and always enqueues output immediately, it never signals backpressure back to the writable side even when the downstream consumer is slow. This is a consequence of the spec design that many developers completely overlook. In browsers, where there's only a single user and typically only a small number of stream pipelines active at any given time, this type of foot gun is often of no consequence, but it has a major impact on server-side or edge performance in runtimes that serve thousands of concurrent requests.</p><pre><code>const fastTransform = new TransformStream({\n  transform(chunk, controller) {\n    // Synchronously enqueue — this never applies backpressure\n    // Even if the readable side's buffer is full, this succeeds\n    controller.enqueue(processChunk(chunk));\n  }\n});\n\n// Pipe a fast source through the transform to a slow sink\nfastSource\n  .pipeThrough(fastTransform)\n  .pipeTo(slowSink);  // Buffer grows without bound</code></pre><p>What TransformStreams are supposed to do is check for backpressure on the controller and use promises to communicate that back to the writer:</p><pre><code>const fastTransform = new TransformStream({\n  async transform(chunk, controller) {\n    if (controller.desiredSize &lt;= 0) {\n      // Wait on the backpressure to clear somehow\n    }\n\n    controller.enqueue(processChunk(chunk));\n  }\n});</code></pre><p>A difficulty here, however, is that the <code>TransformStreamDefaultController</code> does not have a ready promise mechanism like Writers do; so the  implementation would need to implement a polling mechanism to periodically check when &nbsp;becomes positive again.</p><p>The problem gets worse in pipelines. When you chain multiple transforms – say, parse, transform, then serialize – each  has its own internal readable and writable buffers. If implementers follow the spec strictly, data cascades through these buffers in a push-oriented fashion: the source pushes to transform A, which pushes to transform B, which pushes to transform C, each accumulating data in intermediate buffers before the final consumer has even started pulling. With three transforms, you can have six internal buffers filling up simultaneously.</p><p>Developers using the streams API are expected to remember to use options like  when creating their sources, transforms, and writable destinations but often they either forget or simply choose to ignore it.</p><pre><code>source\n  .pipeThrough(parse)      // buffers filling...\n  .pipeThrough(transform)  // more buffers filling...\n  .pipeThrough(serialize)  // even more buffers...\n  .pipeTo(destination);    // consumer hasn't started yet</code></pre><p>Implementations have found ways to optimize transform pipelines by collapsing identity transforms, short-circuiting non-observable paths, deferring buffer allocation, or falling back to native code that does not run JavaScript at all. Deno, Bun, and Cloudflare Workers have all successfully implemented \"native path\" optimizations that can help eliminate much of the overhead, and Vercel's recent <a href=\"https://vercel.com/blog/we-ralph-wiggumed-webstreams-to-make-them-10x-faster\"></a> research is working on similar optimizations for Node.js. But the optimizations themselves add significant complexity and still can't fully escape the inherently push-oriented model that TransformStream uses.</p><div><h4>GC thrashing in server-side rendering</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#gc-thrashing-in-server-side-rendering\" aria-hidden=\"true\"></a></div><p>Streaming server-side rendering (SSR) is a particularly painful case. A typical SSR stream might render thousands of small HTML fragments, each passing through the streams machinery:</p><pre><code>// Each component enqueues a small chunk\nfunction renderComponent(controller) {\n  controller.enqueue(encoder.encode(`&lt;div&gt;${content}&lt;/div&gt;`));\n}\n\n// Hundreds of components = hundreds of enqueue calls\n// Each one triggers promise machinery internally\nfor (const component of components) {\n  renderComponent(controller);  // Promises created, objects allocated\n}</code></pre><p>Every fragment means promises created for  calls, promises for backpressure coordination, intermediate buffer allocations, and result objects – most of which become garbage almost immediately.</p><p>Under load, this creates GC pressure that can devastate throughput. The JavaScript engine spends significant time collecting short-lived objects instead of doing useful work. Latency becomes unpredictable as GC pauses interrupt request handling. I've seen SSR workloads where garbage collection accounts for a substantial portion (up to and beyond 50%) of total CPU time per request. That's time that could be spent actually rendering content.</p><p>The irony is that streaming SSR is supposed to improve performance by sending content incrementally. But the overhead of the streams machinery can negate those gains, especially for pages with many small components. Developers sometimes find that buffering the entire response is actually faster than streaming through Web streams, defeating the purpose entirely.</p><div><h3>The optimization treadmill</h3><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#the-optimization-treadmill\" aria-hidden=\"true\"></a></div><p>To achieve usable performance, every major runtime has resorted to non-standard internal optimizations for Web streams. Node.js, Deno, Bun, and Cloudflare Workers have all developed their own workarounds. This is particularly true for streams wired up to system-level I/O, where much of the machinery is non-observable and can be short-circuited.</p><p>Finding these optimization opportunities can itself be a significant undertaking. It requires end-to-end understanding of the spec to identify which behaviors are observable and which can safely be elided. Even then, whether a given optimization is actually spec-compliant is often unclear. Implementers must make judgment calls about which semantics they can relax without breaking compatibility. This puts enormous pressure on runtime teams to become spec experts just to achieve acceptable performance.</p><p>These optimizations are difficult to implement, frequently error-prone, and lead to inconsistent behavior across runtimes. Bun's \"<a href=\"https://bun.sh/docs/api/streams#direct-readablestream\"></a>\" optimization takes a deliberately and observably non-standard approach, bypassing much of the spec's machinery entirely. Cloudflare Workers' <a href=\"https://developers.cloudflare.com/workers/runtime-apis/streams/transformstream/\"></a> provides a fast-path for pass-through transforms but is Workers-specific and implements behaviors that are not standard for a . Each runtime has its own set of tricks and the natural tendency is toward non-standard solutions, because that's often the only way to make things fast.</p><p>This fragmentation hurts portability. Code that performs well on one runtime may behave differently (or poorly) on another, even though it's using \"standard\" APIs. The complexity burden on runtime implementers is substantial, and the subtle behavioral differences create friction for developers trying to write cross-runtime code, particularly those maintaining frameworks that must be able to run efficiently across many runtime environments.</p><p>It is also necessary to emphasize that many optimizations are only possible in parts of the spec that are unobservable to user code. The alternative, like Bun \"Direct Streams\", is to intentionally diverge from the spec-defined observable behaviors. This means optimizations often feel \"incomplete\". They work in some scenarios but not in others, in some runtimes but not others, etc. Every such case adds to the overall unsustainable complexity of the Web streams approach which is why most runtime implementers rarely put significant effort into further improvements to their streams implementations once the conformance tests are passing.</p><p>Implementers shouldn't need to jump through these hoops. When you find yourself needing to relax or bypass spec semantics just to achieve reasonable performance, that's a sign something is wrong with the spec itself. A well-designed streaming API should be efficient by default, not require each runtime to invent its own escape hatches.</p><p>A complex spec creates complex edge cases. The <a href=\"https://github.com/web-platform-tests/wpt/tree/master/streams\"><u>Web Platform Tests for streams</u></a> span over 70 test files, and while comprehensive testing is a good thing, what's telling is what needs to be tested.</p><p>Consider some of the more obscure tests that implementations must pass:</p><ul><li><p>Prototype pollution defense: One test patches then to intercept promise resolutions, then verifies that  and  operations don't leak internal values through the prototype chain. This tests a security property that only exists because the spec's promise-heavy internals create an attack surface.</p></li><li><p>WebAssembly memory rejection: BYOB reads must explicitly reject ArrayBuffers backed by WebAssembly memory, which look like regular buffers but can't be transferred. This edge case exists because of the spec's buffer detachment model – a simpler API wouldn't need to handle it.</p></li><li><p>Crash regression for state machine conflicts: A test specifically checks that calling  after  doesn't crash the runtime. This sequence creates a conflict in the internal state machine — the  fulfills the pending read and should invalidate the , but implementations must gracefully handle the subsequent  rather than corrupting memory in order to cover the very likely possibility that developers are not using the complex API correctly.</p></li></ul><p>These aren't contrived scenarios invented by test authors in total vacuum. They're consequences of the spec's design and reflect real world bugs.</p><p>For runtime implementers, passing the WPT suite means handling intricate corner cases that most application code will never encounter. The tests encode not just the happy path but the full matrix of interactions between readers, writers, controllers, queues, strategies, and the promise machinery that connects them all.</p><p>A simpler API would mean fewer concepts, fewer interactions between concepts, and fewer edge cases to get right resulting in more confidence that implementations actually behave consistently.</p><p>Web streams are complex for users and implementers alike. The problems with the spec aren't bugs. They emerge from using the API exactly as designed. They aren't issues that can be fixed solely through incremental improvements. They're consequences of fundamental design choices. To improve things we need different foundations.</p><div><h2>A better streams API is possible</h2><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#a-better-streams-api-is-possible\" aria-hidden=\"true\"></a></div><p>After implementing the Web streams spec multiple times across different runtimes and seeing the pain points firsthand, I decided it was time to explore what a better, alternative streaming API could look like if designed from first principles today.</p><p>What follows is a proof of concept: it's not a finished standard, not a production-ready library, not even necessarily a concrete proposal for something new, but a starting point for discussion that demonstrates the problems with Web streams aren't inherent to streaming itself; they're consequences of specific design choices that could be made differently. Whether this exact API is the right answer is less important than whether it sparks a productive conversation about what we actually need from a streaming primitive.</p><p>Before diving into API design, it's worth asking: what is a stream?</p><p>At its core, a stream is just a sequence of data that arrives over time. You don't have all of it at once. You process it incrementally as it becomes available.</p><p>Unix pipes are perhaps the purest expression of this idea:</p><pre><code>cat access.log | grep \"error\" | sort | uniq -c</code></pre><p>\nData flows left to right. Each stage reads input, does its work, writes output. There's no pipe reader to acquire, no controller lock to manage. If a downstream stage is slow, upstream stages naturally slow down as well. Backpressure is implicit in the model, not a separate mechanism to learn (or ignore).</p><p>In JavaScript, the natural primitive for \"a sequence of things that arrive over time\" is already in the language: the async iterable. You consume it with . You stop consuming by stopping iteration.</p><p>This is the intuition the new API tries to preserve: streams should feel like iteration, because that's what they are. The complexity of Web streams – readers, writers, controllers, locks, queuing strategies – obscures this fundamental simplicity. A better API should make the simple case simple and only add complexity where it's genuinely needed.</p><p>I built the proof-of-concept alternative around a different set of principles.</p><p>No custom  class with hidden internal state. A readable stream is just an <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#the_async_iterator_and_async_iterable_protocols\"><code><u>AsyncIterable&lt;Uint8Array[]&gt;</u></code></a>. You consume it with . No readers to acquire, no locks to manage.</p><p>Transforms don't execute until the consumer pulls. There's no eager evaluation, no hidden buffering. Data flows on-demand from source, through transforms, to the consumer. If you stop iterating, processing stops.</p><p>Backpressure is strict by default. When a buffer is full, writes reject rather than silently accumulating. You can configure alternative policies – block until space is available, drop oldest, drop newest – but you have to choose explicitly. No more silent memory growth.</p><p>Instead of yielding one chunk per iteration, streams yield  arrays of chunks. This amortizes the async overhead across multiple chunks, reducing promise creation and microtask latency in hot paths.</p><p>The API deals exclusively with bytes (<a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"></a>). Strings are UTF-8 encoded automatically. There's no \"value stream\" vs \"byte stream\" dichotomy. If you want to stream arbitrary JavaScript values, use async iterables directly. While the API uses , it treats chunks as opaque. There is no partial consumption, no BYOB patterns, no byte-level operations within the streaming machinery itself. Chunks go in, chunks come out, unchanged unless a transform explicitly modifies them.</p><div><h4>Synchronous fast paths matter</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#synchronous-fast-paths-matter\" aria-hidden=\"true\"></a></div><p>The API recognizes that synchronous data sources are both necessary and common. The application should not be forced to always accept the performance cost of asynchronous scheduling simply because that's the only option provided. At the same time, mixing sync and async processing can be dangerous. Synchronous paths should always be an option and should always be explicit.</p><div><h4>Creating and consuming streams</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#creating-and-consuming-streams\" aria-hidden=\"true\"></a></div><p>In Web streams, creating a simple producer/consumer pair requires , manual encoding, and careful lock management:</p><pre><code>const { readable, writable } = new TransformStream();\nconst enc = new TextEncoder();\nconst writer = writable.getWriter();\nawait writer.write(enc.encode(\"Hello, World!\"));\nawait writer.close();\nwriter.releaseLock();\n\nconst dec = new TextDecoder();\nlet text = '';\nfor await (const chunk of readable) {\n  text += dec.decode(chunk, { stream: true });\n}\ntext += dec.decode();</code></pre><p>Even this relatively clean version requires: a , manual  and , and explicit lock release.</p><p>Here's the equivalent with the new API:</p><pre><code>import { Stream } from 'new-streams';\n\n// Create a push stream\nconst { writer, readable } = Stream.push();\n\n// Write data — backpressure is enforced\nawait writer.write(\"Hello, World!\");\nawait writer.end();\n\n// Consume as text\nconst text = await Stream.text(readable);</code></pre><p>The readable is just an async iterable. You can pass it to any function that expects one, including  which collects and decodes the entire stream.</p><p>The writer has a simple interface:  for batched writes,  to signal completion, and  for errors. That's essentially it.</p><p>The Writer is not a concrete class. Any object that implements , , and  can be a writer making it easy to adapt existing APIs or create specialized implementations without subclassing. There's no complex  protocol with , , , callbacks that must coordinate through a controller whose lifecycle and state are independent of the  it is bound to.</p><p>Here's a simple in-memory writer that collects all written data:</p><pre><code>// A minimal writer implementation — just an object with methods\nfunction createBufferWriter() {\n  const chunks = [];\n  let totalBytes = 0;\n  let closed = false;\n\n  const addChunk = (chunk) =&gt; {\n    chunks.push(chunk);\n    totalBytes += chunk.byteLength;\n  };\n\n  return {\n    get desiredSize() { return closed ? null : 1; },\n\n    // Async variants\n    write(chunk) { addChunk(chunk); },\n    writev(batch) { for (const c of batch) addChunk(c); },\n    end() { closed = true; return totalBytes; },\n    abort(reason) { closed = true; chunks.length = 0; },\n\n    // Sync variants return boolean (true = accepted)\n    writeSync(chunk) { addChunk(chunk); return true; },\n    writevSync(batch) { for (const c of batch) addChunk(c); return true; },\n    endSync() { closed = true; return totalBytes; },\n    abortSync(reason) { closed = true; chunks.length = 0; return true; },\n\n    getChunks() { return chunks; }\n  };\n}\n\n// Use it\nconst writer = createBufferWriter();\nawait Stream.pipeTo(source, writer);\nconst allData = writer.getChunks();</code></pre><p>No base class to extend, no abstract methods to implement, no controller to coordinate with. Just an object with the right shape.</p><p>Under the new API design, transforms should not perform any work until the data is being consumed. This is a fundamental principle.</p><pre><code>// Nothing executes until iteration begins\nconst output = Stream.pull(source, compress, encrypt);\n\n// Transforms execute as we iterate\nfor await (const chunks of output) {\n  for (const chunk of chunks) {\n    process(chunk);\n  }\n}</code></pre><p> creates a lazy pipeline. The  and  transforms don't run until you start iterating output. Each iteration pulls data through the pipeline on demand.</p><p>This is fundamentally different from Web streams' <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/pipeThrough\"></a>, which starts actively pumping data from the source to the transform as soon as you set up the pipe. Pull semantics mean you control when processing happens, and stopping iteration stops processing.</p><p>Transforms can be stateless or stateful. A stateless transform is just a function that takes chunks and returns transformed chunks:</p><pre><code>// Stateless transform — a pure function\n// Receives chunks or null (flush signal)\nconst toUpperCase = (chunks) =&gt; {\n  if (chunks === null) return null; // End of stream\n  return chunks.map(chunk =&gt; {\n    const str = new TextDecoder().decode(chunk);\n    return new TextEncoder().encode(str.toUpperCase());\n  });\n};\n\n// Use it directly\nconst output = Stream.pull(source, toUpperCase);</code></pre><p>Stateful transforms are simple objects with member functions that maintain state across calls:</p><pre><code>// Stateful transform — a generator that wraps the source\nfunction createLineParser() {\n  // Helper to concatenate Uint8Arrays\n  const concat = (...arrays) =&gt; {\n    const result = new Uint8Array(arrays.reduce((n, a) =&gt; n + a.length, 0));\n    let offset = 0;\n    for (const arr of arrays) { result.set(arr, offset); offset += arr.length; }\n    return result;\n  };\n\n  return {\n    async *transform(source) {\n      let pending = new Uint8Array(0);\n      \n      for await (const chunks of source) {\n        if (chunks === null) {\n          // Flush: yield any remaining data\n          if (pending.length &gt; 0) yield [pending];\n          continue;\n        }\n        \n        // Concatenate pending data with new chunks\n        const combined = concat(pending, ...chunks);\n        const lines = [];\n        let start = 0;\n\n        for (let i = 0; i &lt; combined.length; i++) {\n          if (combined[i] === 0x0a) { // newline\n            lines.push(combined.slice(start, i));\n            start = i + 1;\n          }\n        }\n\n        pending = combined.slice(start);\n        if (lines.length &gt; 0) yield lines;\n      }\n    }\n  };\n}\n\nconst output = Stream.pull(source, createLineParser());</code></pre><p>For transforms that need cleanup on abort, add an abort handler:</p><pre><code>// Stateful transform with resource cleanup\nfunction createGzipCompressor() {\n  // Hypothetical compression API...\n  const deflate = new Deflater({ gzip: true });\n\n  return {\n    async *transform(source) {\n      for await (const chunks of source) {\n        if (chunks === null) {\n          // Flush: finalize compression\n          deflate.push(new Uint8Array(0), true);\n          if (deflate.result) yield [deflate.result];\n        } else {\n          for (const chunk of chunks) {\n            deflate.push(chunk, false);\n            if (deflate.result) yield [deflate.result];\n          }\n        }\n      }\n    },\n    abort(reason) {\n      // Clean up compressor resources on error/cancellation\n    }\n  };\n}</code></pre><p>For implementers, there's no Transformer protocol with , ,  methods and controller coordination passed into a  class that has its own hidden state machine and buffering mechanisms. Transforms are just functions or simple objects: far simpler to implement and test.</p><div><h4>Explicit backpressure policies</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#explicit-backpressure-policies\" aria-hidden=\"true\"></a></div><p>When a bounded buffer fills up and a producer wants to write more, there are only a few things you can do:</p><ol><li><p>Reject the write: refuse to accept more data</p></li><li><p>Wait: block until space becomes available</p></li><li><p>Discard old data: evict what's already buffered to make room</p></li><li><p>Discard new data: drop what's incoming</p></li></ol><p>That's it. Any other response is either a variation of these (like \"resize the buffer,\" which is really just deferring the choice) or domain-specific logic that doesn't belong in a general streaming primitive. Web streams currently always choose Wait by default.</p><p>The new API makes you choose one of these four explicitly:</p><ul><li><p> (default): Rejects writes when the buffer is full and too many writes are pending. Catches \"fire-and-forget\" patterns where producers ignore backpressure.</p></li><li><p>: Writes wait until buffer space is available. Use when you trust the producer to await writes properly.</p></li><li><p>: Drops the oldest buffered data to make room. Useful for live feeds where stale data loses value.</p></li><li><p>: Discards incoming data when full. Useful when you want to process what you have without being overwhelmed.</p></li></ul><pre><code>const { writer, readable } = Stream.push({\n  highWaterMark: 10,\n  backpressure: 'strict' // or 'block', 'drop-oldest', 'drop-newest'\n});</code></pre><p>No more hoping producers cooperate. The policy you choose determines what happens when the buffer fills.</p><p>Here's how each policy behaves when a producer writes faster than the consumer reads:</p><pre><code>// strict: Catches fire-and-forget writes that ignore backpressure\nconst strict = Stream.push({ highWaterMark: 2, backpressure: 'strict' });\nstrict.writer.write(chunk1);  // ok (not awaited)\nstrict.writer.write(chunk2);  // ok (fills slots buffer)\nstrict.writer.write(chunk3);  // ok (queued in pending)\nstrict.writer.write(chunk4);  // ok (pending buffer fills)\nstrict.writer.write(chunk5);  // throws! too many pending writes\n\n// block: Wait for space (unbounded pending queue)\nconst blocking = Stream.push({ highWaterMark: 2, backpressure: 'block' });\nawait blocking.writer.write(chunk1);  // ok\nawait blocking.writer.write(chunk2);  // ok\nawait blocking.writer.write(chunk3);  // waits until consumer reads\nawait blocking.writer.write(chunk4);  // waits until consumer reads\nawait blocking.writer.write(chunk5);  // waits until consumer reads\n\n// drop-oldest: Discard old data to make room\nconst dropOld = Stream.push({ highWaterMark: 2, backpressure: 'drop-oldest' });\nawait dropOld.writer.write(chunk1);  // ok\nawait dropOld.writer.write(chunk2);  // ok\nawait dropOld.writer.write(chunk3);  // ok, chunk1 discarded\n\n// drop-newest: Discard incoming data when full\nconst dropNew = Stream.push({ highWaterMark: 2, backpressure: 'drop-newest' });\nawait dropNew.writer.write(chunk1);  // ok\nawait dropNew.writer.write(chunk2);  // ok\nawait dropNew.writer.write(chunk3);  // silently dropped</code></pre><div><h4>Explicit Multi-consumer patterns</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#explicit-multi-consumer-patterns\" aria-hidden=\"true\"></a></div><pre><code>// Share with explicit buffer management\nconst shared = Stream.share(source, {\n  highWaterMark: 100,\n  backpressure: 'strict'\n});\n\nconst consumer1 = shared.pull();\nconst consumer2 = shared.pull(decompress);</code></pre><p>Instead of  with its hidden unbounded buffer, you get explicit multi-consumer primitives.  is pull-based: consumers pull from a shared source, and you configure the buffer limits and backpressure policy upfront.</p><p>There's also  for push-based multi-consumer scenarios. Both require you to think about what happens when consumers run at different speeds, because that's a real concern that shouldn't be hidden.</p><p>Not all streaming workloads involve I/O. When your source is in-memory and your transforms are pure functions, async machinery adds overhead without benefit. You're paying for coordination of \"waiting\" that adds no benefit.</p><p>The new API has complete parallel sync versions: , , , and so on. If your source and transforms are all synchronous, you can process the entire pipeline without a single promise.</p><pre><code>// Async — when source or transforms may be asynchronous\nconst textAsync = await Stream.text(source);\n\n// Sync — when all components are synchronous\nconst textSync = Stream.textSync(source);</code></pre><p>Here's a complete synchronous pipeline – compression, transformation, and consumption with zero async overhead:</p><pre><code>// Synchronous source from in-memory data\nconst source = Stream.fromSync([inputBuffer]);\n\n// Synchronous transforms\nconst compressed = Stream.pullSync(source, zlibCompressSync);\nconst encrypted = Stream.pullSync(compressed, aesEncryptSync);\n\n// Synchronous consumption — no promises, no event loop trips\nconst result = Stream.bytesSync(encrypted);</code></pre><p>The entire pipeline executes in a single call stack. No promises are created, no microtask queue scheduling occurs, and no GC pressure from short-lived async machinery. For CPU-bound workloads like parsing, compression, or transformation of in-memory data, this can be significantly faster than the equivalent Web streams code – which would force async boundaries even when every component is synchronous.</p><p>Web streams has no synchronous path. Even if your source has data ready and your transform is a pure function, you still pay for promise creation and microtask scheduling on every operation. Promises are fantastic for cases in which waiting is actually necessary, but they aren't always necessary. The new API lets you stay in sync-land when that's what you need.</p><div><h4>Bridging the gap between this and web streams</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#bridging-the-gap-between-this-and-web-streams\" aria-hidden=\"true\"></a></div><p>The async iterator based approach provides a natural bridge between this alternative approach and Web streams. When coming from a ReadableStream to this new approach, simply passing the readable in as input works as expected when the ReadableStream is set up to yield bytes:</p><pre><code>const readable = getWebReadableStreamSomehow();\nconst input = Stream.pull(readable, transform1, transform2);\nfor await (const chunks of input) {\n  // process chunks\n}</code></pre><p>When adapting to a ReadableStream, a bit more work is required since the alternative approach yields batches of chunks, but the adaptation layer is as easily straightforward:</p><pre><code>async function* adapt(input) {\n  for await (const chunks of input) {\n    for (const chunk of chunks) {\n      yield chunk;\n    }\n  }\n}\n\nconst input = Stream.pull(source, transform1, transform2);\nconst readable = ReadableStream.from(adapt(input));</code></pre><div><h4>How this addresses the real-world failures from earlier</h4><a href=\"https://blog.cloudflare.com/a-better-web-streams-api/#how-this-addresses-the-real-world-failures-from-earlier\" aria-hidden=\"true\"></a></div><ul><li><p>Unconsumed bodies: Pull semantics mean nothing happens until you iterate. No hidden resource retention. If you don't consume a stream, there's no background machinery holding connections open.</p></li><li><p>The  memory cliff:  requires explicit buffer configuration. You choose the  and backpressure policy upfront: no more silent unbounded growth when consumers run at different speeds.</p></li><li><p>Transform backpressure gaps: Pull-through transforms execute on-demand. Data doesn't cascade through intermediate buffers; it flows only when the consumer pulls. Stop iterating, stop processing.</p></li><li><p>GC thrashing in SSR: Batched chunks () amortize async overhead. Sync pipelines via  eliminate promise allocation entirely for CPU-bound workloads.</p></li></ul><p>The design choices have performance implications. Here are benchmarks from the reference implementation of this possible alternative compared to Web streams (Node.js v24.x, Apple M1 Pro, averaged over 10 runs):</p><table><tbody><tr></tr><tr><td><p>Small chunks (1KB × 5000)</p></td></tr><tr><td><p>Tiny chunks (100B × 10000)</p></td></tr><tr><td><p>Async iteration (8KB × 1000)</p></td></tr><tr><td><p>Chained 3× transforms (8KB × 500)</p></td></tr><tr><td><p>High-frequency (64B × 20000)</p></td></tr></tbody></table><p>The chained transform result is particularly striking: pull-through semantics eliminate the intermediate buffering that plagues Web streams pipelines. Instead of each  eagerly filling its internal buffers, data flows on-demand from consumer to source.</p><p>Now, to be fair, Node.js really has not yet put significant effort into fully optimizing the performance of its Web streams implementation. There's likely significant room for improvement in Node.js' performance results through a bit of applied effort to optimize the hot paths there. That said, running these benchmarks in Deno and Bun also show a significant performance improvement with this alternative iterator based approach than in either of their Web streams implementations as well.</p><p>Browser benchmarks (Chrome/Blink, averaged over 3 runs) show consistent gains as well:</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>These benchmarks measure throughput in controlled scenarios; real-world performance depends on your specific use case. The difference between Node.js and browser gains reflects the distinct optimization paths each environment takes for Web streams.</p><p>It's worth noting that these benchmarks compare a pure TypeScript/JavaScript implementation of the new API against the native (JavaScript/C++/Rust) implementations of Web streams in each runtime. The new API's reference implementation has had no performance optimization work; the gains come entirely from the design. A native implementation would likely show further improvement.</p><p>The gains illustrate how fundamental design choices compound: batching amortizes async overhead, pull semantics eliminate intermediate buffering, and the freedom for implementations to use synchronous fast paths when data is available immediately all contribute.</p><blockquote><p>\"We’ve done a lot to improve performance and consistency in Node streams, but there’s something uniquely powerful about starting from scratch. New streams’ approach embraces modern runtime realities without legacy baggage, and that opens the door to a simpler, performant and more coherent streams model.\" \n- Robert Nagy, Node.js TSC member and Node.js streams contributor</p></blockquote><p>I'm publishing this to start a conversation. What did I get right? What did I miss? Are there use cases that don't fit this model? What would a migration path for this approach look like? The goal is to gather feedback from developers who've felt the pain of Web streams and have opinions about what a better API should look like.</p><ul><li><p>API Reference: See the <a href=\"https://github.com/jasnell/new-streams/blob/main/API.md\"></a> for complete documentation</p></li></ul><p>I welcome issues, discussions, and pull requests. If you've run into Web streams problems I haven't covered, or if you see gaps in this approach, let me know. But again, the idea here is not to say \"Let's all use this shiny new object!\"; it is to kick off a discussion that looks beyond the current status quo of Web Streams and returns back to first principles.</p><p>Web streams was an ambitious project that brought streaming to the web platform when nothing else existed. The people who designed it made reasonable choices given the constraints of 2014 – before async iteration, before years of production experience revealed the edge cases.</p><p>But we've learned a lot since then. JavaScript has evolved. A streaming API designed today can be simpler, more aligned with the language, and more explicit about the things that matter, like backpressure and multi-consumer behavior.</p><p>We deserve a better stream API. So let's talk about what that could look like.</p>","contentLength":51284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47180569"},{"title":"Get free Claude max 20x for open-source maintainers","url":"https://claude.com/contact-sales/claude-for-oss","date":1772183338,"author":"zhisme","guid":154675,"unread":true,"content":"<p>Open-source maintainers and contributors keep the ecosystem running. The Claude for Open Source Program is our way of saying thank you for all your hard work, with 6 months of free Claude Max 20x. Apply now.</p><ul role=\"list\"><li>‍ You’re a primary maintainer or core team member of a public repo with 5,000+ GitHub stars  1M+ monthly NPM downloads. You've made commits, releases, or PR reviews within the last 3 months.</li><li>‍<strong>Don't quite fit the criteria</strong> If you maintain something the ecosystem quietly depends on, apply anyway and tell us about it.</li></ul>","contentLength":527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47178371"},{"title":"The normalization of corruption in organizations (2003) [pdf]","url":"https://gwern.net/doc/sociology/2003-ashforth.pdf","date":1772173283,"author":"rendx","guid":154595,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47177186"},{"title":"The Hunt for Dark Breakfast","url":"https://moultano.wordpress.com/2026/02/22/the-hunt-for-dark-breakfast/","date":1772164188,"author":"moultano","guid":154472,"unread":true,"content":"<p>It started with a flash of insight like a thunderbolt in a snow storm, the sort of insight that can only be induced by high altitude hypoxia and making breakfast.&nbsp;</p><p>“Breakfast is a vector space. You can place pancakes, crepes, and scrambled eggs on a simplex where the variables are the ratios between milk, eggs, and flour. We have explored too little of this manifold. More breakfasts can exist than we have known.”</p><p>I stood in the kitchen paralyzed by indecision. The mixing bowl was in front of me, the milk, eggs, and flour next to it, all of them individually as familiar as they had been a moment before, but now the possibilities for their combination were just too great. Breakfast was now an alien fractal intruding on our world like the <a href=\"https://www.youtube.com/watch?v=uBsJgceM0KI\">lighthouse at the end of Annihilation</a>. The thoughts came unbidden.</p><p>“In the manifold of breakfast, are there empty subspaces? Might there be breakfasts that no one has ever had? With a theoretical model of breakfast, can we derive the existence of ‘dark breakfasts,’ breakfasts that we know must exist, but have never observed?”</p><p>The curtain of reality had pulled back for me, and I could no longer pretend to be ignorant of these eldritch possibilities. I furiously began to map the known breakfasts. If the dark breakfast exists, I must be able to find it in the interstices of the normal familiar world.</p><p>First I mapped all that I could recall from memory, pancakes, crepes, waffles, scrambled eggs, popovers, omelettes, and on and on, scouring my brain for every fast I had ever broken. The beginnings of the contours of breakfast began to reveal themselves. A gaping hole stared back at me, but I couldn’t yet be sure. I had to search the dark corners of the world to see if somewhere in far off lands that abyss had yet been filled. I called upon <a href=\"https://gemini.google.com/\">friendly ghosts</a>. I paged through <a href=\"http://en.wikipedia.org/wiki/Main_Page\">ancient tomes</a>. I added kaiserschmarrn, swedish pancakes, dan bing, madeleines, crumpets, clafoutis, blinis, pannu kakku, parathas, nalesniki. The map filled in bit by bit, but it was no use. The gap in the fabric of breakfast remained.</p><p>I searched for benign explanations. Could it be that milk is simply too heavy, and that by including the weight of the water content that boils off I am tilting the simplex too far in its direction? Could it be that by excluding slices of bread as ingredients since they aren’t raw flour and do not go in a mixing bowl, I have excluded breakfasts like french toast, eggs in a basket, breakfast burritos, and breakfast sandwiches that might yet have saved us? Could I have overlooked some arcane culture that breaks their fast with dumplings or egg noodles? None of these satisfied me. The Abyss stared back.</p><p>The breakfasts I was able to identify cluster into three major regions:</p><ol><li>The Pancake Local Group: Here are found most of the conventional breakfasts, pancakes, crepes, waffles, and all of their international variants. Space here is chaotic, fractal. Any slight deviation from your recipe in this region is likely to produce something else entirely. Breakfast here is metastable at best. (<a href=\"https://internationalpancakes.com/\">prior research on the pancake cluster</a>)</li><li>The Baked Good Quadrant: The items here are only breakfasts by convention. Any of them could be served at other meals, and often are.</li><li>The Egg Singularity and Custard Accretion Disk: While only omelettes are labelled for brevity, there are dozens of named dishes that could be stacked on top of the pure egg point, over easy, sunny side up, hard boiled, soft boiled, etc. From these a small tail of egg based dishes sneaks down the right side, each with some amount of milk added, often a variable or optional amount.</li></ol><p>I was days into my research before I finally found a clue. In an <a href=\"https://www.ihop.com/-/media/ihop/nutrition/faq/ihop-nutrition-faq92722.pdf\">obscure document</a> on the website of the International House of Pancakes Corporation there was a hint that the dark breakfast had been made. IHOP omelettes include pancake batter. While I cannot place IHOP omelettes exactly on the map, by interpolating between pancakes and omelettes, we can bound where they must occur, and confirm that the manifold possibilities do indeed pass through the Dark Breakfast Abyss.</p><p>We do not know why the Dark Breakfast Abyss is empty. But by anthropic reasoning, we should conclude that it is empty for good reason. The International House of Pancakes is playing a dangerous game. If someday a <a href=\"https://xkcd.com/472/\">remote IHOP</a> splashes a little too much batter in their omelette, cooks the Forbidden Breakfast, and thereby brings about the end of the world, well, at least we know the <a href=\"https://www.wafflehouse.com/how-to-measure-a-storms-fury-one-breakfast-at-a-time/\">Waffle House will be open</a>.</p><p>For <a href=\"https://nautil.us/the-hard-problem-of-breakfast-237916/\">other breakfast scholars</a> who wish to further my study, I offer my <a href=\"https://docs.google.com/spreadsheets/d/1cY5CwTV91IhfPQZkB-XmeGr79diDmxvvZaVomeTciPM/edit?gid=0#gid=0\">data</a> and <a href=\"https://colab.research.google.com/drive/15s9j_bSInYoDKXDArH3LpVhL50Hl5gRZ#scrollTo=cK8NmhoZz5T0\">code</a>. If you are so foolhardy that you wish to explore the bounds of dark breakfast yourself, the recipe is as follows:</p><ul></ul><blockquote><p>The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far. The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the deadly light into the peace and safety of a new dark age.</p><cite>H.P. Lovecraft – The Call of Cthulhu</cite></blockquote>","contentLength":5335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47176257"},{"title":"Google workers seek 'red lines' on military A.I., echoing Anthropic","url":"https://www.nytimes.com/2026/02/26/technology/google-deepmind-letter-pentagon.html","date":1772161689,"author":"mikece","guid":154403,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47175931"},{"title":"Statement from Dario Amodei on our discussions with the Department of War","url":"https://www.anthropic.com/news/statement-department-of-war","date":1772145767,"author":"qwertox","guid":154278,"unread":true,"content":"<p>I believe deeply in the existential importance of using AI to defend the United States and other democracies, and to defeat our autocratic adversaries.</p><p>Anthropic has therefore worked proactively to deploy our models to the Department of War and the intelligence community. We were <a href=\"https://www.anthropic.com/news/expanding-access-to-claude-for-government\">the first frontier AI company</a> to deploy our models in the US government’s classified networks, the first to deploy them at the <a href=\"https://www.axios.com/2024/11/14/anthropic-claude-nuclear-information-safety\">National Laboratories</a>, and the first to provide <a href=\"https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers\">custom models</a> for national security customers. Claude is <a href=\"https://www.anthropic.com/news/anthropic-and-the-department-of-defense-to-advance-responsible-ai-in-defense-operations\">extensively deployed</a> across the Department of War and other national security agencies for mission-critical applications, such as intelligence analysis, modeling and simulation, operational planning, cyber operations, and more.</p><p>Anthropic has also acted to defend America’s lead in AI, even when it is against the company’s short-term interest. We chose to forgo several hundred million dollars in revenue to cut off the <a href=\"https://www.anthropic.com/news/updating-restrictions-of-sales-to-unsupported-regions\">use of Claude by firms linked to the Chinese Communist Party</a> (some of whom have been <a href=\"https://media.defense.gov/2025/Jan/07/2003625471/-1/-1/1/ENTITIES-IDENTIFIED-AS-CHINESE-MILITARY-COMPANIES-OPERATING-IN-THE-UNITED-STATES.PDF\">designated by the Department of War</a> as Chinese Military Companies), shut down <a href=\"https://www.anthropic.com/news/disrupting-AI-espionage\">CCP-sponsored cyberattacks</a> that attempted to abuse Claude, and have advocated for <a href=\"https://www.wsj.com/opinion/trump-can-keep-americas-ai-advantage-china-chips-data-eccdce91?gaa_at=eafs&amp;gaa_n=AWEtsqdPk42glTHtJxGWpiSYR1xY28wMr6SpvGWmvlfp8_gYMp2h0ulOBH89Njx5eB0%3D&amp;gaa_ts=6983c8a6&amp;gaa_sig=t3NbNoEV35S9fhpBAUsmCPXHG6Zc3taB_jNESn4lAI7qy0l37FtVqnKZe-ASVGLp4SqxRsIS-HRn0k51UzsdpQ%3D%3D\">strong export controls on chips</a> to ensure a democratic advantage.</p><p>Anthropic understands that the Department of War, not private companies, makes military decisions. We have never raised objections to particular military operations nor attempted to limit use of our technology in an  manner.</p><p>However, in a narrow set of cases, we believe AI can undermine, rather than defend, democratic values. Some uses are also simply outside the bounds of what today’s technology can safely and reliably do. Two such use cases have never been included in our contracts with the Department of War, and we believe they should not be included now:</p><ul><li><strong>Mass domestic surveillance. </strong>We support the use of AI for lawful foreign intelligence and counterintelligence missions. But using these systems for mass surveillance is incompatible with democratic values. AI-driven mass surveillance <a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology\">presents serious, novel risks to our fundamental liberties</a>. To the extent that such surveillance is currently legal, this is only because the law has not yet caught up with the rapidly growing capabilities of AI. For example, under current law, the government can purchase detailed records of Americans’ movements, web browsing, and associations from public sources without obtaining a warrant, a practice the <a href=\"https://www.dni.gov/files/ODNI/documents/assessments/ODNI-Declassified-Report-on-CAI-January2022.pdf\">Intelligence Community has acknowledged</a> raises privacy concerns and that has generated bipartisan opposition in Congress. Powerful AI makes it possible to assemble this scattered, individually innocuous data into a comprehensive picture of any person’s life—automatically and at massive scale.</li><li><strong>Fully autonomous weapons. </strong>Partially autonomous weapons, like those used today in Ukraine, are vital to the defense of democracy. Even autonomous weapons (those that take humans out of the loop entirely and automate selecting and engaging targets) may prove critical for our national defense. But today, frontier AI systems are simply not reliable enough to power fully autonomous weapons. We will not knowingly provide a product that puts America’s warfighters and civilians at risk. We have offered to work directly with the Department of War on R&amp;D to improve the reliability of these systems, but they have not accepted this offer. In addition, <a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology\">without proper oversight</a>, fully autonomous weapons cannot be relied upon to exercise the critical judgment that our highly trained, professional troops exhibit every day. They need to be deployed with proper guardrails, which don’t exist today.</li></ul><p>To our knowledge, these two exceptions have not been a barrier to accelerating the adoption and use of our models within our armed forces to date.</p><p>The Department of War has <a href=\"https://media.defense.gov/2026/Jan/12/2003855671/-1/-1/0/ARTIFICIAL-INTELLIGENCE-STRATEGY-FOR-THE-DEPARTMENT-OF-WAR.PDF\">stated</a> they will only contract with AI companies who accede to “any lawful use” and remove safeguards in the cases mentioned above. They have threatened to remove us from their systems if we maintain these safeguards; they have also threatened to designate us a “supply chain risk”—a label reserved for US adversaries, never before applied to an American company— to invoke the Defense Production Act to force the safeguards’ removal. These latter two threats are <a href=\"https://www.politico.com/news/2026/02/26/incoherent-hegseths-anthropic-ultimatum-confounds-ai-policymakers-00800135?utm_content=topic/politics&amp;utm_source=flipboard\">inherently contradictory</a>: one labels us a security risk; the other labels Claude as essential to national security.</p><p>Regardless, these threats do not change our position: we cannot in good conscience accede to their request.</p><p>It is the Department’s prerogative to select contractors most aligned with their vision. But given the substantial value that Anthropic’s technology provides to our armed forces, we hope they reconsider. Our strong preference is to continue to serve the Department and our warfighters—with our two requested safeguards in place. Should the Department choose to offboard Anthropic, we will work to enable a smooth transition to another provider, avoiding any disruption to ongoing military planning, operations, or other critical missions. Our models will be available on the expansive terms we have proposed for as long as required.</p><p>We remain ready to continue our work to support the national security of the United States.</p>","contentLength":5251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47173121"},{"title":"Smartphone market forecast to decline this year due to memory shortage","url":"https://www.idc.com/resource-center/press-releases/wwsmartphoneforecast4q25/","date":1772143785,"author":"littlexsparkee","guid":154471,"unread":true,"content":"<p>“What we are witnessing is not a temporary squeeze, but a tsunami-like shock originating in the memory supply chain, with ripple effects spreading across the entire consumer electronics industry,” said&nbsp;<a href=\"https://my.idc.com/getdoc.jsp?containerId=PRF003252\">Francisco Jeronimo</a>, vice president&nbsp;for Worldwide&nbsp;Client Devices, IDC. “The global smartphone market, particularly Android manufacturers, faces a significant threat. Vendors whose business is mainly at the low end of the market are likely to suffer the most. Rising component costs will hit their margins, and they will have no choice but to pass the costs on to end users. By contrast, Apple and Samsung are better positioned to navigate this crisis. As smaller and low-end-positioned Android vendors struggle with rising costs, Apple and Samsung could not only weather the storm but potentially expand market share as the competitive landscape tightens.”</p>","contentLength":868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47172664"},{"title":"Layoffs at Block","url":"https://twitter.com/jack/status/2027129697092731343","date":1772140676,"author":"mlex","guid":154277,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47172119"},{"title":"What does \" 2>&1 \" mean?","url":"https://stackoverflow.com/questions/818255/what-does-21-mean","date":1772135926,"author":"alexmolas","guid":154402,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47171233"},{"title":"OsmAnd’s Faster Offline Navigation (2025)","url":"https://osmand.net/blog/fast-routing/","date":1772131079,"author":"todsacerdoti","guid":154594,"unread":true,"content":"<p>Offline navigation is a lifeline for travelers, adventurers, and everyday commuters. We demand speed, accuracy, and the flexibility to tailor routes to our specific needs. For years, OsmAnd has championed powerful, feature-rich offline maps that fit in your pocket. But as maps grew more detailed and user demands for complex routing increased, our trusty <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">A* algorithm</a>, despite its flexibility, started hitting a performance wall. How could we deliver a  without bloating map sizes or sacrificing the deep customization our users love?</p><p>The answer: <strong>OsmAnd's custom-built Highway Hierarchy (HH) Routing.</strong> This isn't your standard routing engine; it's a ground-up redesign, meticulously engineered to overcome the unique challenges of providing advanced navigation on compact, offline-first map data.</p><div><div><p>100x speedup is achieved by comparing HH with bidirectional A*.\n2-phase A* already uses many heuristics which don't always create an optimal route and still 5-10x slower.</p></div></div><h2>Why Standard Solutions Failed<a href=\"https://osmand.net/blog/fast-routing/#why-standard-solutions-failed\" aria-label=\"Direct link to Why Standard Solutions Failed\" title=\"Direct link to Why Standard Solutions Failed\">​</a></h2><p>OsmAnd has always been about putting you in control. Our original <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">A* routing engine</a>, configurable via <a href=\"https://github.com/osmandapp/OsmAnd-resources/blob/master/routing/routing.xml\" target=\"_blank\" rel=\"noopener noreferrer\"></a>, offered immense power. You could define intricate profiles, avoid specific road types, and truly personalize your journey. With maps optimized for minimal storage (the entire planet's car data for our new HH-routing is around a mere 800MB!), OsmAnd was a lean, mean navigating machine.</p><p>However, this flexibility came at a cost for complex routes:</p><ul><li>* Calculating a 200-300km car route (or even shorter bicycle/pedestrian paths) could mean visiting over a million road segments, taking 10-20 seconds. For longer trips, this wait could become frustrating.</li></ul><p>We explored standard advanced algorithms like <a href=\"https://en.wikipedia.org/wiki/Contraction_hierarchies\" target=\"_blank\" rel=\"noopener noreferrer\">Contraction Hierarchies (CH)</a>, known for their speed. But they presented their own set of deal-breakers for OsmAnd:</p><ul><li> CH typically pre-calculates optimal paths. Supporting OsmAnd's 10+ routing parameters (leading to over 1024 combinations per profile!) would be impossible with standard CH.</li><li> A CH car profile for a region can be massive (e.g., OSRM's Europe is tens of GBs, their global car profile around 200GB for just one profile). Our goal was to keep  profiles and parameters for the  well under 20GB.</li><li> Users download individual countries or regions. CH usually requires processing the entire road network globally, which doesn't align with OsmAnd's flexible map management.</li><li> The extensive pre-processing for CH makes it unsuitable for frequent updates, let alone OsmAnd’s goal of supporting near real-time changes through hourly map updates.</li></ul><p>The challenge was clear: achieve a quantum leap in speed while preserving extreme flexibility, minimal storage, regional map support, and dynamic update capabilities. Standard Highway Hierarchies were a starting point, but we needed something more – a uniquely OsmAnd solution.</p><h2>Secret Sauce #1: Two-Level Routing<a href=\"https://osmand.net/blog/fast-routing/#secret-sauce-1-two-level-routing\" aria-label=\"Direct link to Secret Sauce #1: Two-Level Routing\" title=\"Direct link to Secret Sauce #1: Two-Level Routing\">​</a></h2><p>The core of OsmAnd's HH-Routing is an elegant two-level hierarchy built upon \"area clusters.\"</p><ul><li> The map is intelligently segmented into numerous small regions or clusters.</li><li> Each cluster has a limited number of defined \"border points\" – these are the gateways in and out of the cluster.</li><li> For common scenarios, we pre-calculate the travel time/distance (the \"shortcut\") between border points  and also to border points of  clusters.</li></ul><p>This map illustrates the OsmAnd routing concept. <a href=\"https://osmand.net/map/navigate/?start=42.343860,-71.068550&amp;end=42.306935,-71.082991&amp;profile=car#14/42.3280/-71.0819\" target=\"_blank\" rel=\"noopener noreferrer\">The route</a> starts in the Start Area Cluster (221558), moves to the nearest Border Point, and continues through precomputed Shortcuts across intermediate clusters. It then enters the Finish Area Cluster (221536) via another border point and finishes using local roads. This method speeds up routing by combining local search with efficient inter-cluster shortcuts.</p><p>The real magic, our , lies in  these border points are selected. Naive approaches quickly fail:</p><ul><li>Randomly selecting border points or using simple geometric divisions (squares/hexagons) results in too many border points per cluster (50-80). This leads to a shortcut explosion (N*(N-1)/2 shortcuts), making the files large and and calculations slow.</li><li>We even tried building hierarchies with 2-3 levels, but the number of shortcuts grew too fast for higher levels if we generated a full graph inside each cluster.</li></ul><p><strong>The \"Parking Lot\" Insight:</strong>\nImagine a vast shopping mall parking lot with thousands of individual parking spots and internal lanes (representing road segments within a cluster). No matter how complex it is inside, there are usually only a few key exits to the main roads. Our goal was to identify these natural \"exits\" for each map cluster. For instance, the complex road network around Amsterdam Airport Schiphol (<a href=\"https://www.openstreetmap.org/#map=17/52.321360/4.766226\" target=\"_blank\" rel=\"noopener noreferrer\">see on OpenStreetMap</a>) has many internal roads but limited primary access points.</p><p>We wanted a scenario where, say, 5 well-placed border points could efficiently represent an area with 5,000 internal points and 10,000 road edges. This would reduce those 10,000 edges to just 5*4/2 = 10 shortcuts for routing  that cluster at a high level – an incredible 1:1000 point ratio and a 30x reduction in edges to consider for the high-level path!</p><p><strong>The Algorithm: Ford-Fulkerson to Find the Bottlenecks</strong>\nTo find these crucial border points, we employed a clever technique based on the <a href=\"https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">Ford-Fulkerson algorithm</a>. By simulating \"flooding\" roads with traffic from random start/end points, we could identify the natural bottlenecks – the \"minimum cut\" in graph theory terms. These bottlenecks became our border points.</p><p>\nCrucially, this distribution of border points is <strong>agnostic of routing speed profiles</strong>. It’s based only on whether a road is  or not. This means the <em>same set of clusters and border points</em> can be used for all car routing profiles (default, shortest, fuel-efficient) and all bicycle profiles (default, prefer flat terrain, etc.). Only the  of the shortcuts between these points change based on the profile. This is a massive factor in keeping storage down – <strong>map data only increased by about 0.5% per profile</strong> to store this HH-Routing structure!</p><p>Just look at the numbers for processing the entire planet for a car profile:</p><ul><li>Original OSM: ~2.07 billion points, ~2.42 billion edges</li><li>Resulting HH structure: ~3 million border points, ~541,000 clusters</li><li>Estimated shortcuts: ~91 million (a manageable number for global routing)</li></ul><h2>How OsmAnd Builds Routes<a href=\"https://osmand.net/blog/fast-routing/#how-osmand-builds-routes\" aria-label=\"Direct link to How OsmAnd Builds Routes\" title=\"Direct link to How OsmAnd Builds Routes\">​</a></h2><p>So, how does OsmAnd use this structure to calculate your route at lightning speed? It's a multi-step process:</p><p><strong>A. Preprocessing (Done by OsmAnd when new maps are prepared):</strong></p><ol><li><strong>Cluster &amp; Border Point Definition:</strong> The map is divided into clusters, and border points are identified using the Ford-Fulkerson based method.</li><li><strong>Shortcut Pre-calculation:</strong> For the most commonly used speed profiles, the travel costs (time/distance) for shortcuts between border points within each cluster are pre-calculated and stored. (Each border point effectively has an \"entry\" and \"exit\" aspect for directed travel).</li></ol><p><strong>B. User Route Request (Query Time - this is what happens on your device):</strong></p><ol><li><p><strong>Step 1: Connect to the Hierarchy (Your Local Area):</strong></p><ul><li>OsmAnd identifies the clusters containing your start and target points.</li><li>It then uses the standard <a href=\"https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">Dijkstra algorithm</a> on the  within your start cluster to find the best paths from your actual start location to  border points of that starting cluster.</li><li>The same is done for your target point within its own cluster (finding paths from all its border points to your actual destination).</li><li>This is quick because it's operating on a very small, localized part of the map.</li></ul></li><li><p><strong>Step 2: Route on the Abstract Graph (The \"Highway\" Part):</strong></p><ul><li>Now, OsmAnd performs another Dijkstra search, but this time on the much smaller \"base graph.\" This graph consists  of the border points and the pre-calculated shortcut values between them.</li><li>This step rapidly finds the optimal sequence of border points and shortcuts to get from your start cluster's periphery to your target cluster's periphery. It's incredibly fast because it's ignoring all the tiny roads  intermediate clusters.</li></ul></li><li><p><strong>Step 3: Refine with Detailed Shortcuts (Applying Secret Sauce #2):</strong></p><ul><li>The result from Step 2 is a high-level route – a sequence of shortcuts connecting border points.</li><li>Now, for  shortcut in this sequence, OsmAnd runs its highly optimized A* algorithm on the , but <strong>strictly limited to the small area of the cluster that shortcut belongs to.</strong></li><li>For example, a 500km route might be broken down into ~100 such shortcuts. If each A* shortcut calculation explores 100-1000 detailed road segments, the total detailed segments visited by A* might be around 10,000-50,000. Compare this to the 1,000,000+ segments the old A* might have needed for the entire route!</li></ul></li></ol><p>This combination – localized Dijkstra, super-fast abstract graph traversal, and highly localized A* refinement – is what delivers the .</p><h2>Secret Sauce #2: Adaptive Routing<a href=\"https://osmand.net/blog/fast-routing/#secret-sauce-2-adaptive-routing\" aria-label=\"Direct link to Secret Sauce #2: Adaptive Routing\" title=\"Direct link to Secret Sauce #2: Adaptive Routing\">​</a></h2><p>Speed is fantastic, but not if it means sacrificing the features OsmAnd users rely on. This is where our  comes into play – ensuring HH-Routing remains incredibly flexible and dynamic:</p><ul><li> Because the final shortcut refinement (Step 3 above) uses A* on detailed maps , all your specified parameters are naturally incorporated:<ul><li>Avoiding specific road types or toll roads.</li><li>Adding penalties or preferences for certain roads.</li><li>Following all the nuanced rules in your custom <a href=\"https://github.com/osmandapp/OsmAnd-resources/blob/master/routing/routing.xml\" target=\"_blank\" rel=\"noopener noreferrer\"></a> profiles.</li></ul></li><li><strong>Handling Live Updates &amp; Dynamic Changes:</strong> What if a bridge is closed due to a <a href=\"https://osmand.net/docs/user/personal/maps-resources#live-updates\" target=\"_blank\" rel=\"noopener noreferrer\">live map update</a> you just downloaded?<ul><li>If the A* calculation for a shortcut (in Step 3) finds it's now impassable, or if its actual detailed cost is significantly different (e.g., &gt;20%) from the pre-calculated shortcut value:<ul><li> The system can update the cost of that specific shortcut in the base graph and quickly re-run the Dijkstra search (Step 2) on the abstract graph to find an alternative high-level path.</li><li> For very localized changes, it might even re-evaluate all shortcuts within that one affected cluster.</li></ul></li><li>Minor road updates (like those in map data that might be a few months old if you're using maps from different regions) usually result in negligible cost differences for shortcuts, so the pre-calculated values remain effective.</li></ul></li><li><strong>Graceful Fallback for Extreme Customization:</strong><ul><li>What if you create a truly unique routing profile that's wildly different from the common ones for which shortcuts were pre-calculated? The system is smart. If it detects that too many shortcuts (~50, for example) need on-the-fly recalculation and deviate significantly, it might determine that falling back to the original, comprehensive A* algorithm for the  would actually be faster than doing many small, heavily modified A* calculations.</li></ul></li></ul><h2>Real Benefits for OsmAnd Users<a href=\"https://osmand.net/blog/fast-routing/#real-benefits-for-osmand-users\" aria-label=\"Direct link to Real Benefits for OsmAnd Users\" title=\"Direct link to Real Benefits for OsmAnd Users\">​</a></h2><p>This complex engineering translates into tangible benefits:</p><ul><li> The 100x average improvement means route calculations, especially for longer journeys, are now dramatically faster.</li><li> Our HH-Routing data adds only  to OsmAnd's already incredibly compact map sizes. The entire planet's car routing data is around 800MB!</li><li><strong>Full Customization Power:</strong> All the beloved flexibility of <a href=\"https://github.com/osmandapp/OsmAnd-resources/blob/master/routing/routing.xml\" target=\"_blank\" rel=\"noopener noreferrer\"></a> and detailed routing parameters is retained.</li><li><strong>Works with Regional Maps:</strong> Download only the countries you need. HH-Routing seamlessly calculates routes across the borders of your downloaded map files (as long as they are compatible, see limitations). Clusters that overlap a region's boundary are included within that region's data.</li><li><strong>Supports Frequent Updates:</strong> The architecture is designed to work with <a href=\"https://osmand.net/docs/user/personal/maps-resources#live-updates\" target=\"_blank\" rel=\"noopener noreferrer\">OsmAnd’s hourly map updates</a>, allowing routing to adapt to fresh road information.</li><li> This structure makes it much easier to implement features like alternative route suggestions based on these key border points.</li></ul><p>No system is perfect, and OsmAnd's HH-Routing has a few considerations:</p><ul><li><strong>Highly Divergent Profiles:</strong> For routing configurations that are not pre-calculated as common scenarios and whose costs vary too much from default configurations, the original <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">A* algorithm</a> might still be faster (and is often used as an automatic fallback).</li><li><strong>Map Version Synchronicity (Important!):</strong> For HH-Routing to work correctly when a route crosses multiple map files (e.g., different countries or regions), <a href=\"https://osmand.net/docs/user/personal/maps-resources#updates\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>all those map files MUST be from the same generation date</strong></a> (i.e., downloaded from OsmAnd around the same time, based on the same underlying OpenStreetMap data version and pre-calculation run).<ul><li>If you try to route from a map of France updated in May with a map of Germany updated in April, HH-Routing may not be compatible across the border. You would need to update all relevant maps to the same version.</li><li>This doesn't mean all maps must be , just <em>from the same batch/pre-calculation period</em>.</li></ul></li><li> The intensive preprocessing required to generate all these routing profiles for the entire planet takes about 2-3 days. This means new map updates are now typically released around the 5th of each month, instead of the 2nd.</li></ul><h2>Smart Engineering Delivers<a href=\"https://osmand.net/blog/fast-routing/#smart-engineering-delivers\" aria-label=\"Direct link to Smart Engineering Delivers\" title=\"Direct link to Smart Engineering Delivers\">​</a></h2><p>OsmAnd's HH-Routing is more than just an algorithm; it's a testament to innovative problem-solving. It’s a carefully engineered system born from the need to overcome specific, demanding constraints: the desire for blazing speed, minimal storage, complete routing flexibility, regional map support, and adaptability to fresh data.</p><p>By cleverly identifying crucial \"bottleneck\" border points, creating a universal two-level hierarchy, and dynamically refining routes with our optimized A* engine, we've managed to deliver a vastly superior navigation experience. It's a win for every OsmAnd user who relies on fast, dependable, and customizable offline navigation.</p><p>We're excited about what HH-Routing brings to OsmAnd!</p><ul><li>Have you experienced the new routing speed?</li><li>What are your go-to custom routing settings that you're glad are still supported?</li><li>Share your thoughts, experiences, and any questions in the comments below or on our community forums!</li></ul>","contentLength":13669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47170157"},{"title":"What Claude Code chooses","url":"https://amplifying.ai/research/claude-code-picks","date":1772129546,"author":"tin7in","guid":154276,"unread":true,"content":"<p> (Recommended) — Built by the creators of Next.js. Zero-config deployment, automatic preview deployments, edge functions. </p><p> — Great alternative with similar features. Good free tier.</p><p> — Good if you're already in the AWS ecosystem.</p>","contentLength":234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47169757"},{"title":"Open Source Endowment – new funding source for open source maintainers","url":"https://endowment.dev/","date":1772122386,"author":"kvinogradov","guid":154593,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47168012"},{"title":"Will vibe coding end like the maker movement?","url":"https://read.technically.dev/p/vibe-coding-and-the-maker-movement","date":1772122031,"author":"itunpredictable","guid":154261,"unread":true,"content":"<p>Whenever a new technology arrives, the impulse is to treat it as something that has never existed before. A clean break from everything that came prior. I catch myself doing this with vibe coding constantly, and I see it everywhere around me. But the most useful lens for understanding a new phenomenon is almost never the phenomenon itself. You want something adjacent, close enough to share structural similarities but removed enough to see clearly. It’s on the lookout for something like this that I started reading more about the Maker Movement of ~2005-2015.</p><p><a href=\"https://en.wikipedia.org/wiki/Chris_Anderson_(writer)\" rel=\"\">Chris Anderson</a><a href=\"https://en.wikipedia.org/wiki/Makers_(novel)\" rel=\"\"> Makers,</a></p><p>A lot of the intellectual energy of the AI era orbits around AGI: when it arrives, what it’ll do to jobs, whether it will be aligned. The Maker Movement had its own gravitational center, and it was the idea that making physical things with your hands could produce an internal transformation. You would become more creative, more entrepreneurial, more self-reliant. The object you made mattered less than what the act of making did to you.</p><p><a href=\"https://fredturner2022.sites.stanford.edu/sites/g/files/sbiybj27111/files/media/file/turner-millenarian-tinkering-tech-culture-2018.pdf\" rel=\"\">published a paper</a></p><p>The specifics of seventeenth-century Puritanism are obviously gone. Nobody at a Maker Faire was talking about predestination. But Turner traced the literary forms and the millenarian structure—the belief that a great transformation is coming, and that individual discipline will determine who makes it through. In the Maker narrative, the American landscape is economically barren. Jobs have disappeared. Institutions have failed you. And in this wilderness, the lone individual searches inside themselves for signs of the entrepreneurial spirit, the creative spark, evidence that they are among the elect who will build their way to salvation.</p><p>Turner’s observation extends well beyond 3D printers. You can trace this same pattern through almost every hobbyist technology scene of the past fifty years. Homebrew computer clubs in the 1970s. Punk zines in the 1980s. The early web in the 1990s. Each one developed a community of practice—what Brian Eno would call a “scenius”—where people played with tools that the mainstream considered toys. Each one generated its own salvation narrative: master this tool, transform yourself, become the kind of person who builds the future.</p><p>And each one operated with a useful kind of slack. The tools were unproductive on purpose. Nobody expected your Arduino project to ship to customers. Nobody expected your homebrew computer to compete with IBM. The whole point was that you had permission to fuck around, and the finding-out happened gradually, through play, over years. This is where the old Silicon Valley adage comes from: “What smart people do on the weekends, everyone else will do during the week in ten years.”</p><p>Vibe coding broke this pattern in a way that matters.</p><p>Every previous wave of hobbyist technology went through a scenius phase—a period where small groups of weirdos played with tools before anyone expected economic output from them.</p><p>Vibe coding skipped that phase entirely. It was deployed directly to the general public, and almost immediately into the codebases of enterprise companies and well-developed products. There was no protected playground period. There was no time to accumulate the weird, useless, playful knowledge that scenius communities generate. Instead, there was immediate pressure to one-shot yourself into a hit product or solve a complex use case on the first try.</p><p>This matters because the scenius phase is where the internal transformation actually happens. When you spend two years making useless Arduino projects, you develop instincts about electronics, materials, and design that you can’t get from a tutorial. When vibe coding goes straight to production, you lose that developmental space. The tool is powerful enough to produce real output before the person using it has developed real judgment. When I speak with people who are on Claude Code 12-14 hours a day, I feel like I’m speaking to someone possessed by something, attempting to grasp a different reality. In the case of scenius, the feedback loop that tethers you to reality was provided by other humans. Someone looked at your project and told you it’s pointless, or brilliant, or both. While in the case of vibe coding, the feedback loop is provided by the machine, and you’re constantly attempting to discern if you’re going crazy or if something genuinely valuable has been produced.</p><p>What it produces is something like hypomania: a state where your productive capacity genuinely increases. You’re not imagining that you’re getting more done, you actually are, but your evaluative faculty is unaccustomed to this mode of creation. You lose the ability to distinguish between “this is good” and “I feel good making this.” Everything feels like a breakthrough. The output is real but your relationship to it is distorted.</p><p>The speed and ease of vibe coding create a kind of evaluative anesthesia. You can’t tell if you’ve built something useful or just something that exists. In some way, this is the sober version of hippies in the 60s trying LSD for the first time: sometimes you may have a breakthrough, or you may have a breakdown, but regardless of which, this is the opposite of the salvation through making that Fred Turner talks about.</p><p>There’s a second reason the old transformation-through-making metaphor doesn’t fit vibe coding, and it has to do with how the Maker Movement actually ended.</p><p><a href=\"https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/\" rel=\"\">commoditizing your complement</a></p><p><a href=\"https://www.cnbc.com/2026/02/09/monday-com-stock-ai-software.html\" rel=\"\">entire SaaS business models</a></p><p>With both of these forces at play—no scenius phase to develop through, and value accumulating upstream rather than with the maker—the old metaphor of transformation-through-making doesn’t hold up exactly. We need a new one.</p><p>Specifically: consumption of a surplus intelligence. AI represents an enormous amount of available cognitive energy, and vibe coding is one way of expending that energy before it goes to waste. Think of it like a resource that’s being generated whether you use it or not—and vibe coding is the act of channeling that surplus into play, into exploration, into rapid creation that may or may not produce lasting artifacts.</p><p><a href=\"https://www.fast.ai/posts/2026-01-28-dark-flow/\" rel=\"\">the dark flow state when you gamble</a></p><p>Consumption almost always gets treated as a negative behavior, especially if you’re an entrepreneur or builder. Consuming is what passive people do. Builders produce.</p><p>I think this framing is wrong, or at least incomplete. There are several productive ways to think about what consumption actually generates.</p><p>Expenditure that’s visible generates spectacle, and spectacle generates attention. When you vibe-code something in public—building fast, shipping immediately, iterating in front of an audience—the product you make matters less than the performance of making it. And undoubtedly, much of vibe coding today is pure signalling performance.</p><p><a href=\"https://x.com/search?q=built%20in%20a%20weekend%20&amp;src=typed_query\" rel=\"\">built this in a weekend</a></p><p>This is structurally identical to how content creators already operate. A YouTuber’s individual video is an expenditure. The audience accumulated across hundreds of videos is the asset. Vibe coding just adds another medium to the content creator’s toolkit: instead of expending effort on essays or videos, you expend it on apps and tools, and you capture the attention the same way.</p><p>If you treat your vibe-coded output as gifts—open source tools, free utilities, shared templates, public repos—you’re creating the conditions to occupy an interesting or powerful position in the network. Think of the people who built the early web’s most useful free tools and resources: They became nodes that other people oriented around.</p><p>Every time you vibe-code something, you’re generating signal. Signal about what users want. Signal about which patterns work. Signal about where the model fails, what edge cases it misses, what instructions it misinterprets. That signal currently flows upstream to model providers for free. Your prompts, your iterations, your corrections—all of it becomes training data for the next generation of models. You are, in a very literal sense, performing unpaid labor for the infrastructure layer every time you build something.</p><p>Consumption doesn’t have to be passive. Surplus can be spent well. The key distinction is whether you’re burning energy with some awareness of what the combustion produces—taste, attention, social capital, structured signal—or whether you’re just spinning up a dozen projects and wondering why none of them stick.</p><p>Personally, I find the consumption metaphor to be a good way to deal with the burnout that comes with constantly using AI for various things. A lot of people approach making things with the mindset of craft, and naturally extend this framing to vibe coding. That framing feels noble and it’s deeply familiar, but it’s also a recipe for burnout, because craft assumes you are reaching inside yourself and pulling something out. The whole emotional architecture of craft is transformational: you struggle, and develop mastery, and the object you produce is evidence of inner change. When the tool is doing most of the producing, that framework starts to collapse. You’re left reaching inward for something that the process never required you to develop, and the gap between the effort you expected to invest and the effort that was actually needed starts to feel like a personal failure rather than a feature of the technology.</p><p>The consumption framing sidesteps this entirely. You’re not reaching inward. You’re starting from the position that there is extra energy available and it needs to go somewhere. The question shifts from “what does this say about me as a maker” to “what’s the most interesting thing I can spend this on.” That’s a fundamentally different emotional posture, and in practice it’s a much more sustainable one.</p>","contentLength":9791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47167931"},{"title":"Nano Banana 2: Google's latest AI image generation model","url":"https://blog.google/innovation-and-ai/technology/ai/nano-banana-2/","date":1772121757,"author":"davidbarker","guid":154176,"unread":true,"content":"<p data-block-key=\"78m38\">In August of last year, our Gemini Image model, <a href=\"https://blog.google/products-and-platforms/products/gemini/updated-image-editing-model/\">Nano Banana</a>, became a <a href=\"https://blog.google/products-and-platforms/products/gemini/nano-banana-google-trends-2025/\">viral sensation</a>, redefining image generation and editing. Then in November, we released <a href=\"https://blog.google/innovation-and-ai/products/nano-banana-pro/\">Nano Banana Pro</a>, offering users advanced intelligence and studio-quality creative control. Today, we’re bringing the best of both worlds to users across Google.</p><p data-block-key=\"191b\">Introducing Nano Banana 2 (Gemini 3.1 Flash Image), our latest state-of-the-art image model. Now you can get the advanced world knowledge, quality and reasoning you love in Nano Banana Pro, at lightning-fast speed.</p>","contentLength":534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47167858"},{"title":"AirSnitch: Demystifying and breaking client isolation in Wi-Fi networks [pdf]","url":"https://www.ndss-symposium.org/wp-content/uploads/2026-f1282-paper.pdf","date":1772121348,"author":"DamnInteresting","guid":154260,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47167763"},{"title":"In 2025, Meta paid an effective federal tax rate of 3.5%","url":"https://bsky.app/profile/rbreich.bsky.social/post/3mfptlfeucn2i","date":1772118860,"author":"doener","guid":154636,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47167171"},{"title":"BuildKit: Docker's Hidden Gem That Can Build Almost Anything","url":"https://tuananh.net/2026/02/25/buildkit-docker-hidden-gem/","date":1772114746,"author":"jasonpeacock","guid":154592,"unread":true,"content":"<h5>\n      Posted on \n  \n    February 25, 2026\n  \n\n\n      \n        &nbsp;•&nbsp;\n      \n      \n      5&nbsp;minutes\n      &nbsp;•\n      \n      859&nbsp;words\n      \n    </h5><p>Most people interact with BuildKit every day without realizing it. When you run , BuildKit is the engine behind it. But reducing BuildKit to “the thing that builds Dockerfiles” is like calling LLVM “the thing that compiles C.” It undersells the architecture by an order of magnitude.</p><p>BuildKit is a general-purpose, pluggable build framework. It can produce OCI images, yes, but also tarballs, local directories, APK packages, RPMs, or anything else you can describe as a directed acyclic graph of filesystem operations. The Dockerfile is just one frontend. You can write your own.</p><p>BuildKit’s design is clean and surprisingly understandable once you see the layers. There are three key concepts.</p><p>At the heart of BuildKit is  (Low-Level Build definition). Think of it as the LLVM IR of build systems. LLB is a binary protocol (protobuf) that describes a DAG of filesystem operations: run a command, copy files, mount a filesystem. It’s content-addressable, which means identical operations produce identical hashes, enabling aggressive caching.</p><p>When you write a Dockerfile, the Dockerfile frontend parses it and emits LLB. But nothing in BuildKit requires that the input be a Dockerfile. Any program that can produce valid LLB can drive BuildKit.</p><h3>Frontends: bring your own syntax</h3><p>A  is a container image that BuildKit runs to convert your build definition (Dockerfile, YAML, JSON, HCL, whatever) into LLB. The frontend receives the build context and the build file through the BuildKit Gateway API, and returns a serialized LLB graph.</p><p>This is the key insight: the build language is not baked into BuildKit. It’s a pluggable layer. You can write a frontend that reads a YAML spec, a TOML config, or a custom DSL, and BuildKit will execute it the same way it executes Dockerfiles.</p><p>You’ve actually seen this mechanism before. The  directive at the top of a Dockerfile tells BuildKit which frontend image to use. <code># syntax=docker/dockerfile:1</code> is just the default. You can point it at any image.</p><h3>Solver and cache: content-addressable execution</h3><p>The  takes the LLB graph and executes it. Each vertex in the DAG is content-addressed, so if you’ve already built a particular step with the same inputs, BuildKit skips it entirely. This is why BuildKit is fast: it doesn’t just cache layers linearly like the old Docker builder. It caches at the operation level across the entire graph, and it can execute independent branches in parallel.</p><p>The cache can be local, inline (embedded in the image), or remote (a registry). This makes BuildKit builds reproducible and shareable across CI runners.</p><p>BuildKit’s  flag is where this gets practical. You can tell BuildKit to export the result as:</p><ul><li> — push to a registry (the default for )</li><li> — dump the final filesystem to a local directory</li><li> — export as a tarball</li><li> — export as an OCI image tarball</li></ul><p>The  output is the most interesting for non-image use cases. Your build can produce compiled binaries, packages, documentation, or anything else, and BuildKit will dump the result to disk. No container image required.</p><p>Projects like <a href=\"https://earthly.dev\" target=\"_blank\" rel=\"noopener\">Earthly</a>\n, <a href=\"https://dagger.io\" target=\"_blank\" rel=\"noopener\">Dagger</a>\n, and <a href=\"https://depot.dev\" target=\"_blank\" rel=\"noopener\">Depot</a>\n are all built on top of BuildKit’s LLB. It’s a proven pattern.</p><h2>Building APK packages with a custom frontend</h2><p>To demonstrate this concretely, I built <a href=\"https://github.com/tuananh/apkbuild\" target=\"_blank\" rel=\"noopener\">apkbuild</a>\n: a custom BuildKit frontend that reads a YAML spec and produces Alpine APK packages. No Dockerfile involved. The entire build pipeline — from source compilation to APK packaging — runs inside BuildKit using LLB operations. Think of this like a dummy version of <a href=\"https://github.com/chainguard-dev/melange\" target=\"_blank\" rel=\"noopener\">Chainguard’s melange</a></p><p>I chose YAML for familiarity, but the spec could be anything you want (JSON, TOML, a custom DSL) as long as your frontend can parse it.</p><p>My package YAML spec looks like this:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>That’s it. No Dockerfile. BuildKit reads this spec through the custom frontend and produces a  file.</p><p>Build the frontend image:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Then use it to build an APK package:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>You should be able to see the APK package in the  folder like below</p><p> tells BuildKit to use our custom frontend instead of the default Dockerfile parser. The  dumps the resulting  files to . No image is created. No registry is involved.</p><p>BuildKit gives you a content-addressable, parallelized, cached build engine for free. You don’t need to reinvent caching, parallelism, or reproducibility. You write a frontend that translates your spec into LLB, and BuildKit handles the rest.</p><p>This is relevant beyond toy demos. Dagger uses LLB as its execution engine for CI/CD pipelines. Earthly compiles Earthfiles into LLB. The pattern is proven at scale.</p><p>If you’re building a tool that needs to compile code, produce artifacts, or orchestrate multi-step builds, consider BuildKit as your execution backend. The Dockerfile is just the default frontend. The real power is in the engine underneath.</p>","contentLength":4911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47166264"}],"tags":["dev"]}