{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"HN","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":30,"items":[{"title":"Anyone can push updates to the doge.gov website","url":"https://www.404media.co/anyone-can-push-updates-to-the-doge-gov-website-2/","date":1739518306,"author":"mahkeiro","guid":211,"unread":true,"content":"<div>\"THESE 'EXPERTS' LEFT THEIR DATABASE OPEN.\"</div>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045835"},{"title":"Zed now predicts your next edit with Zeta, our new open model","url":"https://zed.dev/blog/edit-prediction","date":1739515847,"author":"ahamez","guid":208,"unread":true,"content":"<p>Zed is built for speed. We've always strived for an editing experience that feels . But what's faster than instant?  A tool that anticipates your next move. That's why we're introducing <a href=\"https://zed.dev/edit-prediction\">edit prediction</a> in Zed, powered by <a href=\"https://huggingface.co/zed-industries/zeta\">Zeta</a>, our new open source model.</p><p>Here's a quick walkthrough:</p><div><figure><figcaption>Edit Prediction in action.</figcaption></figure></div><p>As you work, Zed now predicts your next edit, so you can apply it just by hitting . Once you accept a prediction, you can perform multiple follow-up edits by pressing  repeatedly, saving you time and keystrokes. We've received  of requests for this functionality, and we've poured our hearts into making it feel like a natural extension of the Zed experience.</p><p>You can use Zeta for free during this public beta by <a href=\"https://zed.dev/download\">downloading Zed</a> and signing in with your GitHub account. Edit prediction won't be free forever, but right now we're just excited to share and learn.</p><p>Edit prediction transforms  into a magical, universal key. But what about the existing uses of , such as indenting lines? And what happens when there's both an edit prediction  suggestions from your language server? We didn't want a powerful new feature to come at the expense of the existing editing experience in Zed.</p><p>When language server completions are visible, Zed won't preview the predicted edit until you press  or . As soon as you press the modifier, Zed previews the edit and hides the menu to enable an unobstructed review. On macOS, you can just hit  to confirm, or back out by releasing  to restore the language server completions menu.</p><p>On Linux,  is often reserved by the window manager, so we offer  as an alternative default. We chose  because it's on the QWERTY home row and represents rightward movement in Vim. If your Linux window manager doesn't claim , you're free to use that binding as well.</p><p><a href=\"https://huggingface.co/zed-industries/zeta\">Zeta</a> is derived from Qwen2.5-Coder-7B, and is fully open source, including <a href=\"https://huggingface.co/datasets/zed-industries/zeta\">an open dataset</a>. If you're working in an open source repository, we'd love your help improving Zeta by contributing to its dataset. Please bear with us initially, as we will be reviewing the submitted data before publishing to ensure everyone's safety and privacy. We're excited to figure this out and see a community effort form to make edit prediction better everywhere, most especially in Zed!</p><div><div><div><p>How Zed's Open-Source Edit Predictions Work</p></div><p>Richard Feldman and Antonio Scandurra talk about how Zed's new Edit Prediction feature works under the hood. This includes how the Zed team developed and open-sourced both the code and the dataset behind the fine-tuned Zeta language model that powers it!</p></div></div><p>Most coding models are trained on a \"fill in the middle\" task. You give them a prefix and a suffix, and they generate what goes in between.</p><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rs\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><p>This works for completing text at the cursor, but we wanted Zeta to predict edits at arbitrary locations, which doesn't fit into this structure.</p><p>In our experience, models aren't very good at producing granular edits, but they do excel at rewriting larger chunks of code. So that's where we started: given a list of recent edits and the cursor position, we asked the model to rewrite a snippet of text around the cursor, incorporating one or more edit predictions in the rewritten text.</p><p>Before writing a single line of code, we created a set of tests to check if our idea worked. Testing the output of a large language model is tricky because, on every run, you can get slightly different results even when feeding it the exact same input. This can be mitigated by using a temperature of  and, for providers that support it, providing a seed for the RNG.</p><p>That said, code can often be written in many different but equally valid ways. So even when Zeta's output differs from our expected answer, it might still be doing exactly what we want—just taking a different path to get there. This makes traditional unit testing approaches particularly challenging when working with LLMs.</p><p>This led us to take a different approach—instead of strict assertions, we used a larger LLM to evaluate Zeta's edits. By writing our test assertions in plain English and having Claude check if the results matched our intent, we could validate that Zeta was making sensible edits, even when its exact output differed between runs. This ended up being much more practical than trying to make brittle assertions about specific tokens.</p><p>Here's an example taken from our eval suite:</p><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rs\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><p>We took our first stab at making those tests pass by using Qwen2.5-Coder-32B and giving it clear instructions for which types of edits we wanted it to predict. <a href=\"https://github.com/zed-industries/zed/blob/79a70b72b3968d102c6171f8bd2738ec7be8e94f/crates/zeta/src/complete_prompt.md\">Here's the initial system prompt</a> we used and you can look through the history to see how we kept changing it to pass the eval suite.</p><p>This worked out surprisingly well for the first 4-5 evals. However, as soon as we introduced more, we started noticing that it got harder and harder to pass them all consistently. Changing the prompt caused the new evals to pass, but made the old ones fail. Overall, it felt like a flaky process and we didn't feel confident this would lead to the system being robust enough to be used in production.</p><p>Moreover, using a 32b model wasn't really compatible with our strict latency requirements (more on that later).</p><p>After playing around with different approaches, we decided to go with supervised fine-tuning using <a href=\"https://unsloth.ai\">Unsloth</a> and LoRA. The idea was to teach Zeta two key things: figuring out what changes a developer might want next based on their recent edits, and then actually applying those changes cleanly to the code without introducing weird side effects.</p><p>But we had a classic chicken-and-egg problem—we needed data to train the model, but we didn't have any real examples yet. So we started by having Claude generate about 50 synthetic examples that we added to <a href=\"https://huggingface.co/datasets/zed-industries/zeta-dataset\">our dataset</a>. We then used that initial fine-tune to ship an early version of Zeta behind a feature flag and started collecting examples from our own team's usage.</p><p>This approach let us quickly build up a solid dataset of around 400 high-quality examples, which improved the model a lot! However, we kept running into edge cases that would trip the model up. The most annoying ones were when Zeta was working with a small piece of code in a larger file—it would sometimes get confused and make random deletions or insertions that had nothing to do with what the user was trying to do, and it didn't seem like adding more examples steered the model away from those mistakes.</p><p>To handle these edge cases, we conducted another pass using direct preference optimization (DPO). This technique let us go beyond simply showing the model what good edits look like—we could also teach it what edits . With DPO, we could fine-tune Zeta by providing both positive and negative examples, helping it learn the subtle differences between helpful and problematic edits.</p><p>We found that just ~150 carefully selected examples were enough to significantly improve Zeta's behavior on tricky cases. Of course, we think we can make it even better by expanding our training data with more diverse examples, and we're excited to keep pushing the boundaries here.</p><p>Like every feature in Zed, latency was a critical factor for edit prediction. When we started, we set aggressive performance targets: predictions should be delivered in under 200ms for the median case (p50) and under 500ms for the 90th percentile (p90). The challenge was that rewriting complete excerpts, while enabling multi-location edits, requires generating significantly more tokens than simple fill-in-middle approaches. Initially, this put us way over our latency budget.</p><p>However, there's a fascinating insight about how edit predictions work. When we rewrite a text snippet, the output often mirrors the input closely, with changes concentrated in specific spots. This pattern lets us parallelize token generation by using the input as a reference—a technique known as speculative decoding. We use n-gram search to identify promising jumping-off points in the input where we can start parallel token generation, giving us a significant speedup without sacrificing quality.</p><p>For edit predictions to feel responsive, we needed to solve multiple latency challenges in parallel. As discussed above, we tackled the model execution time through speculative decoding, but serving the model at scale presented its own set of hurdles. This was by far the most compute-intensive problem our team has ever tackled.</p><p>A few weeks out from launch, we ran a brief competitive process, and we ended up being really impressed with <a href=\"https://www.baseten.co\">Baseten</a>. Their performance engineers quickly optimized our open source model to run on their flexible infrastructure, achieving our target latencies while letting us retain full visibility into the details of the deployment, both for the Zed team and the entire Zed community. We plan to follow up with a guest post about what they learned optimizing <a href=\"https://huggingface.co/zed-industries/zeta\">our model</a>.</p><p>Latency is not just a function of compute; network transit time is a key driver of perceived speed. To cooperate with the laws of physics, we're launching with GPUs in both North America and Europe, and we hope to add more regions soon. We're also using <a href=\"https://workers.cloudflare.com/\">Cloudflare Workers</a> to handle your requests in a data center located as close to you as possible.</p><p>There's plenty more to explore to make edit predictions more powerful. We'll be fast-following with more experiments. We plan on sending more kinds of context to the model and continuing our experiments with fine-tuning, and we'll share updates as we grow and evolve the Zeta dataset.</p><p>We've learned a lot since we launched Zed AI last fall. The world is changing fast, and we're having a blast exploring and learning to build features that developers love. We're also excited to build with AI the Zed way. From our early days, we've been proponents of an open approach to building software, even when hard, and we see no reason to change that approach when it comes to working with AI. We hope you'll join us as a user, a contributor, or an employee, as we hustle to ship a golden future.</p>","contentLength":9947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045606"},{"title":"Privacy Pass Authentication for Kagi Search","url":"https://blog.kagi.com/kagi-privacy-pass","date":1739476649,"author":"b3n","guid":205,"unread":true,"content":"<p>Today we are announcing a new privacy feature coming to Kagi Search. Privacy Pass is an authentication protocol first introduced by Davidson  in [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">1</a>], and recently standardized by the IETF as RFCs [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2—4</a>].  Our starting point was the excellent Rust implementation of the Privacy Pass protocols by <a href=\"https://github.com/raphaelrobert/privacypass\">Raphael Roberts</a>. At the same time, we are announcing the immediate availability of Kagi’s <a href=\"https://blog.kagi.com/kagi-privacy-pass#tor\">Tor onion service</a>.</p><p>In general terms, Privacy Pass allows “Clients” (generally users) to authenticate to “Servers” (like Kagi) in such a way that while the Server can verify that the connecting Client has the right to access its services, it cannot determine which of its rightful Clients is actually connecting. This is particularly useful in the context of a privacy-respecting paid search engine, where the Server wants to ensure that the Client can access the services, and the Client seeks strong guarantees that, for example, the searches are not associated with them.</p><p>As a privacy-respecting search engine, Kagi’s business model is such that we have no incentive to track what an individual user is searching for. We are in the business of selling a search product, not selling user data or attention.</p><p>Now, Privacy Pass adds another layer of trust: we can verify that you have the right to search without knowing who you are or what you’re searching for. It’s one thing to promise we won’t track you; it’s another to make it technically impossible. We jumped on the opportunity to implement Privacy Pass as soon as the IETF made it an official standard.</p><p>This matters, because for many users, privacy isn’t just about incentives and privacy policies; it’s about proof. When we cannot track you even if we wanted to, that’s genuine privacy.</p><p>Initially, we will be offering Privacy Pass to all our plans with unlimited searches: Professional, Ultimate, Family, and Team plans. Privacy Pass will not be available to Trial and Starter plans due to technical limitations at this moment (see below for more info).</p><p>\nTo get started with Kagi Privacy Pass right away:</p><ul><li>Download the newest version of Kagi’s <a href=\"https://kagi.com/orion\">Orion Browser</a> (for macOS/iOS/iPadOS) with Kagi Privacy Pass natively integrated. You will need at least 0.99.131 for macOS and 1.3.17 for iOS/iPadOS (they are expected to be rolling out globally today).</li></ul><ul><li>Download the newest version of <a href=\"https://play.google.com/store/apps/details?id=com.kagi.search&amp;hl=en_US\">Kagi for Android</a> app with Kagi Privacy Pass natively integrated. You will need to use at least version 0.29 (this is expected to roll out globally today).</li></ul><ul><li>If you are already using the Kagi Search extension, you will want to update it to the latest version (0.7.6 on <a href=\"https://addons.mozilla.org/firefox/addon/kagi-search-for-firefox\">Firefox</a>, 1.2.2.5 on <a href=\"https://chromewebstore.google.com/detail/kagi-search/cdglnehniifkbagbbombnjghhcihifij\">Chrome</a>) to avoid compatibility issues, or simply disable it.</li><li>Safari is not yet supported due to technical limitations, see the F.A.Q. below.</li></ul><p>In addition our implementation of Privacy Pass is open sourced and you can find it <a href=\"https://github.com/kagisearch/privacypass-extension\">here</a>.</p><p>When using Kagi Privacy Pass mode, you’ll be truly anonymous - which means your account settings won’t be available since we can’t identify which user you are.</p><p>But don’t worry - we’ve made it flexible. You can easily toggle Privacy Pass on or off based on your needs. Think of it as two modes: full features with normal privacy, or maximum privacy with core features. You choose what makes sense for you based on your context and needs.</p><p>Privacy Pass uses cryptography to allow a client to authenticate to a server by performing a protocol with two phases: token generation and token redemption.</p><p>In the initial “token generation” phase, the client interacts with the server to generate some authentication “tokens.”</p><p>For the server to willingly participate in this protocol, the client must prove their “right” to generate tokens.</p><p>In the case of Kagi’s users, this can be done by presenting their Kagi session cookie to the server.</p><p>The tokens eventually generated by the client at the end of this phase are indistinguishable from a randomly generated token from the server’s point of view. They cannot be traced back to the user who generated them, or to other tokens generated by the same user at the same or a different time.</p><p>After token generation is performed, a client can initiate a “token redemption” phase.</p><p>During this phase, the client actually accesses the services provided by the server, proving the client’s right to access the services by presenting one of the previously generated tokens.</p><p>Since the previously generated tokens are unknown and unpredictable to the server, the latter can only tell that the client has successfully completed token generation at some point.</p><p>Technically, we say that the techniques used by Privacy Pass result in the two phases being “unlinkable”. While the server is able to tell whether a token presented for redemption was previously generated by interacting with a rightful client, it cannot link the token to a specific token generation phase.</p><p>Crucially, tokens are single-use: servers keep track of which tokens have already been redeemed to avoid multiple redemptions. Furthermore, clients should not present the same token twice to prevent different redemption phases from being linked.</p><p>Tokens have a fixed life span. If they are too old, they will stop being redeemable. In that case, a new token generation phase must be initiated by the client to obtain new tokens.</p><p>As standardized in [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2 - 4</a>], the Privacy Pass protocol is able to accommodate many “architectures.” Our deployment model follows the original architecture presented by Davidson  [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">1</a>], called “Shared Origin, Attester, Issuer” in § 4 of [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2</a>].</p><p>Here, Kagi plays all the “Server roles” (Attester, Issuer, Origin), and Kagi users play the Client role via the new Kagi browser extensions for Privacy Pass, or via native support in Orion. This is what it looks like in practice:</p><ol><li>Once installed, and periodically, the browser extension will generate and store a large number of tokens.</li><li>The user can mark in the extension whether searches should be performed by authenticating classically via a session cookie, or by using Privacy Pass.</li><li>If the user chooses the second option, they will authenticate to Kagi during the search by redeeming one of the tokens it previously generated.</li></ol><p>Using Privacy Pass is as easy as clicking a toggle.</p><p>If you are using the latest version of <a href=\"https://kagi.com/orion\">Orion</a> for macOS,  select Kagi as your search engine in  and then enable the checkbox for showing Privacy Pass options on your toolbar.</p><p>From there you can easily toggle when you want to use Privacy Pass or standard authentication.</p><p>On iOS and iPadOS, Kagi Privacy Pass is natively supported in the latest version of the <a href=\"https://apps.apple.com/us/app/orion-browser-by-kagi/id1484498200\">Orion Browser</a> for iOS and iPadOS and takes just a few clicks to enable.</p><p>Our <a href=\"https://play.google.com/store/apps/details?id=com.kagi.search\">Android app</a> now supports Privacy Pass mode via an app shortcut. Launching the shortcut allows you to browse Kagi seamlessly in Privacy Pass mode. You can also add the shortcut to your home screen for quick access.</p><p>This feature lets you either use Kagi exclusively in Privacy Pass mode or switch effortlessly between modes.</p><h3>Chrome and Firefox browser extensions</h3><p>If you are using the Kagi Privacy Pass extension for <a href=\"https://chromewebstore.google.com/detail/kagi-privacy-pass/mendokngpagmkejfpmeellpppjgbpdaj\">Chrome</a> or <a href=\"https://addons.mozilla.org/firefox/addon/kagi-privacy-pass/\">Firefox</a>, once installed you should see the Kagi Privacy Pass icon on your toolbar.</p><p>Once installed, the extension automatically generates tokens. To use them, click the extension icon, and make sure the toggle is on.</p><p>Note that Safari is not supported at this moment; see the F.A.Q. below for more information.</p><p>As used by Kagi, Privacy Pass tokens offer various security properties (§ 3.3,  of [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2</a>]).</p><p>These can be a little technical to capture. In a few words, they guarantee that users can trust that their searches authenticated via Privacy Pass cannot be linked to their accounts, and Kagi can rest assured that only legitimate users can correctly authenticate using Privacy Pass. Crucially, the guarantee for users is even against malicious servers that attempt to incorrectly implement the server-side computation, as long as the client-side implementation is correct.</p><p>Three of these security properties serve to protect our users:</p><ol><li><p>Generation-redemption unlinkability: Kagi cannot link the tokens presented during token redemption ( during search) with any specific token generation phase. This means that Kagi will not be able to tell who it is serving search results to, only that it is someone who presented a valid Privacy Pass token.</p></li><li><p>Redemption-redemption unlinkability: Kagi cannot link the tokens presented during two different token redemptions. This means that Kagi will not be able to tell from tokens alone whether two searches are being performed by the same user.</p></li><li><p>No redemption hijacking: an eavesdropper that observes any token generation phase, cannot use the observed information alone to “steal” the tokens from the intended user and redeem them themselves. This means that third parties snooping on a user’s token generation interaction will not be able to steal the tokens. This adds a layer of security on top of the confidentiality attained during token generation by using a TLS-protected connection.</p></li></ol><p>Two of these security properties serve to protect Kagi.</p><ol><li><p>Correctness: honestly generated tokens will pass Kagi’s validation.</p></li><li><p>One-more-forgery security: a malicious client cannot use knowledge of a correctly generated token to forge a new one. This means that valid tokens cannot be generated without correctly interacting with Kagi, and therefore valid tokens are evidence that the user owned a valid session cookie for a supported Kagi plan at the moment of generating the token.</p></li></ol><p>Naturally, online interactions are never fully described by a mathematical model.</p><p>While the Privacy Pass protocol  indeed guarantee that the server will not be able to link token generation and token redemption phases , in principle, a malicious server could still attempt to track clients via <a href=\"https://en.wikipedia.org/wiki/Side-channel_attack\">side-channel</a> information.</p><p>For example, if someone were to make the same specific request to a server at the same time every day (say, searching “lunch places near 123 Mulholland Drive, LA” at 11:58 AM), a server that records all searches being made could, in principle, guess that these searches are all made by the same person.</p><p>In this case, Privacy Pass would make it harder for the server to determine who this specific person is, but the server could nonetheless link searches to one another.</p><p>On a level beyond, it is well known that browsers can often have a unique “fingerprint” [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">5-7</a>]. Fingerprinting attacks heavily rely on side-channel signals that evade the Privacy Pass protocol, such as user-agent strings or IP addresses. For example, if a server receives a token generation request from a given IP address, and immediately after a token redemption request from the same address, it can likely conclude that the same individual is behind the request. For this reason, it is <a href=\"https://www.rfc-editor.org/rfc/rfc9576.html#section-4.1\">highly recommended</a> to separate token generation and redemption in time, or “in space” (by using an anonymizing service such as <a href=\"https://www.torproject.org/\">Tor</a> when redeeming tokens, see below).</p><p>Kagi’s Privacy Pass extension and native implementation in Orion take care, as much as we can, to uniform your browser fingerprint, by removing deanonymizing HTTP headers and cookies.</p><p>We see Privacy Pass as an important tool for increasing the anonymity guarantees we can offer to Kagi users.</p><p>Adopting state-of-the-art standards for new privacy enhancing technologies also signals to researchers and standardization bodies that there is a public demand for more privacy and anonymity tools in today’s digital world and incentivizes further scrutiny and development of privacy-enhancing technologies.</p><p>Together with launching Privacy Pass, we are also announcing that we now have a Tor onion service available, which allows access to Kagi directly from the Tor network. Kagi’s onion address is:</p><p>On its own, Tor will obscure your location by hiding your IP address. However, without Privacy Pass, you still need to be logged into your Kagi account to perform searches, making them all theoretically linkable back to a single account. As always, Kagi does not link searches to accounts or permanently record them; see <a href=\"https://kagi.com/privacy\">our Privacy Policy</a> for more info.</p><p>With Tor and Privacy Pass together, Kagi only knows that the search is being issued by a user who previously verified that they have an account authorized to receive tokens, but nothing about the user’s account, or where they’re located.</p><p>Privacy Pass support is provided:</p><p>This should accommodate users who want to install and use the extension across multiple browsers or computers. Please refer to our documentation for <a href=\"https://help.kagi.com/kagi/privacy/privacy-pass.html\">usage instructions</a>.</p><p>At first, Privacy Pass authentication will be available to users on any Kagi plan with unlimited searches. These plans will have a generous allocation of tokens (2000 to begin with) that they can generate monthly.</p><p>We are working on enabling this feature for Trial and Starter plans, which have access to a limited number of monthly searches. Therefore, they risk a worse user experience if their generated tokens are lost (for example, due to uninstalling the extension) and theoretically, users on this plan could redeem more tokens than the limit of searches allowed on their plan (again, we do not know who the user redeeming the tokens is, or what plan they are on). This makes it more technically challenging to support these plans with Privacy Pass, and we have left that for later.</p><p><strong>You mention “tokens.” Are blockchains involved in this protocol?</strong></p><p>Privacy Pass does not rely on any blockchain technology.</p><p>While the protocol makes use of various cryptographic primitives (specifically, elliptic curves and hash functions, as part of a “verifiable oblivious pseudorandom function” construction, [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">8</a>]) and generates “tokens,” these are not generated, stored, or traded on a blockchain.</p><p><strong>You mention the client generating tokens. Is this process energy-intensive or storage-demanding?</strong></p><p>No. The generation of 500 search tokens requires approximately 1 second of computation on a consumer laptop, and is performed in the background when installing the extension. A few extra seconds may be required due to the time required to contact the server and get a response. Each token consists of 216 bytes, for a total of approximately 100 KiB of storage per token generation request.</p><p><strong>Is there a potential impact on the speed of search when using Privacy Pass?</strong></p><p>The initial generation of tokens takes about ~1 second for 500 tokens, plus the time required for contacting the server. This occurs infrequently and is done in the background when possible.</p><p>Currently, the token validation servers are only deployed in our us-central1 region, we plan to expand this shortly after launch.</p><p><strong>How many tokens am I able to generate?</strong></p><p>You can generate 2,000 tokens in one “epoch” (= one month). This should be enough for most users. If you need more than this, you can request additional tokens by contacting support@kagi.com.</p><p><strong>Do you plan to allow purchasing privacy pass tokens without having an account?</strong></p><p>Yes, this makes sense. This is possible because technically the extension does not care if you have an account or not. It just needs to be ‘loaded’ with valid tokens. And you can imagine a mechanism where you could also anonymously purchase them, eg. with monero, without ever creating an account at Kagi.  Let us know <a href=\"https://kagifeedback.org/d/6163-kagi-privacy-pass\">here</a> if you are excited about this, as it will help prioritize it.</p><p><strong>How can I submit feedback for Kagi Privacy Pass?</strong></p><p>We have a feedback thread open <a href=\"https://kagifeedback.org/d/6163-kagi-privacy-pass\">here</a>.</p><p><strong>Even if the extension implements anti-fingerprinting measures, Kagi will still be able to see my IP address, correct?</strong></p><p>Even with Privacy Pass authentication enabled, due to the way the TCP/IP stack works, we will be able to see your search request come from an IP address. As outlined in our <a href=\"https://kagi.com/privacy\">Privacy Policy</a>, your privacy is our priority, whether you are using Privacy Pass to authenticate or otherwise. If you are worried about us seeing your IP address, our suggestion is to connect to Kagi via Tor or through a VPN service you trust.</p><p><strong>How can Privacy Pass increase my privacy, if I have to send a session cookie to authenticate during token generation?</strong></p><p>While token generation is indeed not anonymous, Privacy Pass provides you with anonymity .</p><p>By providing the server with a Privacy Pass token instead of a session cookie when searching, you will guarantee that your searches cannot be  to any specific user account that generated Privacy Pass tokens, or to each other.</p><p>From the point of view of the server, your search query could have come from any of the users who previously generated Privacy Pass tokens.</p><p>The more users do so, the lower the probability that the server can guess it was you specifically who made a given search query.</p><p><strong>Token generation does not work in my Chrome/Firefox private window</strong></p><p>Correct, we need to authenticate you to create tokens (see above), and in the private window, the extension does not have access to your session cookie. Please use a normal browsing window while logged in to Kagi to generate tokens.</p><p>Note, generating tokens while in a private window will work in the <a href=\"https://kagi.com/orion\">Orion browser</a>.</p><p><strong>Can I use Kagi Assistant while using Privacy Pass?</strong></p><p>Not at this time, since Kagi Assistant is only available to Ultimate members. In Privacy Pass, we don’t have any account information, so we can’t validate what plan you’re on. We could issue tokens attached to different keys for different plans, but that also has privacy implications, see the discussion of personalization below.</p><p><strong>What Kagi services will be compatible with Privacy Pass at launch?</strong></p><p>At launch, Privacy Pass will only be used to authenticate Kagi Search. Soon to follow (in the next few weeks), we plan to expand support for Kagi Privacy Pass to:</p><ul><li>Kagi Translate and Kagi Maps</li><li>Kagi universal Summarizer and Ask questions about page</li></ul><p>Please disable Privacy pass to access these services for now.</p><p><strong>Since initial token generation happens in batches and the tokens expire, could tokens with similar expiration dates potentially be used to identify multiple searches from the same user?</strong></p><p>All tokens generated during month X expire at midnight of the first day of month X+2, to avoid this exact issue. Meaning a freshly generated token lasts until the end of the month following its generation (generate today, use all of Feb and March).</p><p><strong>If Kagi cannot track who exactly is performing search queries, will I have access to my account settings including customization and personalization?</strong></p><p>Since Kagi will not know who you are, we will not be able to serve you content tailored to your custom settings via Privacy Pass-protected search.</p><p>We have considered allowing users to send a small configuration with every request <code>(language, region, safe-search)</code> to automatically customize your search experience to some extent. However, we currently believe this would quickly result in a significant loss of anonymity for you and for other users. To illustrate this, we have examined the most common configurations of <code>(language, region, search-safe)</code> used on Kagi.com, and extrapolated how many Privacy Pass users would share them. Looking at the top 35 configurations, we see the following approximate numbers.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Limiting the analysis to only the ten most common language settings, the effect is similar:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>This would mean that someone sending us  as their language configuration would automatically lose redemption-redemption unlinkability guarantees if approximately 1000 Kagi users used Privacy Pass.</p><p>While our extrapolation may be overly conservative, we won’t be enabling this level of “default” customization for users authenticating via Privacy Pass for the time being. We could reconsider if we find a better solution.</p><p>For manual search settings customization, you can always use bangs in your search query to enable basic settings for a specific query. For example, <a href=\"https://help.kagi.com/kagi/features/bangs.html#regional-bangs\">regional bangs</a> will let you focus your query on one region. For example prefixing your search with  will automatically search in the German region.</p><p>To access a fully customized search experience, you can always use the traditional login method and disable the use of Privacy Pass.</p><p><strong>Will Safari be supported?</strong></p><p>The Safari extensions API doesn’t support (as far as we know) removing cookies from requests, which means it will always authenticate with your logged-in account. We’re not aware of a way to change this. The alternative if you want a similar, native, WebKit-based browsing experience, is to use the <a href=\"https://kagi.com/orion\">Orion Browser</a> which has Kagi Privacy Pass natively integrated.</p><ol><li>Davidson, A., Goldberg, I., Sullivan, N., Tankersley, G., &amp; Valsorda, F. (2018). Privacy pass: Bypassing internet challenges anonymously. Proceedings on Privacy Enhancing Technologies. <a href=\"https://doi.org/10.1515/popets-2018-0026\">Paper</a>.</li><li>Davidson, A., Iyengar, J., &amp; A. Wood, C. (2024). The Privacy Pass Architecture. <a href=\"https://www.rfc-editor.org/rfc/rfc9576.html\">RFC 9576</a>.</li><li>Pauly, T., Valdez, S., &amp; A. Wood, C. (2024). The Privacy Pass HTTP Authentication Scheme. <a href=\"https://www.rfc-editor.org/rfc/rfc9577.html\">RFC 9577</a>.</li><li>Celi, S., Davidson, A., Valdez, S., &amp; Wood, C. A. (2024). Privacy Pass Issuance Protocols. <a href=\"https://www.rfc-editor.org/rfc/rfc9578.html\">RFC 9578</a>.</li><li>Eckersley, P. (2010). How unique is your web browser? Proceedings on Privacy Enhancing Technologies. <a href=\"https://dl.acm.org/doi/10.5555/1881151.1881152\">Paper</a>.</li><li>Davidson, A., Faz-Hernandez, A., Sullivan, N., &amp; A. Wood, C. (2023). Oblivious Pseudorandom Functions (OPRFs) Using Prime-Order Groups. <a href=\"https://www.rfc-editor.org/rfc/rfc9497.html\">RFC 9497</a>.</li></ol><blockquote><p>Do you like how this post reads? It was proofread with <a href=\"https://translate.kagi.com\">Kagi Translate’s</a> proofreading option. To proofread any web page, just use <code>https://translate.kagi.com/proofread/[URL]</code>.</p></blockquote>","contentLength":21275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43040521"},{"title":"Phind 2: AI search with visual answers and multi-step reasoning","url":"https://www.phind.com/blog/phind-2","date":1739470829,"author":"rushingcreek","guid":245,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43039308"},{"title":"LibreOffice still kicking at 40, now with browser tricks and real-time collab","url":"https://www.theregister.com/2025/02/13/libreoffice_wasm_zetaoffice/","date":1739465983,"author":"LinuxBender","guid":244,"unread":true,"content":"<p> LibreOffice is a big, mature chunk of code now, but that doesn't make it impossible to teach it impressive new tricks. Some of them could make it more important than ever.</p><p>The open-source office suite had its own <a target=\"_blank\" href=\"https://fosdem.org/2025/schedule/track/libreoffice/\" rel=\"nofollow\">program stream</a> at FOSDEM, including the <a target=\"_blank\" href=\"https://fosdempwa.com/event/5735?year=2025&amp;day=null&amp;test=false\" rel=\"nofollow\">pre-announcement</a> of the new <a target=\"_blank\" href=\"https://wiki.documentfoundation.org/ReleaseNotes/25.2\" rel=\"nofollow\">LibreOffice release 25.2</a>. It has been around in some form since 1985, so this version marks its 40th year. It's middle-aged and, almost inevitably, that means it's big, a bit saggy in places, it definitely has some issues, and it doesn't look as good as it did a couple of decades ago.</p><p>But that doesn't mean it can't be taught interesting things. Even a 40-year-old can acquire new skills and take on entirely new roles.</p><p>It's the same sort of functionality that you get from Google Docs, and indeed this is already possible using the <a target=\"_blank\" href=\"https://www.theregister.com/2022/07/01/collabora_releases_code_2205_webbased/\">Collabora Online web-based version of LibreOffice</a>. The big difference is that such tools run in a browser, so you need to be online. What makes the CRDT implementation different is that this is a local app, working on a local file, but using a network copy to keep changes in sync. The idea is to free you from keeping your apps and data on someone else's computer, without losing the handy collaborative features that web apps bring.</p><p>We also met up with Thorsten for a demo of one of his other babies, <a target=\"_blank\" href=\"https://zetaoffice.net/\" rel=\"nofollow\">ZetaOffice</a>. This is a version of LibreOffice built for the Wasm runtime, which means it can run inside a browser. He demonstrated it to us on an AMD Ryzen-powered developer-spec ThinkPad, but the same binaries could run on any OS and on any CPU, such as ARM hardware. ZetaOffice went into public beta <a target=\"_blank\" href=\"https://blog.allotropia.de/2024/11/08/launching-zetaoffice/\" rel=\"nofollow\">last November</a>, and a first release is looming very soon.</p><p>He showed us it running in multiple modes. You can embed any of the LibreOffice apps into a web page, complete with their normal user interface and so on, much like Google Apps. This enables a website to offer a very rich editing experience of text, spreadsheets, presentations, or anything else. ZetaOffice can also be embedded, scripted, and controlled using the <a target=\"_blank\" href=\"https://github.com/allotropia/zetajs\" rel=\"nofollow\">zetajs wrapper</a>, so you can call it to display embedded rich objects in web pages. We were rather taken by a demo that displayed a moving line chart of ping times to the  website. A script was extracting the times from the ping command, adding lines to a LibreOffice Calc spreadsheet, graphing the last dozen or so lines and embedding the graph in a web page.</p><p>He told us that the Allotropia development team sees ZetaOffice as orthogonal to Collabora Office (or COOL for short). COOL runs in the browser and has simultaneous multiuser editing. ZetaOffice runs in the browser too, but it's perfectly able to run locally and doesn't need an uplink. For now, though, it's single user. The company will make a cross-platform version that is also available for local installation.</p><p>The real power comes from the scriptability and integration with JavaScript, though. We are sure lots of people will find uses for the ability to embed any document LibreOffice can open into any web page, control and automate it from JavaScript, with the choice of a full local user interface – or none, just the content, controlled with a JavaScript UI instead. The code is all under the MIT licence, so it's all-FOSS, and all the work is being upstreamed to LibreOffice itself.</p><p>This is big stuff in more than one way. LibreOffice isn't lightweight anyway. Opening a document in a web page can pull in a gigabyte or so of code, and the memory footprint is a bit more than that. It is still in beta, though, and the team is working hard on modularizing the code into multiple smaller chunks so that it will load much faster and take less memory.</p><p>The potential flexibility here&nbsp;is impressive, though. You could have a portable app on a USB key that was usable on a PC or Mac or Chromebook, regardless of the OS, without installation. Web apps suddenly acquire full rich cross-platform document handling and editing, including, of course, all the main Microsoft files and formats.</p>","contentLength":3989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43038200"},{"title":"Resigning as Asahi Linux project lead","url":"https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/","date":1739460897,"author":"Shank","guid":243,"unread":true,"content":"<p>Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them “jailbreaks” these days) to allow users to run their own unofficial apps on the Nintendo Wii.</p><p>I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other people’s work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didn’t attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.</p><p>When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.</p><p>I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.</p><p>Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which I’ve already spoken at length about and I won’t repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience.</p><p>But then also came the entitled users. This time, it wasn’t about stealing games, it was about features. “When is Thunderbolt coming?” “Asahi is useless to me until I can use monitors over USB-C” “The battery life sucks compared to macOS” (nobody ever complained when compared to x86 laptops…) “I can’t even check my CPU temperature” (yes, I seriously got that one).</p><p>And, of course, “When is M3/M4 support coming?”</p><p>For a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were “alpha” and “unstable” and “not suitable for a daily driver” (despite <a href=\"https://stats.asahilinux.org\">thousands of users</a>, myself included, daily driving it and even using it for servers).</p><p>No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.</p><p>I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.</p><p>Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I won’t go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).</p><p>I did make some progress in 2024, but this left me in a very vulnerable position. I hadn’t gotten nearly as much Asahi work done as I’d liked, and the users weren’t getting any quieter about demanding more features and machine support.</p><p>We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).</p><p>I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. “Full” DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasn’t the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did <a href=\"https://lore.kernel.org/lkml/fda8b831-1ffc-4087-8e7b-d97779b3ecc5@marcan.st/T/#u\">try</a> to bring the subject up on the mailing lists, but it didn’t get much response).</p><p>The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I won’t repeat them in detail here. Suffice it to say, I consider Linus’ handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.</p><p>As you know, this is deeply personal to me, as we’ve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.</p><p>Some might be wondering why we can’t just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But there’s more.</p><p>In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (that’s how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a <a href=\"https://pagure.io/fedora-asahi/mesa-asahi-flatpak\">partial workaround</a> for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.</p><p>I’m not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated <a href=\"https://lore.kernel.org/lkml/20250208204416.GL1130956@mit.edu/\">“We\nare the ‘thin blue line’”</a>, and nobody cared, which just further confirmed to me that I don’t want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to <a href=\"https://lore.kernel.org/lkml/20240828211117.9422-1-wedsonaf@gmail.com/\">quit</a>.</p><p>But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than <a href=\"https://bunny.net/\">bunny.net</a> giving us free CDN credits for the hosting).</p><p>I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say I’m the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they won’t touch the Linux kernel with a 10-foot pole.</p><p>When Apple released the M1, Linus Torvalds <a href=\"https://thenextweb.com/news/linus-torvalds-wants-apples-new-m1-powered-macs-to-run-linux\">wished it could run Linux</a>, but didn’t have much hope it would ever happen. We made it happen, and Linux 5.19 was <a href=\"https://lore.kernel.org/lkml/CAHk-=wgrz5BBk=rCz7W28Fj_o02s0Xi0OEQ3H1uQgOdFvHgx0w@mail.gmail.com/T/#u\">released from an M2 MacBook Air running Asahi Linux</a>. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him <a href=\"https://gist.github.com/marcan/fe70ee6648f3d5ae94eb8332265b8d95\">an invitation</a> to discuss the challenges of kernel contributions and maintenace and see how we could help. He never replied.</p><p>Back in 2011, Con Kolivas <a href=\"https://web.archive.org/web/20110707151924/http://apcmag.com/why_i_quit_kernel_developer_con_kolivas.htm\">left the Linux kernel community</a>. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if you’re not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.</p><p>When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but it’s not fun any more. I have an M3 Pro in a box and I haven’t even turned it on yet. I dread doing the bring-up work. It doesn’t feel worth the trouble.</p><p>I miss having free time where I can relax and not worry about the features we haven’t shipped yet. I miss <a href=\"https://youtube.com/@TsuiokuCircuit\">making music</a>. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we haven’t upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.</p><p>I’m resigning as lead of the Asahi Linux project, effective immediately. The project will <a href=\"https://asahilinux.org/2025/02/passing-the-torch/\">continue on without me</a>, and I’m working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the <a href=\"https://opencollective.com/asahilinux\">Asahi Linux OpenCollective</a> (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).</p><p>I want to thank the entire Asahi Linux team, without whom I would’ve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.</p><p>If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: <a href=\"mailto:marcan@marcan.st\">marcan@marcan.st</a>.</p><p>: A lot of the discussion around this post and the interactions that led to it brings up the term “brigading”. Please read <a href=\"https://hachyderm.io/@chandlerc/114001000657957325\">this excellent Fedi post</a> for a discussion of what is and isn’t brigading.</p>","contentLength":14863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43036904"},{"title":"Why young parents should focus on building trust with their kids","url":"https://desunit.com/blog/marshmallow-test-and-parenting/","date":1739430767,"author":"desunit","guid":242,"unread":true,"content":"<p>Today, when my 1.9-year-old daughter tried to grab and eat an unwashed orange, I asked her to give it to me so I could wash and peel it if she could wait for a moment. She did. It got me thinking about the marshmallow experiment, where the idea was that patience equals success. But … it wasn’t the whole story.</p><p>The marshmallow experiment is famous: a little kid in a room staring at a marshmallow. If they wait 15 minutes, they get two marshmallows instead of just one. Some kids would poke the marshmallow, lick it, or just gobble it up. Others found clever ways to distract themselves – singing, closing their eyes, even falling asleep. The results – children who waited supposedly went on to achieve higher scores in school and better life outcomes. The message was clear: if you can delay gratification, you’re set for life.&nbsp;But later studies revealed some serious holes in that conclusion.</p><p>First of all, it’s not just about willpower. A follow-up study showed that kids from stable, reliable homes were much more likely to wait than kids from unpredictable ones. If you’re a kid and the adults in your life constantly break promises, why would you trust them this time? Why wait for the second marshmallow if history tells you it might not show up? Waiting isn’t a character trait; it’s a strategy. And strategies are shaped by experience.</p><p>Kids are like tiny detectives, watching and learning from every single thing we do. My toddler loves routines – the same bedtime story, the same morning/evening ritual. He thrives on knowing what’s next. So when I tell her to wait and then actually deliver, I’m teaching her something important: trust. But if I say “five more minutes” and don’t follow through, I’m sending a different message – that waiting might not be worth it.</p><p>Back to the marshmallow test. Another big revelation came when researchers looked at economic backgrounds. Kids from wealthier families waited longer than kids from low-income families. Not because they had more self-control, but because their environment made waiting feel safer. If you grow up knowing there will always be food on the table, waiting for an extra marshmallow isn’t a big deal. But if your life is more uncertain, grabbing what you can when you can make total sense.&nbsp;It’s a survival skill, not a lack of discipline.</p><p>This reminds me of a moment last week. My daughter asked me to swing her in my arms, but I was busy and promised her we’d do it once I finished my work. She switched to another task and waited. When I finally asked, “Would you like to have fun now?” she cheerfully screamed, “Yes!” Would she have trusted me if I forgot about it? Probably not.</p><p>Kids crave predictability. That’s why they ask for the same bedtime story 300 nights in a row. It’s why they get upset if the routine changes without warning. Predictability equals safety. And safety builds trust. When they trust us, they’re willing to wait, to try, to take risks.</p><p>So, here’s where the marshmallow test gets it wrong: it’s not just about the kid’s ability to wait; it’s about the adult’s ability to create an environment worth waiting in. If we, as parents, and teachers, aren’t consistent, why would a kid believe us when we say, “Just hold on, something better is coming”?</p><p>Another thing I’ve noticed is how much modeling matters. My daughter watches everything I do. If I tell her to wait and then lose my patience two seconds later because the internet is slow, what’s the lesson there? Waiting is for other people? The best way to teach patience is to live it – easier said than done …</p><p>The marshmallow test also doesn’t account for cultural differences. In some cultures, waiting is baked into daily life. Think about Japan, where kids are often taught to wait quietly for meals or gifts. Compare that to the US, where instant gratification is practically a way of life. These cultural norms shape how kids approach situations like the marshmallow test. It’s not just about personality; it’s about the world they live in.</p><p>In 2012, researchers added a modification to the marshmallow experiment. Before the test, kids were split into two groups. One group experienced a broken promise (they were told they’d get crayons to draw with, but never did). The other group experienced a kept promise (they actually got the crayons). Then came the marshmallow test. Which group waited longer? The kids who saw promises kept. The trust made all the difference.</p><p>So what’s the takeaway here? It’s simple, really: as parents, we set the tone. Our actions, promises, and reliability shape how our kids see the world. Are we building an environment where they feel safe enough to wait? Or are we teaching them that they need to grab what they can, when they can?</p><p>The marshmallow test highlights how our actions shape our kids’ lives. Small gestures – like keeping promises, sticking to “just five more minutes,” or reading bedtime stories – build trust and security. These moments might even help them wait for their own second marshmallow – or decide they don’t need to. Ultimately, it’s about the environment we are creating for them. And who knows, maybe one day they will pass their own version of the marshmallow test – not because they have to, but because they know they can.</p>","contentLength":5341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43033463"},{"title":"Leaking the email of any YouTube user for $10k","url":"https://brutecat.com/articles/leaking-youtube-emails","date":1739359182,"author":"brutecat","guid":241,"unread":true,"content":"<a href=\"https://brutecat.com/\" data-svelte-h=\"svelte-5sfkfb\">&lt; Back</a><h2>Leaking the email of any YouTube user for $10,000</h2><img src=\"https://brutecat.com/assets/youtube-email-disclosure.png\" alt=\"\"><p>Some time ago, I was looking for a research target in Google and was digging through the <a href=\"https://staging-people-pa.sandbox.googleapis.com/$discovery/rest?key=AIzaSyBOh-LSTdP2ddSgqPk6ceLEKTb8viTIvdw\">Internal People API (Staging)</a> discovery document until I noticed something interesting:</p><pre><code></code></pre><p>It seemed the Google-wide block user functionality was based on an obfuscated Gaia ID as well as a display name for that blocked user. The obfuscated Gaia ID is just a Google account identifier.‎</p><p>So, if you block someone on YouTube, you can leak their Google account identifier? I tested it out. I went to a random livestream, blocked a user and sure enough, it showed up in <a href=\"https://myaccount.google.com/blocklist\">https://myaccount.google.com/blocklist</a>‎</p><p>The fallback name was set as their channel name  and the profile ID was their obfuscated Gaia ID ‎</p><p>This was super strange to me because YouTube should never leak the underlying Google account of a YouTube channel. In the past, there's been several bugs to <a href=\"https://sector035.nl/articles/2022-35\">resolve these to an email address</a>, so I was confident there was still a Gaia ID to Email in some old obscure Google product.</p><h3>\n              Escalating this to 4 billion YouTube channels\n            </h3><p>So, we can leak the Gaia ID of any live chat user, but can we escalate this to all channels on YouTube? As it turns out, when you click the 3 dots just to open the context menu, a request is fired:‎</p><pre><code>www.youtube.com\n&lt;redacted&gt;</code></pre><pre><code> OK\napplication/json; charset=UTF-8\nscaffolding on HTTPServer2\n\n</code></pre><p>That  is nothing more than just base64 encoded protobuf, which is a common encoding format used throughout Google.‎</p><p>If we try decoding that  params:</p><pre><code>$ echo -n \"Q2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZMUFBV0FGaUx3b1ZNVEV6T1RBM05EWTJOVE0zTmpjd016Y3dOVGt3RWhaVFJTMWhXVTlpTFhWRFp6QTFPWEZJVW1selgyOTNjQUElM0Q=\" | base64\n -d | sed 's/%3D/=/g' | base64 -d | protoc --decode_raw\n1 {\n  5 {\n    1: \"UChs0pSaEoNLV4mevBFGaoKA\"\n    2: \"36YnV9STBqc\"\n  }\n}\n10: 0\n11: 1\n12 {\n  1: \"113907466537670370590\"\n  2: \"SE-aYOb-uCg059qHRis_ow\"\n}\n14: 0</code></pre><p>It actually just contains the Gaia ID of the user we want to block, we don't even need to block them!</p><p>Let's check out the  requests params too:</p><pre><code>$ echo -n \"R2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZeklhQ2hoVlExTkZMV0ZaVDJJdGRVTm5NRFU1Y1VoU2FYTmZiM2M9\" | base64 -d | sed 's/%3D/=/g' | base64 -d | protoc --decode_raw\n3 {\n  5 {\n    1: \"UChs0pSaEoNLV4mevBFGaoKA\"\n    2: \"36YnV9STBqc\"\n  }\n}\n6 {\n  1: \"UCSE-aYOb-uCg059qHRis_ow\"\n}</code></pre><p>Seems to just contain the channel ID of the channel we're blocking, the livestream video ID and livestream author ID. Let's try to fake the request params with our own target's channel ID.‎</p><pre><code>$  -n  |  -d | sed  |  -d | sed  |  | \nR2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZeklhQ2hoVlEwUXlURnBCVkRGcQpNVVI1VmxoeE1sSXlRbVIxYzFFPQo=</code></pre><p>Testing this on <code>/youtubei/v1/live_chat/get_item_context_menu</code>:</p><pre><code>...\n\"moderateLiveChatEndpoint\":{\"params\":\"Q2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZMUFBV0FGaUx3b1ZNVEF6TWpZeE9UYzBNakl4T0RJNU9Ea3lNVFkzRWhaRU1reGFRVlF4YWpGRWVWWlljVEpTTWtKa2RYTlJjQUElM0Q=\"}\n...</code></pre><pre><code>echo -n \"Q2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZMUFBV0FGaUx3b1ZNVEF6TWpZeE9UYzBNakl4T0RJNU9Ea3lNVFkzRWhaRU1reGFRVlF4YWpGRWVWWlljVEpTTWtKa2RYTlJjQUElM0Q=\" | base64 -d | sed 's/%3D/=/g' | base64 -d | protoc --decode_raw\n1 {\n  5 {\n    1: \"UChs0pSaEoNLV4mevBFGaoKA\"\n    2: \"36YnV9STBqc\"\n  }\n}\n10: 0\n11: 1\n12 {\n  1: \"103261974221829892167\"\n  2: \"D2LZAT1j1DyVXq2R2BdusQ\"\n}\n14: 0</code></pre><p>We can leak the Gaia ID of the channel - </p><h3>\n              The missing puzzle piece: Pixel Recorder\n            </h3><p>I told my friend <a href=\"https://schizo.org\">nathan</a> about the YouTube Gaia ID leak and we started looking into old forgotten Google products since they probably contained some bug or logic flaw to resolve a Gaia ID to an email. <a href=\"https://recorder.google.com\">Pixel Recorder</a> was one of them. Nathan made a test recording on his Pixel phone and synced it to his Google account so we could access the endpoints on the web at <a href=\"https://recorder.google.com\">https://recorder.google.com</a>:‎</p><p>When we tried sharing the recording to a test email, that's when it hit us:</p><pre><code>pixelrecorder-pa.clients6.google.com\n&lt;redacted&gt;\n80\n&lt;redacted&gt;\nAIzaSyCqafaaFzCP07GzWUSRw0oXErxSlrEX2Ro\napplication/json+protobuf\nhttps://recorder.google.com/\n\n</code></pre><pre><code> OK\napplication/json+protobuf; charset=UTF-8\nESF\n138\n\n</code></pre><p>This endpoint was taking in the obfuscated Gaia ID and... ‎</p><p>We tested this with the obfuscated Gaia ID  we got from blocking that user on YouTube a while back and :</p><pre><code> OK\napplication/json+protobuf; charset=UTF-8\nESF\n138\n\n</code></pre><h3>\n              A small problem: preventing notification to the target\n            </h3><p>It seems that whenever we share a recording with a victim, they receive an email that looks like this:‎</p><p>This is , and it would lower the impact of the bug quite a lot. On the share pop-up, there didn't seem to be any option to disable notifications.</p><p>I tried leaking the full request proto via my tool <a href=\"https://github.com/ddd/googleapi_tools/tree/main/req2proto\">req2proto</a>, but there was nothing about disabling the email notification:</p><pre><code>syntax = ;\n\n java.com.google.wireless.android.pixel.recorder.protos;\n\n;\n\n {\n   recording_id = ;\n   delete_obfuscated_gaia_ids = ;\n  ShareUser update_shared_users = ;\n   sharing_message = ;\n}\n\n {\n   obfuscated_gaia_id = ;\n  java.com.google.wireless.android.pixel.recorder.sharedclient.acl.protos.ResourceAccessRole role = ;\n   email = ;\n}</code></pre><p>Even trying to add and remove the user at the same time didn't work, the email was still sent. But that's when we realized - if it's including our recording title in the email subject, perhaps it wouldn't be able to send an email if our recording title was too long.‎</p><p>We hacked together a quick python script to test this out:</p><pre><code> requests\n\nBASE_URL = \n\nheaders = {\n    : ,\n    : ,\n    : ,\n    : \n}\n\n():\n    payload = \n    response = requests.post(BASE_URL +  + , headers=headers, data=payload)\n     response.status_code != :\n        (, response.json())\n        exit()\n    :\n        response = response.json()\n    :\n        (, response.text)\n        exit()\n\n     response[][]\n\n():\n    x = *\n    payload = \n    response = requests.post(BASE_URL +  + , headers=headers, data=payload)\n     response.status_code != :\n        (, response.json())\n        exit()\n\n():\n    share_id = ()\n    headers[] = ( )\n    headers[] = ()\n    uuid = get_recording_uuid(share_id)\n    (, uuid)\n    update_recording_title(uuid)\n    ()\n\n __name__ == :\n    main()</code></pre><p>... and the recording title was now <strong>2.5 million letters long!</strong> There wasn't any server-side limit to the length of a recording name.‎</p><p>Trying to share the recording with a different test user...  No notification email.‎</p><p>We basically have the full attack chain, we just have to put it together.‎</p><ol><li>Leak the obfuscated Gaia ID of the YouTube channel from the Innertube endpoint </li><li>Share the Pixel recording with an extremely long name with the target to convert the Gaia ID to an email</li><li>Remove the target from the Pixel recording (cleanup)‎</li></ol><p>Here's a POC of the exploit in action:</p><ul><li>15/09/24 - Report sent to vendor</li><li>16/09/24 - Vendor triaged report</li><li>03/10/24 - Panel marks it as duplicate of existing-tracked bug, does botched patch of initial YouTube obfuscated Gaia ID disclosure</li><li>03/10/24 - Clarified to vendor that they haven't recognized Pixel recorder as vulnerability itself (since obfuscated Gaia IDs are leaked for Google Maps/Play reviewers) and provided vendor a work-around method to once again leak YouTube channel obfuscated Gaia IDs</li><li>05/11/24 -  Rationale: Exploitation likelihood is medium. Issue qualified as an abuse-related methodology with high impact.</li><li>03/12/24 - Product team sent report back to panel for additional reward consideration, coordinates disclosure for 03/02/2025</li><li>12/12/24 - <strong>Panel awards an additional $7,500.</strong> Rationale: Exploitation likelihood is high. Issue qualified as an abuse-related methodology with high impact. Applied 1 downgrade from the base amount due to complexity of attack chain required.</li><li>29/01/25 - Vendor requests extension for disclosure to 12/02/2025</li><li>09/02/25 - Confirm to vendor that both parts of the exploit have been fixed (T+147 days since disclosure)</li><li>12/02/25 - Report disclosed</li></ul><p data-svelte-h=\"svelte-uqqsu8\">You can contact me via\n\t\t\t\t<a href=\"https://signal.me/#eu/oT3XakzbgN6x5fLe37yBwfssy4KVje3aqZyCVArWxofNQ7FbnOdHfpfs3sn6Lfn6\"><img src=\"data:image/svg+xml,%3c!DOCTYPE%20svg%20PUBLIC%20'-//W3C//DTD%20SVG%201.1//EN'%20'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'%3e%3c!--%20Uploaded%20to:%20SVG%20Repo,%20www.svgrepo.com,%20Transformed%20by:%20SVG%20Repo%20Mixer%20Tools%20--%3e%3csvg%20width='800px'%20height='800px'%20viewBox='0%200%2048%2048'%20id='Layer_2'%20data-name='Layer%202'%20xmlns='http://www.w3.org/2000/svg'%20fill='%23ffffff'%20stroke='%23ffffff'%3e%3cg%20id='SVGRepo_bgCarrier'%20stroke-width='0'/%3e%3cg%20id='SVGRepo_tracerCarrier'%20stroke-linecap='round'%20stroke-linejoin='round'/%3e%3cg%20id='SVGRepo_iconCarrier'%3e%3cdefs%3e%3cstyle%3e.cls-1{fill:none;stroke:%23ffffff;stroke-linejoin:round;}%3c/style%3e%3c/defs%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M27.32,45.25A23.08,23.08,0,0,1,24,45.5a22.26,22.26,0,0,1-3.26-.25m14.44-2.88a21,21,0,0,1-6.08,2.51M41.36,36.7a21.63,21.63,0,0,1-4.66,4.65m5.65-6.16,2.54-6.08a21.25,21.25,0,0,1-2.52,6.07m2.88-14.42A23.33,23.33,0,0,1,45.5,24a22.43,22.43,0,0,1-.25,3.28m-42.46,0A22.46,22.46,0,0,1,2.5,24a22.43,22.43,0,0,1,.25-3.28m39.63-7.89a21.7,21.7,0,0,1,2.51,6.08m-41.69,0a21.19,21.19,0,0,1,2.52-6.06h0m31-6.2a21.19,21.19,0,0,1,4.66,4.65m-34.71,0A21.63,21.63,0,0,1,11.3,6.64M29.09,3.1a21.57,21.57,0,0,1,6.07,2.53h0m-22.33,0a21.07,21.07,0,0,1,6.09-2.51m1.84-.37A21.88,21.88,0,0,1,24,2.5a22.29,22.29,0,0,1,3.27.25'/%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M18.87,44.87a20.92,20.92,0,0,1-5-1.95l-2.24.51'/%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M4.61,36.38l.51-2.2a21.7,21.7,0,0,1-2-5'/%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M10,43.85l-4.08,1a2.19,2.19,0,0,1-2.66-1.56,2.27,2.27,0,0,1,0-1.1l1-4.08'/%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M24,6.41a17.59,17.59,0,0,0-14.83,27l-1.65,7.1,7.16-1.64A17.59,17.59,0,1,0,24,6.41Z'/%3e%3c/g%3e%3c/svg%3e\" alt=\"signal icon\"></a>\n\t\t\t\tor\n\t\t\t\t<a href=\"https://brutecat.com/cdn-cgi/l/email-protection#b1c3dedec5f1d3c3c4c5d4d2d0c59fd2dedc\"><img src=\"data:image/svg+xml,%3c!DOCTYPE%20svg%20PUBLIC%20'-//W3C//DTD%20SVG%201.1//EN'%20'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'%3e%3c!--%20Uploaded%20to:%20SVG%20Repo,%20www.svgrepo.com,%20Transformed%20by:%20SVG%20Repo%20Mixer%20Tools%20--%3e%3csvg%20width='800px'%20height='800px'%20viewBox='0%200%2024%2024'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cg%20id='SVGRepo_bgCarrier'%20stroke-width='0'/%3e%3cg%20id='SVGRepo_tracerCarrier'%20stroke-linecap='round'%20stroke-linejoin='round'/%3e%3cg%20id='SVGRepo_iconCarrier'%3e%3cpath%20d='M4%207.00005L10.2%2011.65C11.2667%2012.45%2012.7333%2012.45%2013.8%2011.65L20%207'%20stroke='%23ffffff'%20stroke-width='1'%20stroke-linecap='round'%20stroke-linejoin='round'/%3e%3crect%20x='3'%20y='5'%20width='18'%20height='14'%20rx='2'%20stroke='%23ffffff'%20stroke-width='1'%20stroke-linecap='round'/%3e%3c/g%3e%3c/svg%3e\" alt=\"email icon\"></a></p>","contentLength":8057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43024221"},{"title":"US and UK refuse to sign AI safety declaration at summit","url":"https://arstechnica.com/ai/2025/02/us-and-uk-refuse-to-sign-ai-safety-declaration-at-summit/","date":1739352809,"author":"miohtama","guid":240,"unread":true,"content":"<p>On Tuesday, Vance told the assembled leaders the US would not relinquish its lead in AI, while also warning countries not to sign AI deals with “authoritarian regimes,” in a thinly veiled jab against China.</p><p>“Partnering with them means chaining your nation to an authoritarian master that seeks to infiltrate, dig in and seize your information infrastructure,” Vance said, referring to CCTV and 5G as previous examples where “cheap tech... [was] heavily subsidized and exported by authoritarian regimes.”</p><p>Among the priorities set out in the joint declaration signed by countries including China, India, and Germany was “reinforcing international co-operation to promote co-ordination in international governance.”</p><p>Such language put off the US, which did not agree on the terminology around multilateralism and international collaboration, said an official from one of the countries that signed the document.</p><p>The US also had concerns that Current AI, the foundation launched by France during the summit, would be used to funnel money to French-speaking countries, the official said.</p><p>Vance’s speech was “a 180-degree turnaround from what we saw with the Biden administration,” said Keegan McBride, a lecturer at the Oxford Internet Institute, who studies the geopolitics of AI.</p><p>The narrative around Macron’s summit was about pushing alternatives to the US-led drive to develop AI technologies. Following the launch of powerful open models from DeepSeek, Europeans had felt like they had a fleeting chance to compete in AI, said Frederike Kaltheuner, senior EU and global governance lead at the AI Now Institute, an AI research institute.</p><p>“Vance just dumped water all over that. [It] was like, ‘Yeah, that’s cute. But guess what? You know you’re actually not the ones who are making the calls here. It’s us,’” said McBride.</p><p><i>Additional reporting by Jim Pickard</i></p>","contentLength":1887,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43023554"},{"title":"Smuggling arbitrary data through an emoji","url":"https://paulbutler.org/2025/smuggling-arbitrary-data-through-an-emoji/","date":1739352248,"author":"paulgb","guid":239,"unread":true,"content":"<blockquote><p>With ZWJ (Zero Width Joiner) sequences you could in theory encode an unlimited amount of data in a single emoji.</p></blockquote><p>Is it really possible to encode arbitrary data in a single emoji?</p><p> yes, although I found an approach without ZWJ. In fact, you can encode data in  unicode character. This sentence has a hidden message󠅟󠅘󠄐󠅝󠅩󠄜󠄐󠅩󠅟󠅥󠄐󠅖󠅟󠅥󠅞󠅔󠄐󠅤󠅘󠅕󠄐󠅘󠅙󠅔󠅔󠅕󠅞󠄐󠅝󠅕󠅣󠅣󠅑󠅗󠅕󠄐󠅙󠅞󠄐󠅤󠅘󠅕󠄐󠅤󠅕󠅨󠅤󠄑. (Try pasting it into <a href=\"https://emoji.paulbutler.org/?mode=decode\">this decoder</a>)</p><p>Unicode represents text as a sequence of , each of which is basically just a number that the Unicode Consortium has assigned meaning to.\nUsually, a specific codepoint is written as , where  is a number represented as uppercase hexadecimal.</p><p>For simple latin-alphabet text, there is a one-to-one mapping between Unicode codepoints and characters that appear on-screen. For example,\n represents the character .</p><p>For other writing systems, some on-screen characters may be represented by multiple codepoints. The character की\n(in <a href=\"https://en.wikipedia.org/wiki/Devanagari\">Devanagari script</a>) is represented by a consecutive pairing of the codepoints  and .</p><p>Unicode designates 256 codepoints as “variation selectors”, named VS-1 to VS-256. These have no on-screen representation of their own, but are used to modify\nthe presentation of the preceeding character.</p><p>Most unicode characters do not have variations associated with them. Since unicode is an evolving standard and aims to be future-compatible,\nvariation selectors are supposed to be preserved during transformations, even if their meaning is not known by the code handling them.\nSo the codepoint  (“g”) followed by  (VS-2) renders as a lowercase “g”, exactly the same as  alone. But if you copy and paste it, the\nvariation selector will tag along with it.</p><p>Since 256 is exactly enough variations to represent a single byte, this gives us a way to “hide” one byte of data in any other unicode codepoint.</p><p>As it turns out, the <a href=\"https://www.unicode.org/versions/Unicode16.0.0/\">Unicode spec</a> does not specifically say anything about sequences\nof multiple variation selectors, except to imply that they should be ignored during rendering.</p><p><em>See where I’m going with this?</em></p><p>We can concatenate a sequence of variation selectors together to represent any arbitrary byte string.</p><p>For example, let’s say we want to encode the data <code>[0x68, 0x65, 0x6c, 0x6c, 0x6f]</code>, which represents the text “hello”. We can do this by\nconverting each byte into a corresponding variation selector, and then concatenating them together.</p><p>To convert from a byte to a variation selector, we can do something like this Rust code:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">(byte: ) -&gt;  {\n     byte  {\n        char::from_u32( byte ).unwrap()\n    }  {\n        char::from_u32( (byte ) ).unwrap()\n    }\n}\n</code></pre></div><p>To encode a series of bytes, we can concatenate a number of these variation selectors after a base character.</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">(base: , bytes: []) -&gt; String {\n     result  String::new();\n    result.push(base);\n     byte  bytes {\n        result.push(byte_to_variation_selector(byte));\n    }\n    result\n}\n</code></pre></div><p>Then to encode the bytes <code>[0x68, 0x65, 0x6c, 0x6c, 0x6f]</code>, we can run:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">() {\n    println(, encode(, [, , , , ]));\n}\n</code></pre></div><p>It just looks like a regular emoji, but try pasting it into <a href=\"https://emoji.paulbutler.org/?mode=decode\">the decoder</a>.</p><p>If we instead use the debug formatter, we see what’s going on:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">() {\n    println(, encode(, [, , , , ]));\n}\n</code></pre></div><pre><code>\"😊\\u{e0158}\\u{e0155}\\u{e015c}\\u{e015c}\\u{e015f}\"\n</code></pre><p>This reveals the characters that were “hidden”&nbsp;in the original output.</p><p>Decoding is similarly straightforward.</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">(variation_selector: ) -&gt; Option {\n     variation_selector  variation_selector ;\n     (..).contains(variation_selector) {\n        Some((variation_selector ) )\n    }  (..).contains(variation_selector) {\n        Some((variation_selector ) )\n    }  {\n        None\n    }\n}\n\n(variation_selectors: ) -&gt; Vec {\n     result  Vec::new();\n    \n     variation_selector  variation_selectors.chars() {\n         Some(byte)  variation_selector_to_byte(variation_selector) {\n            result.push(byte);\n        } result.is_empty() {\n             result;\n        }\n            }\n\n    result\n}\n</code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"rust\"> std::::from_utf8;\n\n() {\n     result  encode(, [, , , , ]);\n    println(, from_utf8(decode(result)).unwrap()); }\n</code></pre></div><p>Note that the base character does not need to be an emoji – the treatment of variation selectors is the same\nwith regular characters. It’s just more fun with emoji.</p><p>To be clear, this is an abuse of unicode and you shouldn’t do it. If your mind is wandering to practical use cases for this, shut it down.</p><p>That said, I can think of a couple of nefarious ways this could be (ab)used:</p><h3>1. Sneaking data past human content filters</h3><p>Since data encoded this way are invisible once rendered, a human moderator or reviewer will not know they are there.</p><p>There are techniques for using subtle variations in text to “watermark” a message, so that if it is sent to a number of people and then\nleaked, it’s possible to trace it to the original recipient. Variation selector sequences are a way to do this that survives most copy/pastes\nand allows arbitrary data density. You could go so far as to watermark  if you wanted to.</p><h2>Addendum: can an LLM decode it?</h2><p>Since this made it on <a href=\"https://news.ycombinator.com/item?id=43023508\">Hacker News</a>, some people have asked about how LLMs\ndeal with this hidden data.</p><p>Generally, tokenizers  seem to preserve the variation selectors as tokens, so in theory the model has access to them.\n<a href=\"https://platform.openai.com/tokenizer\">OpenAI’s tokenizer</a> is a good sanity check of this:</p><p>Overall though, models don’t even seem willing to try decoding them internally. However, when paired with a code interpreter, some models are\nactually able to solve them!</p><p>Here’s an example of Gemini 2 Flash solving one in a remarkable seven seconds, using <a href=\"https://block.github.io/goose/\">Codename Goose</a> and\n<a href=\"https://forevervm.com/\">foreverVM</a> (disclaimer: I work on foreverVM).</p>","contentLength":5705,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43023508"},{"title":"The year I didn't survive","url":"https://bessstillman.substack.com/p/the-year-i-didnt-survive","date":1739326072,"author":"LaurenSerino","guid":238,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43020983"},{"title":"I tasted Honda’s spicy rodent-repelling tape and I will do it again (2021)","url":"https://haterade.substack.com/p/i-tasted-hondas-spicy-rodent-repelling","date":1739286488,"author":"voxadam","guid":237,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43013615"},{"title":"Backblaze Drive Stats for 2024","url":"https://www.backblaze.com/blog/backblaze-drive-stats-for-2024/","date":1739285745,"author":"TangerineDream","guid":236,"unread":true,"content":"<p>As of December 31, 2024, we had 305,180 drives under management. Of that number, there were 4,060 boot drives and 301,120 data drives. This report will focus on those data drives as we review the Q4 2024 annualized failure rates (AFR), the 2024 failure rates, and the lifetime failure rates for the drive models in service as of the end of 2024. Along the way, we’ll share our observations and insights on the data presented, and, as always, we look forward to you doing the same in the comments section at the end of the post.</p><h2>Q4 2024 hard drive failure rates</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. For our evaluation, we removed from consideration 487 drives, as they did not meet the criteria to be included. We’ll discuss the criteria we used in the next section of this report. Removing these drives leaves us with 300,633 hard drives to analyze. The table below shows the annualized failure rates for Q4 2024 for this collection of drives.</p><ul><li>. Seagate 24TB drives (model: ST24000NM002H) arrived in early December. The 1,200 drives filled one Backblaze Vault with no failed drives through the end of Q4. The 24TB Seagate drives join the 20TB Toshiba and 22TB WDC drive models in the 20-plus capacity club as we continue to dramatically increase storage capacity while optimizing existing storage server space.</li><li><strong>Zero failures for the quarter</strong>. Five drive models had zero failures for the quarter starting with the 24TB Seagate drive model noted above. The others are the 4TB HGST (model: HMS5C4040ALE640), the 8TB Seagate (model: ST8000NM000A), the 14TB Seagate (model: ST14000NM000J), and the 16TB Seagate (model: ST16000NM002J). All of the zeroes come with the caveat of having a relatively small number of drives and drive days, but zero failures in a quarter is always a good thing.</li><li><strong>The 4TB drives are nearly extinct</strong>. The 4TB drive count decreased by another 1,774 drives in Q4. (I discussed exactly <a href=\"https://www.backblaze.com/blog/how-backblaze-scales-our-storage-cloud/\" target=\"_blank\" rel=\"noreferrer noopener\">how we migrate them</a> in more detail if you want to dig in.) The remaining ~4,000 drives should be gone by the end of Q1 2025. They will be replaced by the incoming 20TB, 22TB, and 24TB drives. It should be noted that out of the 4TB drives in operation in Q4, only one failed, so those 20-plus TB drives have a lot to live up to from a failure perspective.</li><li><strong>The quarterly failure rate is down.</strong> The AFR for Q4 dropped from 1.89% in Q3 to 1.35% in Q4. While all drive sizes delivered some improvement from Q3 to Q4, one of the primary drivers is the addition of over 14,000 new 20-plus TB drives. As a group, these drives delivered an AFR of 0.77% for the quarter.</li></ul><p>We noted earlier we removed 487 drives from consideration when we produced the table above covering Q4 2024. There are two primary reasons we did not consider these drive models.</p><ul><li>. These are drives of a given model that we monitor and collect Drive Stats data on, but are not considered production drives at this time. For example, drives undergoing certification testing to determine if they are performant enough for our environment are not included in our Drive Stats calculations.</li><li><strong>Insufficient data points. </strong>When we calculate the annualized failure rate for a drive model for a given period of time (quarterly, annual, or lifetime), we want to ensure we have enough data to reliably do so. Therefore we have defined criteria for a drive model to be included in the tables and charts for the specified period of time. Models that do not meet these criteria are not included in the tables and charts for the period in question.</li></ul><table><thead><tr></tr></thead><tbody></tbody></table><p>Regardless of whether or not a given drive model is included in the charts and tables, all of the data for all of the drives we use is included in our Drive Stats dataset which you can download by visiting our <a href=\"https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data\" target=\"_blank\" rel=\"noreferrer noopener\">Drive Stats page</a>.</p><p>As with the Q4 quarterly results, we will apply these criteria to the annual and lifetime charts that follow in this report.</p><h2>2024 annual hard drive failure rates</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. We removed nine drive models consisting of 2,012 drives from consideration as they did not meet the annual criteria we have defined. This leaves us with 298,954 drives divided across 27 different drive models. The table below shows the AFRs for 2024 for this collection of drives.</p><ul><li>. There were no qualifying drive models with zero failures in 2024. That said, the 16TB Seagate (model: ST16000NM002J) got close by recording just one drive failure back in Q3, giving the drive an AFR of 0.22% for 2024.&nbsp;</li><li>. During 2024, our data center techs installed 53,337 drives. If we assume there are 2,080 work hours a year (52 weeks times 40 hours), that math is , and that means our intrepid DC techs installed 26 drives per hour. Busy, busy, busy!&nbsp;</li><li>While there were 1,200 new 24TB Seagate drives added in 2024, they were installed in early December and did not accumulate enough drive days to make the cut for the annual, or lifetime, tables. Including the 24TB Seagate drive, there were three models that missed out on being included in the 2024 annual tables, these drive models are listed below.</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>As a reminder, a drive model needs to have over 250 drives by the end of Q4 and accumulate at least 50,000 drive days during 2024 to be included in the annual tables.</p><h2>Comparing Drive Stats for 2022, 2023, and 2024</h2><p>The table below compares the annual failure rates by drive model for each of the last three years. The table includes just those drive models which met the annual criteria as of the end of 2024. The data for each year is inclusive of that year only for the operational drive models present at the end of each year. The table is sorted by drive size and then AFR.</p><ul><li> The 2024 AFR for all drives listed was 1.57%, this is down from 1.70% in 2023.&nbsp; We expect the overall failure rates to continue to fall in 2025, but we will be watching the following for indicators.\n<ul><li><strong>The failure rates of the 8TB and 12TB drive models. </strong>All of the models will exceed their five years of service. In general, the failure rate will noticeably increase as the drives exceed five years of service. And, while there are outliers like the current HGST 4TB drives, you can’t assume that will happen.</li><li><strong>The failure rates of the 14TB and 16TB drive models.</strong> These models are approaching middle age—three to five years in operation. This is where, according to <a href=\"https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/\" target=\"_blank\" rel=\"noreferrer noopener\">the bathtub curve</a>, their failure rates could gradually increase—but not as severely as when they exceed five years.&nbsp;</li><li><strong>The failure rates for the 20TB, 22TB, and 24TB drives models.</strong> These drives will enter the flat portion of the bathtub curve, that is where their failure rate should be the lowest.</li></ul></li></ul><h2>Annualized failure rates vs. drive size</h2><p>Now, we can dig into the numbers to see what else we can learn. We’ll start by looking at the quarterly annualized failure rate by drive size over the last three years.</p><p>Let’s take a look at the different drive sizes and how they affect the overall annualized failure rate over time.</p><p>. The 4TB (blue line) drives and 10TB (gold line) drives have had little impact over the last year on the overall failure rate as each finished the year with a relatively small number of drives. Still, the wild ride delivered by the 10TB drives keeps our DC techs on their toes.&nbsp;</p><p>. The 8TB (gray line) drives and 12TB (purple line) drives range in age from five to eight years and as such their overall failure rates should be increasing over time. The 12TB drives are following that pattern moving up from about 1% AFR back in 2021 to just about 3% in 2024. The failure rates of the 8TB drives, while erratic from quarter-to-quarter, have a nearly flat trendline over the same period.</p><p>. The 14TB (green line) and 16TB (azure* line) drives comprise 57% of the drives in service and on average they range in age from two to four years. They are in the prime of their working lives. As such, they should have low and stable failure rates, and as you can see, they do.</p><p>*&nbsp; Maybe azure isn’t quite right, but robin’s egg blue seemed a bit pretentious.</p><p>. The 22TB (orange line) drives are in their early days as we continue to add more drives on a regular basis. Once the drive population settles down, we’ll have a better sense of the AFR direction. Still, the early results are solid with a lifetime AFR of 1.06%.</p><h2>Annualized failure rates vs. manufacturer</h2><p>One of the more popular ways we can look at this data is by the drive manufacturer as we’ve done below.</p><p>To complete the picture, the chart below uses the same data, but displays just the linear trendlines for each of the manufacturers over the same three-year period.</p><p>. While the HGST trendline is not pretty, it doesn’t tell the entire story. Looking at the first chart, until Q4 2023, the HGST drives were at or below the average for all of the drives, that is all manufacturers. At that point, HGST has exceeded the average, and then some. The table below contains results for just the HGST drives for 2024. We’ve sorted them, high to low, by the 2024 AFR.</p><p>As you can see, there are two 12TB drive models driving the high AFR for the HGST drives. The HUH721212ALN604 model began showing signs of an increased quarterly AFR in Q1 2023 and the HUH721212ALE604 model followed suit in Q3 2024. Without these drive models, the 2024 AFR for HGST drive would be 0.55%.</p><p>. The quarterly AFR trendline decreased for the Seagate drives from 2022 through 2024. While the decrease was slight, from 2.25% to 2.0%, Seagate was the only manufacturer to do so. The decrease appears, at least in part, to be due to the removal of the Seagate 4TB drives during that period.&nbsp;</p><p>. Over the 2022 to 2024 period, the quarterly AFR for the Toshiba drive models varied within a fairly narrow range between 0.80% and 1.52%, with most quarters hovering slightly around 1.2%. Most importantly, none of the individual drive models were outliers, as the highest quarterly AFR for any Toshiba drive model was 1.58%. We like consistency.&nbsp;</p><p>. While WDC drive models delivered a similar level of consistency as the Toshiba models, they did so with a lower AFR each quarter. From 2022 through 2024, the range of quarterly AFR values for the WDC models was 0.0% to 0.85%. The 0.0% AFR was in Q1 2022 when none of the 12,207 WDC drives in operation failed during that quarter.</p><h2>Lifetime hard drive stats</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. Applying our drive criteria noted above for the lifetime period, we removed 11 drive models consisting of 2,736 drives from consideration as they did not meet the lifetime criteria we defined. This leaves us with 298,230 drives divided across 25 different drive models. The table below shows the lifetime AFRs for this collection of drives.</p><p>The current lifetime AFR for all of the drives is 1.31%. This is down from 1.46% in 2023. The drop is primarily due to the completion of the migration of the 4TB Seagate drives in 2024, which left us with only two of these drives still in operation as of the end of 2024. As a consequence, the 79 million drive days and over 5,600 drive failures racked up by the 4TB Seagate drives by the end of 2023 are not included in the data presented in the 2024 lifetime table above.&nbsp;&nbsp;</p><p>In the final table below, we’ve taken the lifetime table and sorted out the drive models that have a lifetime AFR of 1.50% or less by drive size.</p><p>A couple of caveats as you review the table.</p><ul><li>There is enough data for each model to say the AFR values are solid. That said, everything could change tomorrow. In general, the hard drive failure rate follows the bathtub curve as the drives age—unless it doesn’t. Some drives refuse to fail as they age, like the 4TB HGST drives. Other drives are great, and then “hit the wall” and bend the failure curve upward, fast.</li><li>A drive model with a 1% annualized failure rate means that you can expect one drive out of 100 to fail in a year. If you’re a personal drive user, that one drive could be yours. If you have exactly one drive, your personal annualized failure rate is 100%. In other words, always have a backup, and don’t forget to test it.</li></ul><p>I have been authoring the various Drive Stats reports for the past ten years and this will be my last one. I am retiring, or perhaps in Drive Stats vernacular, it would be “migrating.” Either way, after 10 years in the U.S. Air Force and 30+ years in Silicon Valley Tech, it is time. Drive Stats will continue with <a href=\"https://www.backblaze.com/blog/author/stephanie/\" target=\"_blank\" rel=\"noreferrer noopener\">Stephanie Doyle</a> and <a href=\"https://www.backblaze.com/blog/author/davidjohnson/\" target=\"_blank\" rel=\"noreferrer noopener\">David Johnson</a> as the replacement drive models beginning with the Q1 2025 report. I wish them well.</p><p>I want to say thank you to each of you who have taken your time to peruse and engage with the Drive Stats reports and data over the last 10 years. And, thank you as well for the comments, questions, and discussions that raced and raged across the various communities that care about something as mundane and awesome as a hard drive. It has been quite the ride—thanks again.</p><h2>The Hard Drive Stats data</h2><p>The complete data set used to create the tables and charts in this report is available on our&nbsp;<a href=\"https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/\" target=\"_blank\" rel=\"noreferrer noopener\">Hard Drive Test Data</a>&nbsp;page. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data itself to anyone; it is free.</p><p>Good luck, and let us know if you find anything interesting.</p>","contentLength":13388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43013431"},{"title":"Boring tech is mature, not old","url":"https://rubenerd.com/boring-tech-is-mature-not-old/","date":1739282902,"author":"mikece","guid":235,"unread":true,"content":"<p>I’ve talked before about how I think NetBSD is “boring”, and that it’s among the highest forms of praise I can give tech as a sysadmin and architect. But I’ve never elaborated why that is.</p><p>The opposite of being bored is to be surprised, and that’s not something a sysadmin desires when building, maintaining, scaling, troubleshooting, upgrading, or even replacing a complex system… especially when you’re woken up at 03:30 by a monitoring server. That’s also why the phrase <em>work doing what you love and you never work a day again in your life</em> is an empty platitude of nonsense. But I digress.</p><p>Boring tech behaves in predictable ways. It’s a well trodden path others have evaluated, optimised, troubleshooted, and understood. Using tech that has been subjected to all those people hours of use means you’re less likely to run into edge cases, unexpected behaviour, or attributes and features that lack documentation or community knowledge. In other words, when something goes wrong, can you turn to someone or something?</p><p>Likewise, tech (generally) doesn’t exist in a vacuum. It interacts with other components and systems, some of which are even conveniently under our control, sometimes. Multiply out the potential for surprises by the number of components and their relative maturity, and your head can start to spin.</p><p>This isn’t to say there isn’t room for innovation, or that staying put is a guaranteed recipe for success. What it does teach is that it pays to make informed decisions, and that often times the understood, reliable, boring tech will get you there over something new, shiny or propped up with marketing spin. The number of people I’ve talked with who’ve replaced complicated K8s clusters with a few VMs and seen massive improvements in reliability, cost, and uptime would make some people at the Orange Peanut Gallery more than a little perturbed, for example.</p><blockquote><p>I used to agree with this. Now I don’t think so any more. You should prefer “boring” tech, and boring should be read as has been around for a while and therefore is well understood.</p><p>Ubiquity is a bad test for well-understood technology. Age as a test for ubiquity is also bad.</p></blockquote><p>There are a few assumptions here:</p><ul><li><p>Boring should be read as something being around for a while. I wouldn’t necessarily agree with that. I’ve been a DBA, and I never in a million years would call Oracle “boring”. It’s fiendishly complicated and difficult to maintain, and commands above-average salaries in part for that reason. A sign saying “there be Ellison dragons” isn’t boring, it’s frankly terrifying.</p></li><li><p>Ubiquity is a test for “boringness”. He’s right here; while age gives something an opportunity to become ubiquitous, it’s not a guarantee. I’d look to the BSDs here; I’d consider them boring, but they’re not exactly widely deployed compared to Penguins.</p></li><li><p>Boring should be read as well-understood. That’s true. Time gives something more of a chance of being well-understood, until suddenly it doesn’t and nobody is around with sufficient knowledge and inclination to maintain your COBOL stack. At that stage, I’d say you have a decidedly  issue on your hands.</p></li></ul><p>I’d conclude by suggesting boring tech isn’t old, but . Maturity not just in the software, but its documentation, community, and track record.  is often used as an analogue for maturity, but it’s not the same thing. Otherwise I’d be more mature than my Zoomer friends, and I very much doubt that to be the case. .</p><p>This is why I don’t hesitate to call NetBSD boring, and why I say that’s a compliment.</p>","contentLength":3598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43012862"},{"title":"Firing programmers for AI is a mistake","url":"https://defragzone.substack.com/p/techs-dumbest-mistake-why-firing","date":1739266962,"author":"frag","guid":234,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43010814"},{"title":"Musk-led group makes $97B bid for control of OpenAI","url":"https://www.reuters.com/markets/deals/elon-musk-led-group-makes-974-billion-bid-control-openai-wsj-reports-2025-02-10/","date":1739220138,"author":"jdoliner","guid":233,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43004889"},{"title":"Undergraduate shows that searches within hash tables can be much faster","url":"https://www.quantamagazine.org/undergraduate-upends-a-40-year-old-data-science-conjecture-20250210/","date":1739207109,"author":"Jhsto","guid":232,"unread":true,"content":"<p>Together, Krapivin (now a graduate student at the University of Cambridge), Farach-Colton (now at New York University) and Kuszmaul demonstrated in a <a href=\"https://arxiv.org/abs/2501.02305\">January 2025 paper</a> that this new hash table can indeed find elements faster than was considered possible. ln so doing, they had disproved a conjecture long held to be true.</p><p>“It’s an important paper,” said <a href=\"https://ajhconway.com/\">Alex Conway</a> of Cornell Tech in New York City. “Hash tables are among the oldest data structures we have. And they’re still one of the most efficient ways to store data.” Yet open questions remain about how they work, he said. “This paper answers a couple of them in surprising ways.”</p><p>Hash tables have become ubiquitous in computing, partly because of their simplicity and ease of use. They’re designed to allow users to do exactly three things: “query” (search for) an element, delete an element, or insert one into an empty slot. The first hash tables date back to the early 1950s, and computer scientists have studied and used them ever since. Among other things, researchers wanted to figure out the speed limits for some of these operations. How fast, for example, could a new search or insertion possibly be?</p><p>The answer generally depends on the amount of time it takes to find an empty spot in a hash table. This, in turn, typically depends on how full the hash table is. Fullness can be described in terms of an overall percentage — this table is 50% full, that one’s 90% — but researchers often deal with much fuller tables. So instead, they may use a whole number, denoted by , to specify how close the hash table is to 100% full. If  is 100, then the table is 99% full. If  is 1,000, the table is 99.9% full. This measure of fullness offers a convenient way to evaluate how long it should take to perform actions like queries or insertions.</p><p>Researchers have long known that for certain common hash tables, the expected time required to make the worst possible insertion — putting an item into, say, the last remaining open spot — is proportional to . “If your hash table is 99% full,” Kuszmaul said, “it makes sense that you would have to look at around 100 different positions to find a free slot.”</p><p>In a <a href=\"https://dl.acm.org/doi/10.1145/3828.3836\">1985 paper</a>, the computer scientist <a href=\"https://amturing.acm.org/award_winners/yao_1611524.cfm\">Andrew Yao</a>, who would go on to win the A.M. Turing Award, asserted that among hash tables with a specific set of properties, the best way to find an individual element or an empty spot is to just go through potential spots randomly — an approach known as uniform probing. He also stated that, in the worst-case scenario, where you’re searching for the last remaining open spot, you can never do better than . for 40 years, most computer scientists assumed that Yao’s conjecture was true.</p><p>Krapivin was not held back by the conventional wisdom for the simple reason that he was unaware of it. “I did this without knowing about Yao’s conjecture,” &nbsp;he said. His explorations with tiny pointers led to a new kind of hash table — one that did not rely on uniform probing. And for this new hash table, the time required for worst-case queries and insertions is proportional to (log ) — far faster than . This result directly contradicted Yao’s conjecture. Farach-Colton and Kuszmaul helped Krapivin show that (log ) is the optimal, unbeatable bound for the popular class of hash tables Yao had written about.</p><p>“This result is beautiful in that it addresses and solves such a classic problem,” said <a href=\"http://www.cs.cmu.edu/~guyb/\">Guy Blelloch</a> of Carnegie Mellon.</p><p>“It’s not just that they disproved [Yao’s conjecture], they also found the best possible answer to his question,” said <a href=\"https://cs.uwaterloo.ca/about/people/sassadi\">Sepehr Assadi</a> of the University of Waterloo. &nbsp;“We could have gone another 40 years before we knew the right answer.”</p>","contentLength":3730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43002511"},{"title":"The Anthropic Economic Index","url":"https://www.anthropic.com/news/the-anthropic-economic-index","date":1739196862,"author":"meetpateltech","guid":231,"unread":true,"content":"<p>In the coming years, AI systems will have a major impact on the ways people work. For that reason, we're launching the <a href=\"https://www.anthropic.com/economic-index\">Anthropic Economic Index</a>, an initiative aimed at understanding AI's effects on labor markets and the economy over time.</p><p>The Index’s <a href=\"https://assets.anthropic.com/m/2e23255f1e84ca97/original/Economic_Tasks_AI_Paper.pdf\">initial report</a> provides first-of-its-kind data and analysis based on millions of anonymized conversations on <a href=\"http://claude.ai\">Claude.ai</a>, revealing the clearest picture yet of how AI is being incorporated into real-world tasks across the modern economy.</p><p>We're also <a href=\"https://huggingface.co/datasets/Anthropic/EconomicIndex/\">open sourcing the dataset</a>used for this analysis, so researchers can build on and extend our findings. Developing policy responses to address the coming transformation in the labor market and its effects on employment and productivity will take a range of perspectives. To that end, we are also inviting economists, policy experts, and other researchers to <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfDEdY-mT5lcXPaDSv-0Ci1rSXGlbIJierxkUbNB7_07-kddw/viewform?usp=dialog\">provide input</a> on the Index.</p><p>The main findings from the Economic Index’s first paper are:</p><ul><li>Today, usage is concentrated in software development and technical writing tasks. Over one-third of occupations (roughly 36%) see AI use in at least a quarter of their associated tasks, while approximately 4% of occupations use it across three-quarters of their associated tasks.</li><li>AI use leans more toward augmentation (57%), where AI collaborates with and enhances human capabilities, compared to automation (43%), where AI directly performs tasks.</li><li>AI use is more prevalent for tasks associated with mid-to-high wage occupations like computer programmers and data scientists, but is lower for both the lowest- and highest-paid roles. This likely reflects both the limits of current AI capabilities, as well as practical barriers to using the technology.</li></ul><p>See below for further details on our initial findings.</p><h2>Mapping AI usage across the labor market</h2><p>Our <a href=\"https://assets.anthropic.com/m/2e23255f1e84ca97/original/Economic_Tasks_AI_Paper.pdf\">new paper</a> builds on a long line of research on the labor market impact of technologies, from the Spinning Jenny of the Industrial Revolution to the car-manufacturing robots of the present day. We focus on the ongoing impact of AI. We don’t survey people on their AI use, or attempt to forecast the future; instead, we have direct data on how AI is actually being used.</p><h3><strong>Analyzing occupational tasks</strong></h3><p>Our research began with an important <a href=\"https://academic.oup.com/qje/article-abstract/118/4/1279/1925105\">insight from the economics literature</a>: sometimes it makes sense to focus onrather than. Jobs often share certain tasks and skills in common: for example, visual pattern recognition is a task performed by designers, photographers, security screeners, and radiologists.</p><p>Certain tasks lend themselves better to being automated or augmented by a new technology than others. We’d therefore expect AI to be adopted selectively for different tasks across different occupations, and that analyzing tasks—in addition to jobs as a whole—would give us a fuller picture of how AI is being integrated into the economy.</p><h3><strong>Using Clio to match AI use to tasks</strong></h3><p>This research was made possible by Claude insights and observations, or\"<a href=\"https://www.anthropic.com/research/clio\">Clio</a>\", an automated analysis tool that allows us to analyze conversations with Claude while preserving user privacy. We used Clio on a dataset of approximately one million conversations with Claude (specifically, Free and Pro conversations on <a href=\"http://claude.ai\">Claude.ai</a>), and used it to organize the conversations by occupational task.</p><p>We chose tasks according to the classification made by the U.S. Department of Labor, which maintains a database of around 20,000 specific work-related tasks called the Occupational Information Network, or <a href=\"https://www.onetonline.org/\">O*NET</a>. Clio matched each conversation with the O*NET task that best represented the role of the AI in the conversation (the process is summarized in the figure below). We then followed the O*NET scheme for grouping the tasks into the occupations they best represented, and the occupations into a small set of overall categories:  and so on.</p><p> The tasks and occupations with by far the largest adoption of AI in our dataset were those in the “computer and mathematical” category, which in large part covers software engineering roles. 37.2% of queries sent to Claude were in this category, covering tasks like software modification, code debugging, and network troubleshooting.</p><p>The second largest category was “arts, design, sports, entertainment, and media” (10.3% of queries), which mainly reflected people using Claude for various kinds of writing and editing. Unsurprisingly, occupations involving a high degree of physical labor, such as those in the “farming, fishing, and forestry” category (0.1% of queries), were least represented.</p><p>We also compared the rates in our data to the rates at which each occupation appeared in the labor market in general. The comparisons are shown in the figure below.</p><p><strong>Depth of AI use within occupations.</strong> Our analysis found that very few occupations see AI use across most of their associated tasks: only approximately 4% of jobs used AI for at least 75% of tasks. However, more moderate use of AI is much more widespread: roughly 36% of jobs had some use of AI for at least 25% of their tasks.</p><p>As we predicted, there wasn’t evidence in this dataset of jobs being entirely automated: instead, AI was diffused across the many tasks in the economy, having stronger impacts for some groups of tasks than others.</p><p> The O*NET database provides the median U.S. salary for each of the occupations listed. We added this information to our analysis, allowing us to compare professions’ median salaries and the level of AI use in their corresponding tasks.</p><p>Interestingly, both low-paying and very-high-paying jobs had very low rates of AI use (these were generally jobs involving a large degree of manual dexterity, such as shampooers and obstetricians). It was specific occupations in the mid-to-high median salary ranges, like computer programmers and copywriters, who were—in our data—among the heaviest users of AI.</p><p><strong>Automation versus augmentation.</strong> We also looked in more detail at  the tasks were being performed—specifically, at which tasks involved “automation” (where AI directly performs tasks such as formatting a document) versus “augmentation” (where AI collaborates with a user to perform a task).</p><p>Overall, we saw a slight lean towards augmentation, with 57% of tasks being augmented and 43% of tasks being automated. That is, in just over half of cases, AI was not being used to replace people doing tasks, but instead worked  them, engaging in tasks like validation (e.g., double-checking the user’s work), learning (e.g., helping the user acquire new knowledge and skills), and task iteration (e.g., helping the user brainstorm or otherwise doing repeated, generative tasks).</p><p>Our study provides a unique glimpse into how AI is changing the labor market. But as with all studies it has important limitations. Some of these include:</p><ul><li>We can’t know for certain whether someone using Claude for a task was completing a task for work. Someone asking Claude for writing or editing advice  be doing so at work, but they could also be doing so for the novel they’re writing as a hobby.</li><li>Relatedly, we don’t know  the users were using the responses from Claude. Were they, for instance, copy-pasting code snippets? Were they fact-checking responses or accepting them uncritically? Some of what appears in our data to be automation could, in fact, be augmentation: for example, a user might ask Claude to write a full memo for them (which would appear as automation), but then edit it themselves afterwards (which would be augmentation).</li><li>We also only analyze data from <a href=\"http://claude.ai\">Claude.ai</a> Free and Pro plans, rather than API, Team, or Enterprise users. While Claude.ai data contains some non-work conversations, we used a language model to filter this data to only contain conversations relevant to an occupational task, which helps to mitigate this concern.</li><li>The sheer number of different tasks means it is possible that Clio classified some conversations incorrectly (please see the full paper, in particular Appendix B, for details on how we validated the analysis);</li><li>Claude can’t generate images (except indirectly via code), and so some creative uses won’t be referenced in the data;</li><li>Given that Claude is advertised for use as a state-of-the-art coding model, we might expect coding to be overrepresented as a use case. For that reason, we don’t argue that the uses in our dataset are a representative sample of AI use in general.</li></ul><h2>Conclusions and future research</h2><p>AI use is rapidly expanding, and models are becoming ever-more capable. The labor-market picture may look quite different within a relatively short time. For that reason, we’ll repeat many of the analyses above over time to help track the societal and economic changes that are likely to occur. We’ll regularly release the results and the associated datasets as part of the Anthropic Economic Index.</p><p>These kinds of longitudinal analyses can give us new insights into AI and the job market. For example, we’ll be able to monitor changes in the depth of AI use within occupations. If it remains the case that AI is used only for certain tasks, and only a few jobs use AI for the vast majority of their tasks, the future might be one where most current jobs evolve rather than disappear. We can also monitor the ratio of automation to augmentation, providing signals of areas where automation is becoming more prevalent.</p><p>Our research gives data on how AI is being used, but it doesn’t provide policy prescriptions. Answers to questions about how to prepare for AI’s impact on the labor market can’t come directly from research in isolation; instead, they’ll come from a combination of evidence, values, and experience from broad perspectives. We look forward to using our new methodology to shed more light on these issues.</p><p>Read <a href=\"https://assets.anthropic.com/m/2e23255f1e84ca97/original/Economic_Tasks_AI_Paper.pdf\">the full paper</a> for more details of our analyses and results.</p><h2>Open data and call for input</h2><p>The most important contribution of this paper, and of the Anthropic Economic Index, is its new methodology providing detailed data on the impacts of AI. We’re immediately openly sharing the dataset we used for the above analyses, and we plan to share further such datasets as they become available in the future.</p><p>The full dataset can be downloaded <a href=\"https://huggingface.co/datasets/Anthropic/EconomicIndex\">here</a>.</p><p>A form for researchers to provide feedback on our data and suggest new research directions is <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfDEdY-mT5lcXPaDSv-0Ci1rSXGlbIJierxkUbNB7_07-kddw/viewform?usp=dialog\">here</a>.</p><p>We appreciate the productive comments and discussion on early findings and drafts of the paper from Jonathon Hazell, Anders Humlum, Molly Kinder, Anton Korinek, Benjamin Krause, Michael Kremer, John List, Ethan Mollick, Lilach Mollick, Arjun Ramani, Will Rinehart, Robert Seamans, Michael Webb, and Chenzi Xu.</p><p>If you’re interested in working at Anthropic to research the effects of AI on the labor market, we encourage you to apply for our Societal Impacts <a href=\"https://boards.greenhouse.io/anthropic/jobs/4524032008\">Research Scientist</a> and <a href=\"https://boards.greenhouse.io/anthropic/jobs/4251453008\">Research Engineer</a> roles.</p>","contentLength":10759,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000529"},{"title":"Surnames from nicknames nobody has any more","url":"https://blog.plover.com/lang/etym/nickname-names.html","date":1739195678,"author":"JNRowe","guid":230,"unread":true,"content":"<p>English has a pattern of common patronymic names.  For example, \"John\nPeters\" and \"John Peterson\" are someone whose father was named\n\"Peter\".  (\"Peters\" should be understood as \"Peter's\".)  Similarly we\nhave John Williams and John Williamson, John Roberts and John\nRobertson, John Richards and John Richardson, John James and John\nJameson, John Johns and John Johnson, and so on.</p><p>Often Dad's name was a nickname.  For example, a common nickname for\n\"John\" is \"Jack\" and we have (less commonly) John Jacks and (more\ncommonly) John Jackson. John Bills and John Bilson, John Wills and\nJohn Wilson, and John Willis and John Willison are Bill, Will, and\nWille, all short for William.</p><p>\"Richard\" is \"Dick\", and we have John Dicks (or Dix) and John Dickson\n(or Dixon).  \"Nicholas\" is \"Nick\" and we have John Nicks (or Nix) and John\nNickson (or Nixon).</p><p>Sometimes the name has the diminutive suffix “-kin” inserted.  Wilkins\nis little Will's son, as is Wilkinson; Peterkins is little Peter's\nson.</p><p>These patterns are so common that if you find surnames that follow\nthem you can almost always infer a forename, although it may be one\nthat is no longer common, or that is spelled differently.  For\nexample, many people are named Pierce, Pearse, Pierson, or Pearson,\nwhich is from the name Pierre, Piers or Pierce, still used in English\nalthough much less common than in the past.  (It is from the same root\nas Peter.)  Perkins is little Pierre.  \nRobin used to be a nickname for\nRobert (it's “Robkin” with the difficult “-bk-” simplified to just\n“-b-”) and we have John Robins and John Robinson. </p><p>Sometimes, the pattern is there but the name is unclear because it is\na nickname that is now so uncommon that it is neatly forgotten.  The\nfathers of John Watts, Watson, and Watkins were called Wat, which used\nto be short for Walter.  John Hobbs, John Hobson, and Hobkins are\nnamed for Hob, which was short for Robert in the same way that Rob and\nBob are still.  (I had a neighbor who was called Hob, and told me his\nfamily claimed that it was short for Robert, but that he wasn't sure.\nI assured him that they were correct.)  “Daw”, an archaic nickname for\n“David”, gives us Dawes, Dawkins, and Dawson.</p><p>Back in September when I started this article I thought on John Gibbs\nand John Gibson.  Who's named \"Gib\", and why?  Is it archaic nickname?\nYes!  It was short for Gilbert.  Then I forgot about the draft article\nuntil today when I woke up wondering about John Simpson (and, I\nrealize now, John Simms and John Simkins). And it transpired \"Sim\" or\n\"Simme\" was once a common nickname for Simon.</p><p>I would welcome further examples.</p><p>Vicki Rosenzweig reminds me that another lost nickname for Richard is\n\"Hick\" (like \"Rick\" and \"Dick\") and from this we get Hicks, Hix,\nHickson, Hixon, Hickman, and (she informs me) \"Hickmott\".</p><p>\"-mott\" perhaps indicates a male in-law, according to the Dictionary\nof American Family Names via\n<a href=\"https://www.ancestry.com/name-origin?surname=hickmott\">Ancestrry.com</a>.\nThe only other clear example of this that I noticed was \"Willmott\".\nNote that Gaelic names like Dermott are not of this type; the\nresemblance is coincidental.</p><p>We get Hodge / Hodges / Hodgson / Hodgkins from \"Hodge\", an arcahic\nnickname for Roger.</p><p>Lucy Keer suggests that Bateson / Bates is from \"Bat\", an archair\nnickname for Bartholomew.</p><p>Christopher Taylor reminds me that in Atkins and Atkinson the \"At-\" is\nshort for \"Adam\", and   similarly in \"Addis\" and \"Addison\".  I know a\nguy named Atteson but I'm not sure it is the same thing.</p><p>Response to this article has been too much for me to keep up with.</p><ul><li>\"Dob\" is another old nickname for \"Robert\", giving us \"Dobson\";</li><li>\"Dodge\" another nickname for Roger, hence Dodgson, as in Louis Carrol's real name, Charles Dodgson;</li><li>\"Tibb\" is an old nickname for Theobald, giving surnames like \"Tibbs\" and \"Tibbets\";</li><li>\"Hud\" for \"Hugh\", giving us the Hudsons.</li></ul><p>I also asked Claude for suggestions:</p><blockquote><p>I'm going to paste a blog article that discusses common English\n  surnames that are patronymic versions of forenames that no longer\n  exist.  For example, the common surname \"Watson\" is from \"Wat\", and\n  \"Wat\" was once a common nickname for \"Walter\", but no longer used.\n  I would like you to list ten similar examples that are not mentioned\n  in the article.</p></blockquote><blockquote><p>Here are ten examples of common English surnames derived from obsolete\n  forenames or nicknames, different from those mentioned in the article:</p><ol><li>Higgins/Higgs - from \"Hick,\" a medieval pet form of Richard</li><li>Timms/Timson - from \"Tim,\" originally a medieval nickname for Timothy</li><li>Hodgson/Hodges - from \"Hodge,\" once a common nickname for Roger</li><li>Atkins/Atkinson - from \"At\" or \"Atte,\" medieval diminutive of Adam</li><li>Parkins/Parkinson - from \"Perkin,\" diminutive of Peter/Pierre</li><li>Judson/Judd - from \"Judd,\" medieval nickname for Jordan</li><li>Malkins/Malkinson - from \"Malk,\" diminutive of Malcolm</li><li>Larkins/Larkin - from \"Larry\" or \"Laurence,\" medieval form of Lawrence</li><li>Collins/Colson - from \"Col,\" medieval nickname for Nicholas</li><li>Huggins/Huggett - from \"Hug,\" medieval pet form of Hugh</li></ol></blockquote><p>Remember that some of these might be made up.  For example,\n<a href=\"https://en.wiktionary.org/wiki/Malkin#English\">Wiktionary claims that #7 is </a>,\nfrom , a nickname for Mary.  Who's right?  I don't know.  The\nonly way to settle this is with a cage match!  I hope.</p><p>I complained that #2 was not to spec because “Tim” is still common,\nand demanded a replacement:</p><blockquote><ol><li>Rankins/Rankin - from \"Ran,\" a medieval diminutive of Randolph</li></ol></blockquote><p>I also asked Claude if I had made any errors, and it said:</p><blockquote><p>The article claims that \"Robin\" comes from \"Robkin\" with \"bk\"\n  simplified to \"b\". This is incorrect.</p><p>\"Robin\" is actually a French\n  diminutive of Robert, coming from Old French Robin/Rovin.&nbsp;…</p></blockquote>","contentLength":5588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000316"},{"title":"Show HN: HTML visualization of a PDF file's internal structure","url":"https://github.com/desgeeko/pdfsyntax/blob/main/docs/browse.md","date":1739195573,"author":"desgeeko","guid":191,"unread":true,"content":"<p>Hi,\nI've just finished a rebuild of this function and added a lot of new features: info, page index, minimap, inverted index,... \nI think it may be useful for inspection, debugging or just as a learning resource showcasing the PDF file format.\nThis is a pet project and I would be happy to receive some feedback!\nRegards</p>","contentLength":320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000303"},{"title":"To buy a Tesla Model 3, only to end up in hell","url":"https://www.myteslaexperience.com/2025-02-01/to-buy-a-tesla-mode-3-only-to-end-up-in-hell","date":1739175913,"author":"lleims","guid":229,"unread":true,"content":"<div>\n    \n        My Tesla Experience\n    \n\n    \n      </div><p>In October 2024 I decided to buy a <a href=\"https://www.tesla.com/model3\">Tesla Model 3</a>. When I picked up the car, many of its features did not work (cameras, GPS navigation system, autopilot, software updates, etc.). It turned out to be <a href=\"https://electrek.co/2024/12/16/tesla-major-issue-self-driving-computer-inside-new-cars/\">a major issue with Tesla’s brand-new on-board computer</a>. This is a common failure in Tesla Model 3 cars manufactured in the last quarter of 2024. The issue has affected thousands of Tesla customers. Two weeks later, the car began to . Several drivers experiencing the same failure have also reported <strong>problems with the braking (ABS) and suspension (EPS) systems</strong>.</p><p>Tesla acknowledges all faults in the vehicle (except for battery losses, a fact on which it has not yet formally commented). The company refuses to accept a return of the vehicle without penalty, even though the problems were reported immediately. The only solution they offer is to make me wait 3 months for a miraculous replacement part. Ignored and without any type of compensation, I live attached to a car that needs a <strong>full recharge every 12 days to not die</strong>. For my own safety, and that of other drivers, I have stopped driving the vehicle.</p><p>In this blog I tell you how it happened. If you are the owner of a Tesla car, or you are thinking about buying one, I encourage you to read my blog and see up close how  treats their own customers. It is possible that if you have problems with your car, you will end up living in hell.</p><p>It’s been over 4 years since I stopped driving regularly.</p><p>At the end of 2020, my partner and I moved to  (China). My old car, a , stayed in Spain. I had bought it second-hand in 2007, and it served me well for more than 10 years. I never had big problems. Upon returning from China, at the end of 2023, my wife and I went to live in , where she is from. That’s when I made the decision that in 2024 I would buy a new car.</p><p>I’m not a motor enthusiast. I like to drive, and I have had the opportunity to take long car trips through Spain and other countries, but I know little about cars. Therefore, when purchasing a new vehicle, I decided to follow the advice of what experts call <a href=\"https://en.wikipedia.org/wiki/Wisdom_of_the_crowd\">“the wisdom of the crowd”</a>. Of the list of the <a href=\"https://www.xataka.com/movilidad/coches-vendidos-2023-2024-espana\">10 best-selling cars in Spain</a>, the one that caught my attention the most was the . It is a car that I had already driven as a rental vehicle on several occasions. The decision was almost made, my next car would be a Toyota Corolla.</p><p>It was during a family gathering in August when I decided to ask my cousins, much more educated than me in the world of motors, for advice. One of them had a <a href=\"https://www.autocar.co.uk/car-review/kia/ev6\">Kia EV6</a>. He was very satisfied with it, so the idea of buying an electric car began to float around in my head.</p><p>I started reading blogs and online magazines about electric cars. I soon came to the conclusion that the  is, today, the best EV in terms of quality and price. It is the electric car with the lowest consumption (12.5 kW / 100 km), and it is also the best-selling EV in the world (again ). In addition, there is the incentive of the <a href=\"https://en.wikipedia.org/wiki/Tesla_Supercharger\">Tesla Charger Network</a>. After reading an article on my friend Alberto Ruibal’s blog in which he argues <a href=\"https://www.alonsoruibal.com/el-tesla-model-3-sale-mas-barato-que-un-dacia-sandero/\">why the Tesla Model 3 is cheaper than a Dacia Sandero</a> (in Spanish), I ended up being completely convinced. My next car would be a Tesla Model 3.</p><h2>Buying a Tesla where there is no Tesla</h2><p>A Tesla car is always purchased through <a href=\"https://www.tesla.com\">Tesla’s website</a>. If you live in a country where Tesla is officially present, you would simply go to the website and buy your car from your country (for example, ). Unfortunately, Tesla is still not present in some countries in Europe. That is the case of countries such as Slovenia, Latvia, Lithuania or , where I live. If Tesla is not present in your country of residence, you would go to Tesla’s website and buy your car at , which is physically located in  (Holland). In short, when you live in a European country where there is no Tesla you have to import your car from Holland (See ).</p><p>Importing the car from Holland implies an additional investment of time and money. More time, because you will have to complete <strong>more bureaucratic procedures to register the car</strong> in your country of residence. More money, because you will need to hire a transport company to <strong>bring your car from Holland</strong>. It is not possible to travel to Holland, get to Tilburg, and drive the car from there.</p><p>Another problem with importing the car is that, in case there is some type of subsidy for the acquisition of EVs in your country of residence, it is likely you will not be able to use it if the vehicle is imported.</p><p>I estimate that the total import costs were slightly over 2000 EUR.</p><p>: In some Internet online boards I read that it is possible to buy the car in any other country in Europe, but normally users who choose this option usually have problems with VAT refunds.</p><h2>Transporting a car from Holland</h2><p>I made my reservation on . Three weeks later, specifically on , the car was already available for pickup. I then contacted a vehicle transport company, <a href=\"https://www.tradisa.com\">Tradisa</a>, to organize the delivery.</p><p>Incidentally, just when the car was arriving, I was in Spain. Therefore, Tradisa suggested me to contact <a href=\"https://www.drivetesla.sk\">DriveTesla.sk</a>. This is a company in Slovakia that is dedicated to importing Tesla cars. They offer services such as vehicle purchase, registration, insurance, etc. I contracted several services with them: insurance, car registration and deposit service (in addition to winter tires). On , Tradisa dropped the car off at DriveTesla facilities and three weeks later, once I was back in Slovakia, I went to Bratislava to pick up the car. It was . After almost 2 months since the reservation, the car was finally in my hands!</p><p>But not everything was going to be good news. One of DriveTesla’s staff members warned me: <em>“By the way, the cameras are not working. Probably it can be fixed by updating the Operating System.”</em> I didn’t know it at the time, but my nightmare was about to begin…</p><p>Making the jump from a  to a  is not trivial. I’d driven automatic cars before, but at least they had a physical gear-shifter! :) For 45 minutes, one of the DriveTesla staff members taught me the gist of how the car worked. We also took the opportunity to try to update the Operating System and solve the , but <strong>the download got stuck halfway through</strong>. It seems like I wasn’t going to be able to fix the cameras, but did I really need them? <em>“My old Clio didn’t have cameras, I’m not going to miss them”</em>, I thought.</p><p>I was expecting a long trip, so before hitting the road I decided to take a few laps to practice. I practiced until I felt comfortable. Before leaving, I looked up my destination on the car’s . Bad luck, the GPS didn’t work either. <em>“What difference does it make? My old Clio didn’t have a GPS navigator either. I’m not going to miss it”</em>, I thought again. I pulled my phone out of my pocket, looked for my destination on Google Maps (a small town in the middle of Slovakia where I would meet my wife), and this time, I hit the road.</p><p>On the way it started to rain. I remembered what I was told me during the instruction: <em>“the Tesla Model 3 has a <strong>windshield wiper that comes into operation automatically</strong> when it starts raining”</em>. It was raining and raining, but the automatic windshield wipers didn’t kick in. On the steering wheel hub, there is also a physical button to activate the wipers manually. I had no choice but to start pressing that button, and continue doing it often because unfortunately it wasn’t raining, it was pouring. From the Tesla’s main screen, it’s possible to set the windshield wiper speed. However, I prefer to <strong>not take my eyes off the road</strong> when I drive. After a long time of almost continuously pressing the button, I discovered that the speed can also be set by pressing the button twice in a row very quickly.</p><p>After an hour and a half of driving, I stopped the car to rest and stretch my legs. That’s when I understood what was happening. Since the cameras were not working, the automatic windshield wipers did not work either. Paying more attention to the information screen, I realized of several notifications warning that other functionalities in the car were not available. For example, ,  or . That is to say, <strong>everything that depended on the car’s cameras didn’t work</strong>, including  obviously (but luckily it didn’t occur to me to try it lol). Another thing that didn’t work was . I mean, I managed to pair Spotify from my phone to the Tesla, but when I started playing music, the song stopped after 10 seconds. This always happened, regardless of which song.</p><p>Even so, I still had hope inside of me all this could be solved by updating the Operating System. Because we already know that hope is the last thing ever lost and we human beings are forever optimists.</p><p>On Sunday , when I got home, I tried to update the Operating System again. No luck. Same problem again, the download hung halfway.</p><p>The next day, I gave it another try with the same result. So on Tuesday, , I gave up completely and opened a ticket, through the Tesla app, at the  (my closest Tesla Service Center). I described all the technical problems I had observed in the car so far:</p><ul><li>The  are not working.</li><li>The  is not working.</li><li>Nothing that depends on the cameras works: , <strong>automatic windshield wipers</strong>, , , , etc.</li><li> are not working</li></ul><p>After four days without receiving a response from the , I texted them again. On the sixth day, terribly worried, I called . After a long wait, I managed to speak to an agent. As I was describing the problem, he already knew what the issue was about. This article, authored by  for , explains the problem in the car: <a href=\"https://electrek.co/2024/12/16/tesla-major-issue-self-driving-computer-inside-new-cars\">Tesla is having major issue with its self-driving computer inside new cars</a>. The article describes a fault present in many of the Tesla Model 3 manufactured in the last quarter of 2024. A short circuit when calibrating the cameras leaves the new on-board computer, known as HW4, unusable.</p><p><em>“The problem is linked to a new version of Tesla’s HW4 (sometimes called AI4) onboard self-driving computer. Internally, some refer to the new version as AI4.1. The computers are short-circuiting.”</em></p><p><em>“Tesla drivers are reporting computer failures after driving off with their brand-new cars over just the first few tens to hundreds of miles. Wide-ranging features powered by the computer, like active safety features, cameras, and even GPS, navigation, and range estimations, fail to work.”</em></p><p>These are exactly the problems that I had observed in the vehicle, along with other additional dysfunctions such as the inability to update the Operating System and the data transfer problems in Spotify.</p><p>The article also explains that Tesla <strong>does not have replacement stock</strong> for the new on-board computer, and is making its customers believe that the car is drivable in this state and poses no risk:</p><p><em>“One source said that Tesla service is being told to play down any safety concerns related to this problem to avoid people believing their brand-new cars are not drivable. It’s unclear if Tesla reported the issue to NHTSA despite the fact that the broken rear-view camera goes against federal safety regulations, which should force a recall.”</em></p><p><em>“Tesla service is currently being overwhelmed by the issue, and Tesla is pushing service appointments to next year.”</em></p><p>On , I received a response from the Budapest Service Center <strong>confirming the problems in the vehicle</strong> and that <strong>the on-board computer had to be replaced</strong>. As the article points out, they informed me that at the moment they did not have the part in stock and they did not know when it would be available.</p><p>Exactly one week later, I received a message in Hungarian from the Budapest Service Center letting me choose a date for my appointment. The earliest date, .</p><p>Almost 3 months of waiting to, hypothetically, repair the vehicle. Apparently, Tesla considers OK their customers to drive vehicles with the following features not working:</p><ul><li>: essential for parking, surveillance, etc.</li><li>: a crucial feature in the Tesla Model 3, as when planning a route, the car indicates where to stop for charging. Additionally, when approaching a charging point, the vehicle automatically preheats the battery for a faster recharge.</li><li><strong>Braking (ABS) and suspension (EPS) systems</strong>: I have no way of knowing whether these systems are working correctly in my vehicle or not, as I never had the chance to drive the car in perfect condition. Regardless, I don’t believe I have to risk my own safety, or that of others, to find out whether these systems are working reliably.</li><li> and  (e.g., from Spotify).</li><li><strong>Other systems that rely on cameras</strong>: Autopilot, automatic windshield wipers, automatic lights, lane departure detection, drowsiness detection, etc.</li></ul><h2>Renault Clio vs Tesla Model 3</h2><p>The <a href=\"https://electrek.co/2024/12/16/tesla-major-issue-self-driving-computer-inside-new-cars\">Electrotek article</a> cited above was published on . Until I read it, I wasn’t fully aware of the impact of the breakdown. Especially, the part that affects the  (ABS) and  (EPS) systems. In fact, I kept driving the car and joked with my friends that I had an . That is, on the outside the car looked like a Tesla Model 3, but on the inside it worked like my old . Let’s compare:</p><table><tbody><tr></tr><tr><td>Automatic windshield wiper</td></tr><tr></tr><tr><td>Surveillance (Sentry mode)</td></tr><tr></tr></tbody></table><p>In fact, a 20-year-old Clio DCI is slightly superior to this . It is true that the Clio does not have heated seats and steering wheel, but that does not justify the price difference I think.</p><p>The comparison may seem humorous, but it reveals something obvious: <strong>a Tesla Model 3 is superior to other cars for these and other features</strong>. These functionalities are not extras in the vehicle, they are an integral part of it, and have a cost for which customers pay. Some of these features are essential in a Tesla Model 3, such as  or the . Why should I take risks, whether or not I’m going to hit the car, when parking it? Planning a long-distance trip is much more difficult without GPS navigation since I have to schedule the stops myself to recharge the car and I cannot preheat the battery. When I charge on a Tesla charger, I do not know the cost since this information is only available in the GPS navigator, etc. I have left out of the picture the problems, reported by other users, in the braking (ABS) and suspension (EPS) systems, but without a doubt, they pose a risk to the safety of drivers.</p><p>But this story doesn’t end here. The worst was yet to come…</p><h2>Doctor, my car is losing battery. Is it serious?</h2><p>It is expected for an EV to lose battery while it remains parked. There’s even a technical term for this. It is called <a href=\"https://www.ev.guide/lesson-articles/what-is-vampire-drain\"></a> or .</p><p>The  is the <strong>discharge that occurs in an electric car while we are not using it</strong>. It is something similar to what happens with mobile phones. Even if we are not using a cell phone, it loses battery unless it remains plugged in. The same thing happens in an EV. I don’t know what the average expected battery loss value is for a Tesla Model 3. During the 3 weeks the car was parked in Bratislava, the battery went from 83% to 79%. That is, 4% loss in 3 weeks. I think it’s a pretty reasonable number.</p><p>In addition to , EV owners need to be more aware about the care and maintenance of their vehicle’s battery. For example, the usual thing is to <strong>charge the car up to 80-85%</strong>, and . If the car falls below 20%, it is recommended to recharge it immediately. Electric car batteries, unlike laptop batteries, <strong>do not have a memory effect</strong>. Therefore, the battery is not damaged if the car is charged from 20% to 50%, for example. On the other hand, Tesla recommends <strong>charging the battery up to 100% at least once a week</strong>.</p><p>I don’t know whether it was a coincidence or not, but just when I charged the car for the first time to 100%, the battery started to drain. I charged the battery to 100% and just , without having used the car, . That means, the car had lost 4% battery daily. Just that day, my wife and I had planned a long trip to the .</p><p>While driving, my wife was searching the Internet for information about what could have caused this sudden battery loss. <em>«Could it have been the cold?»</em> The car sleeps in a garage, and although it was winter the temperature never dropped below 0 degrees. It is true that extreme temperatures (very high or very low) can affect the battery, but <a href=\"https://www.visitnorway.com/plan-your-trip/getting-around/by-car/electric-cars\">Norway has the highest number of EVs in the world per person</a>. Common sense tells me that it must be colder in Norway than in Slovakia. Therefore, it couldn’t be the cold. Another possibility could be some feature that had been left activated. It is known that the  of the Tesla Model 3 consumes 1% every 2.5 hours. That means, if you inadvertently leave  activated, the car <strong>can lose almost 10% of battery in a day</strong>. But  was not activated.</p><p>Either way, this unexpected setback was a huge inconvenience to our trip. Arriving at our first stop, we had to spend all of our time finding a Tesla charger before resuming our journey. Fortunately, one recharge was enough to get me back home without having to stop again.</p><p>As soon as I returned, I went to charge the car. This time I decided to charge it up to 82-83%. This happened on Saturday .</p><p>The next day, I was flying from Budapest (Hungary) to Porto (Portugal). It was a bumpy flight, and I ended up arriving in Porto on Monday morning. Upon arrival, I opened the Tesla app to see if the car was still losing battery at 4%. My surprise was huge when I checked the . That is to say, the car was losing battery at a greater pace than expected, around !! A chill ran through my entire body.</p><p>I was going to spend approximately a week in Spain. On  I would take a plane from Vigo to London to spend New Year’s Eve there with my wife and some friends. Until  I would not be back in Slovakia. I did a quick calculation. With a battery loss of 8% per day <strong>the battery would end up being consumed</strong> between  and . Once I had made this calculation, the first thing that crossed my mind was to  and  to Slovakia . It was an absolutely irrational thought. I was putting my life, well-being and relationships with family and friends before the needs of a car. I had bought a car so I could move around more freely. But now the car was conditioning me. My life and my plans revolved around an object. I didn’t have a car, I had a . I had to take care of it, I had to see if it had slept well, and above all, I had to check if it still had battery left.</p><p>Going back to my senses, I looked for a . I called my wife, who was still in Slovakia, and told her the bad news. The idea was to somehow charge the car enough so that it wouldn’t die before returning. I knew it wasn’t going to be easy for her because she hadn’t driven in years. In addition, she didn’t have much time since she was taking a flight to London that same day. Even so, with the help of a friend, she managed to charge the car up to 98% before leaving.</p><p>I spent Christmas  and trying to figure out <strong>why the car had started losing 8% battery daily</strong>. I remember that on Christmas Eve I managed to speak with Tesla Spain. When I asked whether it was normal for the car to lose 8% of battery daily, the agent on the other end of the line answered: . I was stunned. Now I really didn’t understand anything. <em>“Was it within expectations that the car would lose 8% battery per day? But, who could live with a car like that?”</em>, I thought. In my phone call with Tesla Spain, I insisted that they speak to the  so that they could get back me. I had notified them immediately when I saw the car was losing battery, but I still had not received a response. I also sent an email to  (my only contact when purchasing the vehicle), demanding either a replacement of the vehicle or a full refund of the amount, since the car was defective from the very first day I laid my hands on it and now there was this huge battery problem. I also tried to contact the , without success.</p><p>The days I went through were not pleasant at all. My head was constantly spinning around the car. Every day since , I started sending a screenshot to the  showing how the car was losing 8% battery per day. I asked them directly, repeatedly, what was causing this problem. But I never received a response. They had deliberately decided to ignore me. To this day, and as I write these lines, I am still waiting for the Budapest Service Center to explain to me what the battery losses are due to. I continue sending screenshots daily.</p><p>When I returned to Slovakia, the first thing I did was to charge the car. Then, I called Tesla again. On the other end of the line, an agent told me the battery losses were due to previous problems already existing in the car. I insisted that the  confirmed this diagnosis, but as I have explained previously . I asked for solutions or mitigations to this situation. The only solution they gave me was to keep waiting for that miraculous replacement part. There are no contingency plans, no replacement vehicles, no compensation for these problems.</p><p>The lack of management and planning at Tesla left me greatly disappointed and in disbelief. A car that is known to be defective should be evaluated at a mechanic immediately. Making a customer to wait 3 months for a proper diagnosis or replacement part is unacceptable. Forcing thousands of customers to drive a dysfunctional car, which depends so much on technology, is not only unacceptable but also . It poses a serious risk to your safety, that of your companions, passers-by and other drivers. Forcing someone to live with a car that loses 8% battery a day is like forcing someone to live tied to a chain. Even charging the car to 100%, the battery does not last more than 12 days. <strong>I can’t go on vacation for 2 weeks because otherwise this car would die</strong>.</p><p>During my conversation with the Tesla agent, I asked whether the  could handle my case more quickly. The agent told me that he could not check the waiting times of the centers, and suggested me not to switch Service Centers as I would lose my turn. All Tesla’s customers have their hands tied, even though they do not know it yet. I have absolutely lost my trust in this company, and the worst of all is that I feel like a prisoner of it.</p><p>I came to the conclusion that, even if Tesla were able to fix the problems in my car, I would not like to live within their . I don’t want to plan my life around the ineptitude and inefficiency of a company, which also  and in a banal way . On  I called <strong>Tesla Netherlands Sales Department</strong> to request the return of the vehicle and a refund of my money. Nobody picked up the phone on the other end of the line. Finally, after several calls, I managed to talk to an agent from another department. I requested a refund of my money since the vehicle was delivered broken and now the battery problems were added. The agent was not sure whether I was entitled to a full refund since the car had more than 100 km. He was also not sure whether I could leave the car in Budapest or should I transport it back to Tilburg. He promised me that on Monday, , someone from the Sales Department would contact me. We agreed on a time, between 4 and 6 PM. After almost a month, I’m still waiting for that call.</p><p>All of this can be summarized in a single sentence: <strong>The Head of the Department of Government Efficiency is the CEO of an disfunctional company</strong>.</p><p>Since the battery problems occurred I have stopped driving the vehicle. Even more so, after reading the <a href=\"https://www.motorpasion.com/tesla/abs-esp-limpiaparabrisas-tesla-nuevos-se-estan-quedando-ayudas-marca-dice-que-sus-coches-electricos-siguen-siendo-seguros\">article by Daniel Murias for Motorpasión</a> (in Spanish). I consider it a risk to drive the car in this state. And I don’t think I should risk my life, or the lives of other people, to find out whether the car’s braking or suspension systems are working properly. This should be done by a mechanic, either at a garage or a Tesla Service Center. But this is Tesla, and it seems like <strong>no one is behind the wheel</strong>.</p>","contentLength":23616,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42998024"},{"title":"Why blog if nobody reads it?","url":"https://andysblog.uk/why-blog-if-nobody-reads-it/","date":1739123410,"author":"alexgiann","guid":228,"unread":true,"content":"<p>Back in the day, advertising legend Bill Bernbach said, “The most powerful element in advertising is the truth.”</p><p>Let’s tell the truth, then: Nobody reads your blog.</p><p>At least, not as many as you’d like. Maybe a handful, maybe none. You pour your thoughts into it, craft each sentence, pick the right image — then silence. No likes, no shares, no engagement.</p><p>There’s two lies we tell ourselves:</p><ol><li><em>If I write it, they will come.</em> They won’t. There are billions of blog posts out there. The internet is an infinite void, and your blog is a whisper in a hurricane.</li><li>If nobody reads it, it’s a waste of time.</li></ol><p>There is a hidden value in blogging.\nThere’s an old Zen saying: “Chop wood, carry water.” You do it not for the applause but because it needs doing.</p><p>Blogging forces clarity. It makes you structure your thoughts, sharpen your perspective. You stop writing fluff because — let’s be honest — you’re writing for yourself. And if you can’t keep yourself interested, nobody else stands a chance.</p><p>When you write, you think better. When you think better, you create better.</p><p>So, who’s there real audience?</p><p>You’re not just writing for today’s invisible audience. You’re writing for:</p><ul><li>Future you. Your posts become a time capsule of your evolving mind.</li><li>One right person. Maybe one day, someone stumbles across your words at exactly the right moment. And that changes something for them.</li><li>The work itself. Consistency beats virality. A hundred posts with depth will outlast a single viral hit.</li></ul><p>And there’s another thing. My other passion is street photography. Which is a bit like blogging.</p><p>You walk through the city, camera in hand. You see a scene — a moment of light, shadow, humanity. You capture it.</p><p>But that’s not why you did it. You did it because you saw something.</p><p>Like blogging. You write because you think, because you observe, because you need to put it somewhere.</p><p>If someone reads it? Bonus. If not? The work still got done.</p><p>And that’s the real point.</p>","contentLength":1978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42992159"},{"title":"Modern-Day Oracles or Bullshit Machines? How to thrive in a ChatGPT world","url":"https://thebullshitmachines.com/","date":1739089457,"author":"ctbergstrom","guid":227,"unread":true,"content":"<p>In a series of five- to ten-minute lessons, we will explain what these machines are, how they work, and how to thrive in a world where they are everywhere.</p><p>You will learn when these systems can save you a lot of time and effort. You will learn when they are likely to steer you wrong. And you will discover how to see through the hype to tell the difference. </p>","contentLength":358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42989320"},{"title":"Jacksonpollock.org (2003)","url":"https://jacksonpollock.org/","date":1739049769,"author":"memalign","guid":226,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42986320"},{"title":"We are destroying software","url":"https://antirez.com/news/145","date":1739026105,"author":"antirez","guid":225,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42983275"},{"title":"Teen on Musk's DOGE team graduated from 'The Com'","url":"https://krebsonsecurity.com/2025/02/teen-on-musks-doge-team-graduated-from-the-com/","date":1739007264,"author":"mmsc","guid":224,"unread":true,"content":"<p> reported this week that a 19-year-old working for ‘s so-called <strong>Department of Government Efficiency</strong> (DOGE) was given access to sensitive US government systems even though his past association with cybercrime communities should have precluded him from gaining the necessary security clearances to do so. As today’s story explores, the DOGE teen is a former denizen of ‘,’ an archipelago of Discord and Telegram chat channels that function as a kind of distributed cybercriminal social network for facilitating instant collaboration.</p><p>Since President Trump’s second inauguration, Musk’s DOGE team has gained access to a truly staggering amount of personal and sensitive data on American citizens, moving quickly to seize control over databases at the , the <strong>Office of Personnel Management</strong>, the , and the <strong>Department of Health and Human Resources</strong>, among others.</p><p> first <a href=\"https://archive.ph/HCCA4\" target=\"_blank\" rel=\"noopener\">reported on Feb. 2</a> that one of the technologists on Musk’s crew is a 19-year-old high school graduate named , who reportedly goes by the nickname “Big Balls” online. One of the companies Coristine founded, , was set up in 2021, when he would have been around 16 years old.</p><p>“Tesla.Sexy LLC controls dozens of web domains, including at least two Russian-registered domains,”  reported. “One of those domains, which is still active, offers a service called Helfie, which is an AI bot for Discord servers targeting the Russian market. While the operation of a Russian website would not violate US sanctions preventing Americans doing business with Russian companies, it could potentially be a factor in a security clearance review.”</p><p>Mr. Coristine has not responded to requests for comment. In <a href=\"https://archive.ph/VAD48\" target=\"_blank\" rel=\"noopener\">a follow-up story</a> this week,  found that someone using a Telegram handle tied to Coristine solicited a DDoS-for-hire service in 2022, and that he worked for a short time at a company that specializes in protecting customers from DDoS attacks.</p><div><img aria-describedby=\"caption-attachment-70369\" decoding=\"async\" src=\"https://krebsonsecurity.com/wp-content/uploads/2025/02/coristine.png\" alt=\"\" width=\"637\" height=\"636\"><p>A profile photo from Coristine’s WhatsApp account.</p></div><p>Internet routing records <a href=\"https://bgp.he.net/net/23.133.104.0/24#_whois\" target=\"_blank\" rel=\"noopener\">show</a> that Coristine runs an Internet service provider called  (AS400495). Also known as “,” Packetware <a href=\"https://bgp.he.net/net/23.133.104.0/24#_dnsrecords\" target=\"_blank\" rel=\"noopener\">currently hosts</a> tesla[.]sexy and diamondcdn[.]com, among other domains.</p><p>DiamondCDN was advertised and claimed by someone who used the nickname “” on several Com-based Discord channels over the years. A review of chat logs from some of those channels show other members frequently referred to Rivage as “Edward.”</p><p>From late 2020 to late 2024, Rivage’s conversations would show up in multiple Com chat servers that are closely monitored by security companies. In November 2022, Rivage could be seen requesting recommendations for a reliable and powerful DDoS-for-hire service.</p><p>Rivage made that request in the cybercrime channel “,” a core Com hub where users could buy and sell attack services. <a href=\"https://www.bleepingcomputer.com/news/security/ddos-site-dstatcc-seized-and-two-suspects-arrested-in-germany/\" target=\"_blank\" rel=\"noopener\">Dstat’s website dstat[.]cc was seized</a> in 2024 as part of “Operation PowerOFF,” an international law enforcement action against DDoS services.</p><p>Coristine’s LinkedIn profile said that in 2022 he worked at an anti-DDoS company called , which  generously described as a “network monitoring firm known for hiring reformed blackhat hackers.”  wrote:</p><p>“At Path Network, Coristine worked as a systems engineer from April to June of 2022, according to his now-deleted LinkedIn résumé. Path has at times listed as employees , also known as Cosmo the God, a well-known former cybercriminal and member of the hacker group UGNazis, as well as Matthew Flannery, an Australian convicted hacker whom police allege was a member of the hacker group LulzSec. It’s unclear whether Coristine worked at Path concurrently with those hackers, and WIRED found no evidence that either Coristine or other Path employees engaged in illegal activity while at the company.”</p><p>The other founder of BackConnect Security LLC was , a Georgia man who <a href=\"https://krebsonsecurity.com/2020/01/ddos-mitigation-firm-founder-admits-to-ddos/\" target=\"_blank\" rel=\"noopener\">pleaded guilty in 2020</a> to paying a DDoS-for-hire service to launch attacks against others.</p><p>The aforementioned Path employee Eric Taylor <a href=\"https://krebsonsecurity.com/2017/02/men-who-sent-swat-team-heroin-to-my-home-sentenced/\" target=\"_blank\" rel=\"noopener\">pleaded guilty in 2017</a> to charges including <a href=\"http://krebsonsecurity.com/2013/03/the-world-has-no-room-for-cowards/\" target=\"_blank\" rel=\"noopener\">an attack on our home in 2013</a>. Taylor was among several men involved in making a false report to my local police department about a supposed hostage situation at our residence in Virginia. In response, a heavily-armed police force surrounded my home and put me in handcuffs at gunpoint before the police realized it was all a dangerous hoax known as “swatting.”</p><p>CosmoTheGod rocketed to Internet infamy in 2013 when he and a number of other hackers <a href=\"https://krebsonsecurity.com/2013/03/credit-reports-sold-for-cheap-in-the-underweb/\" target=\"_blank\" rel=\"noopener\">set up the Web site exposed[dot]su</a>, which “doxed” dozens of public officials and celebrities by publishing the address, Social Security numbers and other personal information on the former First Lady Michelle Obama, the then-director of the FBI and the U.S. attorney general, among others. The group also swatted many of the people they doxed.</p><p> noted that Coristine only worked at Path for a few months in 2022, but the story didn’t mention why his tenure was so short. A screenshot shared on the website <a href=\"https://www.pathtruths.com\" target=\"_blank\" rel=\"noopener\">pathtruths.com</a> includes a snippet of conversations in June 2022 between Path employees discussing Coristine’s firing.</p><p>According to that record, Path founder Marshal Webb dismissed Coristine for leaking internal documents to a competitor. Not long after Coristine’s termination, someone leaked an abundance of internal Path documents and conversations. Among other things, those chats revealed that one of Path’s technicians was a Canadian man named  who was convicted in 2017 of <a href=\"https://krebsonsecurity.com/2017/09/canadian-man-gets-9-months-detention-for-serial-swattings-bomb-threats/\" target=\"_blank\" rel=\"noopener\">perpetrating dozens of swatting attacks and fake bomb threats</a> — including at least two attempts against our home in 2014.</p><div><img aria-describedby=\"caption-attachment-70370\" decoding=\"async\" loading=\"lazy\" src=\"https://krebsonsecurity.com/wp-content/uploads/2025/02/pathchat.png\" alt=\"\" width=\"585\" height=\"808\"><p>A snippet of text from an internal Path chat room, wherein members discuss the reason for Coristine’s termination: Allegedly, leaking internal company information. Source: Pathtruths.com.</p></div><p>On May 11, 2024, Rivage posted on a Discord channel for a DDoS protection service that is chiefly marketed to members of The Com. Rivage expressed frustration with his time spent on Com-based communities, suggesting that its profitability had been oversold.</p><p>“I don’t think there’s a lot of money to be made in the com,” Rivage lamented. “I’m not buying Heztner [servers] to set up some com VPN.”</p><p>Rivage largely stopped posting messages on Com channels after that.  reports that Coristine subsequently spent three months last summer working at , Elon Musk’s brain implant startup.</p><p>The trouble with all this is that even if someone sincerely intends to exit The Com after years of consorting with cybercriminals, they are often still subject to personal attacks, harassment and hacking long after they have left the scene.</p><p>That’s because a huge part of Com culture involves harassing, swatting and hacking other members of the community. These internecine attacks are often for financial gain, but just as frequently they are perpetrated by cybercrime groups to exact retribution from or assert dominance over rival gangs.</p><p>Experts say it is extremely difficult for former members of violent street gangs to gain a security clearance needed to view sensitive or classified information held by the U.S. government. That’s because ex-gang members are highly susceptible to extortion and coercion from current members of the same gang, and that alone presents an unacceptable security risk for intelligence agencies.</p><p>And make no mistake: The Com is the English-language cybercriminal hacking equivalent of a violent street gang. KrebsOnSecurity has published numerous stories detailing how feuds within the community <a href=\"https://krebsonsecurity.com/2022/09/violence-as-a-service-brickings-firebombings-shootings-for-hire/\" target=\"_blank\" rel=\"noopener\">periodically spill over into real-world violence</a>.</p><p>When Coristine’s name surfaced in ‘s report this week, members of The Com immediately took notice.&nbsp;In the following segment from a February 5, 2025 chat in a Com-affiliated hosting provider, members criticized Rivage’s skills, and discussed harassing his family and notifying authorities about incriminating accusations that may or may not be true.</p><blockquote><p>2025-02-05 16:29:44 UTC vperked#0 they got this nigga on indiatimes man\n2025-02-05 16:29:46 UTC alexaloo#0 Their cropping is worse than AI could have done<p>\n2025-02-05 16:29:48 UTC hebeatsme#0 bro who is that</p>\n2025-02-05 16:29:53 UTC hebeatsme#0 yalla re talking about<p>\n2025-02-05 16:29:56 UTC xewdy#0 edward</p>\n2025-02-05 16:29:56 UTC .yarrb#0 rivagew<p>\n2025-02-05 16:29:57 UTC vperked#0 Rivarge</p>\n2025-02-05 16:29:57 UTC xewdy#0 diamondcdm<p>\n2025-02-05 16:29:59 UTC vperked#0 i cant spell it</p>\n2025-02-05 16:30:00 UTC hebeatsme#0 rivage<p>\n2025-02-05 16:30:08 UTC .yarrb#0 yes</p>\n2025-02-05 16:30:14 UTC hebeatsme#0 i have him added<p>\n2025-02-05 16:30:20 UTC hebeatsme#0 hes on discord still</p>\n2025-02-05 16:30:47 UTC .yarrb#0 hes focused on stroking zaddy elon<p>\n2025-02-05 16:30:47 UTC vperked#0 https://en.wikipedia.org/wiki/Edward_Coristine</p>\n2025-02-05 16:30:50 UTC vperked#0 no fucking way<p>\n2025-02-05 16:30:53 UTC vperked#0 they even made a wiki for him</p>\n2025-02-05 16:30:55 UTC vperked#0 LOOOL<p>\n2025-02-05 16:31:05 UTC hebeatsme#0 no way</p>\n2025-02-05 16:31:08 UTC hebeatsme#0 hes not a good dev either<p>\n2025-02-05 16:31:14 UTC hebeatsme#0 like????</p>\n2025-02-05 16:31:22 UTC hebeatsme#0 has to be fake<p>\n2025-02-05 16:31:24 UTC xewdy#0 and theyre saying ts</p>\n2025-02-05 16:31:29 UTC xewdy#0 like ok bro<p>\n2025-02-05 16:31:51 UTC .yarrb#0 now i wanna know what all the other devs are like…</p>\n2025-02-05 16:32:00 UTC vperked#0 “`Coristine used the moniker “bigballs” on LinkedIn and @Edwardbigballer on Twitter, according to The Daily Dot.[“`<p>\n2025-02-05 16:32:05 UTC vperked#0 LOL</p>\n2025-02-05 16:32:06 UTC hebeatsme#0 lmfaooo<p>\n2025-02-05 16:32:07 UTC vperked#0 bro</p>\n2025-02-05 16:32:10 UTC hebeatsme#0 bro<p>\n2025-02-05 16:32:17 UTC hebeatsme#0 has to be fake right</p>\n2025-02-05 16:32:22 UTC .yarrb#0 does it mention Rivage?<p>\n2025-02-05 16:32:23 UTC xewdy#0 He previously worked for NeuraLink, a brain computer interface company led by Elon Musk</p>\n2025-02-05 16:32:26 UTC xewdy#0 bro what<p>\n2025-02-05 16:32:27 UTC alexaloo#0 I think your current occupation gives you a good insight of what probably goes on</p>\n2025-02-05 16:32:29 UTC hebeatsme#0 bullshit man<p>\n2025-02-05 16:32:33 UTC xewdy#0 this nigga got hella secrets</p>\n2025-02-05 16:32:37 UTC hebeatsme#0 rivage couldnt print hello world<p>\n2025-02-05 16:32:42 UTC hebeatsme#0 if his life was on the line</p>\n2025-02-05 16:32:50 UTC xewdy#0 nigga worked for neuralink<p>\n2025-02-05 16:32:54 UTC hebeatsme#0 bullshit</p>\n2025-02-05 16:33:06 UTC Nashville Dispatch ##0000 ||@PD Ping||<p>\n2025-02-05 16:33:07 UTC hebeatsme#0 must have killed all those test pigs with some bugs</p>\n2025-02-05 16:33:24 UTC hebeatsme#0 ur telling me the rivage who failed to start a company<p>\n2025-02-05 16:33:28 UTC hebeatsme#0 https://cdn.camp</p>\n2025-02-05 16:33:32 UTC hebeatsme#0 who didnt pay for servers<p>\n2025-02-05 16:33:34 UTC hebeatsme#0 ?</p>\n2025-02-05 16:33:42 UTC hebeatsme#0 was too cheap<p>\n2025-02-05 16:33:44 UTC vperked#0 yes</p>\n2025-02-05 16:33:50 UTC hebeatsme#0 like??<p>\n2025-02-05 16:33:53 UTC hebeatsme#0 it aint adding up</p>\n2025-02-05 16:33:56 UTC alexaloo#0 He just needed to find his calling idiot.<p>\n2025-02-05 16:33:58 UTC alexaloo#0 He found it.</p>\n2025-02-05 16:33:59 UTC hebeatsme#0 bro<p>\n2025-02-05 16:34:01 UTC alexaloo#0 Cope in a river dude</p>\n2025-02-05 16:34:04 UTC hebeatsme#0 he cant make good money right<p>\n2025-02-05 16:34:08 UTC hebeatsme#0 doge is about efficiency</p>\n2025-02-05 16:34:11 UTC hebeatsme#0 he should make $1/he<p>\n2025-02-05 16:34:15 UTC hebeatsme#0 $1/hr</p>\n2025-02-05 16:34:25 UTC hebeatsme#0 and be whipped for better code<p>\n2025-02-05 16:34:26 UTC vperked#0 prolly makes more than us</p>\n2025-02-05 16:34:35 UTC vperked#0 with his dad too<p>\n2025-02-05 16:34:52 UTC hebeatsme#0 time to report him for fraud</p>\n2025-02-05 16:34:54 UTC hebeatsme#0 to donald trump<p>\n2025-02-05 16:35:04 UTC hebeatsme#0 rivage participated in sim swap hacks in 2018</p>\n2025-02-05 16:35:08 UTC hebeatsme#0 put that on his wiki<p>\n2025-02-05 16:35:10 UTC hebeatsme#0 thanks</p>\n2025-02-05 16:35:15 UTC hebeatsme#0 and in 2021<p>\n2025-02-05 16:35:17 UTC hebeatsme#0 thanks</p>\n2025-02-05 16:35:19 UTC chainofcommand#0 i dont think they’ll care tbh</p></blockquote><p>Given the speed with which Musk’s DOGE team was allowed access to such critical government databases, it strains credulity that Coristine could have been properly cleared beforehand. After all, he’d recently been dismissed from a job <em>for allegedly leaking internal company information to outsiders</em>.</p><p>According to the national security adjudication <a href=\"https://www.dni.gov/files/NCSC/documents/Regulations/SEAD-4-Adjudicative-Guidelines-U.pdf\" target=\"_blank\" rel=\"noopener\">guidelines</a> (PDF) released by the <strong>Director of National Intelligence</strong> (DNI), eligibility determinations take into account a person’s stability, trustworthiness, reliability, discretion, character, honesty, judgment, and ability to protect classified information.</p><p>The DNI policy further states that “eligibility for covered individuals shall be granted only when facts and circumstances indicate that eligibility is clearly consistent with the national security interests of the United States, and any doubt shall be resolved in favor of national security.”</p><p>On Thursday, 25-year-old DOGE staff member  resigned after being linked to a deleted social media account that advocated racism and eugenics. Elez resigned after <a href=\"https://www.wsj.com/tech/doge-staffer-resigns-over-racist-posts-d9f11a93\" target=\"_blank\" rel=\"noopener\">asked</a> the White House about his connection to the account.</p><p>“Just for the record, I was racist before it was cool,” the account posted in July. “You could not pay me to marry outside of my ethnicity,” the account wrote on X in September. “Normalize Indian hate,” the account wrote the same month, in reference to a post noting the prevalence of people from India in Silicon Valley.</p><p>Elez’s resignation came a day after the Department of Justice agreed to limit the number of DOGE employees who have access to federal payment systems. The DOJ said access would be limited to two people, Elez and , the CEO of a company called Cloud Software Group.</p><p>Earlier today, Musk <a href=\"https://thehill.com/business/5133430-elon-musk-doge-staffer-racist-posts/\" target=\"_blank\" rel=\"noopener\">said he planned to rehire Elez</a> after  and  reportedly endorsed the idea. Speaking at The White House today, Trump said he wasn’t concerned about the security of personal information and other data accessed by DOGE, adding that he was “very proud of the job that this group of young people” are doing.</p><p>A White House official <a href=\"https://www.reuters.com/world/us/staffer-with-elon-musks-doge-amplified-white-supremacists-online-2025-02-07/\" target=\"_blank\" rel=\"noopener\">told</a> on Wednesday that Musk and his engineers have appropriate security clearances and are operating in “full compliance with federal law, appropriate security clearances, and as employees of the relevant agencies, not as outside advisors or entities.”</p><p><a href=\"https://www.npr.org/2025/02/07/nx-s1-5288988/doge-elon-musk-staff-trump\" target=\"_blank\" rel=\"noopener\">reports</a> Trump added that his administration’s cost-cutting efforts would soon turn to the Education Department and the Pentagon, “where he suggested without evidence that there could be ‘trillions’ of dollars in wasted spending within the $6.75 trillion the federal government spent in fiscal year 2024.”</p><p>GOP leaders in the Republican-controlled House and Senate have largely shrugged about Musk’s ongoing efforts to seize control over federal databases, dismantle agencies mandated by Congress, freeze federal spending on a range of already-appropriated government programs, and threaten workers with layoffs.</p><p>Meanwhile, multiple parties have sued to stop DOGE’s activities. ABC News <a href=\"https://abcnews.go.com/US/judge-decide-block-doge-accessing-sensitive-labor-department/story?id=118575362\" target=\"_blank\" rel=\"noopener\">says</a> a federal judge was to rule today on whether DOGE should be blocked from accessing Department of Labor records, following a lawsuit alleging Musk’s team sought to illegally access highly sensitive data, including medical information, from the federal government.</p><p>At least 13 state attorneys general say they plan to file a lawsuit to stop DOGE from accessing federal payment systems containing Americans’ sensitive personal information, <a href=\"https://www.mychamplainvalley.com/news/local-news/vermont-joins-lawsuit-over-doge-access-to-government-payment-system/\" target=\"_blank\" rel=\"noopener\">reports</a>.</p><p>Reuters <a href=\"https://www.reuters.com/world/us/us-treasury-temporarily-agrees-limit-doge-access-amid-privacy-lawsuit-2025-02-06/\" target=\"_blank\" rel=\"noopener\">reported Thursday</a> that the U.S. Treasury Department had agreed not to give Musk’s team access to its payment systems while a judge is hearing arguments in a lawsuit by employee unions and retirees alleging Musk illegally searched those records.</p><p><a href=\"https://arstechnica.com/tech-policy/2025/02/lawsuit-tries-to-block-doges-access-to-student-loan-data/\" target=\"_blank\" rel=\"noopener\">writes</a> that The Department of Education (DoE) was sued Friday by a California student association demanding an “immediate stop” to DOGE’s “unlawfully” digging through student loan data to potentially dismantle the DoE.</p>","contentLength":15941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42981756"},{"title":"VSCode’s SSH agent is bananas","url":"https://fly.io/blog/vscode-ssh-wtf/","date":1738977932,"author":"zdyxry","guid":223,"unread":true,"content":"<p>We’re interested in getting integrated into the flow VSCode uses to do remote editing over SSH, because everybody is using VSCode now, and, in particular, they’re using forks of VSCode that generate code with LLMs. </p><div><p>”hallucination” is what we call it when LLMs get code wrong; “engineering” is what we call it when people do.</p></div><p>LLM-generated code is <a href=\"https://nicholas.carlini.com/writing/2024/how-i-use-ai.html\" title=\"\">useful in the general case</a> if you know what you’re doing. But it’s ultra-useful if you can close the loop between the LLM and the execution environment (with an “Agent” setup). There’s lots to say about this, but for the moment: it’s a semi-effective antidote to hallucination: the LLM generates the code, the agent scaffolding runs the code, the code generates errors, the agent feeds it back to the LLM, the process iterates. </p><p>So, obviously, the issue here is you don’t want this iterative development process happening on your development laptop, because LLMs have boundary issues, and they’ll iterate on your system configuration just as happily on the Git project you happen to be working in. A thing you’d really like to be able to do: run a closed-loop agent-y (“agentic”? is that what we say now) configuration for an LLM, on a clean-slate Linux instance that spins up instantly and that can’t screw you over in any way. You get where we’re going with this.</p><p>Anyways! I would like to register a concern.</p><p>Emacs hosts the spiritual forebearer of remote editing systems, a blob of hyper-useful Elisp called <a href=\"https://www.gnu.org/software/tramp/\" title=\"\">“Tramp”</a>. If you can hook Tramp up to any kind of interactive environment — usually, an SSH session — where it can run Bourne shell commands, it can extend Emacs to that environment.</p><p>So, VSCode has a feature like Tramp. Which, neat, right? You’d think, take Tramp, maybe simplify it a bit, switch out Elisp for Typescript.</p><p>Unlike Tramp, which lives off the land on the remote connection, VSCode mounts a full-scale invasion: it runs a Bash snippet stager that downloads an agent, including a binary installation of Node. </p><p>The agent runs over port-forwarded SSH. It establishes a WebSockets connection back to your running VSCode front-end. The underlying protocol on that connection can:</p><ul><li>Wander around the filesystem\n</li><li>Launch its own shell PTY processes\n</li></ul><p>In security-world, there’s a name for tools that work this way. I won’t say it out loud, because that’s not fair to VSCode, but let’s just say the name is murid in nature.</p><p>I would be a little nervous about letting people VSCode-remote-edit stuff on dev servers, and apoplectic if that happened during an incident on something in production. </p><p>It turns out we don’t have to care about any of this to get a custom connection to a Fly Machine working in VSCode, so none of this matters in any kind of deep way, but: we’ve decided to just be a blog again, so: we had to learn this, and now you do too.</p>","contentLength":2855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42979467"},{"title":"Obscure islands I find interesting","url":"https://amanvir.com/obscure-islands","date":1738967011,"author":"venusgirdle","guid":222,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42978199"},{"title":"Do-nothing scripting: the key to gradual automation (2019)","url":"https://blog.danslimmon.com/2019/07/15/do-nothing-scripting-the-key-to-gradual-automation/","date":1738957696,"author":"tehnub","guid":221,"unread":true,"content":"<p>Every ops team has some manual procedures that they haven’t gotten around to automating yet. <a href=\"https://landing.google.com/sre/sre-book/chapters/eliminating-toil/\">Toil</a> can never be totally eliminated.</p><p>Very often, the biggest toil center for a team at a growing company will be its procedure for modifying infrastructure or its procedure for provisioning user accounts. Partial instructions for the latter might look like this:</p><ol><li>Create an SSH key pair for the user.</li><li>Commit the public key to Git and push to master.</li><li>Wait for the build job to finish.</li><li>Find the user’s email address in the employee directory.</li><li>Send the user their private key via 1Password.</li></ol><p>This is a relatively short example. Sometimes there are 20 steps in the process. Sometimes there are branches and special cases to keep track of as you go. Over time, these procedures can become unmanageably large and complex.</p><p>Procedures like this are frustrating because they’re focus-intensive yet require very little thought. They demand our full attention, but our attention isn’t rewarded with interesting problems or satisfying solutions – just another checkbox checked. I have a word for a procedure like this: a .</p><p>We know that this procedure is ripe for automation. We can easily see how to automate any given step. And we know that a computer could carry out the instructions with far greater speed and accuracy than we can, and with less tendency toward <a href=\"https://risk-engineering.org/concept/Rasmussen-practical-drift\">practical drift</a>.</p><p>However, automating slogs sometimes feels like an all-or-nothing proposition. Sure, we could write a script to handle step 2, or step 5. But that wouldn’t  make the procedure any less cumbersome. It would lead to a proliferation of single-purpose scripts with different conventions and expectations, and you’d still have to follow a documented multi-step procedure for using those scripts.</p><p>This perception of futility is the problem we really need to solve in order to escape from these manual slogs. I’ve found an approach that works pretty reliably: .</p><p>Almost any slog can be turned into a . A do-nothing script is a script that encodes the instructions of a slog, encapsulating each step in a function. For the example procedure above, we could write the following do-nothing script:</p><div><pre title=\"\">import sys\n\ndef wait_for_enter():\n    raw_input(\"Press Enter to continue: \")\n\nclass CreateSSHKeypairStep(object):\n    def run(self, context):\n        print(\"Run:\")\n        print(\"   ssh-keygen -t rsa -f ~/{0}\".format(context[\"username\"]))\n        wait_for_enter()\n\nclass GitCommitStep(object):\n    def run(self, context):\n        print(\"Copy ~/new_key.pub into the `user_keys` Git repository, then run:\")\n        print(\"    git commit {0}\".format(context[\"username\"]))\n        print(\"    git push\")\n        wait_for_enter()\n\nclass WaitForBuildStep(object):\n    build_url = \"http://example.com/builds/user_keys\"\n    def run(self, context):\n        print(\"Wait for the build job at {0} to finish\".format(self.build_url))\n        wait_for_enter()\n\nclass RetrieveUserEmailStep(object):\n    dir_url = \"http://example.com/directory\"\n    def run(self, context):\n        print(\"Go to {0}\".format(self.dir_url))\n        print(\"Find the email address for user `{0}`\".format(context[\"username\"]))\n        context[\"email\"] = raw_input(\"Paste the email address and press enter: \")\n\nclass SendPrivateKeyStep(object):\n    def run(self, context):\n        print(\"Go to 1Password\")\n        print(\"Paste the contents of ~/new_key into a new document\")\n        print(\"Share the document with {0}\".format(context[\"email\"]))\n        wait_for_enter()\n\nif __name__ == \"__main__\":\n    context = {\"username\": sys.argv[1]}\n    procedure = [\n        CreateSSHKeypairStep(),\n        GitCommitStep(),\n        WaitForBuildStep(),\n        RetrieveUserEmailStep(),\n        SendPrivateKeyStep(),\n    ]\n    for step in procedure:\n        step.run(context)\n    print(\"Done.\")\n</pre></div><p>This script doesn’t actually  any of the steps of the procedure. That’s why it’s called a do-nothing script. It feeds the user a step at a time and waits for them to complete each step manually.</p><p>At first glance, it might not be obvious that this script provides value. Maybe it looks like all we’ve done is make the instructions harder to read. But the value of a do-nothing script is immense:</p><ul><li>It’s now much less likely that you’ll lose your place and skip a step. This makes it easier to maintain focus and power through the slog.</li><li>Each step of the procedure is now encapsulated in a function, which makes it possible to replace the text in any given step with code that performs the action automatically.</li><li>Over time, you’ll develop a library of useful steps, which will make future automation tasks more efficient.</li></ul><p>A do-nothing script doesn’t save your team any manual effort. It lowers the activation energy for automating tasks, which allows the team to eliminate toil over time.</p>","contentLength":4783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42976698"},{"title":"Asahi Linux lead developer Hector Martin resigns from Linux kernel","url":"https://lkml.org/lkml/2025/2/7/9","date":1738933323,"author":"Mond_","guid":220,"unread":true,"content":"I no longer have any faith left in the kernel development process orcommunity management approach.<p>Apple/ARM platform development will continue downstream. If I feel like</p>sending some patches upstream in the future myself for whatever subtree<p>I may, or I may not. Anyone who feels like fighting the upstreaming</p>fight themselves is welcome to do so.<p>Signed-off-by: Hector Martin &lt;marcan@marcan.st&gt;</p>--- 1 file changed, 1 deletion(-)<p>diff --git a/MAINTAINERS b/MAINTAINERS</p>index 1e930c7a58b13d8bbe6bf133ba7b36aa24c2b5e0..c9623439998709c9d6d6944cbd87e025356422da 100644+++ b/MAINTAINERS<p>@@ -2177,7 +2177,6 @@ F:\tsound/soc/codecs/cs42l84.*</p> F:\tsound/soc/codecs/ssm3515.c<p> ARM/APPLE MACHINE SUPPORT</p>-M:\tHector Martin &lt;marcan@marcan.st&gt;<p> M:\tSven Peter &lt;sven@svenpeter.dev&gt;</p> R:\tAlyssa Rosenzweig &lt;alyssa@rosenzweig.io&gt;---<p>base-commit: 40384c840ea1944d7c5a392e8975ed088ecf0b37</p>change-id: 20250207-rm-maint-af7cccc22871-- <p>Hector Martin &lt;marcan@marcan.st&gt;</p>","contentLength":928,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42972062"}],"tags":["dev","hn"]}