{"id":"MvwLKznkcWQJt9LV3qspiNNstpReRGojdXM3bsYDh","title":"Kubernetes Blog","displayTitle":"Dev - Kubernetes Blog","url":"https://kubernetes.io/feed.xml","feedLink":"https://kubernetes.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":3,"items":[{"title":"Spotlight on SIG Apps","url":"https://kubernetes.io/blog/2025/03/12/sig-apps-spotlight-2025/","date":1741737600,"author":"","guid":486,"unread":true,"content":"<p>In our ongoing SIG Spotlight series, we dive into the heart of the Kubernetes project by talking to\nthe leaders of its various Special Interest Groups (SIGs). This time, we focus on\n,\nthe group responsible for everything related to developing, deploying, and operating applications on\nKubernetes. <a href=\"https://www.linkedin.com/in/sandipanpanda\">Sandipan Panda</a>\n(<a href=\"https://www.devzero.io/\">DevZero</a>) had the opportunity to interview <a href=\"https://github.com/soltysh\">Maciej\nSzulik</a> (<a href=\"https://defenseunicorns.com/\">Defense Unicorns</a>) and <a href=\"https://github.com/janetkuo\">Janet\nKuo</a> (<a href=\"https://about.google/\">Google</a>), the chairs and tech leads of\nSIG Apps. They shared their experiences, challenges, and visions for the future of application\nmanagement within the Kubernetes ecosystem.</p><p><strong>Sandipan: Hello, could you start by telling us a bit about yourself, your role, and your journey\nwithin the Kubernetes community that led to your current roles in SIG Apps?</strong></p><p>: Hey, my name is Maciej, and Iâ€™m one of the leads for SIG Apps. Aside from this role, you\ncan also find me helping\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-cli#readme\">SIG CLI</a> and also being one of\nthe Steering Committee members. Iâ€™ve been contributing to Kubernetes since late 2014 in various\nareas, including controllers, apiserver, and kubectl.</p><p>: Certainly! I'm Janet, a Staff Software Engineer at Google, and I've been deeply involved\nwith the Kubernetes project since its early days, even before the 1.0 launch in 2015. It's been an\namazing journey!</p><p>My current role within the Kubernetes community is one of the chairs and tech leads of SIG Apps. My\njourney with SIG Apps started organically. I started with building the Deployment API and adding\nrolling update functionalities. I naturally gravitated towards SIG Apps and became increasingly\ninvolved. Over time, I took on more responsibilities, culminating in my current leadership roles.</p><p><em>All following answers were jointly provided by Maciej and Janet.</em></p><p><strong>Sandipan: For those unfamiliar, could you provide an overview of SIG Apps' mission and objectives?\nWhat key problems does it aim to solve within the Kubernetes ecosystem?</strong></p><p>As described in our\n<a href=\"https://github.com/kubernetes/community/blob/master/sig-apps/charter.md#scope\">charter</a>, we cover a\nbroad area related to developing, deploying, and operating applications on Kubernetes. That, in\nshort, means weâ€™re open to each and everyone showing up at our bi-weekly meetings and discussing the\nups and downs of writing and deploying various applications on Kubernetes.</p><p><strong>Sandipan: What are some of the most significant projects or initiatives currently being undertaken\nby SIG Apps?</strong></p><p>At this point in time, the main factors driving the development of our controllers are the\nchallenges coming from running various AI-related workloads. Itâ€™s worth giving credit here to two\nworking groups weâ€™ve sponsored over the past years:</p><h2>Best practices and challenges</h2><p><strong>Sandipan: SIG Apps plays a crucial role in developing application management best practices for\nKubernetes. Can you share some of these best practices and how they help improve application\nlifecycle management?</strong></p><ol><li><p>Implementing <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\">health checks and readiness probes</a>\nensures that your applications are healthy and ready to serve traffic, leading to improved\nreliability and uptime. The above, combined with comprehensive logging, monitoring, and tracing\nsolutions, will provide insights into your application's behavior, enabling you to identify and\nresolve issues quickly.</p></li><li><p><a href=\"https://kubernetes.io/docs/concepts/workloads/autoscaling/\">Auto-scale your application</a> based\non resource utilization or custom metrics, optimizing resource usage and ensuring your\napplication can handle varying loads.</p></li><li><p>Use Deployment for stateless applications, StatefulSet for stateful applications, Job\nand CronJob for batch workloads, and DaemonSet for running a daemon on each node. Use\nOperators and CRDs to extend the Kubernetes API to automate the deployment, management, and\nlifecycle of complex applications, making them easier to operate and reducing manual\nintervention.</p></li></ol><p><strong>Sandipan: What are some of the common challenges SIG Apps faces, and how do you address them?</strong></p><p>The biggest challenge weâ€™re facing all the time is the need to reject a lot of features, ideas, and\nimprovements. This requires a lot of discipline and patience to be able to explain the reasons\nbehind those decisions.</p><p><strong>Sandipan: How has the evolution of Kubernetes influenced the work of SIG Apps? Are there any\nrecent changes or upcoming features in Kubernetes that you find particularly relevant or beneficial\nfor SIG Apps?</strong></p><p>The main benefit for both us and the whole community around SIG Apps is the ability to extend\nkubernetes with <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">Custom Resource Definitions</a>\nand the fact that users can build their own custom controllers leveraging the built-in ones to\nachieve whatever sophisticated use cases they might have and we, as the core maintainers, havenâ€™t\nconsidered or werenâ€™t able to efficiently resolve inside Kubernetes.</p><p><strong>Sandipan: What opportunities are available for new contributors who want to get involved with SIG\nApps, and what advice would you give them?</strong></p><p>We get the question, \"What good first issue might you recommend we start with?\" a lot :-) But\nunfortunately, thereâ€™s no easy answer to it. We always tell everyone that the best option to start\ncontributing to core controllers is to find one you are willing to spend some time with. Read\nthrough the code, then try running unit tests and integration tests focusing on that\ncontroller. Once you grasp the general idea, try breaking it and the tests again to verify your\nbreakage. Once you start feeling confident you understand that particular controller, you may want\nto search through open issues affecting that controller and either provide suggestions, explaining\nthe problem users have, or maybe attempt your first fix.</p><p>Like we said, there are no shortcuts on that road; you need to spend the time with the codebase to\nunderstand all the edge cases weâ€™ve slowly built up to get to the point where we are. Once youâ€™re\nsuccessful with one controller, youâ€™ll need to repeat that same process with others all over again.</p><p><strong>Sandipan: How does SIG Apps gather feedback from the community, and how is this feedback\nintegrated into your work?</strong></p><p>We always encourage everyone to show up and present their problems and solutions during our\nbi-weekly <a href=\"https://github.com/kubernetes/community/tree/master/sig-apps#meetings\">meetings</a>. As long\nas youâ€™re solving an interesting problem on top of Kubernetes and you can provide valuable feedback\nabout any of the core controllers, weâ€™re always happy to hear from everyone.</p><p><strong>Sandipan: Looking ahead, what are the key focus areas or upcoming trends in application management\nwithin Kubernetes that SIG Apps is excited about? How is the SIG adapting to these trends?</strong></p><p>Definitely the current AI hype is the major driving factor; as mentioned above, we have two working\ngroups, each covering a different aspect of it.</p><p><strong>Sandipan: What are some of your favorite things about this SIG?</strong></p><p>Without a doubt, the people that participate in our meetings and on\n<a href=\"https://kubernetes.slack.com/messages/sig-apps\">Slack</a>, who tirelessly help triage issues, pull\nrequests and invest a lot of their time (very frequently their private time) into making kubernetes\ngreat!</p><p>SIG Apps is an essential part of the Kubernetes community, helping to shape how applications are\ndeployed and managed at scale. From its work on improving Kubernetes' workload APIs to driving\ninnovation in AI/ML application management, SIG Apps is continually adapting to meet the needs of\nmodern application developers and operators. Whether youâ€™re a new contributor or an experienced\ndeveloper, thereâ€™s always an opportunity to get involved and make an impact.</p><p>If youâ€™re interested in learning more or contributing to SIG Apps, be sure to check out their <a href=\"https://github.com/kubernetes/community/tree/master/sig-apps\">SIG\nREADME</a> and join their bi-weekly <a href=\"https://github.com/kubernetes/community/tree/master/sig-apps#meetings\">meetings</a>.</p>","contentLength":7391,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spotlight on SIG etcd","url":"https://kubernetes.io/blog/2025/03/04/sig-etcd-spotlight/","date":1741046400,"author":"","guid":485,"unread":true,"content":"<p><strong>Frederico: Hello, thank you for the time! Letâ€™s start with some introductions, could you tell us a\nbit about yourself, your role and how you got involved in Kubernetes.</strong></p><p> Hello, I am Benjamin. I am a SIG etcd Tech Lead and one of the etcd maintainers. I\nwork for VMware, which is part of the Broadcom group. I got involved in Kubernetes &amp; etcd &amp; CSI\n(<a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md\">Container Storage Interface</a>)\nbecause of work and also a big passion for open source. I have been working on Kubernetes &amp; etcd\n(and also CSI) since 2020.</p><p> Hey team, Iâ€™m James, a co-chair for SIG etcd and etcd maintainer. I work at Red Hat as a\nSpecialist Architect helping people adopt cloud native technology. I got involved with the\nKubernetes ecosystem in 2019. Around the end of 2022 I noticed how the etcd community and project\nneeded help so started contributing as often as I could. There is a saying in our community that\n\"you come for the technology, and stay for the people\": for me this is absolutely real, itâ€™s been a\nwonderful journey so far and Iâ€™m excited to support our community moving forward.</p><p> Hey everyone, I'm Marek, the SIG etcd lead. At Google, I lead the GKE etcd team, ensuring\na stable and reliable experience for all GKE users. My Kubernetes journey began with <a href=\"https://github.com/kubernetes/community/tree/master/sig-instrumentation\">SIG\nInstrumentation</a>, where I\ncreated and led the <a href=\"https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/\">Kubernetes Structured Logging effort</a>.\nI'm still the main project lead for <a href=\"https://kubernetes-sigs.github.io/metrics-server/\">Kubernetes Metrics Server</a>,\nproviding crucial signals for autoscaling in Kubernetes. I started working on etcd 3 years ago,\nright around the 3.5 release. We faced some challenges, but I'm thrilled to see etcd now the most\nscalable and reliable it's ever been, with the highest contribution numbers in the project's\nhistory. I'm passionate about distributed systems, extreme programming, and testing.</p><p> Hi there, my name is Wenjia, I am the co-chair of SIG etcd and one of the etcd\nmaintainers. I work at Google as an Engineering Manager, working on GKE (Google Kubernetes Engine)\nand GDC (Google Distributed Cloud). I have been working in the area of open source Kubernetes and\netcd since the Kubernetes v1.10 and etcd v3.1 releases. I got involved in Kubernetes because of my\njob, but what keeps me in the space is the charm of the container orchestration technology, and more\nimportantly, the awesome open source community.</p><p><strong>Frederico: Excellent, thank you. I'd like to start with the origin of the SIG itself: SIG etcd is\na very recent SIG, could you quickly go through the history and reasons behind its creation?</strong></p><p>: Absolutely! SIG etcd was formed because etcd is a critical component of Kubernetes,\nserving as its data store. However, etcd was facing challenges like maintainer turnover and\nreliability issues. <a href=\"https://etcd.io/blog/2023/introducing-sig-etcd/\">Creating a dedicated SIG</a>\nallowed us to focus on addressing these problems, improving development and maintenance processes,\nand ensuring etcd evolves in sync with the cloud-native landscape.</p><p><strong>Frederico: And has becoming a SIG worked out as expected? Better yet, are the motivations you just\ndescribed being addressed, and to what extent?</strong></p><p>: It's been a positive change overall. Becoming a SIG has brought more structure and\ntransparency to etcd's development. We've adopted Kubernetes processes like KEPs\n(<a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/README.md\">Kubernetes Enhancement Proposals</a>\nand PRRs (<a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md\">Production Readiness Reviews</a>,\nwhich has improved our feature development and release cycle.</p><p><strong>Frederico: On top of those, what would you single out as the major benefit that has resulted from\nbecoming a SIG?</strong></p><p>: The biggest benefits for me was adopting Kubernetes testing infrastructure, tools like\n<a href=\"https://docs.prow.k8s.io/\">Prow</a> and <a href=\"https://testgrid.k8s.io/\">TestGrid</a>. For large projects like\netcd there is just no comparison to the default GitHub tooling. Having known, easy to use, clear\ntools is a major boost to the etcd as it makes it much easier for Kubernetes contributors to also\nhelp etcd.</p><p>: Totally agree, while challenges remain, the SIG structure provides a solid foundation\nfor addressing them and ensuring etcd's continued success as a critical component of the Kubernetes\necosystem.</p><p>The positive impact on the community is another crucial aspect of SIG etcd's success that Iâ€™d like\nto highlight. The Kubernetes SIG structure has created a welcoming environment for etcd\ncontributors, leading to increased participation from the broader Kubernetes community. We have had\ngreater collaboration with other SIGs like <a href=\"https://github.com/kubernetes/community/blob/master/sig-api-machinery/README.md\">SIG API\nMachinery</a>,\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-scalability\">SIG Scalability</a>,\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-scalability\">SIG Testing</a>,\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle\">SIG Cluster Lifecycle</a>, etc.</p><p>This collaboration helps ensure etcd's development aligns with the needs of the wider Kubernetes\necosystem. The formation of the <a href=\"https://github.com/kubernetes/community/blob/master/wg-etcd-operator/README.md\">etcd Operator Working Group</a>\nunder the joint effort between SIG etcd and SIG Cluster Lifecycle exemplifies this successful\ncollaboration, demonstrating a shared commitment to improving etcd's operational aspects within\nKubernetes.</p><p><strong>Frederico: Since you mentioned collaboration, have you seen changes in terms of contributors and\ncommunity involvement in recent months?</strong></p><p>: Yes -- as showing in our\n<a href=\"https://etcd.devstats.cncf.io/d/23/prs-authors-repository-groups?orgId=1&amp;var-period=m&amp;var-repogroup_name=All&amp;from=1422748800000&amp;to=1738454399000\">unique PR author data</a>\nwe recently hit an all time high in March and are trending in a positive direction:</p><p><strong>Frederico: That's quite telling, thank you. In terms of the near future, what are the current\npriorities for SIG etcd?</strong></p><p>: Reliability is always top of mind -â€“ we need to make sure etcd is rock-solid. We're also\nworking on making etcd easier to use and manage for operators. And we have our sights set on making\netcd a viable standalone solution for infrastructure management, not just for Kubernetes. Oh, and of\ncourse, scaling -â€“ we need to ensure etcd can handle the growing demands of the cloud-native world.</p><p>: I agree that reliability should always be our top guiding principle. We need to ensure\nnot only correctness but also compatibility. Additionally, we should continuously strive to improve\nthe understandability and maintainability of etcd. Our focus should be on addressing the pain points\nthat the community cares about the most.</p><p><strong>Frederico: Are there any specific SIGs that you work closely with?</strong></p><p>: SIG API Machinery, for sure â€“ they own the structure of the data etcd stores, so we're\nconstantly working together. And SIG Cluster Lifecycle â€“ etcd is a key part of Kubernetes clusters,\nso we collaborate on the newly created etcd operator Working group.</p><p>: Other than SIG API Machinery and SIG Cluster Lifecycle that Marek mentioned above, SIG\nScalability and SIG Testing is another group that we work closely with.</p><p><strong>Frederico: In a more general sense, how would you list the key challenges for SIG etcd in the\nevolving cloud native landscape?</strong></p><p>: Well, reliability is always a challenge when you're dealing with critical data. The\ncloud-native world is evolving so fast that scaling to meet those demands is a constant effort.</p><p><strong>Frederico: We're almost at the end of our conversation, but for those interested in in etcd, how\ncan they get involved?</strong></p><p>: We'd love to have them! The best way to start is to join our\n<a href=\"https://github.com/kubernetes/community/blob/master/sig-etcd/README.md#meetings\">SIG etcd meetings</a>,\nfollow discussions on the <a href=\"https://groups.google.com/g/etcd-dev\">etcd-dev mailing list</a>, and check\nout our <a href=\"https://github.com/etcd-io/etcd/issues\">GitHub issues</a>. We're always looking for people to\nreview proposals, test code, and contribute to documentation.</p><p>: I love this question ðŸ˜€ . There are numerous ways for people interested in contributing\nto SIG etcd to get involved and make a difference. Here are some key areas where you can help:</p><ul><li>: Tackle existing issues in the etcd codebase. Start with issues labeled \"good first\nissue\" or \"help wanted\" to find tasks that are suitable for newcomers.</li><li>: Contribute to the development of new features and enhancements. Check the\netcd roadmap and discussions to see what's being planned and where your skills might fit in.</li><li>: Help ensure the quality of etcd by writing tests, reviewing code\nchanges, and providing feedback.</li><li>: Improve <a href=\"https://etcd.io/docs/\">etcd's documentation</a> by adding new content,\nclarifying existing information, or fixing errors. Clear and comprehensive documentation is\nessential for users and contributors.</li><li>: Answer questions on forums, mailing lists, or <a href=\"https://kubernetes.slack.com/archives/C3HD8ARJ5\">Slack channels</a>.\nHelping others understand and use etcd is a valuable contribution.</li></ul><ul><li>: Start by joining the etcd community on Slack,\nattending SIG meetings, and following the mailing lists. This will\nhelp you get familiar with the project, its processes, and the\npeople involved.</li><li>: If you're new to open source or etcd, consider\nfinding a mentor who can guide you and provide support. Stay tuned!\nOur first cohort of mentorship program was very successful. We will\nhave a new round of mentorship program coming up.</li><li>: Don't be afraid to start with small contributions. Even\nfixing a typo in the documentation or submitting a simple bug fix\ncan be a great way to get involved.</li></ul><p>By contributing to etcd, you'll not only be helping to improve a\ncritical piece of the cloud-native ecosystem but also gaining valuable\nexperience and skills. So, jump in and start contributing!</p><p><strong>Frederico: Excellent, thank you. Lastly, one piece of advice that\nyou'd like to give to other newly formed SIGs?</strong></p><p>: Absolutely! My advice would be to embrace the established\nprocesses of the larger community, prioritize collaboration with other\nSIGs, and focus on building a strong community.</p><p>: Here are some tips I myself found very helpful in my OSS\njourney:</p><ul><li>: Open source development can take time. Don't get\ndiscouraged if your contributions aren't accepted immediately or if\nyou encounter challenges.</li><li>: The etcd community values collaboration and\nrespect. Be mindful of others' opinions and work together to achieve\ncommon goals.</li><li>: Contributing to open source should be\nenjoyable. Find areas that interest you and contribute in ways that\nyou find fulfilling.</li></ul><p><strong>Frederico: A great way to end this spotlight, thank you all!</strong></p><p>For more information and resources, please take a look at :</p>","contentLength":9586,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NFTables mode for kube-proxy","url":"https://kubernetes.io/blog/2025/02/28/nftables-kube-proxy/","date":1740700800,"author":"","guid":484,"unread":true,"content":"<p>A new nftables mode for kube-proxy was introduced as an alpha feature\nin Kubernetes 1.29. Currently in beta, it is expected to be GA as of\n1.33. The new mode fixes long-standing performance problems with the\niptables mode and all users running on systems with reasonably-recent\nkernels are encouraged to try it out. (For compatibility reasons, even\nonce nftables becomes GA, iptables will still be the .)</p><h2>Why nftables? Part 1: data plane latency</h2><p>The iptables API was designed for implementing simple firewalls, and\nhas problems scaling up to support Service proxying in a large\nKubernetes cluster with tens of thousands of Services.</p><p>In general, the ruleset generated by kube-proxy in iptables mode has a\nnumber of iptables rules proportional to the sum of the number of\nServices and the total number of endpoints. In particular, at the top\nlevel of the ruleset, there is one rule to test each possible Service\nIP (and port) that a packet might be addressed to:</p><pre tabindex=\"0\"><code># If the packet is addressed to 172.30.0.41:80, then jump to the chain\n# KUBE-SVC-XPGD46QRK7WJZT7O for further processing\n-A KUBE-SERVICES -m comment --comment \"namespace1/service1:p80 cluster IP\" -m tcp -p tcp -d 172.30.0.41 --dport 80 -j KUBE-SVC-XPGD46QRK7WJZT7O\n# If the packet is addressed to 172.30.0.42:443, then...\n-A KUBE-SERVICES -m comment --comment \"namespace2/service2:p443 cluster IP\" -m tcp -p tcp -d 172.30.0.42 --dport 443 -j KUBE-SVC-GNZBNJ2PO5MGZ6GT\n# etc...\n-A KUBE-SERVICES -m comment --comment \"namespace3/service3:p80 cluster IP\" -m tcp -p tcp -d 172.30.0.43 --dport 80 -j KUBE-SVC-X27LE4BHSL4DOUIK\n</code></pre><p>This means that when a packet comes in, the time it takes the kernel\nto check it against all of the Service rules is  in the number\nof Services. As the number of Services increases, both the average and\nthe worst-case latency for the first packet of a new connection\nincreases (with the difference between best-case, average, and\nworst-case being mostly determined by whether a given Service IP\naddress appears earlier or later in the  chain).</p><p>By contrast, with nftables, the normal way to write a ruleset like\nthis is to have a  rule, using a \"verdict map\" to do the\ndispatch:</p><pre tabindex=\"0\"><code>table ip kube-proxy {\n# The service-ips verdict map indicates the action to take for each matching packet.\nmap service-ips {\ntype ipv4_addr . inet_proto . inet_service : verdict\ncomment \"ClusterIP, ExternalIP and LoadBalancer IP traffic\"\nelements = { 172.30.0.41 . tcp . 80 : goto service-ULMVA6XW-namespace1/service1/tcp/p80,\n172.30.0.42 . tcp . 443 : goto service-42NFTM6N-namespace2/service2/tcp/p443,\n172.30.0.43 . tcp . 80 : goto service-4AT6LBPK-namespace3/service3/tcp/p80,\n... }\n}\n# Now we just need a single rule to process all packets matching an\n# element in the map. (This rule says, \"construct a tuple from the\n# destination IP address, layer 4 protocol, and destination port; look\n# that tuple up in \"service-ips\"; and if there's a match, execute the\n# associated verdict.)\nchain services {\nip daddr . meta l4proto . th dport vmap @service-ips\n}\n...\n}\n</code></pre><p>Since there's only a single rule, with a roughly  map lookup,\npacket processing time is more or less constant regardless of cluster\nsize, and the best/average/worst cases are very similar:</p><p>But note the huge difference in the vertical scale between the\niptables and nftables graphs! In the clusters with 5000 and 10,000\nServices, the p50 (average) latency for nftables is about the same as\nthe p01 (approximately best-case) latency for iptables. In the 30,000\nService cluster, the p99 (approximately worst-case) latency for\nnftables manages to beat out the p01 latency for iptables by a few\nmicroseconds! Here's both sets of data together, but you may have to\nsquint to see the nftables results!:</p><h2>Why nftables? Part 2: control plane latency</h2><p>While the improvements to data plane latency in large clusters are\ngreat, there's another problem with iptables kube-proxy that often\nkeeps users from even being able to grow their clusters to that size:\nthe time it takes kube-proxy to program new iptables rules when\nServices and their endpoints change.</p><p>With both iptables and nftables, the total size of the ruleset as a\nwhole (actual rules, plus associated data) is  in the combined\nnumber of Services and their endpoints. Originally, the iptables\nbackend would rewrite every rule on every update, and with tens of\nthousands of Services, this could grow to be hundreds of thousands of\niptables rules. Starting in Kubernetes 1.26, we began improving\nkube-proxy so that it could skip updating  of the unchanged\nrules in each update, but the limitations of  as an\nAPI meant that it was still always necessary to send an update that's\n in the number of Services (though with a noticeably smaller\nconstant than it used to be). Even with those optimizations, it can\nstill be necessary to make use of kube-proxy's  config\noption to ensure that it doesn't spend every waking second trying to\npush iptables updates.</p><p>The nftables APIs allow for doing much more incremental updates, and\nwhen kube-proxy in nftables mode does an update, the size of the\nupdate is only  in the number of Services and endpoints that\nhave changed since the last sync, regardless of the total number of\nServices and endpoints. The fact that the nftables API allows each\nnftables-using component to have its own private table also means that\nthere is no global lock contention between components like with\niptables. As a result, kube-proxy's nftables updates can be done much\nmore efficiently than with iptables.</p><p>(Unfortunately I don't have cool graphs for this part.)</p><p>All that said, there are a few reasons why you might not want to jump\nright into using the nftables backend for now.</p><p>First, the code is still fairly new. While it has plenty of unit\ntests, performs correctly in our CI system, and has now been used in\nthe real world by multiple users, it has not seen anything close to as\nmuch real-world usage as the iptables backend has, so we can't promise\nthat it is as stable and bug-free.</p><p>Second, the nftables mode will not work on older Linux distributions;\ncurrently it requires a 5.13 or newer kernel. Additionally, because of\nbugs in early versions of the  command line tool, you should not\nrun kube-proxy in nftables mode on nodes that have an old (earlier\nthan 1.0.0) version of  in the host filesystem (or else\nkube-proxy's use of nftables may interfere with other uses of nftables\non the system).</p><p>Third, you may have other networking components in your cluster, such\nas the pod network or NetworkPolicy implementation, that do not yet\nsupport kube-proxy in nftables mode. You should consult the\ndocumentation (or forums, bug tracker, etc.) for any such components\nto see if they have problems with nftables mode. (In many cases they\nwill not; as long as they don't try to directly interact with or\noverride kube-proxy's iptables rules, they shouldn't care whether\nkube-proxy is using iptables or nftables.) Additionally, observability\nand monitoring tools that have not been updated may report less data\nfor kube-proxy in nftables mode than they do for kube-proxy in\niptables mode.</p><p>Finally, kube-proxy in nftables mode is intentionally not 100%\ncompatible with kube-proxy in iptables mode. There are a few old\nkube-proxy features whose default behaviors are less secure, less\nperformant, or less intuitive than we'd like, but where we felt that\nchanging the default would be a compatibility break. Since the\nnftables mode is opt-in, this gave us a chance to fix those bad\ndefaults without breaking users who weren't expecting changes. (In\nparticular, with nftables mode, NodePort Services are now only\nreachable on their nodes' default IPs, as opposed to being reachable\non all IPs, including , with iptables mode.) The\n<a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#migrating-from-iptables-mode-to-nftables\">kube-proxy documentation</a> has more information about this, including\ninformation about metrics you can look at to determine if you are\nrelying on any of the changed functionality, and what configuration\noptions are available to get more backward-compatible behavior.</p><p>Ready to try it out? In Kubernetes 1.31 and later, you just need to\npass  to kube-proxy (or set  in\nyour kube-proxy config file).</p><p>You can also convert existing clusters from iptables (or ipvs) mode to\nnftables by updating the kube-proxy configuration and restarting the\nkube-proxy pods. (You do not need to reboot the nodes: when restarting\nin nftables mode, kube-proxy will delete any existing iptables or ipvs\nrules, and likewise, if you later revert back to iptables or ipvs\nmode, it will delete any existing nftables rules.)</p><p>As mentioned above, while nftables is now the  kube-proxy mode,\nit is not the , and we do not yet have a plan for changing\nthat. We will continue to support the iptables mode for a long time.</p><p>The future of the IPVS mode of kube-proxy is less certain: its main\nadvantage over iptables was that it was faster, but certain aspects of\nthe IPVS architecture and APIs were awkward for kube-proxy's purposes\n(for example, the fact that the  device needs to have\n Service IP address assigned to it), and some parts of\nKubernetes Service proxying semantics were difficult to implement\nusing IPVS (particularly the fact that some Services had to have\ndifferent endpoints depending on whether you connected to them from a\nlocal or remote client). And now, the nftables mode has the same\nperformance as IPVS mode (actually, slightly better), without any of\nthe downsides:</p><p>(In theory the IPVS mode also has the advantage of being able to use\nvarious other IPVS functionality, like alternative \"schedulers\" for\nbalancing endpoints. In practice, this ended up not being very useful,\nbecause kube-proxy runs independently on every node, and the IPVS\nschedulers on each node had no way of sharing their state with the\nproxies on other nodes, thus thwarting the effort to balance traffic\nmore cleverly.)</p><p>While the Kubernetes project does not have an immediate plan to drop\nthe IPVS backend, it is probably doomed in the long run, and people\nwho are currently using IPVS mode should try out the nftables mode\ninstead (and file bugs if you think there is missing functionality in\nnftables mode that you can't work around).</p>","contentLength":10096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","k8s"]}