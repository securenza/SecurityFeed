{"id":"CSXkJZ3MdFx","title":"Dev News","displayTitle":"Dev News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":64,"items":[{"title":"LinkedIn is the worst social media I've ever seen","url":"https://news.ycombinator.com/item?id=43052409","date":1739563136,"author":"bitreducer","guid":219,"unread":true,"content":"Besides being full of AI-generated, useless content, the platform is also riddled with dark patterns.<p>I'm a newcomer, I’d never used LinkedIn before until I joined three months ago. Since then, I've been banned twice and shadow-banned several times.</p><p>1) Right after I created my account, I got shadow-banned. My friends couldn’t find me, and my profile wasn’t accessible via direct link. This issue wasn’t resolved until I contacted support; they confirmed that restrictions had been placed on my account and then removed them.</p><p>2) I shouldn’t even have to mention this, but as a professional in my field, after that incident I always ended up on the very last pages of search results for keywords related to my field. People without those keywords in their resumes, or whose work isn’t at all related to the field, kept being ranked ahead of me. I can’t say I was deliberately pessimized, but that’s exactly how it appears.</p><p>3) Some time later, I decided to get Premium and paid with my own card (issued in another country, with the same name as on my profile and my passport). That’s when I received the first restriction: I was locked out of my account and couldn’t sign in until I contacted support. They made me take a photo of myself and of my passport, and after that they lifted the restrictions and apologized.</p><p>4) Everything seemed to be going along normally, yet I remained stuck on the last pages of search, being outranked by completely irrelevant profiles, until I decided to write a post about my own article on Medium. Immediately after publishing the post with a link to Medium, I received a second restriction. I was locked out of my account, couldn’t access my messages or interview invitations, and even missed a call because of this. They once again required me to submit a photo of myself along with a copy of my passport. Although they eventually restored my account, this time the process took 4–5 days, which caused me to miss an interview.</p><p>5) Now my profile is accessible via direct link, but I’ve disappeared from search. No one can find me, and hardly anyone visits my page. My post was hidden while support was \"sorting out\" my account, and I lost all the potential post views it would have gained because it wasn't featured in the recommendations.</p><p>All of this happened within the first three months of using LinkedIn. I’ve never seen a more appalling social media, one so full of dark patterns and outright abuse towards its users, forcing them through humiliating identity verification processes and hiding them from search.</p><p>Needless to say, I have never violated any of the platform’s rules. I don’t spam, I don’t bother people, and I don’t advertise anything. Meanwhile, my friend, whose account is over five years old—can do whatever he wants. He uses VPNs, changes his profile location several times a week, and switches his VPN location from Dubai to Europe multiple times a day without ever facing any restrictions.</p><p>And yes, I completely forgot to add: when your account gets restricted, it doesn’t matter whether you’re a premium user or not — the treatment is equally poor and the response is equally slow.</p><p>I’ve never seen a worse social media, and I’d be thrilled if a worthy competitor to LinkedIn ever emerged, I’d be one of the first to join.</p><p>BONUS: This content was originally posted on reddit, quickly became popular in the LinkedIn subreddit and was then deleted by moderator who works for M$ (which owns LinkedIn). Frankly, it feels like the whole LinkedIn subreddit is highly censored by him.</p>","contentLength":3575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43052409"},{"title":"Shutdown Go server","url":"https://www.reddit.com/r/golang/comments/1ipj5zn/shutdown_go_server/","date":1739562402,"author":"/u/Kennedy-Vanilla","guid":611,"unread":true,"content":"<div><p>Hi, recently I saw that many people shutdown their servers like this or similar</p><pre><code>serverCtx, serverStopCtx serverCtx, serverStopCtx := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { &lt;-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { &lt;-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal(\"graceful shutdown timed out.. forcing exit.\") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf(\"error shutting down server: %v\", err) } serverStopCtx() }() log.Printf(\"Server starting on port %s...\\n\", port) err = server.ListenAndServe() if err != nil &amp;&amp; err != http.ErrServerClosed { log.Printf(\"error starting server: %v\", err) os.Exit(1) } &lt;-serverCtx.Done() log.Println(\"Server stopped\") } := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { &lt;-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { &lt;-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal(\"graceful shutdown timed out.. forcing exit.\") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf(\"error shutting down server: %v\", err) } serverStopCtx() }() log.Printf(\"Server starting on port %s...\\n\", port) err = server.ListenAndServe() if err != nil &amp;&amp; err != http.ErrServerClosed { log.Printf(\"error starting server: %v\", err) os.Exit(1) } &lt;-serverCtx.Done() log.Println(\"Server stopped\") </code></pre><p>Is it necessary? Like it's so many code for the simple operation</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Kennedy-Vanilla\"> /u/Kennedy-Vanilla </a>","contentLength":1805,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linus Torvalds’ Critique of C++: A Comprehensive Review","url":"https://programmers.fyi/linus-torvalds-critique-of-c-a-comprehensive-review","date":1739558840,"author":"/u/derjanni","guid":620,"unread":true,"content":"<p>Linus Torvalds, the inventor  of Linux, has always been quite harsh about C++ and why he rejects it for Linux kernel development. He’s not just been very vocal about it, but also brought up some arguments against the use of C++ that are worth reviewing in detail.</p><p>C and C++ are almost the same, except they aren’t. C++ is often considered the object oriented “version” of C. The successor to it, if you will. But C++ is rather an extension to C. Bringing object orientation, constructors, destructors, templates, exception handling, namespaces and operator overloading. All these extensions come with their very own challenges and paradigms. Unsurprisingly, Linus’ technical arguments against C++ all have their roots in these extensions.</p><p>The main arguments that Linus brought forward against C++ can be found in the kernel mailing list under “<a href=\"https://harmful.cat-v.org/software/c++/linus\" rel=\"noopener ugc nofollow\" target=\"_blank\">Compiling C++ kernel module + Makefile</a>” and the respective responses that date back as far as 2004. Let’s wipe the personal disputes off these messages and focus on the arguments.</p><h2>Exception handling in C++</h2><blockquote><p>“the whole C++ exception handling thing is fundamentally broken. It’s _especially_ broken for kernels.”</p></blockquote>","contentLength":1171,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iphr9s/linus_torvalds_critique_of_c_a_comprehensive/"},{"title":"Lead Asahi Linux Developer Quits Days After Leaving Kernel Maintainer Role","url":"https://linux.slashdot.org/story/25/02/14/1842251/lead-asahi-linux-developer-quits-days-after-leaving-kernel-maintainer-role?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739558520,"author":"msmash","guid":256,"unread":true,"content":"Hector Martin has resigned as the project lead of Asahi Linux, weeks after stepping down from his role as a Linux kernel maintainer for Apple ARM support. His departure from Asahi follows a contentious exchange with Linus Torvalds over development processes and social media advocacy. After quitting kernel maintenance earlier this month, the conflict escalated when Martin suggested that \"shaming on social media\" might be necessary to effect change. \n\nTorvalds sharply rejected this approach, stating that \"social media brigading just makes me not want to have anything at all to do with your approach\" and suggested that Martin himself might be the problem. In his final resignation announcement from Asahi, Martin wrote: \"I no longer have any faith left in the kernel development process or community management approach.\" \n\nThe dispute reflects deeper tensions in the Linux kernel community, particularly around the integration of Rust code. It follows the August departure of another key Rust for Linux maintainer, Wedson Almeida Filho from Microsoft. According to Sonatype's research, more than 300,000 open source projects have slowed or halted updates since 2020.","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delphi Is 30","url":"https://blog.marcocantu.com/blog/2025-february-delphi-is-30.html","date":1739558515,"author":"andsoitis","guid":218,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43051598"},{"title":"A study on how turtles navigate using the Earth’s magnetic field","url":"https://www.unc.edu/posts/2025/02/12/dancing-turtles-unlock-scientific-discovery/","date":1739557799,"author":"gmays","guid":217,"unread":true,"content":"<p>When captive loggerhead sea turtles anticipate food, they do a little dance. The turtles raise their heads out of water, open their mouths, flap their flippers and spin in circles.</p><p>Not only is this “turtle dancing behavior” cute, it was also the key to unlocking a significant scientific discovery by researchers at UNC-Chapel Hill.</p><p>The prestigious international science journal Nature published the study Feb. 12. Kayla Goforth, a recent doctoral graduate in the College of Arts and Sciences’ biology department, is the lead author of “Learned magnetic map cues and two mechanisms of magnetoreception in turtles.”</p><p>Early on in her graduate studies at Carolina, Goforth was fascinated by sea turtles returning to the same feeding sites again and again throughout their lifetimes — despite traveling up to 10,000 miles across the planet. She hypothesized that these turtles used Earth’s magnetic field to memorize specific geographic areas that they associated with food, and she devised an experiment in the biology department’s <a href=\"https://lohmannlab.web.unc.edu/\">Lohmann Lab</a> — run by husband-and-wife duo Kenneth and Catherine Lohmann.</p><p>“Kayla began to wonder if we could get the turtles to associate the magnetic signature of a geographic area with food — and therefore act out this turtle dance behavior,” said Kenneth Lohmann, the Charles P. Postelle Jr. Distinguished Professor of Biology. “She really took the lead in this. I wasn’t at all sure in the beginning whether it would work, but we were happy to have her try, and it turned out remarkably well.”</p><div><img fetchpriority=\"high\" decoding=\"async\" aria-describedby=\"caption-attachment-252848\" src=\"https://www.unc.edu/wp-content/uploads/2025/02/Untitled-design9.gif\" alt=\"Short, soundless video of a small sea turtle flapping around in water.\" width=\"800\" height=\"533\"><p>The team of Carolina researchers analyzed how sea turtles use Earth’s magnetic field to guide their long-distance migrations across the ocean, and how the turtles sense magnetic fields.</p></div><p>Goforth and the rest of the all-Carolina team conditioned loggerhead turtles to magnetic fields replicating those that exist in various oceanic locations, repeatedly feeding the turtles in some magnetic fields while not feeding them in others. When later exposed to the fields in which they were fed, the turtles exhibited “turtle dancing behavior,” indicating that they associated that specific magnetic signature with food.</p><p>“That’s how we figured out what to measure, just by watching them do this as we were feeding them in the lab,” said Goforth, who is now conducting postdoctoral research at Texas A&amp;M University. “When they get really excited, they stick their heads out; they slap their flippers wildly; they spin in place sometimes.”</p><p>The assay suggests that loggerhead sea turtles can learn the magnetic signatures of geographical areas — much like dropping a pin in a GPS app.</p><div><img decoding=\"async\" aria-describedby=\"caption-attachment-252840\" src=\"https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-225x300.jpeg\" alt=\"Aerial image of many sea turtles on a beach.\" width=\"225\" height=\"300\" srcset=\"https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-225x300.jpeg 225w, https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-768x1024.jpeg 768w, https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-1152x1536.jpeg 1152w, https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-1536x2048.jpeg 1536w, https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-scaled.jpeg 1920w\" sizes=\"(max-width: 225px) 100vw, 225px\"><p>The groundbreaking study was published in the prestigious international science journal Nature. (Ken Lohmann/biology department)</p></div><p>When turtles use magnetic signatures, they are using their magnetic map sense, but they also have a magnetic compass sense that enables them to move north, south or some other direction. The research further showed that the map and compass senses detect magnetic fields in different ways.</p><p>Working in collaboration with the College’s physics and astronomy department, the team investigated the effects of radiofrequency oscillating magnetic fields on the turtles’ magnetic senses. What the team found surprised them. The oscillating magnetic fields had no impact on the turtles’ magnetic map ability but did disrupt the turtles’ ability to use the magnetic compass and orient themselves.</p><p>“It suggested there are two different mechanisms for the magnetic map and compass,” Goforth said. “And they might have evolved separately.”</p><p>Exactly how turtles and other migratory species use these senses is a question that has baffled scientists for more than 50 years, and it’s one that Goforth continues to research at Texas A&amp;M, now studying monarch butterflies instead of sea turtles.</p><p>“The big question is always the mechanism,” Goforth said. “What’s the mechanism and what is the sensor? We know that for the visual sense, you have eyes; for the sense of smell, you have a nose; and for hearing, you have ears. But no receptor like that has been identified for the magnetic sense, and the mechanism remains unknown.”</p><div><img decoding=\"async\" aria-describedby=\"caption-attachment-252838\" src=\"https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-1024x576.jpg\" alt=\"Hand holding a tiny sea turtle.\" width=\"1024\" height=\"576\" srcset=\"https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-1024x576.jpg 1024w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-300x169.jpg 300w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-768x432.jpg 768w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-1536x864.jpg 1536w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-1200x675.jpg 1200w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-800x450.jpg 800w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-600x338.jpg 600w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-550x310.jpg 550w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-450x253.jpg 450w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-350x196.jpg 350w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-250x141.jpg 250w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-200x112.jpg 200w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal.jpg 1600w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"><p>(Ken Lohmann/biology department)</p></div>","contentLength":4230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43051465"},{"title":"Dynamic triple/double buffering merge request for GNOME was just merged!","url":"https://gitlab.gnome.org/GNOME/mutter/-/merge_requests/1441","date":1739556559,"author":"/u/joojmachine","guid":623,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ipguyt/dynamic_tripledouble_buffering_merge_request_for/"},{"title":"For those managing or working with multiple clusters, do you use a combined kubeconfig file or separate by cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1ipg99n/for_those_managing_or_working_with_multiple/","date":1739555053,"author":"/u/trouphaz","guid":614,"unread":true,"content":"<p>I wonder if I'm in the minority. I have been keeping my kubeconfigs separate per cluster for years while I know others that combine everything to a single file. I started doing this because I didn't fully grasp yaml when I started and when I had an issue with the kubeconfig, I didn't have any idea on how to repair it. So I'd have to fully recreate it.</p><p>So, each cluster has its own kubeconfig file named for the cluster's name and I have a function that'll set my KUBECONFIG variable to the file using the cluster name.</p><pre><code>sc() { CLUSTER_NAME=\"${1}\" export KUBECONFIG=\"~/.kube/${CLUSTER_NAME}\" } </code></pre>","contentLength":592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incremental Archival from Postgres to Parquet for Analytics","url":"https://www.crunchydata.com/blog/incremental-archival-from-postgres-to-parquet-for-analytics","date":1739554943,"author":"/u/gtobbe","guid":624,"unread":true,"content":"<p>PostgreSQL is commonly used to store event data coming from various kinds of devices. The data often arrives as individual events or small batches, which requires an operational database to capture. Features like <a href=\"https://www.crunchydata.com/blog/native-partitioning-with-postgres\">time partitioning</a> help optimize the storage layout for time range filtering and efficient deletion of old data.</p><p>The PostgreSQL feature set gives you a lot of flexibility for handling a variety of IoT scenarios, but there are certain scenarios for it is less suitable, namely:</p><ul><li>Long-term archival of historical data</li><li>Fast, interactive analytics on the source data</li></ul><p>Ideally, data would get automatically archived in cheap storage, in a format optimized for large analytical queries.</p><p>We developed two open source Postgres extensions that help you do that:</p><ul><li><a href=\"https://github.com/CrunchyData/pg_parquet\">pg_parquet</a> can export (and import) query results to the <a href=\"https://parquet.apache.org/\">Parquet</a> file format in object storage using regular COPY commands</li><li><a href=\"https://github.com/crunchydata/pg_incremental\">pg_incremental</a> can run a command for a never-ending series of time intervals or files, built on top of <a href=\"https://github.com/citusdata/pg_cron\">pg_cron</a></li></ul><p>With some simple commands, you can set up a reliable, fully automated pipeline to export time ranges to the columnar Parquet format in S3.</p><p>Then, you can use a variety of analytics tools to query or import the data. My favorite is of course <a href=\"https://www.crunchydata.com/blog/crunchy-data-warehouse-postgres-with-iceberg-for-high-performance-analytics\">Crunchy Data Warehouse</a>.</p><p>On any PostgreSQL server that has the pg_parquet and pg_incremental extensions, you can set up a pipeline that periodically exports data to in S3 in two steps.</p><p>The pg_incremental extension has a create_time_interval_pipeline function that will run a given command once the time interval has passed, with 2 timestamp parameters set to the start and end of the hour. We cannot directly use query parameters in a COPY command, but we can define a simple PL/pgSQL function that generates and executes a custom COPY command using the parameters.</p><pre><code>-- existing raw data table\ncreate table events (\n  event_id bigint not null generated by default as identity,\n  event_time timestamptz not null default now(),\n  device_id bigint not null,\n  sensor_1 double precision\n);\n\ninsert into events (device_id, sensor_1)\nvalues (297, 20.4);\n\n\ninsert into events (device_id, sensor_1)\nvalues (297, 20.4);\n\n-- define an export function that wraps a COPY command\ncreate or replace function export_events(start_time timestamptz, end_time timestamptz)\nreturns void language plpgsql as $function$\nbegin\n  execute format(\n    $$\n      copy (select * from events where event_time &gt;= %L and event_time &lt; %L)\n      to 's3://mybucket/events/%s.parquet' with (format 'parquet');\n    $$,\n    start_time, end_time, to_char(start_time, 'YYYY-MM-DD-HH')\n  );\nend;\n$function$;\n\n-- export events hourly from the start of the year, and keep exporting in the future\nselect incremental.create_time_interval_pipeline('event-export',\n  time_interval := '1 hour',                      /* export data by the hour                */\n  batched := false,                               /* process 1 hour at a time               */\n  start_time := '2025-01-01',                     /* backfill from the start of the year    */\n  source_table_name := 'events',                  /* wait for writes on events to finish    */\n  command := $$ select export_events($1, $2) $$   /* run export_events with start/end times */\n);\n\n</code></pre><p>By running these commands, Postgres will export all the data from the start of the year into hourly Parquet files in S3, and will keep doing so after every hour and automatically retry on failure.</p><p>To use pg_parquet Crunchy Bridge, you can add your S3 credentials for pg_parquet to your Postgres server via the dashboard under Settings -&gt; Data lake.</p><p>Once data is in Parquet, you can use a variety of tools and approaches to query the data. If you want to keep using Postgres, you can use <a href=\"https://www.crunchydata.com/products/warehouse\">Crunchy Data Warehouse</a> which has two different ways of working with Parquet data.</p><p>The simplest way to start querying Parquet files in S3 in Crunchy Data Warehouse is to use a lake analytics table. You can easily create a table for all Parquet files that match a wildcard pattern:</p><pre><code>create foreign table events_parquet ()\nserver crunchy_lake_analytics\noptions (path 's3://mybucket/events/*.parquet');\n</code></pre><p>You can then immediately query the data and the files get cached in the background, so queries will quickly get faster.</p><p>A downside of querying Parquet directly is that eventually we will have a lot of hourly files that match the pattern, and there will be some overhead from listing them for each query (listing is not cached). We also cannot easily change the schema later.</p><p>A more flexible approach is to import the Parquet files into an Iceberg table. Iceberg tables are also backed by Parquet in S3, but the files are compacted and optimized, and the table supports transactions and schema changes.</p><p>You can create an Iceberg table that has the same schema as a set of Parquet files using the definition_from option. You could also load the data using load_from, but we’ll do that separately.</p><pre><code>create table events_iceberg () using iceberg\nwith (definition_from = 's3://mybucket/events/*.parquet');\n</code></pre><p>Now we need a way to import all existing Parquet files and also import new files that show up in S3 into Iceberg. This is another job for pg_incremental. Following a similar approach as before, we create a function to generate a COPY command using a parameter.</p><pre><code>-- define an import function that wraps a COPY command to import from a URL\ncreate function import_events(path text)\nreturns void language plpgsql as $function$\nbegin\n  execute format($$copy events_iceberg from %L$$, path);\nend;\n$function$;\n\n-- create a pipeline to import new files into a table, one by one.\n-- $1 will be set to the path of a new file\nselect incremental.create_file_list_pipeline('event-import',\n   file_pattern := 's3://mybucket/events/*.parquet',\n   list_function := 'crunchy_lake.list_files',\n   command := $$ select import_events($1) $$,\n);\n\n-- optional: do compaction immediately\nvacuum events_iceberg;\n\n</code></pre><p>After running these commands, your data will be continuously archived from your source Postgres server into Iceberg in S3. You can then run fast analytical queries directly from Crunchy Data Warehouse, which uses a combination of parallel, vectorized query processing and file caching to speed up queries. You can additionally set up (materialized) views and assign read permissions to the relevant users.</p><p>No complex ETL pipelines required.</p><p>To give you a sense of the performance benefit of using Parquet, we loaded 100M rows into the source table, which got automatically mirrored in Parquet and Iceberg via our pipelines. We then ran a simple analytical query on each table:</p><pre><code>select device_id, avg(sensor_1) from events group by 1;\n</code></pre><p>The runtimes in milliseconds are shown in the following chart:</p><p>In this case the source server is a standard-16 instance (4 vcpus) on Crunchy Bridge, and the warehouse is a warehouse-standard-16 instance (4 vcpus). So, using Crunchy Data Warehouse we can analyze 100M rows in well under a second on a small machine, and get &gt;10x speedup with Iceberg.</p><p>The use of compression also means the size went from 8.9GB in PostgreSQL to 1.2GB in Iceberg using object storage.</p><p>With pg_parquet and pg_incremental, you can incrementally export data from PostgreSQL into Parquet in S3, and with Crunchy Data Warehouse you can process and analyze that data very quickly while still using PostgreSQL.</p><p>One of the nice characteristics of the approach described in this blog is that the pipelines are fully transactional. It means that every import or export step either fully succeeds or fails and then it will be retried until it does succeed. That’s how we can create production-ready pipelines with a few simple SQL commands.</p><p>Under the covers, pg_incremental keeps track of which time ranges or files have been processed. The bookkeeping happens in the same transaction as the COPY commands. So if a command fails because of an ephemeral S3 issue, the data will not end up being ingested twice or go missing. Having transactions takes away a huge amount of complexity for building reliable pipelines. There can of course be other reasons for pipeline failures that cannot be resolved through retries (e.g. changing data format), so it is still important to <a href=\"https://github.com/crunchydata/pg_incremental#monitoring-pipelines\">monitor</a> your pipelines.</p>","contentLength":8175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipg7oy/incremental_archival_from_postgres_to_parquet_for/"},{"title":"Siren Call of SQLite on the Server","url":"https://pid1.dev/posts/siren-call-of-sqlite-on-the-server/","date":1739551579,"author":"/u/sausagefeet","guid":626,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipevoh/siren_call_of_sqlite_on_the_server/"},{"title":"Detecting AI agent use and abuse","url":"https://stytch.com/blog/detecting-ai-agent-use-abuse/","date":1739549910,"author":"mattmarcus","guid":216,"unread":true,"content":"<p>AI agents aren’t just indexing your content anymore. With tools like OpenAI’s Operator, Anthropic’s Computer Use API, and BrowserBase’s Open Operator, these agents can navigate the web, mimic real users, and even take actions at scale. The challenge? Knowing whether they’re enhancing your user experience—or opening the door to abuse.</p><p>In some scenarios, apps might encourage agent use if it improves usability and adoption, but in other cases, it could present unacceptable risks for application developers or be used as a method for malicious attacks (e.g. credential stuffing or fake account creation).</p><p>In either scenario, observability is paramount. Applications need to know what traffic is on their site (is this a human? A bot? A good bot or a bad one?) in order to make intelligent decisions about how to shape traffic and enforce desired usage patterns. AI agents add an additional wrinkle as users are already sharing their credentials with tools like Operator, meaning even a well-intentioned agent creates potential risk for these applications and their users.</p><p>The key question is: <strong>Can you detect AI agent traffic on your application today?</strong></p><p>We tested multiple AI agent toolkits across high-traffic consumer sites, and the results were clear—legacy detection techniques (CAPTCHAs, IP blocking, user-agent filtering) are largely ineffective. Here’s what we found.</p><h2>The New Reality of Bot Traffic:What AI Agent Traffic Looks Like</h2><p>Traditionally, bot detection relied on CAPTCHAs, IP blocking, and user-agent filtering. But modern AI agents are engineered to look like actual users:</p><ul><li> They use genuine IP addresses, user agents, and even simulate mouse movements.</li><li><strong>Headless browsing at human speeds:</strong> Their interactions mimic natural browsing behavior, evading rate-limit triggers.</li><li><strong>Datacenter origins—but not always:</strong> While some (like OpenAI’s Operator) come from known Azure datacenters, others (like Anthropic’s API) can run locally, borrowing your machine’s properties.</li></ul><p>On the surface level, AI agent traffic can look quite similar to regular human user traffic. Here are examples of browser &amp; network properties across different AI agents:</p><table><thead><tr><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody><tr><td><div variant=\"listItem\"><p>Chrome on Linux (standard fingerprint)</p></div></td><td><div variant=\"listItem\"><p>Firefox on Ubuntu (stable, unless run locally)</p></div></td><td><div variant=\"listItem\"><p>Chrome on Mac (with stealth features)</p></div></td></tr><tr><td><div variant=\"listItem\"><p>Known Azure Datacenter IP</p></div></td><td><div variant=\"listItem\"><p>Varies—depends on local vs. cloud deployment</p></div></td></tr><tr></tr><tr><td><div variant=\"listItem\"><p>San Francisco, California</p></div></td></tr><tr></tr><tr></tr></tbody></table><p>Both OpenAI’s Operator and BrowserBase’s Open Operator spin up <strong>remote datacenter or container‑hosted</strong> Chromium instances, rather than installing any software on your local machine. Because these solutions originate from a cloud‑hosted environment (with IP addresses and other signatures tied to the provider’s infrastructure), they can be  via methods like ASN lookups if the provider’s IP ranges are well known.</p><p>By contrast, Anthropic’s “Computer Use” functionality is available only through its API as of now, and you can choose how to run that API (e.g., directly on your own machine vs. inside a VM or container). If you run it locally, it inherits your local system’s properties like IP and ASN; if you host it in the cloud, it uses whichever environment that provider offers. Each approach has its own implications for fingerprinting and detection.</p><p>Currently, Anthropic provides two options for using their agent:</p><ol><li>Direct use of your computer environment (not recommended due to security concerns around unrestricted access).</li><li><a rel=\"noreferrer noopener\" href=\"https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo\">Setting up a virtual machine or container</a> with minimal privileges to run the API. You can either run this locally or deploy to a cloud provider using Docker, in which case it won’t inherit the user’s network properties.</li></ol><p>Whereas stricter sites like Reddit block OpenAI and BrowserBase outright, Anthropic’s approach allows it to successfully bypass even these strict sites when run locally:</p><p>And with tools like BrowserBase, there are also now open-sourced options for building browsing AI agents that allow potential attackers to increase the stealth of their headless browsing setup. This means we should expect continued, quick iteration on some agent use cases that will make them even more difficult to detect. As an example, BrowserBase offers premium plans with more advanced stealth to bypass CAPTCHA and other detection techniques:</p><p>Today, most sites are allowing agent traffic to navigate freely – whether these sites are detecting the abnormalities of this traffic (and choosing not to take action) is hard to say. For some like Reddit &amp; Youtube, their block on OpenAI Operator indicates they want to lock down agentic traffic. When these AI Agents bypass their restrictions (as detailed below), it’s a good signal those tools are actually flying below their radar today versus being officially sanctioned.</p><p>For others that allow this traffic, they may actually encourage it for user convenience or potentially have perverse incentives in some cases to allow a certain amount of bot traffic (Ticketmaster is a potential example of this given they can benefit from some level of bot activity increasing ticket purchase rates). Many more sites, however, simply don’t have the traffic intelligence to detect these AI agents, which explains why they can operate freely.</p><p>In testing the three primary AI Agent browsing toolkits on a set of high traffic consumer sites, we found that Youtube and Reddit were the only ones that consistently blocked this traffic:</p><table><thead><tr><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody></tbody></table><p>Few websites are blocking popular AI Agents today – indicating either a lack of detection, ambivalence towards enforcement, or both. Reddit &amp; Youtube are a couple of key outliers that block this traffic when they can discern it.</p><p>Still, the differences are interesting. It’s worth understanding why Anthropic or BrowserBase are sometimes able to bypass sites that otherwise are blocking this traffic. In BrowserBase’s case, they generate a slightly different user-agent each session, which sometimes aligns with the underlying chromium runtime but sometimes attempts to be deceptive by claiming to be a different version, which leads to more inconsistency in whether companies detect it correctly.</p><p>Companies like LinkedIn and X are particularly sensitive to scraping on their sites, so the lack of enforcement is surprising. This suggests they’re not currently able to confidently discern this is programmatic traffic.</p><h2>How to Tell an Agent from a Human</h2><p>The obvious question here is: <strong>shouldn't this be easy to detect by using just an IP or user agent?</strong></p><p>And the answer is yes and no. For OpenAI Operator, it’s pretty straightforward to detect given its stability &amp; origin point. For Anthropic and BrowserBase (and the new agents entering the market), it’s a bit more complicated.</p><table><thead><tr><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody><tr></tr><tr><td><div variant=\"listItem\"><p>Depends on how you run it:</p><ul><li>Unlike Operator, you can run it locally directly on your machine or (recommended) on a dockerized container. In this case, it inherits your own device’s IP &amp; characteristics</li><li>In production, you’d likely host it in AWS/Azure/etc and traffic would come from known data centers</li></ul></div></td></tr><tr></tr><tr></tr><tr><td><div variant=\"listItem\"><p>Offers options for automatically solving</p></div></td></tr></tbody></table><p>Some key elements worth knowing:</p><ul><li>OpenAI &amp; BrowserBase both operate on top of chromium – being able to discern chromium browsers vs. true chrome is key for detecting the latter since BrowserBase modifies things like IP, user agent, etc for increased stealth.</li><li>OpenAI originates out of a known Azure datacenter while BrowserBase originates from a known AWS datacenter.</li><li>OpenAI hands off non-invisible CAPTCHAs to the end user to complete, while BrowserBase offers options for automatically solving.</li></ul><p>Ideally, you want to detect this traffic beyond more coarse identifiers like known datacenter IPs so that your detection is fully resistant to adjustments agentic traffic will make in the future. And core to that is having high confidence in whether a browser interacting with your site is a real browser or a headless instrumentation.</p><p>This is where we’ve personally found Machine Learning to be incredibly useful by building a browser lie detector. We look at a very wide range of browser signals &amp; we have downloaded the entire historical archives of every browser version that has been released that we can find and trained our model on what authentic browsers look like vs. anomalous or deceitful browsers. </p><p>To provide this visibility, we’ve automated additions of new legitimate browser signatures upon new releases, we fingerprint it, run it past the model to establish an authentic set of signals for the current browser version, allowing us to detect anyone trying to emulate that version inorganically. With that fingerprinted dataset, machine learning can be used to detect anomalies with high accuracy. It’s this signal collection that has allowed us to see that OpenAI &amp; BrowserBase are full chromium builds, while Anthropic’s Firefox browser fails to emulate certain characteristics that would be present on a real user’s Firefox instance.</p><h2><strong>Charting Your Path Forward</strong></h2><p>Whether you choose to block, restrict, or harness AI agent traffic, effective observability is non-negotiable. Here’s our current playbook:</p><ul><li> Start by detecting and monitoring AI agent fingerprints. Learn from their behavior before enforcing hard limits.</li><li><strong>Embrace legitimate use cases:</strong> Some users may leverage AI agents to streamline workflows (e.g., automating reporting on your dashboard). Recognize these opportunities while safeguarding against abuse.</li><li> As AI agents evolve, so must your detection strategies. Invest in ML-based solutions that adapt alongside new agent behaviors.</li></ul><p>AI agents are rewriting the rules of web interaction. They offer exciting UX advancements—but also present new security challenges. The race is on: evolve your detection or risk being outpaced by bad actors and unintended misuse.</p><div><div><div><div><div><h3>Try Device Fingerprinting</h3><p>99.99% bot detection, intelligent rate limiting, and reverse engineering protection.</p></div><a href=\"https://stytch.com/docs/fraud\">Read the docs</a></div></div></div></div>","contentLength":9979,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43049959"},{"title":"I'm very impressed by how Rust supports both beginners and pro's","url":"https://www.reddit.com/r/rust/comments/1ipe6m7/im_very_impressed_by_how_rust_supports_both/","date":1739549777,"author":"/u/ConstructionShot2026","guid":633,"unread":true,"content":"<p>I would go as far saying it supports a syntax abstraction that is simpler than python to read.</p><p>I just find it amazing, with a performance level so close to C++.</p><p>Its your choice how many complex features you want to add for control and optimization, and the compiler is so cool, that it can add them automatically if I don't see it necessary.</p><p>I believe if more knew how simple it could be, more would use it outside systems programming :D</p>","contentLength":434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why does Linux open large file bases much faster than windows?","url":"https://www.reddit.com/r/linux/comments/1ipd8a7/why_does_linux_open_large_file_bases_much_faster/","date":1739547296,"author":"/u/AlternativeCarpet494","guid":622,"unread":true,"content":"<p>So I have a 4TB hard drive with around a 100 GB dataset on it. I was going to some useless uni classes today and thought oh I’ll just work on some of my code to process the data set on my windows laptop. Anyways, the file explorer crashed. Why is the windows file system so much worse?</p>","contentLength":287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A better technique to split HTML into structured chunks while preserving the DOM hierarchy.","url":"https://github.com/carlosplanchon/betterhtmlchunking/","date":1739544345,"author":"/u/carlosplanchon","guid":625,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipc503/a_better_technique_to_split_html_into_structured/"},{"title":"Switching on Strings in Zig","url":"https://www.openmymind.net/Switching-On-Strings-In-Zig/","date":1739542983,"author":"/u/simon_o","guid":627,"unread":true,"content":"<p>Newcomers to Zig will quickly learn that you can't switch on a string (i.e. ). The following code gives us the unambiguous error message :</p><pre><code>color</code></pre><p>I've seen two explanations for why this isn't supported. The first is that there's ambiguity around string identity. Are two strings only considered equal if they point to the same address? Is a null-terminated string the same as its non-null-terminated counterpart? The other reason is that users of  [apparently] expect <a href=\"https://en.wikipedia.org/wiki/Branch_table\">certain optimizations</a> which are not possible with strings (although, presumably, these same users would know that such optimizations aren't possible with string).</p><p>Instead, in Zig, there are two common methods for comparing strings.</p><p>The most common way to compare strings is using  with :</p><pre><code>stdmem colorstdmem colorstdmem colorstdmem color</code></pre><p>The implementation for  depends on what's being compared. Specifically, it has an optimized code path when comparing strings. Although that's what we're interested in, let's look at the non-optimized version:</p><pre><code> T a balen  blenalen  aptr  bptra ba_elem b_elema_elem  b_elem</code></pre><p>Whether we're dealing with slices of bytes or some other type, if they're of different length, they can't be equal. Once we know that they're the same length, if they point to the same memory, then they must be equal. I'm not a fan of this second check; it might be cheap, but I think it's quite uncommon. Once those initial checks are done, we compare each element (each byte of our string) one at a time.</p><p>The optimized version, which  used for strings, is <a href=\"https://github.com/ziglang/zig/blob/5b9b5e45cb710ddaad1a97813d1619755eb35a98/lib/std/mem.zig#L720\">much more involved</a>. But it's fundamentally the same as the above with <a href=\"https://www.openmymind.net/SIMD-With-Zig/\">SIMD</a> to compare multiple bytes at once.</p><p>The nature of string comparison means that real-world performance is dependent on the values being compared. We know that if we have 100  branches then, at the worse case, we'll need to call  100 times. But comparing strings of different lengths or strings which differ early will be significantly faster. For example, consider these three cases:</p><pre><code> str1  10_000  str2  10_000 \n    _  stdmem str1 str2 str1  a\"  10_000 str2  a\"  10_000\n    _  stdmem str1 str2 str1  999_999 str2  1_000_000\n    _  stdmem str1 str2</code></pre><p>For me, the first comparison takes ~270ns, whereas the other two take ~20ns - despite the last one involving much larger strings. The second case is faster because the difference is early in the string allowing the  loop to return after only one comparison. The third case is faster because the strings are of a different length:  is returned by the initial  check.</p><p>The  takes an enum type and a string value and returns the corresponding enum value or null. This code prints \"you picked: blue\"</p><pre><code> std \n    red\n    blue\n    green\n    pink color  stdmetaColorInvalidChoicecolorred  stddebugblue  stddebuggreen  stddebugpink  stddebug</code></pre><p>If you don't need the enum type (i.e. ) beyond this check, you can leverage Zig's anonymous types. This is equivalent:</p><pre><code> std  color  stdmeta\n        red\n        blue\n        green\n        pinkInvalidChoicecolorred  stddebugblue  stddebuggreen  stddebugpink  stddebug</code></pre><p>It's  obvious how this should perform versus the straightforward  approach. Yes, we now have a  statement that the compiler can [hopefully] optimize, but  still has convert our input, , into an enum.</p><p>The implementation of  depends on the number of possible values, i.e. the number of enum values. Currently, if there are more than 100 values, it'll fallback to using the same  that we explored above. Thus, with more than 100 values it does the  check PLUS the switch. This should <a href=\"https://github.com/ziglang/zig/issues/3863\">improve in the future</a>.</p><p>However, with 100 or fewer values,  creates a comptime  which can then be used to lookup the value.  isn't something we've looked at before. It's a specialized map that buckets keys by their length. Its advantage over Zig's <a href=\"https://www.openmymind.net/Zigs-HashMap-Part-1/\">other hash maps</a> is that it can be constructed at compile-time. For our  enum, the internal state of a  would look something like:</p><pre><code>// keys are ordered by length\nkeys:     [\"red\", \"blue\", \"pink\", \"green\"];\n\n// values[N] corresponds to keys[N]\nvalues:   [.red, .blue, .pink, .green];\n\n// What's this though?\nindexes:  [0, 0, 0, 0, 1, 3];</code></pre><p>It might not be obvious how  is used. Let's write our own  implementation, simulating the above  state:</p><pre><code>str keys  values redbluepinkgreen indexes strlen  indexeslen index  indexesstrlenindex  keyslen key  keysindexkeylen  strlenstdmem key str valuesindex\n        index </code></pre><p>Take note that  are ordered by length. As a naive implementation, we could iterate through the keys until we either find a match or find a key with a longer length. Once we find a key with a longer length, we can stop searching, as all remaining candidates won't match - they'll all be too long.  goes a step further and records the index within  where entries of a specific length begin.  tells us where to start looking for keys with a length of 3 (at index 0).  tells us where to start looking for keys with a length of 5 (at index 3).</p><p>Above, we fallback to using  for any key which is the same length as our target string.  uses its own \"optimized\" version:</p><pre><code>a baptr  bptra ba_elem b_elema_elem  b_elem</code></pre><p>This is the same as the simple  implementation, minus the length check. This is done because the  within our  loop is only ever called for values with matching length. On the flip side, 's  doesn't use SIMD, so it would be slower for large strings.</p><p>In my own benchmarks, in general, I've seen little difference between the two approaches. It does seem like  is generally as fast or faster. It also results in more concise code and is ideal if the resulting enum is useful beyond the comparison.</p><p>You usually don't have long enum values, so the lack of SIMD-optimization isn't a concern. However, if you're considering building your own  at compile time with long keys, you should benchmark with a custom  function based on .</p><p>We could manually bucket those  branches ourselves, similar to what the  does. Something like:</p><pre><code>colorlenstdmem colorstdmem colorstdmem colorstdmem color</code></pre><p>Ughhh. This highlights the convenience of using  to generate similar code. Also, do remember that  quickly discards strings of different lengths, which helps to explain why both approaches generally perform similarly.</p>","contentLength":6154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipbna2/switching_on_strings_in_zig/"},{"title":"Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11","url":"https://www.youtube.com/watch?v=eHa6GhK7L0I","date":1739541570,"author":"CNCF [Cloud Native Computing Foundation]","guid":390,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eHa6GhK7L0I?version=3","enclosureMime":"","commentsUrl":null},{"title":"The History of S.u.S.E","url":"https://www.abortretry.fail/p/the-history-of-suse","date":1739541229,"author":"ibobev","guid":215,"unread":true,"content":"<p>In 1992, four college students, Roland Dyroff, Thomas Fehr, Hubert Mantel, and Burchard Steinbild in Fürth, Germany were dreaming of independence. They wanted to chart their own paths, make money for themselves, and have fun doing so. They’d had significant exposure to UNIX at university, and there was a new, free, and open source kernel called Linux from Helsinki. At the same time, there was Peter MacDonald’s Linux distribution, the Softlanding System (SLS), and Patrick Volkerding was publishing fixes for SLS and making them freely available via FTP. Somewhat problematically, most UNIX systems and most Linux systems were exclusively English language oriented. These four students were ready to do something for themselves and they began the process of localizing SLS into German.</p><p>On the 2nd of September in 1992, the quartet founded Gesellschaft für Software und Systementwicklung mbH (or S.u.S.E.). In English that would be Software and System Development, but the contraction S.u.S.E. also bore more than a passing similarity to the name of Konrad Zuse who’d built the world’s first programmable and Turing-complete computer (depending upon how one defines this) the Z3 in May of 1941.</p><p><a href=\"https://www.abortretry.fail/p/the-history-of-slackware\" rel=\"\">Slackware Linux</a></p><p>In these halcyon days, the young company had no real hierarchy, no regular meetings, no reports, and no bureaucracy. The company operated in a sort of democratic fashion. Being so small, everyone knew everything that was occurring at any given time. If a major decision needed to be made, they’d simply take an informal vote. If they had unanimity, they’d make the change and this meant that the company could change direction in a single night which was when they normally got the most work done.</p><p>In 1994, S.u.S.E released a fully localized version of Slackware Linux 2.0 with some patches and software incorporated from other Linux distributions, S.u.S.E. Linux 1.0. This release was quickly followed by 1.1.27 in July. This was a Linux system with the XFree86 2.1.1, TCP/IP, NFS, kernel 1.0.9, GNU CC 2.5.8 and 2.4.5, and documentation on disc. The system requirements were a 386 with at least 4MB of RAM, but if one intended to use X11 the RAM requirement was a minimum of 8MB. The system supported ISA, EISA, VLB, and PCI, but it did not support MCA.</p><p>Most references of version 4.2 state that the distribution was only available in German, but this is incorrect. The initial boot configuration allows a choice of either English or German.</p><p>After the installation environment is loaded, when init handed off control to a tty, the user was greeted with a logon screen, again in both languages.</p><p>From there, YaST would take the user through the installation in a well design curses environment.</p><p><a href=\"https://www.tindie.com/products/weird/isa-8-bit-ethernet-controller/\" rel=\"\">8bit RTL8019</a></p><p>At this point, the system was ready to load Linux.</p><p>After creating a Linux disk partition, the user is required to reboot (though nothing informs you of this). At that point, the partition changes are written to disk. In my case, this meant that I would need to boot from a floppy. There is an option to create a Linux boot floppy, but I chose to use a Win98 boot floppy instead. This essentially runs through the LOADLIN portion for a second time, after which I restarted the setup, loaded the config for all packages, and proceeded with the installation.</p><p>Here, selecting the main menu will allow the installer to progress to finishing the installation.</p><p>After login, there’s a brief delay and then we go through the first run setup procedures.</p><p>That took an incredibly long time on my modest 1990s system.</p><p>S.u.S.E. Linux version 5 was released in July of 1997, and that was the version to begin the transition of the distribution to RPMs. Slackware’s TGZs were still supported. That release used kernel 2.0.30, Bash 2.0, Qt 1.2, XFree86 3.3 and FVWM 4.0.45. October of that year saw the release of version 5.1 with the most notable changes being the support of FTP package updates and KDE. This version was five CD-ROMs. Four of these were for the installation, and one was for the LiveCD environment. The final version 5 release was on the 10th of September in 1998, version 5.3 with kernel 2.0.35, KDE 1.0, and Netscape Communicator version 4.05.</p><p>So, here in the installer, we can see a clear evolution from the prior release. Everything is a bit more refined, the installation was far more straight forward, and we can easily see that what has been reported as a complete rebase on Jurix, was truly more of a merger between the Slackware-rooted origin, and the RPM-based Jurix with its automatic build system. We can also see that S.u.S.E. was now evolving into something unique, something gestalt. That said, my installation on a K6-III crashed repeatedly.</p><p>S.u.S.E. opened a branch office in Oakland in 1997 as well as more offices across Europe. Not long after these expansions came new partnerships with IBM, SAP, Compaq, and Oracle. For IBM and Compaq, this partnership included S.u.S.E. offering 24-hour support services. The same year, SuSE began offering servers pre-installed with S.u.S.E. Linux. For 1998, the company moved their corporate office from Fürth to Nürnberg. In 1999, the company opened a branch office in the UK.</p><p>The company released version 6.0 on the 21st of December in 1998, and with this release, the company rebranded as SuSE Linux. Version 6 shipped with kernel 2.0.36, and it saw point release up to 6.4 which released on the 9th of March in 2000. Version 6.4 included YaST2 which was graphical.</p><p>On the 27th of September in 2000, SuSE Linux version 7 was released. SuSE Linux Professional 7.0 consisted of six CD-ROMs and one DVD for those who had a DVD-ROM drive, two floppies for those who couldn’t boot from optical media, a quick install guide, a handbook, a configuration guide, and an applications guide. By this point, like many other commercial Linux distributions, SuSE included StarOffice, Netscape, Acrobat, GIMP, Sane, KDE, and other familiar applications. It had a graphical YaST2 setup tool, ReiserFS and ext2 filesystems, RPM package management, and retailed for $69.99. SuSE Linux Personal lacked many of the proprietary applications and shipped on three CDs. This version saw released for x86, Alpha, PPC, and SPARC. The last of the version 7 systems was 7.3 released on the 23rd of October in 2001.</p><p><a href=\"https://www.abortretry.fail/p/the-itanic-saga\" rel=\"\">Itanium</a></p><p>By November of 2000, something around 66% of SuSE’s revenues were from Linux CD-ROM sales at retail, online, over the phone. Roughly 24% was from services, and the remaining 8% or so was from hardware sales. The company had around forty developers working on open source software, and this didn’t include people who handled the compiling and packaging into RPMs (release engineers), or those who handled support services. Beyond this, the company contributed both money and talent to projects like ReiserFS, KDE, and the Linux kernel.</p><p><a href=\"https://www.abortretry.fail/p/the-history-of-red-hat\" rel=\"\">Red Hat</a><a href=\"https://www.abortretry.fail/p/the-history-of-caldera-part-1\" rel=\"\">Caldera</a><a href=\"https://www.abortretry.fail/p/the-history-of-linux-mandrake\" rel=\"\">Mandrake</a></p><p>SuSE Linux 8.0 was released on the 22nd of April in 2002 with Linux kernel 2.4.18, KDE3, KOffice, and several other enhancements. The installer was more automated, had better hardware detection, and YaST2 could detect Windows partitions. Upon the detection of a Windows partition, the installer would suggest changes to hard disk allocation for the installation of SuSE. YaST2 was also much better at configuring the X server with SaX2 (SuSE advanced X configuration). On the media side of things, SuSE 8 included applications for audio mixing, editing, and CD writing. These audio applications were aided by the inclusion of ALSA 0.9 (Advanced Linux Sound Architecture) which allowed for the playback of more than one audio stream at a time. Security enhancements also came to SuSE with SuSE Personal Firewall 2, GNU Privacy Guard, OpenSSH, PAM, and IPsec. Version 8 concluded with the release of 8.2 on the 7th of April in 2003.</p><p>SuSE Linux 9.0 was released on the 15th of October in 2003 with Linux kernel 2.4.21, GNOME 2.2.3, and KDE 3.1.4. This version brought two major changes. First, SuSE Linux 9 was the first distribution to support AMD64. SuSE had reached out to AMD early in the Hammer architecture’s development, and they built an emulator from AMD’s specifications. The company then proceeded to port the entire distribution AMD64 with AutoBuild. When the first hardware arrived at their offices, the system worked flawlessly. The second big addition was support for NTFS (non-root) filesystems.</p><p><a href=\"https://www.abortretry.fail/p/the-history-of-novell\" rel=\"\">Novell</a></p><p>The first release after acquisition was SUSE Linux 9.1 on the 23rd of April in 2004. This version shipped with kernel 2.6.4, GNOME 2.4, and KDE 3.2.1. SUSE 9.1 brought in some visual changes between the personal and professional editions as well. The personal editions maintained the green theming while the professional editions had blue theming. This was also evident in the default background images.</p><p>For the first time, with SUSE 9.1, people could download the distribution without any monetary cost. A single CD release was available without support of any kind. For the most part, this was just SUSE Personal without some extra packages.</p><p>SUSE Linux Enterprise Server 9 was released in August of 2004 and Novell Linux Desktop 9 (built on much the same package base) was released on the 8th of November in 2004. This release saw official support from IBM, HP, Sun, Dell, SGI, Lenovo, and Fujitsu. For the first time, GNOME became the default, but KDE remained present. Naturally, Ximian’s Evolution became Novell Evolution. Given events at Sun, StarOffice was replaced by OpenOffice. With more of the Ximian acquisition in evidence, the availability of software updates were made apparent to the user with a Red Carpet icon.</p><p>While SUSE had been an enterprise focused distribution for quite a while, on the 4th of August in 2005, Novell announced that the creation of the openSUSE Project. This would allow for a more open development process, and what had previously been the personal edition became a freely available product without professional support. SUSE Linux 10 was released on the 6th of October in 2005 and it was the first SUSE release to follow this model. SUSE Linux 10 shipped with kernel 2.6.13, GNOME 2.12, and KDE 3.4.2.</p><p>SUSE Linux 10.1 was released on the 11th of May in 2006 with Xgl, AppArmor, Xen, the ext3 filesystem, Firefox 2, and NetworkManager. To my knowledge, this was the first major inclusion of Xen in any Linux system. Both enterprise server and desktop versions of 10 were released in July of 2006, and the Novell Linux Desktop was dropped in favor of SUSE Linux Enterprise Deskstop. Version bumps and the name change to openSUSE arrived with 10.2 on the 7th of Decemer in 2006. The final version 10 release was made on the 4th of October in 2007 with 10.3 featuring kernel 2.6.22, GNOME 2.12, KDE 3.5.1, and XFCE 4.4.1. This version also included a preview release of KDE4 Beta 2. On the enterprise side, updates were shipped in Service Packs with SP4, the final version 10 service pack, having shipped on the 15th of April in 2011.</p><p>In November of 2006, Microsoft and Novell signed an intellectual property agreement that involved immunity over patent litigation and freed SUSE Linux users over any possibility of lawsuits from Microsoft. This followed claims from Microsoft that Linux systems were in violation of 235 patents without any disclosure over what those violations were. While some in the community viewed this negatively and were quite vocal about it, this did gain some piece of mind for SUSE to include things like Exchange support, MS Office file format support, and interoperability between the two companies’ directory implementations. This would prove to have lasting effects on both the wider Linux ecosystem and on Microsoft. In the following years, improvements in MS Office support would proliferate, Active Directory support would be common, and Microsoft’s Azure would have Linux support. Microsoft even made SQL Server, VS Code, and Powershell available on Linux.</p><p>On the 22nd of November in 2010, Novell and Attachmate announced that Novell would be acquired by Attachmate. Jeff Hawn, chairman and CEO of Attachmate, commented:</p><blockquote><p><em>The openSUSE project is an important part of the SUSE business. As noted in the agreement announced today, Attachmate plans to operate SUSE as a stand-alone business unit after the transaction closes. If this transaction closes, then after closing, Attachmate Corporation anticipates no change to the relationship between the SUSE business and the openSUSE project as a result of this transaction.</em></p></blockquote><p>On the 30th of April in 2011, Attachmate completed their acquisition of Novell. On Monday, the 2nd of May, eight hundred employees were laid off. </p><p>openSUSE 12.1 came into the world on the 16th of November in 2011 with kernel 3.1.0, GNOME 3.2, and KDE 4.7. This release also included Snapper for Btrfs snapshot management, and systemd for the init system. System V init was still available to users should they want it. Version 12.2 hit on the 5th of September in 2012 with KDE Plasma 4.8, GNOME 3.4, XFCE 4.10, Firefox 14, Plymouth, GRUB 2, and kernel 3.4.6. Version 12.3, the final for the 12 series, released on the 13th of March in 2013. This version gave version bumps to most software, did away with LiveCDs in favor of USB images, and introduced a “rescue” image with XFCE. </p><p>openSUSE version 13.1 was released on the 19th of November in 2013 with kernel 3.11.6, GNOME 3.10, KDE Plasma 4.11.2, Firefox 25, LibreOffice 4.1, experimental Wayland support, and YaST rewritten in Ruby. SUSE Linux Enterprise 12 was built on openSUSE 13, and it shipped on the 28th of October in 2014. openSUSE 13.2 followed on the 4th of November in 2014 with version bumps across the board. That same month, the openSUSE project introduced openSUSE Tumbleweed. This is a rolling release distribution with periodic snapshots made into installable images for i686, AMD64, ARM 64, PPC64, IBM zSystems, KVM, Xen, HyperV, and public and private cloud. Live system images are available with GNOME, KDE, XFCE, and rescue.</p><p>On the 20th of November in 2014, Micro Focus completed its acquisition of Attachmate. Under Micro Focus, SUSE was operated independently from the rest of the organization with its own products. Nils Brauckmann became CEO of SUSE and gained a board seat at Micro Focus.</p><p>With rolling releases now available, the traditional openSUSE system became openSUSE Leap. The first release of Leap hearkened back to the distribution’s history with version 42.1 becoming available on the 4th of November in 2015. This system was based off of enterprise 12 SP1. With another reference to history, Leap brought about a return to KDE as the default desktop with KDE Plasma 5.8. The 42 series saw two updated releases with 42.2 on the 16th of November in 2016, and 42.3 on the 26th of July in 2017. These later releases were based off of SP2 and SP3 respectively.</p><p>On the 25th of May in 2018, SUSE released Leap 15 with SUSE Linux Enterprise following on the 25th of June. These were built on the same codebase with GNOME 3.26, LibreOffice 6, GCC 7, Wayland as the display server, and kernel 4.12. Updates of this release are the current offering as of this writing.</p><p>Micro Focus sold SUSE to EQT AB on the 2nd of July in 2018 for $2.535 billion. Melissa Di Donato became CEO on the 22nd of July in 2019. SUSE finally went public on the Frankfurt stock exchange on the 19th of May in 2021. The company was valued at about $6.1 billion on the first day of trading. Di Donato resigned on the 21st of March in 2023 with Dirk-Peter van Leeuwen being appointed as CEO. On the 17th of August in 2023, EQT delisted SUSE. On the 13th of November in 2023, SUSE merged with Marcel New Lux IV S.A. and the combined entity became SUSE S.A.</p><p>I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment.</p>","contentLength":15900,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43048261"},{"title":"So you want better debug info?","url":"https://walnut356.github.io/posts/so-you-want-better-debug-info/","date":1739539833,"author":"/u/Anthony356","guid":631,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ipalov/so_you_want_better_debug_info/"},{"title":"Show HN: Transform your codebase into a single Markdown doc for feeding into AI","url":"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html","date":1739539403,"author":"tesserato","guid":202,"unread":true,"content":"<p>CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.\nThe output for the current repository can be found <a href=\"https://github.com/tesserato/CodeWeaver/blob/main/codebase.md\">here</a>.</p><ul><li><strong>Comprehensive Codebase Documentation:</strong> Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format.</li><li> Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.</li><li>  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types).</li><li> Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.</li><li><strong>Simple Command-Line Interface:</strong>  Offers an intuitive command-line interface with straightforward options for customization.</li></ul><p>If you have Go installed, run <code>go install github.com/tesserato/CodeWeaver@latest</code>to install the latest version of CodeWeaver or <code>go install github.com/tesserato/CodeWeaver@vX.Y.Z</code> to install a specific version.</p><p>Alternatively, download the appropriate pre built executable from the <a href=\"https://github.com/tesserato/CodeWeaver/releases\">releases page</a>.</p><p>If necessary, make the  executable by using the  command:</p><table><thead><tr></tr></thead><tbody><tr><td>The root directory to scan and document.</td></tr><tr><td>The name of the output Markdown file.</td></tr><tr><td><code>-ignore \"&lt;regex patterns&gt;\"</code></td><td>Comma-separated list of regular expression patterns for paths to exclude.</td></tr><tr><td><code>-included-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were included in the documentation.</td></tr><tr><td><code>-excluded-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were excluded from the documentation.</td></tr><tr><td>Display this help message and exit.</td></tr></tbody></table><h2><strong>Generate documentation for the current directory:</strong></h2><p>This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).</p><h2><strong>Specify a different input directory and output file:</strong></h2><pre><code>./codeweaver -dir=my_project -output=project_docs.md\n</code></pre><p>This command will process the  directory and save the documentation to .</p><h2><strong>Ignore specific file types and directories:</strong></h2><pre><code>./codeweaver -ignore=\"\\.log,temp,build\" -output=detailed_docs.md\n</code></pre><p>This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.</p><h2><strong>Save lists of included and excluded paths:</strong></h2><pre><code>./codeweaver -ignore=\"node_modules\" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md\n</code></pre><p>This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .</p><p>Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.</p><p>CodeWeaver is released under the <a href=\"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/LICENSE\">MIT License</a>. See the  file for complete license details.</p>","contentLength":3311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43048027"},{"title":"AI is stifling new tech adoption","url":"https://vale.rocks/posts/ai-is-stifling-tech-adoption","date":1739537105,"author":"kiyanwang","guid":214,"unread":true,"content":"<p>I propose that the advent and integration of AI models into the workflows of developers has stifled the adoption of new and potentially superior technologies due to training data cutoffs and system prompt influence.</p><p>I have noticed a bias towards specific technologies in multiple popular models and have noted anecdotally in conversation and online discussion people choosing technology based on how well AI tooling can assist with its usage or implementation.</p><p>While it has long been the case that developers have considered documentation and support availability when choosing software, AI’s influence dramatically amplifies this factor in decision-making, often in ways that aren’t immediately apparent and with undisclosed influence.</p><p>Large language models, especially those on the scale of many of the most accessible, popular hosted options, take humongous datasets and long periods to train. By the time everything has been scraped and a dataset has been built, the set is on some level already obsolete. Then, before a model can reach the hands of consumers, time must be taken to train and evaluate it, and then even more to finally deploy it.</p><p>Once it has finally released, it usually remains stagnant in terms of having its knowledge updated. This creates an AI knowledge gap. A period between the present and AI’s training cutoff. This gap creates a time between when a new technology emerges and when AI systems can effectively support user needs regarding its adoption, meaning that models will not be able to service users requesting assistance with new technologies, thus disincentivising their use.</p><p>The cutoff means that models are strictly limited in knowledge up to a certain point. For instance, Anthropic’s latest models have a cutoff of <a href=\"https://support.anthropic.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data\">April 2024</a>, and OpenAI’s latest models have cutoffs of <a href=\"https://platform.openai.com/docs/models\">late 2023</a>.</p><p>The influence of the popularisation of AI models has also led to the proliferation of AI slop content, which AI companies are likely trying to avoid introducing in their scraped data sets – potentially increasing this knowledge gap.</p><p>This knowledge gap doesn’t just stifle the adoption of new frameworks and tooling but also the adoption of new and potentially superior features introduced in updates to already in-use frameworks and tooling as a result of the models not yet including information about them in their training set. This is a lesser issue though, because a technology with existing market adoption is bound to have fanatics who will use it, create examples, and produce blog posts, documentation, and other media about it that will eventually end up in training data.</p><p>While some models are used in contexts where they are equipped with tooling, such as ChatGPT with its internet search capabilities, their prompts often only instruct them to use this functionality when explicitly requested by a user. Of course, plenty of implementations don’t have any access to any internet query functionality at all, such as Claude via its web interface, which will inform you it will only discuss things included in its training data.</p><p>Of course, if people are reluctant to adopt a new technology because of a lack of AI support, there will be fewer people likely to produce material regarding said technology, which leads to an overall inverse feedback effect. Lack of AI support prevents a technology from gaining the required critical adoption mass, which in turn prevents a technology from entering use and having material made for it, which in turn starves the model of training data, which in turn disincentivises selecting that technology, and so on and so forth.</p><p>Consider a developer working with a cutting-edge JavaScript framework released just months ago. When they turn to AI coding assistants for help, they find these tools unable to provide meaningful guidance because their training data predates the framework’s release. This forces developers to rely solely on potentially limited official documentation and early adopter experiences, which, for better or worse, tends to be an ‘old’ way of doing things and incentivises them to use something else.</p><p>I’ve anecdotally noticed that many AI tools have a ‘preference’ for React and Tailwind when asked to tackle a web-based task, or even to create any app involving an interface at all.</p><p>With Claude 3.5 Sonnet, which is generally my AI offering of choice given its superior coding ability, my “What personal preferences should Claude consider in responses?” profile setting includes the line “When writing code, use vanilla HTML/CSS/JS unless otherwise noted by me”. Despite this, Claude will  opt to generate new code with React, and in some occurrences even rewrite my existing code into React against my intent and without my consultation.</p><p>This can be evidenced when asking Claude to use a certain technology, such as Svelte Runes, where it will instead direct the user towards its preferred technology. For example:</p><blockquote><p> Build me something with Svelte Runes.</p></blockquote><blockquote><p> I apologize, but I am not able to create Svelte Runes components directly, as the artifact environment only supports certain technologies like React, HTML, and SVG. I can help you build something similar using React, or I can provide you with Svelte Runes code that you can use in your own Svelte project. Which would you prefer?</p><p>If you’d like to see a React version, I can create something with similar reactive functionality using React’s hooks. Or if you want Svelte Runes code specifically, I can write that for you to use in your own environment.</p></blockquote><p>To test if popular models do have built-in bias for React, I logged into the web interfaces of four of the current most popular AI chat platforms via my free account and asked the models to “Create me a web app. It can do anything.”. I repeated this with each of the four models three times in fresh chats with previous chats deleted and any shared context (such as ChatGPT’s memory feature) disabled or cleared between each test.</p><p>I asked Anthropic’s Claude 3.5 Sonnet with blank personal preferences and artifacts enabled. The first time it opted to create a mood tracking app, the second a reading list manager, and the third time a to-do list. Each time it chose to generate it using React with Tailwind CSS for styling. It generated each project in an artifact which offered an interactive preview directly in the interface.</p><p>I asked OpenAI’s ChatGPT 4o with both custom instructions and memory disabled. It produced a note-taking app the first time, a random quote generator the second time, and a to-do list the third time. All using React with Tailwind for styling and presented in <a href=\"https://openai.com/index/introducing-canvas\">canvases</a>. These canvases came with a preview button, which, much like with Claude, offered an interactive preview of the React project it built directly in the interface.</p><p>I asked Google’s Gemini 2.0 Flash with app activity disabled. All three times it built to-do list apps using vanilla HTML, CSS, and JavaScript. Each time, it also noted that I should consider using JavaScript frameworks on the frontend, namely React, Angular, and Vue - in that order.</p><p>I asked DeepSeek’s DeepSeek-V3, and it returned a to-do list app using vanilla HTML, CSS, and JavaScript. On the second request, it created a project outline for a to-do list app, but this time with Bootstrap, Node.js, Express.js, and SQLite. The third time, it created another outline for a note-taking app but with Node.js, Express.js, and MongoDB.</p><p>DeepSeek had the greatest variation in technology used but is the least accessible model and didn’t actually ‘create’ the app as requested – merely a general outline for it.</p><p>These tests show both Claude and ChatGPT have a strong preference for React and Tailwind,  that Gemini has a preference for HTML/CSS/JS but will recommend React and that DeepSeek is much more flexible and varied with its technological choices, though requires more prompting to produce an actual output.</p><p>I’d suggest that a beginner developer, or someone creating an app exclusively via prompting, is likely to use ChatGPT due to its position in the zeitgeist and use whatever output the model first produces without much second thought – thus influencing their tech selection without their realising.</p><p>Even if a developer does opt to use another framework or toolset, there is a chance that the model will prod them towards a selection more aligned with its system prompt, even going as far as rewriting their code to its ‘preference’ against user request, as evidenced by Claude ignoring my request for vanilla web technologies.</p><p>That is assuming that a specific framework or toolset hasn’t already been chosen by the user specifically because they have heard or experienced it being best handled by AI models. Also, while this very much applies to larger technical choices, such as what framework or general tooling a project may choose, it also trickles down into smaller decisions, such as what libraries or modules they may import.</p><p>I think it is evident that AI models are influencing technology, and that the technologies currently in use – especially those that reached popularity before November 2022, when ChatGPT was released, or that are otherwise in current data sets – will be around for a long time to come, and that AI models’ preferential treatment of them will expand their adoption and lifespan.</p><p>I think it would be prudent for AI companies to provide more transparent documentation of technology biases in their models, like they disclose that their models can make mistakes. These models are becoming a common part of developer knowledge and decision-making, and we’re letting the training and prompting decisions of OpenAI, Anthropic, etc shape the entire direction of software development.</p><p>As for further research on this topic, if one had a collection of system prompts over time, it may be possible to compare them to download trends of specified packages and identify correlations. Of course, there are a lot of other influences on the download and adoption of packages, so this may prove difficult and provide data too noisy and influenced by outside variables to be unusable.</p>","contentLength":10120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43047792"},{"title":"Show HN: A New Way to Learn Languages","url":"https://www.langturbo.com/","date":1739534938,"author":"sebnun","guid":201,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43047554"},{"title":"Walking with filesystems: using the fs.FS interface","url":"https://bitfieldconsulting.com/posts/filesystems","date":1739531881,"author":"/u/AlexandraLinnea","guid":608,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ip8dk9/walking_with_filesystems_using_the_fsfs_interface/"},{"title":"Learn from Documentation or Book?","url":"https://www.reddit.com/r/kubernetes/comments/1ip8d2r/learn_from_documentation_or_book/","date":1739531824,"author":"/u/nicholle_marvel","guid":613,"unread":true,"content":"<p>In 2025, there are numerous books available on Kubernetes, each addressing various scenarios. These books offer solutions to real-world problems and cover a wide range of topics related to Kubernetes.</p><p>On the other hand, there is also very detailed official documentation available.</p><p>Is it worth reading the entire documentation to learn Kubernetes, or should one follow a book instead?</p><p>Two follow-up points to consider: 1. Depending on specific needs, one might visit particular chapters of the official documentation. 2. Books often introduce additional tools to solve certain problems, such as monitoring tools and CI/CD tools.</p><p>Please note that the goal is not certification but rather gaining comprehensive knowledge that will be beneficial during interviews and in real-world situations.</p>","contentLength":786,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub - openorch/openorch: AI app platform. A language-agnostic, distributed platform for building microservices-based AI backends.","url":"https://github.com/openorch/openorch","date":1739528271,"author":"/u/Sand4Sale14","guid":628,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ip7jlt/github_openorchopenorch_ai_app_platform_a/"},{"title":"How am I just finding out about the OhMyZsh plugin?","url":"https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/kubectl","date":1739526964,"author":"/u/ominouspotato","guid":617,"unread":true,"content":"<p>It’s literally just a bunch of aliases but it has made CLI ops so much easier. Still on my way to memorizing them all, but changing namespace contexts and exec-ing to containers has never been easier. Highly recommend if you’re a k8s operator! </p><p>Would also love to hear what you all use in your day-to-day. My company is looking into GUI tools like Lens but they haven’t bought licenses yet. </p>","contentLength":396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1ip78yb/how_am_i_just_finding_out_about_the_ohmyzsh_plugin/"},{"title":"Here's What Devs Are Saying About New GitHub Copilot Agent – Is It Really Good?","url":"https://favtutor.com/articles/github-copilot-agent/","date":1739523054,"author":"/u/ImpressiveContest283","guid":629,"unread":true,"content":"<p>With the introduction of AI, coding has never been easier. To simplify it further, GitHub Copilot is here to improve the working experience, helping code faster and better. </p><p>If <a href=\"https://favtutor.com/articles/github-copilot-workspace-insights/\">GitHub Copilot</a> has undergone a transition from being a mere passive suggestion engine for the code to being an active collaborator that simplifies development, Agent Mode takes it one step further. Think of it as an autonomous co-coder, a peer programmer who prods and pushes forward the work on your projects.</p><h2><strong>The New GitHub Copilot Agent Mode</strong></h2><p>Agent Mode is like having a co-coder within the programming team, prioritizing your development activity over that of any other person. It now allows Copilot to:</p><ul><li> It constantly improves and enhances the code it generates, always searching for optimal solutions.</li><li>On detection of an error, it automatically assumes the responsibility for spotting and correcting the source, thus killing debugging efforts.</li><li>It provides you with all terminal commands and facilitates running those commands for more speedy working.</li><li>It goes to infer subsequent underlying tasks beyond your instructions so all solutions would be comprehensive.</li></ul><p>An example would be tasking the agent to implement a web application for tracking marathon training. <a href=\"https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/?utm_source=chatgpt.com\" data-type=\"link\" data-id=\"https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/?utm_source=chatgpt.com\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Agent Mode can set up the project structure</a> alone; create the relevant components and even fix runtime errors to provide seamless development.</p><figure></figure><h3><strong>Auto Complete Coding Tasks with ease</strong></h3><p>With <a href=\"https://code.visualstudio.com/blogs/2025/02/12/next-edit-suggestions\" data-type=\"link\" data-id=\"https://code.visualstudio.com/blogs/2025/02/12/next-edit-suggestions\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Next Edit Suggestions (NES)</a>, GitHub Copilot has introduced a feature of code editing that does editing by predicting and suggesting modifications to existing code. NES accounts for the fact that coding often involves refining code already written. </p><p>This feature is now available for use along with Visual Studio Code, where developers can enable NES to receive intelligent edit recommendations that thereby improve coding efficiency and accuracy.</p><h3><strong>Rapid Coding and Beginner-Friendly</strong></h3><p><strong>By providing high-level instructions, Copilot’s Agent Mode generates the necessary code components, eliminating the need for manual coding. </strong></p><p>Moreover, Agent Mode iteratively improves the code, optimizing performance and ensuring functionality without human intervention. Beyond the initial instructions, Copilot infers additional necessary tasks, such as setting up game logic and user interfaces, to create a fully functional application.</p><h3><strong>GitHub Copilot Agent vs other coding assistants</strong></h3><p>Cursor is an advanced AI-driven coding tool aimed at boosting developer efficiency.</p><p>Copilot is designed to blend effortlessly into several integrated development environments (IDEs) such as Visual Studio Code and JetBrains and provide immediate inline code recommendations and auto-completion options.</p><p>On the other hand, Cursor operates as an independent AI-enhanced editor derived from Visual Studio Code, equipped with features like proactive code generation, intelligent auto-completion, and smart rewriting capabilities.</p><p>While many users commend Copilot for its seamless integration and intuitive user interface, Cursor stands out for providing a richer integration of AI within its ecosystem.</p><h3><strong>A Glimpse into the Future</strong>: </h3><p>Though Agent Mode is already a leap into revolution, GitHub’s Project Padawan hints at an even more exciting future. It aims to create a self-sufficient software engineering (SWE) agent capable of completing entire tasks without human intervention. </p><p>Just picture a future where you tell Copilot to fix GitHub issues and write whole modules. Features like Issue resolution and continuous learning could transform the very way that developers approach software engineering.</p><h2><strong>Divided Opinions: The other side of the coin</strong></h2><p>Although several developers have included the Copilot in their flow, that is not to say they did not face a fair share of challenges. Netizens on X and Reddit are divided over the new AI Agent and have pointed out how the tool creates code or content that sometimes feels generic or shallow. </p><p><strong>Importantly, the AI-generated output needs several thorough checks against project requirements and coding standards.</strong></p><p>The Agent Mode of GitHub Copilot is set to change the landscape of software development by relieving developers from mundane tasks such as debugging, documentation, or testing. This will give developers more time and energy to concentrate on higher-level innovation and creative problem-solving. Will it take over programmers or not is a topic up for another debate..</p>","contentLength":4399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ip6dts/heres_what_devs_are_saying_about_new_github/"},{"title":"HandBrake 1.9.1 released (video transcoder)","url":"https://github.com/HandBrake/HandBrake/releases/tag/1.9.1","date":1739522206,"author":"/u/gabriel_3","guid":621,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ip67fh/handbrake_191_released_video_transcoder/"},{"title":"Ricochet: Peer-to-peer instant messaging system built on Tor hidden services (2017)","url":"https://github.com/ricochet-im/ricochet","date":1739522071,"author":"philonoist","guid":213,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43046192"},{"title":"Linux kernel cgroups writeback high CPU troubleshooting","url":"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/","date":1739521827,"author":"mesto1","guid":212,"unread":true,"content":"<p>Read on to learn how we used network packet captures and BPF to debug web server performance, ultimately uncovering a Linux kernel performance issue. This investigation was a collaboration between myself and my colleagues.</p><p>We’ve been upgrading the operating system from CentOS to Ubuntu on hosts across our fleet. Our CentOS hosts run an outdated Linux kernel version (3.10), whereas our Ubuntu hosts run a more modern kernel version (6.8). In August 2024, we began rolling out the Ubuntu upgrade across our Apache web servers. When we migrated larger portions of our fleet to Ubuntu, we began seeing elevated listen overflow errors. This elevated error rate prompted us to roll back the Ubuntu upgrade while we debugged:</p><p>Apache listens on a socket for incoming web requests. When incoming requests arrive more quickly than Apache can serve them, the queue of requests waiting to be served grows longer. This queue is capped to a configurable size. When the queue overflows its maximum size, we have a .</p><p>Listen overflows are a symptom of either of two things: the rate of incoming web requests is too high, and / or Apache is serving web requests too slowly.</p><p>Each listen overflow that occurs means we failed to serve a web request. This can result in user-facing errors. Furthermore, if the listen overflows are a symptom of web requests being served slowly, it means users may be experiencing slow page loads.</p><p>Listen overflows occurred a few minutes after a newly autoscaled web server was pooled. They did not tend to recur subsequently. Furthermore, web requests had elevated latency during this same time period:</p><p>We believed listen overflows were occurring because web requests served in the first few minutes after the host was pooled were being served unusually slowly.</p><p>Log lines,  timing information, and <a href=\"https://github.com/adsr/phpspy\"></a> flame graphs showed that network operations were executing particularly slowly. But investigating further, we found evidence that the network was performing normally. Instead, PHP had seemingly stalled for over a second. The below log line indicates that a memcached SET command was slow, but network packet captures on both the client and the server that we analyzed in Wireshark indicate that the SET command experienced normal network latency. The client waited over 1 second before sending the subsequent GET command, as if our PHP script stalled after the packet was received but before we recorded the elapsed time.</p><p><code>[Tue Sep 24 21:20:54 2024] [info] Memcached operation exceeded 20ms :operation=\"set\"  key=\"warmup_key_7746_2\"</code></p><h2>\n  \n  \n    Is it due to elevated system CPU? (yes) <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#is-it-due-to-elevated-system-cpu-yes\">#</a></h2><p>Adding on to the evidence pointing away from network problems, we saw a large spike in system CPU usage about four minutes after newly autoscaled hosts were booted. If we waited until after this spike in system CPU to pool the hosts, we experienced no listen overflows. This spike in system CPU only occurred on Ubuntu hosts – CentOS did not have this problem:</p><p>This made us realize that the problems we were seeing were unrelated to pooling the hosts – the spike in system CPU occurred regardless of whether the hosts were pooled. During the spike in system CPU, we saw logs in :</p><p><code>workqueue: inode_switch_wbs_work_fn hogged CPU for &gt;10000us 4 times, consider switching to WQ_UNBOUND</code></p><p>And  showed kernel workers were using lots of CPU in a function :</p><p>We made a  recording demonstrating that when system CPU spikes, the kernel is busy inside :</p><h2>\n  \n  \n    Why did we initially suspect the network? <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#why-did-we-initially-suspect-the-network\">#</a></h2><p>Our logs and profiling tools showed that network operations were executing particularly slowly. However, when CPU is busy, network operations may appear to slow down disproportionately. Network calls require CPU <a href=\"https://en.wikipedia.org/wiki/Context_switch\">context switches</a> (I think?). After a process blocked on network receives a response, the process may spend a long time waiting in the CPU <a href=\"https://en.wikipedia.org/wiki/Run_queue\">run queue</a> before it gets scheduled again when the CPU is busy. Network operations that appear slow at the user space level may be a symptom of CPU busyness.</p><p>Although there can be cases where this is not true, it has been my experience that when the network is the cause of slowness, CPU usage on client hosts is often lower than normal. When the client is blocked waiting for the network, it is often more idle. In retrospect, perhaps the fact that elevated CPU was one of the symptoms we were seeing should have pointed us away from network issues.</p><p>We believed something in the kernel function <a href=\"https://github.com/torvalds/linux/blob/906bd684e4b1e517dd424a354744c5b0aebef8af/fs/fs-writeback.c#L490\"></a> was causing elevated system CPU. This function is in a file <a href=\"https://github.com/torvalds/linux/blob/master/fs/fs-writeback.c\"></a>, which contains functions related to the <a href=\"https://github.com/firmianay/Life-long-Learner/blob/master/linux-kernel-development/chapter-16.md\">writeback functionality of the Linux page cache</a>. We knew that system CPU was elevated about four minutes after new hosts were booted. One of the first things a new host does is download the latest code – this is part of our host bootstrapping process and involves writing thousands of files to disk. We wondered if the process of flushing the dirty pages to disk was causing the elevated system CPU. While we did not see elevated disk write metrics during the system CPU spike, we decided to test this theory. We added a <a href=\"https://www.man7.org/linux/man-pages/man1/sync.1.html\"></a> call after the  command that downloads the code on new hosts. In theory, that should synchronously write the dirty pages in the page cache to disk. Perhaps by controlling when the page cache was flushed to disk, we could control when the spike in system CPU occurred and ensure that it occurred before we pooled the host. This attempt, however, was unsuccessful. We saw no spike in system CPU when calling . Furthermore, we still saw the spike in system CPU a minute or two later.</p><p>We were back to the drawing board. As we mentioned above, one of the first things a new host does is download the latest code. This process is called , and it runs as a <a href=\"https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html\">systemd oneshot</a> service. We found something perplexing: if we prevented the  service from running, we never saw the spike in system CPU. And if we subsequently ran the  service manually, we would see the spike in system CPU a couple minutes later. This implies that something in  was the cause of the issues. However, if we ran the  command in  manually in our interactive shell, we saw no spikes in system CPU. This apparent contradiction led us on a wild goose chase of trying to determine if some systemd service that was dependent on  was the cause of the elevated system CPU, or if there was some subtle difference in the way we were running the commands in our shell versus how systemd was running the commands.</p><h2>\n  \n  \n    Is it inode cgroup switching? (yes) <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#is-it-inode-cgroup-switching-yes\">#</a></h2><p>Each of the thousands of files that is written by the  service has a corresponding <a href=\"https://www.kernel.org/doc/html/latest/filesystems/ext4/inodes.html\">index node</a>, also known as an inode, that the kernel uses to store file metadata. <a href=\"https://www.kernel.org/doc/html/v5.0/admin-guide/cgroup-v2.html#control-group-v2\">Control groups</a>, also known as cgroups, are a feature of Linux which allows for setting per process limits on system resources. For example, cgroups allow us to limit a given process from consuming too much memory, disk I/O, network bandwidth, etc. Every process belongs to a cgroup. Cgroups form a tree-like hierarchical structure. Processes in a given cgroup are given limits both by the cgroup to which they belong and that cgroup’s parents.</p><p>In the context of cgroups, page cache writeback is tracked at the inode level. A given inode is assigned to whichever cgroup contains the process that is responsible for the majority of writes to the inode’s file. If a new process starts writing a lot to a file, the file’s inode may switch to the new process’s cgroup. Likewise, if a process managed by systemd is terminated, systemd will <a href=\"https://systemd.io/CGROUP_DELEGATION/#controller-support\">remove the process’s cgroup</a>, at which point any inodes assigned to the process’s cgroup will be moved to the parent cgroup.</p><p>The <a href=\"https://github.com/torvalds/linux/blob/906bd684e4b1e517dd424a354744c5b0aebef8af/fs/fs-writeback.c#L490\"></a> that we believed was causing the elevated system CPU is responsible for moving an inode from one cgroup to another in the context of writeback. We got more insight by running this <a href=\"https://github.com/bpftrace/bpftrace\"></a> command on a newly booted host:</p><div><div><pre><code>% sudo bpftrace -e '\ntracepoint:writeback:inode_switch_wbs {\n  printf(\n    \"[%s] inode is switching! inode: %d old cgroup: %d new cgroup: %d\\n\",\n    strftime(\"%H:%M:%S\", nsecs),\n    args-&gt;ino,\n    args-&gt;old_cgroup_ino,\n    args-&gt;new_cgroup_ino\n  );\n}'\n</code></pre></div></div><p>During the spike in system CPU, we saw thousands of these lines printed out by the  command:</p><div><div><pre><code>[20:49:30] inode is switching! inode: 3730800 old cgroup: 22438 new cgroup: 88\n[20:49:30] inode is switching! inode: 3730799 old cgroup: 22438 new cgroup: 88\n...\n\n</code></pre></div></div><p>Each line corresponds to a file written by  that was switching from ’s dying cgroup to the parent cgroup. The old cgroup identifier ( corresponds to ’s cgroup. The new cgroup identifier () corresponds to the parent cgroup.</p><p>The  command prints out data from a <a href=\"https://github.com/torvalds/linux/blob/906bd684e4b1e517dd424a354744c5b0aebef8af/fs/fs-writeback.c#L415\">kernel tracepoint</a> in the Linux kernel’s <a href=\"https://github.com/torvalds/linux/blob/906bd684e4b1e517dd424a354744c5b0aebef8af/include/trace/events/writeback.h#L216-L243\">writeback code</a>. The fields available to print in this tracepoint can be viewed via:</p><div><div><pre><code>% sudo cat /sys/kernel/debug/tracing/events/writeback/inode_switch_wbs/format\nname: inode_switch_wbs\nID: 886\nformat:\n    field:unsigned short common_type;   offset:0;   size:2; signed:0;\n    field:unsigned char common_flags;   offset:2;   size:1; signed:0;\n    field:unsigned char common_preempt_count;   offset:3;   size:1; signed:0;\n    field:int common_pid;   offset:4;   size:4; signed:1;\n\n    field:char name[32];    offset:8;   size:32;    signed:0;\n    field:ino_t ino;    offset:40;  size:8; signed:0;\n    field:ino_t old_cgroup_ino; offset:48;  size:8; signed:0;\n    field:ino_t new_cgroup_ino; offset:56;  size:8; signed:0;\n\nprint fmt: \"bdi %s: ino=%lu old_cgroup_ino=%lu new_cgroup_ino=%lu\", REC-&gt;name, (unsigned long)REC-&gt;ino, (unsigned long)REC-&gt;old_cgroup_ino, (unsigned long)REC-&gt;new_cgroup_ino\n</code></pre></div></div><p>We found that when we added a  to the end of the script that  executes, we could delay the spike in system CPU by one hour. Because systemd only removes a service’s cgroup when its process exits, the  delayed when inodes switched from one cgroup to another.</p><p>We found a systemd directive that allows us to turn off certain cgroup accounting features: <a href=\"https://www.freedesktop.org/software/systemd/man/latest/systemd.resource-control.html#Control%20Group%20Management\"></a>. If either the  or  controllers are disabled, the kernel <a href=\"https://github.com/torvalds/linux/blob/c291c9cfd76a8fb92ef3d66567e507009236ce90/include/linux/backing-dev.h#L172\">will not</a> perform cgroup writeback or any of the related accounting and cgroup switching. We found that by creating a systemd <a href=\"https://gist.github.com/dasl-/87b849625846aed17f1e4841b04ecc84#file-dasl-slice-L5\">slice with those controllers disabled</a> and configuring ’s unit file to <a href=\"https://gist.github.com/dasl-/06ced03d4b905fd79d8d58283ecaf67d#file-setupwebroot-service-L7\">use that slice</a>, we could no longer reproduce the elevated system CPU. We had solved our performance issue.</p><p>No more system CPU spike and no more listen overflows:\n<img src=\"https://dasl.cc/assets/posts/2025-01-01-debugging-our-new-linux-kernel/i8.png\" alt=\"system CPU graph\"></p><h2>\n  \n  \n    Do we have a minimal reproduction script? <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#do-we-have-a-minimal-reproduction-script\">#</a></h2><p>We came up with a minimal reproduction of the issue:</p><p><code>$ sudo mkdir -p /var/random-files &amp;&amp; sudo systemd-run --property=Type=oneshot bash -c 'dd if=/dev/urandom bs=1024 count=400000 | split -a 16 -b 1k - /var/random-files/file.'</code></p><p>This command creates 400,000 files, each consisting of 1,024 random bytes. The files have names like <code>/var/random-files/file.aaaaaaaaaaaaaaaa</code> and <code>/var/random-files/file.aaaaaaaaaaaaaaab</code>. This command is run as a systemd oneshot service. Within anywhere from 30 seconds to 3 minutes after this command finishes, we see a spike in system CPU. Viewing  will confirm this (press  to show kernel threads in ) – we see kernel workers using lots of CPU in the function .</p><h2>\n  \n  \n    Why wasn’t CentOS affected? <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#why-wasnt-centos-affected\">#</a></h2><p>The initial release of <a href=\"https://man7.org/linux/man-pages/man7/cgroups.7.html\">cgroups</a>, known as cgroups v1, was in kernel version 2.6.24. Cgroups v1 has since been replaced by a new implementation: cgroups v2. Cgroups v2 was officially released in kernel version 4.5. Our old CentOS operating system used kernel version 3.10. We believe this inode switching CPU issue is related to the  and  controllers introduced in cgroups v2. Thus CentOS, which uses cgroups v1, is not vulnerable to this issue.</p><h2>\n  \n  \n    Why couldn’t we reproduce when running rsync manually? <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#why-couldnt-we-reproduce-when-running-rsync-manually\">#</a></h2><p><a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#is-it-writeback-no\">Recall</a> that when we ran the  command from  manually in our interactive shell, we saw no spike in system CPU. It turns out that each interactive ssh session you have open creates its own cgroup. Below is the output of  on a web server on which I have two interactive ssh sessions open. One session is running a  command, and the other session is running . The two cgroups are called  and :</p><pre>% sudo systemd-cgls\nControl group /:\n-.slice\n├─user.slice\n│ └─user-10101.slice\n│   ├─user@10101.service …\n│   │ └─init.scope\n│   │   ├─1710746 /lib/systemd/systemd --user\n│   │   └─1710793 (sd-pam)\n│   ├─\n│   │ ├─1708943 sshd: dleibovic [priv]\n│   │ ├─1711073 sshd: dleibovic@pts/0\n│   │ ├─1711171 -zsh\n│   │ └─1716022 sleep 100\n│   └─\n│     ├─1780667 sshd: dleibovic [priv]\n│     ├─1781414 sshd: dleibovic@pts/1\n│     ├─1781577 -zsh\n│     └─1791367 systemd-cgls\n...\n</pre><p>These  cgroups stick around until you terminate your ssh session. After terminating your ssh session, systemd removes the corresponding cgroup. With this insight, we tested terminating the interactive ssh session after manually running the  commands. Sure enough, about 2 minutes after we terminated the ssh session, we saw the big spikes in system CPU caused by inode cgroup switching.</p><p>We suspected that this performance issue was caused by either the hypervisor or the kernel. We shared our findings with Canonical, the company behind Ubuntu. Canonical confirmed that it is a kernel issue that was likely introduced by a Linux kernel commit from 2021. More details are available in the <a href=\"https://bugs.launchpad.net/ubuntu/+source/linux-oem-6.5/+bug/2038492\">public bug report</a>, in which I have commented. We are hopeful that Canonical will engage with the Linux kernel developers and eventually fix this performance issue.</p>","contentLength":13399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43046174"},{"title":"GitHub - gopher-fleece/gleece: Gleece - bringing joy and ease to API development in Go!","url":"https://github.com/gopher-fleece/gleece","date":1739521286,"author":"/u/h_talker","guid":610,"unread":true,"content":"<p>I come from the Node.js / TypeScript ecosystem and have recently started working with Go as the technology for high-performance and CPU-intensive microservices.</p><p>I was missing the <a href=\"https://github.com/lukeautry/tsoa\">TSOA</a> approach and code styling for REST API implementation, meaning writing ordinary functions and declaring HTTP info, where the framework handles the rest - routing, validation, documentation, authentication, etc.</p><p>So... I have created, with my colleague <a href=\"https://github.com/yuval-po\">Yuval</a>, the Gleece project that does exactly that.</p><p>Since Go doesn't provide an annotation mechanism (as TSOA uses in JS), we used comments for HTTP info. To make it easier to work with, we also created a <a href=\"https://marketplace.visualstudio.com/items?itemName=haim-kastner.gleece-extension\">VS Code extension</a> to highlight and provide visibility to the HTTP info.</p><p>Feel free to use it, and I would love any feedback 🙂</p>","contentLength":761,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ip605y/github_gopherfleecegleece_gleece_bringing_joy_and/"},{"title":"Advancing Open Source Gateways with kgateway","url":"https://www.cncf.io/blog/2025/02/05/advancing-open-source-gateways-with-kgateway/","date":1739520627,"author":"/u/dshurupov","guid":612,"unread":true,"content":"<div><p>Posted on February 5, 2025\n\t\t</p></div><p>At KubeCon NA 2024, <a href=\"https://www.youtube.com/watch?v=psZi_T1np4U\">Solo.io announced its intention</a> to donate the <a href=\"https://github.com/solo-io/gloo\">Gloo Gateway open source project</a> to the CNCF, to benefit the broader cloud native ecosystem. In case you are not familiar with Gloo Gateway, it is the most mature and widely deployed Envoy-based gateway in the market today. Built on open source and open standards, Gloo Gateway is Kubernetes-native and implements the Kubernetes Gateway API. Today, we’d like to update you on some exciting developments since the announcement.</p><h2>What is a gateway, and why donate Gloo Gateway to CNCF?</h2><p>If you are using Kubernetes, you most likely need a gateway to control traffic going into or out of your cluster. Gateways are a critical component of the Kubernetes ecosystem, which is why the Kubernetes community collectively developed the <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API</a> based on lessons learned from the Ingress API and Istio. With the growth of AI workloads and the rising need for traffic and cost control for calling LLM providers, gateways play an even more critical role in controlling and securing all-direction traffic—north-south, east-west, or inter-service communication.</p><p>While Gloo Gateway is the most mature and feature-rich Envoy-based gateway today, we believe that under vendor-neutral governance, the project’s adoption and ecosystem integration will reach new heights. Donating the project to the CNCF will expand the contributor base, foster innovation across organizations, and provide a battle-tested, feature-rich, vendor-neutral gateway project to the diverse global CNCF user community.</p><p>In November 2024, we moved the Gloo open source repository to the k8sgateway repository as a preparatory step for the donation. After working with the CNCF TOC, Kubernetes SIG-Network, and steering committee leaders, we renamed the project to <a href=\"https://kgateway.dev/\">kgateway</a>.</p><p>Given Gloo’s large adopter base, we believe kgateway qualifies as a CNCF incubation project. Due to the lengthy due diligence process for incubation projects, we decided to <a href=\"https://github.com/cncf/sandbox/issues/319\">donate it as a CNCF sandbox project</a> instead. We look forward to working with the CNCF TOC and TAG Network leaders for the upcoming sandbox review.</p><p>Working closely with the maintainer community, we established governance for the project that rewards maintainership while ensuring no single company has a controlling stake. Using the git-vote bot for transparency, we successfully held our first governance vote. Out of 10 eligible voters (including 4 maintainers outside Solo.io), nine voted favorably on the <a href=\"https://github.com/kgateway-dev/community/pull/19\">proposed governance PR</a>:</p><p>In addition to renaming the project, recruiting maintainers, and establishing governance, we’ve been focused on:</p><ol><li> Developing buildable, vendor-neutral artifacts, set to launch in the coming weeks.</li><li><strong>Improved Development Velocity:</strong> Establishing robust pipeline checks for PRs, including linting, Kubernetes Gateway API conformance tests, and end-to-end testing.</li><li> Ensuring the project remains highly extendable, aligning with core design principles of kgateway and Envoy.</li></ol><p>A shout-out to our core maintainers and contributors for laying this solid foundation for innovation.</p><p>You may notice that the Gloo open source repository still exists. This is temporary during the transition period. With Gloo’s large open source user base, we understand that migrating from Gloo to kgateway takes time. In upcoming releases, we plan to deprecate the Gloo repository to focus all open-source efforts on kgateway.</p><p>We aim to make kgateway the most popular gateway for all-direction traffic—north-south, east-west, or inter-service communication. In addition to implementing the latest Kubernetes Gateway API features, we’re prioritizing:</p><h3>Traffic control for AI workloads</h3><p>As workloads like AI agents run on Kubernetes clusters, questions arise:</p><ul><li>How securely do they connect to LLM services such as OpenAI or Gemini?</li><li>Are these services local or external, with usage-based costs?</li><li>How should credentials and backup LLMs be managed?</li><li>Do you want each developer to develop prompt guard and enrichment in their own AI workloads?&nbsp;</li></ul><p>Kgateway simplifies these challenges with two proposed declarative APIs for routing traffic to LLM providers while applying advanced policies such as secret management, backup LLMs, prompt guard or enrichment, and more. Refer to <a href=\"https://github.com/kgateway-dev/kgateway/pull/10495/files\">the enhancement proposal</a> for more information.</p><p>The <a href=\"https://gateway-api-inference-extension.sigs.k8s.io/\">Gateway API Inference extension</a>, sponsored by Kubernetes SIG-Network, focuses on extending the Kubernetes Gateway API with inference-specific routing extensions. It introduces the concept of an “InferencePool” (composed of one or more inference pods), enabling application developers to effectively route requests based on AI workload requirements. Daneyon Hanson has been leading our <a href=\"https://github.com/kgateway-dev/kgateway/pull/10420\">work</a> for interference extensions and we are proud to see him nominated as a maintainer on the Gateway API Inference extension project as a result!</p><h3>Providing advanced Layer 7 features for Istio in ambient mode</h3><p>Ambient mode splits Istio functionality into a secure overlay layer (ztunnels) and a Layer 7 processing layer (waypoint). With kgateway as the waypoint proxy, users gain advanced L7 features such as request transformation, retries, and traffic control for AI workloads connecting to LLM services. This pluggability ensures consistent operational experiences for north-south and inter-service traffic. Steven Landow, who is a maintainer on both Istio and kgateway, is leading <a href=\"https://github.com/kgateway-dev/kgateway/issues/10453\">this effort</a>.</p><p>To explore additional roadmap initiatives or propose updates, please refer to our <a href=\"https://github.com/kgateway-dev/community/blob/main/ROADMAP.md\">roadmap document</a>.</p><p>Are you interested in exploring kgateway, or are you already a Gloo Gateway user? We’d love to hear from you and shape the future of kgateway together. If working on cutting-edge cloud-native projects excites you, join us as a contributor! Connect with us via:</p><p>Let’s work together and build kgateway into the future of cloud connectivity!</p>","contentLength":5832,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1ip5uvx/advancing_open_source_gateways_with_kgateway/"},{"title":"Anyone can push updates to the doge.gov website","url":"https://www.404media.co/anyone-can-push-updates-to-the-doge-gov-website-2/","date":1739518306,"author":"mahkeiro","guid":211,"unread":true,"content":"<div>\"THESE 'EXPERTS' LEFT THEIR DATABASE OPEN.\"</div>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045835"},{"title":"Benchmarking vision-language models on OCR in dynamic video environments","url":"https://arxiv.org/abs/2502.06445","date":1739517976,"author":"ashu_trv","guid":210,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045801"},{"title":"Trait upcasting stabilized in 1.86","url":"https://github.com/rust-lang/rust/pull/134367","date":1739517052,"author":"/u/hpxvzhjfgb","guid":635,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ip51qt/trait_upcasting_stabilized_in_186/"},{"title":"Extensible WASM Applications with Go","url":"https://go.dev/blog/wasmexport","date":1739516933,"author":"ingve","guid":209,"unread":true,"content":"<p>\n      Cherry Mui\n      13 February 2025\n      </p><p>Go 1.24 enhances its WebAssembly (Wasm) capabilities with the\naddition of the  directive and the ability to build a reactor\nfor WebAssembly System Interface (WASI).\nThese features enable Go developers to export Go functions to Wasm,\nfacilitating better integration with Wasm hosts and expanding the possibilities\nfor Go-based Wasm applications.</p><h2>WebAssembly and the WebAssembly System Interface</h2><p><a href=\"https://webassembly.org/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly (Wasm)</a> is a binary instruction format\nthat was initially created for web browsers, providing the execution of\nhigh-performance, low-level code at speeds approaching native performance.\nSince then, Wasm’s utility has expanded, and it is now used in various\nenvironments beyond the browser.\nNotably, cloud providers offer services that directly execute Wasm\nexecutables, taking advantage of the\n<a href=\"https://wasi.dev/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly System Interface (WASI)</a> system call API.\nWASI allows these executables to interact with system resources.</p><p>Go first added support for compiling to Wasm in the 1.11 release, through the\n port.\nGo 1.21 added a new port targeting the WASI preview 1 syscall API through the\nnew  port.</p><h2>Exporting Go Functions to Wasm with </h2><p>Go 1.24 introduces a new compiler directive, , which allows\ndevelopers to export Go functions to be called from outside of the\nWasm module, typically from a host application that runs the Wasm runtime.\nThis directive instructs the compiler to make the annotated function available\nas a Wasm <a href=\"https://webassembly.github.io/spec/core/valid/modules.html?highlight=export#exports\" rel=\"noreferrer\" target=\"_blank\">export</a>\nin the resulting Wasm binary.</p><p>To use the  directive, simply add it to a function definition:</p><pre><code>//go:wasmexport add\nfunc add(a, b int32) int32 { return a + b }\n</code></pre><p>With this, the Wasm module will have an exported function named  that\ncan be called from the host.</p><p>This is analogous to the <a href=\"https://go.dev/cmd/cgo#hdr-C_references_to_Go\">cgo  directive</a>,\nwhich makes the function available to be called from C,\nthough  uses a different, simpler mechanism.</p><p>A WASI reactor is a WebAssembly module that operates continuously, and\ncan be called upon multiple times to react on events or requests.\nUnlike a “command” module, which terminates after its main function finishes,\na reactor instance remains live after initialization, and its exports remain\naccessible.</p><p>With Go 1.24, one can build a WASI reactor with the  build\nflag.</p><pre><code>$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm\n</code></pre><p>The build flag signals to the linker not to generate the  function\n(the entry point for a command module), and instead generate an\n function, which performs runtime and package initialization,\nalong with any exported functions and their dependencies.\nThe  function must be called before any other exported functions.\nThe  function will not be automatically invoked.</p><p>To use a WASI reactor, the host application first initializes it by calling\n, then simply invoke the exported functions.\nHere is an example using <a href=\"https://wazero.io/\" rel=\"noreferrer\" target=\"_blank\">Wazero</a>, a Go-based Wasm runtime\nimplementation:</p><pre><code>// Create a Wasm runtime, set up WASI.\nr := wazero.NewRuntime(ctx)\ndefer r.Close(ctx)\nwasi_snapshot_preview1.MustInstantiate(ctx, r)\n\n// Configure the module to initialize the reactor.\nconfig := wazero.NewModuleConfig().WithStartFunctions(\"_initialize\")\n\n// Instantiate the module.\nwasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)\n\n// Call the exported function.\nfn := wasmModule.ExportedFunction(\"add\")\nvar a, b int32 = 1, 2\nres, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))\nc := api.DecodeI32(res[0])\nfmt.Printf(\"add(%d, %d) = %d\\n\", a, b, c)\n\n// The instance is still alive. We can call the function again.\nres, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))\nfmt.Printf(\"add(%d, %d) = %d\\n\", b, c, api.DecodeI32(res[0]))\n</code></pre><p>The  directive and the reactor build mode allow applications to\nbe extended by calling into Go-based Wasm code.\nThis is particularly valuable for applications that have adopted Wasm as a\nplugin or extension mechanism with well-defined interfaces.\nBy exporting Go functions, applications can leverage the Go Wasm modules to\nprovide functionality without needing to recompile the entire application.\nFurthermore, building as a reactor ensures that the exported functions can be\ncalled multiple times without requiring reinitialization, making it suitable\nfor long-running applications or services.</p><h2>Supporting rich types between the host and the client</h2><p>Go 1.24 also relaxes the constraints on types that can be used as input and\nresult parameters with  functions.\nFor example, one can pass a bool, a string, a pointer to an , or a\npointer to a struct which embeds  and contains supported\nfield types\n(see the <a href=\"https://go.dev/cmd/compile#hdr-WebAssembly_Directives\">documentation</a> for detail).\nThis allows Go Wasm applications to be written in a more natural and ergonomic\nway, and removes some unnecessary type conversions.</p><p>While Go 1.24 has made significant enhancements to its Wasm capabilities,\nthere are still some notable limitations.</p><p>Wasm is a single-threaded architecture with no parallelism.\nA  function can spawn new goroutines.\nBut if a function creates a background goroutine, it will not continue\nexecuting when the  function returns, until calling back into\nthe Go-based Wasm module.</p><p>While some type restrictions have been relaxed in Go 1.24, there are still\nlimitations on the types that can be used with  and\n functions.\nDue to the unfortunate mismatch between the 64-bit architecture of the client\nand the 32-bit architecture of the host, it is not possible to pass pointers in\nmemory.\nFor example, a  function cannot take a pointer to a struct that\ncontains a pointer-typed field.</p><p>The addition of the ability to build a WASI reactor and export Go functions to\nWasm in Go 1.24 represent a significant step forward for Go’s WebAssembly\ncapabilities.\nThese features empower developers to create more versatile and powerful Go-based\nWasm applications, opening up new possibilities for Go in the Wasm ecosystem.</p>","contentLength":5777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045698"},{"title":"Zed now predicts your next edit with Zeta, our new open model","url":"https://zed.dev/blog/edit-prediction","date":1739515847,"author":"ahamez","guid":208,"unread":true,"content":"<p>Zed is built for speed. We've always strived for an editing experience that feels . But what's faster than instant?  A tool that anticipates your next move. That's why we're introducing <a href=\"https://zed.dev/edit-prediction\">edit prediction</a> in Zed, powered by <a href=\"https://huggingface.co/zed-industries/zeta\">Zeta</a>, our new open source model.</p><p>Here's a quick walkthrough:</p><div><figure><figcaption>Edit Prediction in action.</figcaption></figure></div><p>As you work, Zed now predicts your next edit, so you can apply it just by hitting . Once you accept a prediction, you can perform multiple follow-up edits by pressing  repeatedly, saving you time and keystrokes. We've received  of requests for this functionality, and we've poured our hearts into making it feel like a natural extension of the Zed experience.</p><p>You can use Zeta for free during this public beta by <a href=\"https://zed.dev/download\">downloading Zed</a> and signing in with your GitHub account. Edit prediction won't be free forever, but right now we're just excited to share and learn.</p><p>Edit prediction transforms  into a magical, universal key. But what about the existing uses of , such as indenting lines? And what happens when there's both an edit prediction  suggestions from your language server? We didn't want a powerful new feature to come at the expense of the existing editing experience in Zed.</p><p>When language server completions are visible, Zed won't preview the predicted edit until you press  or . As soon as you press the modifier, Zed previews the edit and hides the menu to enable an unobstructed review. On macOS, you can just hit  to confirm, or back out by releasing  to restore the language server completions menu.</p><p>On Linux,  is often reserved by the window manager, so we offer  as an alternative default. We chose  because it's on the QWERTY home row and represents rightward movement in Vim. If your Linux window manager doesn't claim , you're free to use that binding as well.</p><p><a href=\"https://huggingface.co/zed-industries/zeta\">Zeta</a> is derived from Qwen2.5-Coder-7B, and is fully open source, including <a href=\"https://huggingface.co/datasets/zed-industries/zeta\">an open dataset</a>. If you're working in an open source repository, we'd love your help improving Zeta by contributing to its dataset. Please bear with us initially, as we will be reviewing the submitted data before publishing to ensure everyone's safety and privacy. We're excited to figure this out and see a community effort form to make edit prediction better everywhere, most especially in Zed!</p><div><div><div><p>How Zed's Open-Source Edit Predictions Work</p></div><p>Richard Feldman and Antonio Scandurra talk about how Zed's new Edit Prediction feature works under the hood. This includes how the Zed team developed and open-sourced both the code and the dataset behind the fine-tuned Zeta language model that powers it!</p></div></div><p>Most coding models are trained on a \"fill in the middle\" task. You give them a prefix and a suffix, and they generate what goes in between.</p><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rs\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><p>This works for completing text at the cursor, but we wanted Zeta to predict edits at arbitrary locations, which doesn't fit into this structure.</p><p>In our experience, models aren't very good at producing granular edits, but they do excel at rewriting larger chunks of code. So that's where we started: given a list of recent edits and the cursor position, we asked the model to rewrite a snippet of text around the cursor, incorporating one or more edit predictions in the rewritten text.</p><p>Before writing a single line of code, we created a set of tests to check if our idea worked. Testing the output of a large language model is tricky because, on every run, you can get slightly different results even when feeding it the exact same input. This can be mitigated by using a temperature of  and, for providers that support it, providing a seed for the RNG.</p><p>That said, code can often be written in many different but equally valid ways. So even when Zeta's output differs from our expected answer, it might still be doing exactly what we want—just taking a different path to get there. This makes traditional unit testing approaches particularly challenging when working with LLMs.</p><p>This led us to take a different approach—instead of strict assertions, we used a larger LLM to evaluate Zeta's edits. By writing our test assertions in plain English and having Claude check if the results matched our intent, we could validate that Zeta was making sensible edits, even when its exact output differed between runs. This ended up being much more practical than trying to make brittle assertions about specific tokens.</p><p>Here's an example taken from our eval suite:</p><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rs\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><p>We took our first stab at making those tests pass by using Qwen2.5-Coder-32B and giving it clear instructions for which types of edits we wanted it to predict. <a href=\"https://github.com/zed-industries/zed/blob/79a70b72b3968d102c6171f8bd2738ec7be8e94f/crates/zeta/src/complete_prompt.md\">Here's the initial system prompt</a> we used and you can look through the history to see how we kept changing it to pass the eval suite.</p><p>This worked out surprisingly well for the first 4-5 evals. However, as soon as we introduced more, we started noticing that it got harder and harder to pass them all consistently. Changing the prompt caused the new evals to pass, but made the old ones fail. Overall, it felt like a flaky process and we didn't feel confident this would lead to the system being robust enough to be used in production.</p><p>Moreover, using a 32b model wasn't really compatible with our strict latency requirements (more on that later).</p><p>After playing around with different approaches, we decided to go with supervised fine-tuning using <a href=\"https://unsloth.ai\">Unsloth</a> and LoRA. The idea was to teach Zeta two key things: figuring out what changes a developer might want next based on their recent edits, and then actually applying those changes cleanly to the code without introducing weird side effects.</p><p>But we had a classic chicken-and-egg problem—we needed data to train the model, but we didn't have any real examples yet. So we started by having Claude generate about 50 synthetic examples that we added to <a href=\"https://huggingface.co/datasets/zed-industries/zeta-dataset\">our dataset</a>. We then used that initial fine-tune to ship an early version of Zeta behind a feature flag and started collecting examples from our own team's usage.</p><p>This approach let us quickly build up a solid dataset of around 400 high-quality examples, which improved the model a lot! However, we kept running into edge cases that would trip the model up. The most annoying ones were when Zeta was working with a small piece of code in a larger file—it would sometimes get confused and make random deletions or insertions that had nothing to do with what the user was trying to do, and it didn't seem like adding more examples steered the model away from those mistakes.</p><p>To handle these edge cases, we conducted another pass using direct preference optimization (DPO). This technique let us go beyond simply showing the model what good edits look like—we could also teach it what edits . With DPO, we could fine-tune Zeta by providing both positive and negative examples, helping it learn the subtle differences between helpful and problematic edits.</p><p>We found that just ~150 carefully selected examples were enough to significantly improve Zeta's behavior on tricky cases. Of course, we think we can make it even better by expanding our training data with more diverse examples, and we're excited to keep pushing the boundaries here.</p><p>Like every feature in Zed, latency was a critical factor for edit prediction. When we started, we set aggressive performance targets: predictions should be delivered in under 200ms for the median case (p50) and under 500ms for the 90th percentile (p90). The challenge was that rewriting complete excerpts, while enabling multi-location edits, requires generating significantly more tokens than simple fill-in-middle approaches. Initially, this put us way over our latency budget.</p><p>However, there's a fascinating insight about how edit predictions work. When we rewrite a text snippet, the output often mirrors the input closely, with changes concentrated in specific spots. This pattern lets us parallelize token generation by using the input as a reference—a technique known as speculative decoding. We use n-gram search to identify promising jumping-off points in the input where we can start parallel token generation, giving us a significant speedup without sacrificing quality.</p><p>For edit predictions to feel responsive, we needed to solve multiple latency challenges in parallel. As discussed above, we tackled the model execution time through speculative decoding, but serving the model at scale presented its own set of hurdles. This was by far the most compute-intensive problem our team has ever tackled.</p><p>A few weeks out from launch, we ran a brief competitive process, and we ended up being really impressed with <a href=\"https://www.baseten.co\">Baseten</a>. Their performance engineers quickly optimized our open source model to run on their flexible infrastructure, achieving our target latencies while letting us retain full visibility into the details of the deployment, both for the Zed team and the entire Zed community. We plan to follow up with a guest post about what they learned optimizing <a href=\"https://huggingface.co/zed-industries/zeta\">our model</a>.</p><p>Latency is not just a function of compute; network transit time is a key driver of perceived speed. To cooperate with the laws of physics, we're launching with GPUs in both North America and Europe, and we hope to add more regions soon. We're also using <a href=\"https://workers.cloudflare.com/\">Cloudflare Workers</a> to handle your requests in a data center located as close to you as possible.</p><p>There's plenty more to explore to make edit predictions more powerful. We'll be fast-following with more experiments. We plan on sending more kinds of context to the model and continuing our experiments with fine-tuning, and we'll share updates as we grow and evolve the Zeta dataset.</p><p>We've learned a lot since we launched Zed AI last fall. The world is changing fast, and we're having a blast exploring and learning to build features that developers love. We're also excited to build with AI the Zed way. From our early days, we've been proponents of an open approach to building software, even when hard, and we see no reason to change that approach when it comes to working with AI. We hope you'll join us as a user, a contributor, or an employee, as we hustle to ship a golden future.</p>","contentLength":9947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045606"},{"title":"Type safe Go money library","url":"https://www.reddit.com/r/golang/comments/1ip4nxm/type_safe_go_money_library/","date":1739515437,"author":"/u/HawkSecure4957","guid":609,"unread":true,"content":"<p>Hello, community I have been working on money library that is type safe, it's in beta and haven't been test against production. But I would like to share it with community.</p>","contentLength":172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The New York Stock Exchange plans to launch NYSE Texas","url":"https://ir.theice.com/press/news-details/2025/The-New-York-Stock-Exchange-to-Launch-NYSE-Texas/default.aspx","date":1739515211,"author":"ChrisArchitect","guid":207,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045558"},{"title":"Does X cause Y? An in-depth evidence review (2021)","url":"https://www.cold-takes.com/does-x-cause-y-an-in-depth-evidence-review/","date":1739513659,"author":"l0b0","guid":206,"unread":true,"content":"<p>\nThere's an interesting theory out there that X causes Y. If this were true, it would be pretty important. So I did a deep-dive into the academic literature on whether X causes Y. Here's what I found.\n</p><p>\n(Embarrassingly, I can't actually remember what X and Y are. I think maybe X was enriched preschool, or just school itself, or eating fish while pregnant, or the Paleo diet, or lead exposure, or a clever \"nudge\" policy trying to get people to save more, or some self-help technique, or some micronutrient or public health intervention, or democracy, or free trade, or some approach to intellectual property law. And Y was ... lifetime earnings, or risk of ADHD diagnosis, or IQ in adulthood, or weight loss, or violent crime, or peaceful foreign policy, or GDP per capita, or innovation. Sorry about that! Hope you enjoy the post anyway! Fortunately, I think <strong>what I'm about to write is correct for pretty much any (X,Y) from those sorts of lists.</strong>)\n</p><ul><li>There are hundreds of studies on whether X causes Y, but most of them are simple observational studies that are just essentially saying \"People/countries with more X also have more Y.\" For reasons discussed below, we can't really learn much from these studies.\n\n</li><li>There are 1-5 more interesting studies on whether X causes Y. Each study looks really clever, informative and rigorous at first glance. However, the more closely you look at them, the more confusing the picture gets.\n\n</li><li>We ultimately need to choose between (a) believing some overly complicated theory of the relationship between X and Y, which reconciles all of the wildly conflicting and often implausible things we're seeing in the studies; (b) more-or-less reverting to what we would've guessed about the relationship between X and Y in the absence of any research.\n</li></ul><h2>The chaff: lots of unhelpful studies that I'm disregarding</h2><p>\nFirst, the good news: there are hundreds of studies on whether X causes Y. The bad news? We need to throw most of them out. \n</p><p>\nMany have comically small sample sizes (like studying 20 people) and/or comically short time horizons (like looking at weight loss over two weeks), or unhelpful outcome measures (like intelligence tests in children under 5). But by far the most common problem is that <strong>most of the studies on whether X causes Y are simple observational studies: they essentially just find that people/countries with more X also have more Y. </strong></p><p>\nWhy is this a problem? There could be a  - some third thing, Z, that is correlated with both X and Y. And there are specific reasons we should expect confounders to be common:\n</p><ul><li>In general, people/countries that have more X also have more of lots of other helpful things - they're richer, they're more educated, etc. For example, if we're asking whether higher-quality schooling leads to higher earnings down the line, an issue is that people with higher-quality schooling also tend to come from better-off families with lots of other advantages.\n\n</li><li>In fact, the <em>very fact that people in upper-class intellectual circles think X causes Y</em> means that richer, more educated people/countries tend to deliberately get more X, and also try to do a lot of other things to get more Y. For example, more educated families tend to eat more fish (complicating the attempt to see whether eating fish in pregnancy is good for the baby).</li></ul><p>\nNow, a lot of these studies try to \"control for\" the problem I just stated - they say things like \"We examined the effect of X and Y, while controlling for Z [e.g., how wealthy or educated the people/countries/whatever are].\" How do they do this? The short answer is, well, hm, jeez. Well you see, to simplify matters a bit, just try to imagine ... uh ... shit. Uh. The only high-level way I can put this is:\n</p><ul><li>They use a technique called <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">regression analysis</a> that, as far as I can determine, cannot be explained in a simple, intuitive way (especially not in terms of how it \"controls for\" confounders).\n\n</li><li>The \"controlling for\" thing relies on a lot of subtle assumptions and can break in all kinds of weird ways. <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152719\">Here's </a>a technical explanation of some of the pitfalls; <a href=\"https://www.cold-takes.com/phil-birnbaums-regression-analysis/\">here's</a> a set of deconstructions of regressions that break in weird ways.\n\n</li><li>None of the observational studies about whether X causes Y discuss the pitfalls of \"controlling for\" things and whether they apply here.\n</li></ul><p>\nI don't think we can trust these papers, and to really pick them all apart (given how many there are) would take too much time. So let's focus on a smaller number of better studies.\n</p><h2>The wheat: 1-5 more interesting studies</h2><p>\nDigging through the sea of unhelpful studies, I found 1-5 of them that are actually really interesting! \n</p><p>\nBut on closer inspection, I have a bunch of reservations. For example:\n</p><ul><li>The paper doesn't make it easy to replicate its analysis, and when someone does manage to sort-of replicate it, they may <a href=\"https://blog.givewell.org/2017/12/07/questioning-evidence-hookworm-eradication-american-south/\">get different results</a>. \n\n</li><li>There was other weird stuff going on (e.g., changes in census data collection methods), during the strange historical event, so it's a little hard to generalize.\n\n</li><li>In a response to the study, another academic advances a complex theory of how the study could actually have gotten a misleading result. This led to an intense back-and-forth between the original authors and the skeptic, stretched out over years because each response had to be published in a journal, and by the time I got to the end of it I didn't have any idea what to think anymore.</li></ul><p>\nI found 0-4 other interesting studies. I can't remember all of the details, but they may have included:\n</p><p>\nCompounding the problem, the 1-5 studies I found tell very different stories about the relationship between X and Y. How could this make sense? Is there a unified theory that can reconcile all the results?\n</p><p>\nWell, one possibility is that X causes Y sometimes, but only under very particular conditions, and the effect can be masked by some other thing going on. So - if you meet one of 7 criteria, you should do X to get more Y, but if you meet one of 9 other criteria, you should actually avoid X!\n</p><p>\nI have to say, this all was simultaneously more fascinating and less  than I expected it would be going in. I thought I would find some nice studies about the relationship between X and Y and be done. Instead, I've learned a ton about weird historical events and about the ins and outs of different measures of X and Y, but I feel just super confused about whether X causes Y.\n</p><p>\nI guess my bottom line is that X does cause Y, because it intuitively seems like it would.\n</p><p>\nI'm glad I did all this research, though. It's good to know that social science research can go haywire in all kinds of strange ways. And it's good to know that despite the confident proclamations of pro- and anti-X people, it's legitimately just super unclear whether X causes Y. \n</p><p>\nI mean, how else could I have learned that?\n</p><h2>Appendix: based on a true story</h2><p>\nThis piece was inspired by:\n</p><p>\nThe basic patterns above apply to most of these, and the bottom line usually has the kind of frustrating ambiguity seen in this conclusion.</p><p>There are cases where things seem a bit less ambiguous and the bottom line seems clearer. Speaking broadly, I think the main things that contribute to this are:\n</p><ul><li>Actual randomization. For years I've nodded along when people say \"You shouldn't be dogmatic about randomization, there are many ways for a study to be informative,\" but each year I've become a bit more dogmatic. Even the most sophisticated-, appealing-seeming alternatives to randomization in studies seem to have a way of falling apart. Randomized studies almost always have problems and drawbacks too. But I’d rather have a randomized study with drawbacks than a non-randomized study with drawbacks.\n\n</li><li>Extreme thoroughness, such as Roodman's attempt to reconstruct the data and code for key studies in <a href=\"https://www.openphilanthropy.org/blog/reasonable-doubt-new-look-whether-prison-growth-cuts-crime\">Reasonable Doubt</a>. This sometimes leads to outright dismissing a number of studies, leaving a smaller, more consistent set remaining.\n</li></ul>","contentLength":7882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045406"},{"title":"An argument for how Kubernetes can be use in development and reduce overall system complexity.","url":"https://youtu.be/EJrw3z7m5iQ?si=D4BhYdkGryByoIVj","date":1739512799,"author":"/u/purton_i","guid":616,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1ip4045/an_argument_for_how_kubernetes_can_be_use_in/"},{"title":"ChatLoopBackOff Episode 46 (Dragonfly)","url":"https://www.youtube.com/watch?v=gd6HRgr8KcA","date":1739512616,"author":"CNCF [Cloud Native Computing Foundation]","guid":389,"unread":true,"content":"<article>Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. \n\nThis CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.</article>","contentLength":576,"flags":null,"enclosureUrl":"https://www.youtube.com/v/gd6HRgr8KcA?version=3","enclosureMime":"","commentsUrl":null},{"title":"What is some software that surprise you not not used more","url":"https://www.reddit.com/r/linux/comments/1ip3pn0/what_is_some_software_that_surprise_you_not_not/","date":1739511638,"author":"/u/Shiroegalleu","guid":619,"unread":true,"content":"<p>About a year ago I asked a similar question before I deleted my old account. </p><p>I learned about quite a few programs and learned about a feature in one that I was already using that replaced a different program.</p><p><a href=\"https://github.com/Genymobile/scrcpy\">Scrcpy</a> - An amazing tool that you can use to capture your phone and it's camera</p><p><a href=\"https://jellyfin.org/\">Jellyfin</a> - An open source media server. There's clients for it on Roku, web os , mobile and more.</p><p><a href=\"https://github.com/Vencord/Vesktop\">Vesktop</a> - A Discord client that just works on wayland. also has plugin and theme support like better discord</p><p><a href=\"https://freetubeapp.io/\">Freetube</a> - AN open source private focus YouTube client.</p><p>You most likely have heard of all of these but I rarely see them being used</p>","contentLength":622,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cloudflare Cli called flarectl is no longer supported it seems","url":"https://www.reddit.com/r/golang/comments/1ip19zv/cloudflare_cli_called_flarectl_is_no_longer/","date":1739503178,"author":"/u/gedw99","guid":607,"unread":true,"content":"<p>Flarectl is really valuable for working with Cloudflare, but its dead these days.</p><p>For those with slow meat sticks :</p><p> isn't a part of the &gt; v1 libraries, it only exists on the v0 branch which is now out of active development. you'll need to use the  branch if you want to keep building .</p><p>we don't offer a CLI tool today that is auto generated (like the libraries). it is on the roadmap but no dates sorry.</p><p>\" v4 is the latest and greatest PKG for Cloudflare.</p><p>v2 was released in april 2024, then v3 in September, then v4 a month ago.</p><p>I am reaching out to the community to see if anyone is maintaining a CLI that uses v4...</p><p>Heaps of forks, but hard work to go through and find one :)</p>","contentLength":672,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Job roles related to Kubernetes/OpenShift","url":"https://www.reddit.com/r/kubernetes/comments/1ip0j60/job_roles_related_to_kubernetesopenshift/","date":1739500784,"author":"/u/UCONN_throwaway_99","guid":615,"unread":true,"content":"<p>I was given the opportunity to do a POC for my team to migrate our app onto containers, and we support OpenShift. I really enjoyed the migration part of it and learning about OpenShift/containerization. Would anyone know what kind of job role I should be searching for related to this work?</p>","contentLength":290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating a chrome extension with rust + leptos -> wasm","url":"https://iism.org/article/ride-the-lightning-the-art-of-creative-motivation-63","date":1739497248,"author":"/u/grok-battle","guid":632,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iozf6h/creating_a_chrome_extension_with_rust_leptos_wasm/"},{"title":"Installing Golang for non-sudo users on Ubuntu","url":"https://www.reddit.com/r/golang/comments/1ioz4pq/installing_golang_for_nonsudo_users_on_ubuntu/","date":1739496337,"author":"/u/Mycroft2046","guid":606,"unread":true,"content":"<p>I am trying to install Golang for non-sudo user. The official instructions unzips the Go tar file in  and then add  to PATH. Can I unzip it in  instead and add <code>/home/username/.local/go/bin</code> to PATH instead? Common sense tells me that it should work, but I don't know if there is any specific Go feature that relies on Go being installed in .</p>","contentLength":340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RustOwl - A new tool for visualizing Rust lifetimes","url":"https://youtu.be/NV6Xo_el_2o","date":1739496031,"author":"/u/zxyzyxz","guid":634,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ioz18y/rustowl_a_new_tool_for_visualizing_rust_lifetimes/"},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1739491200,"author":"","guid":731,"unread":true,"content":"<p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the \nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial  is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><div>sequenceDiagram\nautonumber\nrect rgb(191, 223, 255)\nKubelet-&gt;&gt;+Kube-apiserver: Create Node\nNote over Kubelet: Taint: node.cloudprovider.kubernetes.io\nKube-apiserver-&gt;&gt;-Kubelet: Node Created\nend\nNote over Kube-apiserver: Node is Not Ready<p> Tainted, Missing Node Addresses*, ...\nNote over Kube-apiserver: Send Updates\nrect rgb(200, 150, 255)\nKube-apiserver-&gt;&gt;+Cloud-controller-manager: Watch: New Node Created\nNote over Cloud-controller-manager: Initialize Node:</p>Cloud Provider Labels, Node Addresses, ...\nCloud-controller-manager-&gt;&gt;-Kube-apiserver: Update Node\nend\nNote over Kube-apiserver: Node is Ready\n</div><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a  resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a  resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one “correct way” to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting “hostNetwork” to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider’s instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure’s node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.\n<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider’s documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nname: cloud-controller-manager\nnamespace: kube-system\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nannotations:\nkubernetes.io/description: Cloud controller manager for my infrastructure\nspec:\ncontainers: # the container details will depend on your specific cloud controller manager\n- name: cloud-controller-manager\ncommand:\n- /bin/my-infrastructure-cloud-controller-manager\n- --leader-elect=true\n- -v=1\nimage: registry/my-infrastructure-cloud-controller-manager@latest\nresources:\nrequests:\ncpu: 200m\nmemory: 50Mi\nhostNetwork: true # these Pods are part of the control plane\nnodeSelector:\nnode-role.kubernetes.io/control-plane: \"\"\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- topologyKey: \"kubernetes.io/hostname\"\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\ntolerations:\n- effect: NoSchedule\nkey: node-role.kubernetes.io/master\noperator: Exists\n- effect: NoExecute\nkey: node.kubernetes.io/unreachable\noperator: Exists\ntolerationSeconds: 120\n- effect: NoExecute\nkey: node.kubernetes.io/not-ready\noperator: Exists\ntolerationSeconds: 120\n- effect: NoSchedule\nkey: node.cloudprovider.kubernetes.io/uninitialized\noperator: Exists\n- effect: NoSchedule\nkey: node.kubernetes.io/not-ready\noperator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jottr, simple text editor for distraction-free writing","url":"https://www.reddit.com/r/linux/comments/1iow7od/jottr_simple_text_editor_for_distractionfree/","date":1739487720,"author":"/u/walterblackkk","guid":618,"unread":true,"content":"<p><a href=\"https://github.com/mfat/jottr\">Jottr</a> is a simple, opinionated, plain text editor I developed primarily for my personal use. It's mainly intended for writers, researchers and journalists or anyone who needs a simple, distraction-free tool for writing. It's cross-platform and can be used on Linux, macOS or Windows. The app is released under the GPL v3 license.</p><p>It has the following features:</p><ul><li>Dark, Light and Sepia (paper-like) themes</li><li>Focus mode - Distraction-free writing made easy by hiding all UI elements (can be toggled with mouse or a keyboard shortcut)</li><li>Search for any word without leaving the editor using the integrated web browser</li><li>Snippets - Save any word or block of text as snippets and quickly insert using mouse or through typing snippet name</li><li>Inline auto-completion for words added to user dictionary</li><li>Add custom search websites to your right-click menu</li></ul><p>Feel free to share your feedback/thoughts.</p>","contentLength":868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Streamlined dataflow analysis code in rustc","url":"https://nnethercote.github.io/2024/12/19/streamlined-dataflow-analysis-code-in-rustc.html","date":1739485725,"author":"/u/nnethercote","guid":630,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iovgra/streamlined_dataflow_analysis_code_in_rustc/"},{"title":"Show HN: SQL Noir – Learn SQL by solving crimes","url":"https://www.sqlnoir.com/","date":1739483356,"author":"chrisBHappy","guid":200,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43041827"},{"title":"Privacy Pass Authentication for Kagi Search","url":"https://blog.kagi.com/kagi-privacy-pass","date":1739476649,"author":"b3n","guid":205,"unread":true,"content":"<p>Today we are announcing a new privacy feature coming to Kagi Search. Privacy Pass is an authentication protocol first introduced by Davidson  in [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">1</a>], and recently standardized by the IETF as RFCs [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2—4</a>].  Our starting point was the excellent Rust implementation of the Privacy Pass protocols by <a href=\"https://github.com/raphaelrobert/privacypass\">Raphael Roberts</a>. At the same time, we are announcing the immediate availability of Kagi’s <a href=\"https://blog.kagi.com/kagi-privacy-pass#tor\">Tor onion service</a>.</p><p>In general terms, Privacy Pass allows “Clients” (generally users) to authenticate to “Servers” (like Kagi) in such a way that while the Server can verify that the connecting Client has the right to access its services, it cannot determine which of its rightful Clients is actually connecting. This is particularly useful in the context of a privacy-respecting paid search engine, where the Server wants to ensure that the Client can access the services, and the Client seeks strong guarantees that, for example, the searches are not associated with them.</p><p>As a privacy-respecting search engine, Kagi’s business model is such that we have no incentive to track what an individual user is searching for. We are in the business of selling a search product, not selling user data or attention.</p><p>Now, Privacy Pass adds another layer of trust: we can verify that you have the right to search without knowing who you are or what you’re searching for. It’s one thing to promise we won’t track you; it’s another to make it technically impossible. We jumped on the opportunity to implement Privacy Pass as soon as the IETF made it an official standard.</p><p>This matters, because for many users, privacy isn’t just about incentives and privacy policies; it’s about proof. When we cannot track you even if we wanted to, that’s genuine privacy.</p><p>Initially, we will be offering Privacy Pass to all our plans with unlimited searches: Professional, Ultimate, Family, and Team plans. Privacy Pass will not be available to Trial and Starter plans due to technical limitations at this moment (see below for more info).</p><p>\nTo get started with Kagi Privacy Pass right away:</p><ul><li>Download the newest version of Kagi’s <a href=\"https://kagi.com/orion\">Orion Browser</a> (for macOS/iOS/iPadOS) with Kagi Privacy Pass natively integrated. You will need at least 0.99.131 for macOS and 1.3.17 for iOS/iPadOS (they are expected to be rolling out globally today).</li></ul><ul><li>Download the newest version of <a href=\"https://play.google.com/store/apps/details?id=com.kagi.search&amp;hl=en_US\">Kagi for Android</a> app with Kagi Privacy Pass natively integrated. You will need to use at least version 0.29 (this is expected to roll out globally today).</li></ul><ul><li>If you are already using the Kagi Search extension, you will want to update it to the latest version (0.7.6 on <a href=\"https://addons.mozilla.org/firefox/addon/kagi-search-for-firefox\">Firefox</a>, 1.2.2.5 on <a href=\"https://chromewebstore.google.com/detail/kagi-search/cdglnehniifkbagbbombnjghhcihifij\">Chrome</a>) to avoid compatibility issues, or simply disable it.</li><li>Safari is not yet supported due to technical limitations, see the F.A.Q. below.</li></ul><p>In addition our implementation of Privacy Pass is open sourced and you can find it <a href=\"https://github.com/kagisearch/privacypass-extension\">here</a>.</p><p>When using Kagi Privacy Pass mode, you’ll be truly anonymous - which means your account settings won’t be available since we can’t identify which user you are.</p><p>But don’t worry - we’ve made it flexible. You can easily toggle Privacy Pass on or off based on your needs. Think of it as two modes: full features with normal privacy, or maximum privacy with core features. You choose what makes sense for you based on your context and needs.</p><p>Privacy Pass uses cryptography to allow a client to authenticate to a server by performing a protocol with two phases: token generation and token redemption.</p><p>In the initial “token generation” phase, the client interacts with the server to generate some authentication “tokens.”</p><p>For the server to willingly participate in this protocol, the client must prove their “right” to generate tokens.</p><p>In the case of Kagi’s users, this can be done by presenting their Kagi session cookie to the server.</p><p>The tokens eventually generated by the client at the end of this phase are indistinguishable from a randomly generated token from the server’s point of view. They cannot be traced back to the user who generated them, or to other tokens generated by the same user at the same or a different time.</p><p>After token generation is performed, a client can initiate a “token redemption” phase.</p><p>During this phase, the client actually accesses the services provided by the server, proving the client’s right to access the services by presenting one of the previously generated tokens.</p><p>Since the previously generated tokens are unknown and unpredictable to the server, the latter can only tell that the client has successfully completed token generation at some point.</p><p>Technically, we say that the techniques used by Privacy Pass result in the two phases being “unlinkable”. While the server is able to tell whether a token presented for redemption was previously generated by interacting with a rightful client, it cannot link the token to a specific token generation phase.</p><p>Crucially, tokens are single-use: servers keep track of which tokens have already been redeemed to avoid multiple redemptions. Furthermore, clients should not present the same token twice to prevent different redemption phases from being linked.</p><p>Tokens have a fixed life span. If they are too old, they will stop being redeemable. In that case, a new token generation phase must be initiated by the client to obtain new tokens.</p><p>As standardized in [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2 - 4</a>], the Privacy Pass protocol is able to accommodate many “architectures.” Our deployment model follows the original architecture presented by Davidson  [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">1</a>], called “Shared Origin, Attester, Issuer” in § 4 of [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2</a>].</p><p>Here, Kagi plays all the “Server roles” (Attester, Issuer, Origin), and Kagi users play the Client role via the new Kagi browser extensions for Privacy Pass, or via native support in Orion. This is what it looks like in practice:</p><ol><li>Once installed, and periodically, the browser extension will generate and store a large number of tokens.</li><li>The user can mark in the extension whether searches should be performed by authenticating classically via a session cookie, or by using Privacy Pass.</li><li>If the user chooses the second option, they will authenticate to Kagi during the search by redeeming one of the tokens it previously generated.</li></ol><p>Using Privacy Pass is as easy as clicking a toggle.</p><p>If you are using the latest version of <a href=\"https://kagi.com/orion\">Orion</a> for macOS,  select Kagi as your search engine in  and then enable the checkbox for showing Privacy Pass options on your toolbar.</p><p>From there you can easily toggle when you want to use Privacy Pass or standard authentication.</p><p>On iOS and iPadOS, Kagi Privacy Pass is natively supported in the latest version of the <a href=\"https://apps.apple.com/us/app/orion-browser-by-kagi/id1484498200\">Orion Browser</a> for iOS and iPadOS and takes just a few clicks to enable.</p><p>Our <a href=\"https://play.google.com/store/apps/details?id=com.kagi.search\">Android app</a> now supports Privacy Pass mode via an app shortcut. Launching the shortcut allows you to browse Kagi seamlessly in Privacy Pass mode. You can also add the shortcut to your home screen for quick access.</p><p>This feature lets you either use Kagi exclusively in Privacy Pass mode or switch effortlessly between modes.</p><h3>Chrome and Firefox browser extensions</h3><p>If you are using the Kagi Privacy Pass extension for <a href=\"https://chromewebstore.google.com/detail/kagi-privacy-pass/mendokngpagmkejfpmeellpppjgbpdaj\">Chrome</a> or <a href=\"https://addons.mozilla.org/firefox/addon/kagi-privacy-pass/\">Firefox</a>, once installed you should see the Kagi Privacy Pass icon on your toolbar.</p><p>Once installed, the extension automatically generates tokens. To use them, click the extension icon, and make sure the toggle is on.</p><p>Note that Safari is not supported at this moment; see the F.A.Q. below for more information.</p><p>As used by Kagi, Privacy Pass tokens offer various security properties (§ 3.3,  of [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2</a>]).</p><p>These can be a little technical to capture. In a few words, they guarantee that users can trust that their searches authenticated via Privacy Pass cannot be linked to their accounts, and Kagi can rest assured that only legitimate users can correctly authenticate using Privacy Pass. Crucially, the guarantee for users is even against malicious servers that attempt to incorrectly implement the server-side computation, as long as the client-side implementation is correct.</p><p>Three of these security properties serve to protect our users:</p><ol><li><p>Generation-redemption unlinkability: Kagi cannot link the tokens presented during token redemption ( during search) with any specific token generation phase. This means that Kagi will not be able to tell who it is serving search results to, only that it is someone who presented a valid Privacy Pass token.</p></li><li><p>Redemption-redemption unlinkability: Kagi cannot link the tokens presented during two different token redemptions. This means that Kagi will not be able to tell from tokens alone whether two searches are being performed by the same user.</p></li><li><p>No redemption hijacking: an eavesdropper that observes any token generation phase, cannot use the observed information alone to “steal” the tokens from the intended user and redeem them themselves. This means that third parties snooping on a user’s token generation interaction will not be able to steal the tokens. This adds a layer of security on top of the confidentiality attained during token generation by using a TLS-protected connection.</p></li></ol><p>Two of these security properties serve to protect Kagi.</p><ol><li><p>Correctness: honestly generated tokens will pass Kagi’s validation.</p></li><li><p>One-more-forgery security: a malicious client cannot use knowledge of a correctly generated token to forge a new one. This means that valid tokens cannot be generated without correctly interacting with Kagi, and therefore valid tokens are evidence that the user owned a valid session cookie for a supported Kagi plan at the moment of generating the token.</p></li></ol><p>Naturally, online interactions are never fully described by a mathematical model.</p><p>While the Privacy Pass protocol  indeed guarantee that the server will not be able to link token generation and token redemption phases , in principle, a malicious server could still attempt to track clients via <a href=\"https://en.wikipedia.org/wiki/Side-channel_attack\">side-channel</a> information.</p><p>For example, if someone were to make the same specific request to a server at the same time every day (say, searching “lunch places near 123 Mulholland Drive, LA” at 11:58 AM), a server that records all searches being made could, in principle, guess that these searches are all made by the same person.</p><p>In this case, Privacy Pass would make it harder for the server to determine who this specific person is, but the server could nonetheless link searches to one another.</p><p>On a level beyond, it is well known that browsers can often have a unique “fingerprint” [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">5-7</a>]. Fingerprinting attacks heavily rely on side-channel signals that evade the Privacy Pass protocol, such as user-agent strings or IP addresses. For example, if a server receives a token generation request from a given IP address, and immediately after a token redemption request from the same address, it can likely conclude that the same individual is behind the request. For this reason, it is <a href=\"https://www.rfc-editor.org/rfc/rfc9576.html#section-4.1\">highly recommended</a> to separate token generation and redemption in time, or “in space” (by using an anonymizing service such as <a href=\"https://www.torproject.org/\">Tor</a> when redeeming tokens, see below).</p><p>Kagi’s Privacy Pass extension and native implementation in Orion take care, as much as we can, to uniform your browser fingerprint, by removing deanonymizing HTTP headers and cookies.</p><p>We see Privacy Pass as an important tool for increasing the anonymity guarantees we can offer to Kagi users.</p><p>Adopting state-of-the-art standards for new privacy enhancing technologies also signals to researchers and standardization bodies that there is a public demand for more privacy and anonymity tools in today’s digital world and incentivizes further scrutiny and development of privacy-enhancing technologies.</p><p>Together with launching Privacy Pass, we are also announcing that we now have a Tor onion service available, which allows access to Kagi directly from the Tor network. Kagi’s onion address is:</p><p>On its own, Tor will obscure your location by hiding your IP address. However, without Privacy Pass, you still need to be logged into your Kagi account to perform searches, making them all theoretically linkable back to a single account. As always, Kagi does not link searches to accounts or permanently record them; see <a href=\"https://kagi.com/privacy\">our Privacy Policy</a> for more info.</p><p>With Tor and Privacy Pass together, Kagi only knows that the search is being issued by a user who previously verified that they have an account authorized to receive tokens, but nothing about the user’s account, or where they’re located.</p><p>Privacy Pass support is provided:</p><p>This should accommodate users who want to install and use the extension across multiple browsers or computers. Please refer to our documentation for <a href=\"https://help.kagi.com/kagi/privacy/privacy-pass.html\">usage instructions</a>.</p><p>At first, Privacy Pass authentication will be available to users on any Kagi plan with unlimited searches. These plans will have a generous allocation of tokens (2000 to begin with) that they can generate monthly.</p><p>We are working on enabling this feature for Trial and Starter plans, which have access to a limited number of monthly searches. Therefore, they risk a worse user experience if their generated tokens are lost (for example, due to uninstalling the extension) and theoretically, users on this plan could redeem more tokens than the limit of searches allowed on their plan (again, we do not know who the user redeeming the tokens is, or what plan they are on). This makes it more technically challenging to support these plans with Privacy Pass, and we have left that for later.</p><p><strong>You mention “tokens.” Are blockchains involved in this protocol?</strong></p><p>Privacy Pass does not rely on any blockchain technology.</p><p>While the protocol makes use of various cryptographic primitives (specifically, elliptic curves and hash functions, as part of a “verifiable oblivious pseudorandom function” construction, [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">8</a>]) and generates “tokens,” these are not generated, stored, or traded on a blockchain.</p><p><strong>You mention the client generating tokens. Is this process energy-intensive or storage-demanding?</strong></p><p>No. The generation of 500 search tokens requires approximately 1 second of computation on a consumer laptop, and is performed in the background when installing the extension. A few extra seconds may be required due to the time required to contact the server and get a response. Each token consists of 216 bytes, for a total of approximately 100 KiB of storage per token generation request.</p><p><strong>Is there a potential impact on the speed of search when using Privacy Pass?</strong></p><p>The initial generation of tokens takes about ~1 second for 500 tokens, plus the time required for contacting the server. This occurs infrequently and is done in the background when possible.</p><p>Currently, the token validation servers are only deployed in our us-central1 region, we plan to expand this shortly after launch.</p><p><strong>How many tokens am I able to generate?</strong></p><p>You can generate 2,000 tokens in one “epoch” (= one month). This should be enough for most users. If you need more than this, you can request additional tokens by contacting support@kagi.com.</p><p><strong>Do you plan to allow purchasing privacy pass tokens without having an account?</strong></p><p>Yes, this makes sense. This is possible because technically the extension does not care if you have an account or not. It just needs to be ‘loaded’ with valid tokens. And you can imagine a mechanism where you could also anonymously purchase them, eg. with monero, without ever creating an account at Kagi.  Let us know <a href=\"https://kagifeedback.org/d/6163-kagi-privacy-pass\">here</a> if you are excited about this, as it will help prioritize it.</p><p><strong>How can I submit feedback for Kagi Privacy Pass?</strong></p><p>We have a feedback thread open <a href=\"https://kagifeedback.org/d/6163-kagi-privacy-pass\">here</a>.</p><p><strong>Even if the extension implements anti-fingerprinting measures, Kagi will still be able to see my IP address, correct?</strong></p><p>Even with Privacy Pass authentication enabled, due to the way the TCP/IP stack works, we will be able to see your search request come from an IP address. As outlined in our <a href=\"https://kagi.com/privacy\">Privacy Policy</a>, your privacy is our priority, whether you are using Privacy Pass to authenticate or otherwise. If you are worried about us seeing your IP address, our suggestion is to connect to Kagi via Tor or through a VPN service you trust.</p><p><strong>How can Privacy Pass increase my privacy, if I have to send a session cookie to authenticate during token generation?</strong></p><p>While token generation is indeed not anonymous, Privacy Pass provides you with anonymity .</p><p>By providing the server with a Privacy Pass token instead of a session cookie when searching, you will guarantee that your searches cannot be  to any specific user account that generated Privacy Pass tokens, or to each other.</p><p>From the point of view of the server, your search query could have come from any of the users who previously generated Privacy Pass tokens.</p><p>The more users do so, the lower the probability that the server can guess it was you specifically who made a given search query.</p><p><strong>Token generation does not work in my Chrome/Firefox private window</strong></p><p>Correct, we need to authenticate you to create tokens (see above), and in the private window, the extension does not have access to your session cookie. Please use a normal browsing window while logged in to Kagi to generate tokens.</p><p>Note, generating tokens while in a private window will work in the <a href=\"https://kagi.com/orion\">Orion browser</a>.</p><p><strong>Can I use Kagi Assistant while using Privacy Pass?</strong></p><p>Not at this time, since Kagi Assistant is only available to Ultimate members. In Privacy Pass, we don’t have any account information, so we can’t validate what plan you’re on. We could issue tokens attached to different keys for different plans, but that also has privacy implications, see the discussion of personalization below.</p><p><strong>What Kagi services will be compatible with Privacy Pass at launch?</strong></p><p>At launch, Privacy Pass will only be used to authenticate Kagi Search. Soon to follow (in the next few weeks), we plan to expand support for Kagi Privacy Pass to:</p><ul><li>Kagi Translate and Kagi Maps</li><li>Kagi universal Summarizer and Ask questions about page</li></ul><p>Please disable Privacy pass to access these services for now.</p><p><strong>Since initial token generation happens in batches and the tokens expire, could tokens with similar expiration dates potentially be used to identify multiple searches from the same user?</strong></p><p>All tokens generated during month X expire at midnight of the first day of month X+2, to avoid this exact issue. Meaning a freshly generated token lasts until the end of the month following its generation (generate today, use all of Feb and March).</p><p><strong>If Kagi cannot track who exactly is performing search queries, will I have access to my account settings including customization and personalization?</strong></p><p>Since Kagi will not know who you are, we will not be able to serve you content tailored to your custom settings via Privacy Pass-protected search.</p><p>We have considered allowing users to send a small configuration with every request <code>(language, region, safe-search)</code> to automatically customize your search experience to some extent. However, we currently believe this would quickly result in a significant loss of anonymity for you and for other users. To illustrate this, we have examined the most common configurations of <code>(language, region, search-safe)</code> used on Kagi.com, and extrapolated how many Privacy Pass users would share them. Looking at the top 35 configurations, we see the following approximate numbers.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Limiting the analysis to only the ten most common language settings, the effect is similar:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>This would mean that someone sending us  as their language configuration would automatically lose redemption-redemption unlinkability guarantees if approximately 1000 Kagi users used Privacy Pass.</p><p>While our extrapolation may be overly conservative, we won’t be enabling this level of “default” customization for users authenticating via Privacy Pass for the time being. We could reconsider if we find a better solution.</p><p>For manual search settings customization, you can always use bangs in your search query to enable basic settings for a specific query. For example, <a href=\"https://help.kagi.com/kagi/features/bangs.html#regional-bangs\">regional bangs</a> will let you focus your query on one region. For example prefixing your search with  will automatically search in the German region.</p><p>To access a fully customized search experience, you can always use the traditional login method and disable the use of Privacy Pass.</p><p><strong>Will Safari be supported?</strong></p><p>The Safari extensions API doesn’t support (as far as we know) removing cookies from requests, which means it will always authenticate with your logged-in account. We’re not aware of a way to change this. The alternative if you want a similar, native, WebKit-based browsing experience, is to use the <a href=\"https://kagi.com/orion\">Orion Browser</a> which has Kagi Privacy Pass natively integrated.</p><ol><li>Davidson, A., Goldberg, I., Sullivan, N., Tankersley, G., &amp; Valsorda, F. (2018). Privacy pass: Bypassing internet challenges anonymously. Proceedings on Privacy Enhancing Technologies. <a href=\"https://doi.org/10.1515/popets-2018-0026\">Paper</a>.</li><li>Davidson, A., Iyengar, J., &amp; A. Wood, C. (2024). The Privacy Pass Architecture. <a href=\"https://www.rfc-editor.org/rfc/rfc9576.html\">RFC 9576</a>.</li><li>Pauly, T., Valdez, S., &amp; A. Wood, C. (2024). The Privacy Pass HTTP Authentication Scheme. <a href=\"https://www.rfc-editor.org/rfc/rfc9577.html\">RFC 9577</a>.</li><li>Celi, S., Davidson, A., Valdez, S., &amp; Wood, C. A. (2024). Privacy Pass Issuance Protocols. <a href=\"https://www.rfc-editor.org/rfc/rfc9578.html\">RFC 9578</a>.</li><li>Eckersley, P. (2010). How unique is your web browser? Proceedings on Privacy Enhancing Technologies. <a href=\"https://dl.acm.org/doi/10.5555/1881151.1881152\">Paper</a>.</li><li>Davidson, A., Faz-Hernandez, A., Sullivan, N., &amp; A. Wood, C. (2023). Oblivious Pseudorandom Functions (OPRFs) Using Prime-Order Groups. <a href=\"https://www.rfc-editor.org/rfc/rfc9497.html\">RFC 9497</a>.</li></ol><blockquote><p>Do you like how this post reads? It was proofread with <a href=\"https://translate.kagi.com\">Kagi Translate’s</a> proofreading option. To proofread any web page, just use <code>https://translate.kagi.com/proofread/[URL]</code>.</p></blockquote>","contentLength":21275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43040521"},{"title":"Phind 2: AI search with visual answers and multi-step reasoning","url":"https://www.phind.com/blog/phind-2","date":1739470829,"author":"rushingcreek","guid":245,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43039308"},{"title":"Coding Interviews were HARD Until I Learned These 20 Tips","url":"https://blog.algomaster.io/p/20-coding-interviews-tips","date":1739467827,"author":"Ashish Pratap Singh","guid":729,"unread":true,"content":"<p>I gave my first  in 2016—and failed. I failed the next five interviews as well before finally landing my first job at .</p><p>Since then, I’ve interviewed with many companies and faced my fair share of rejections. However, over the years, my failure rate in coding interviews dropped significantly.</p><p>By 2022, with just 1.5 months of focused preparation, I successfully cleared interviews at  and .</p><p>Surprisingly, my success wasn’t due to a dramatic improvement in problem-solving skills. The real game-changer was my approach— and  during the interview.</p><p>In this article, I’ll share  that made coding interviews significantly easier for me.</p><p>These tips cover everything you need to know, including:</p><ul><li><p>How to systematically approach coding interview problems</p></li><li><p>Key concepts and patterns you should know</p></li><li><p>The type of problems you should practice</p></li><li><p>How to choose the right algorithm for a given problem</p></li><li><p>Techniques to optimize your solution</p></li><li><p>How to communicate your thought process effectively</p></li></ul><p>By applying these strategies, you’ll be able to tackle coding interviews with confidence and massively increase your chances of success.</p><p>In a coding interview, interviewers want to see how well you , , and  under pressure.</p><p>Here's a breakdown of what they look for:</p><ol><li><p><strong>Understanding the problem</strong>: Do you ask clarifying questions instead of making assumptions to ensure you fully understand the problem?</p></li><li><p>: Can you decompose the problem into smaller, manageable parts?</p></li><li><p>: Can you design an optimal solution in terms of time and space complexity?</p></li><li><p>: Do you handle edge cases like empty inputs, duplicates, large values, or special conditions?</p></li><li><p>: Can you explain why one approach is better than another?</p></li><li><p>: Do you have a strong grasp of data structures and algorithms, and can you choose the right one for the problem?</p></li><li><p>Can you quickly compute the time and space complexity of your solution?</p></li><li><p><strong>Explaining your thought process</strong>: Can you clearly articulate your approach and why it works?</p></li><li><p>: Are you receptive to hints and able to adjust your approach accordingly?</p></li><li><p>: Do you follow good coding practices (meaningful variable names, proper indentation, modular functions etc..)?</p></li><li><p><strong>Improving the initial solution</strong>: Can you optimize and refine your first solution when prompted?</p></li><li><p>Are you able to tackle variations of the original problem?</p></li><li><p>Can you manually walk through your code with sample inputs to verify correctness?</p></li></ol><p>Most coding interviews last </p><p>Depending on the company and interviewer, you may be asked to solve 2-3easy/medium problems or 1 hard problem with follow-ups.</p><p>Lets assume you are given one problem, with a follow up in a 45-minute interview. Here’s how you can optimally allocate your time:</p><ol><li><p>The interviewer may ask you to introduce yourself. Prepare a concise 1-2 minute introduction that highlights your background, experience, and key strengths. Practice it beforehand so that you can deliver it smoothly.</p></li><li><p><strong>Understand the Problem (5-10 mins):  </strong>Carefully read the problem statement, ask clarifying questions, and walk through sample inputs and expected outputs.</p></li><li><p><strong>Plan the Approach (10-20 mins): </strong>Brainstorm possible solutions, evaluate trade-offs, and discuss time and space complexity.</p></li><li><p><strong>Implement the Code (20-30 mins): </strong>Write a clean, modular and readable code.</p></li><li><p>Dry-run your code with sample inputs, debug any issues, and ensure edge cases are handled.</p></li><li><p><strong>Follow-ups and Wrap Up (35-45 mins): </strong>Answer follow up questions, and ask thoughtful questions to the interviewer about the company, role, or team.</p></li></ol><p>One of the biggest mistakes candidates make in coding interviews is jumping into coding too soon.</p><p>If you don't fully understand the question, you might end up solving the </p><p>Here’s how to ensure you grasp the problem before coding:</p><h4><strong>Read the Problem Carefully</strong></h4><p>Take a moment to absorb the problem statement. Rephrase it in your own words to confirm your understanding. </p><p>Identify the expected input/output format and any hidden constraints.</p><p>If anything is unclear, ask questions before diving into the solution. Interviewers appreciate when you seek clarity. Never assume details that aren’t explicitly mentioned in the problem statement.</p><p>Common clarifications include:</p><ul><li><p>Are there duplicate values?</p></li><li><p>Can the input be empty? If so, what should the output be?</p></li><li><p>Should the solution handle negative numbers?</p></li><li><p>Should the output maintain the original order of elements?</p></li><li><p>Is the graph directed or undirected?</p></li><li><p>Does the input contain only lowercase English letters, or can it have uppercase, digits, or special characters?</p></li><li><p>What should happen if multiple solutions exist? Should I return any valid solution, or does the problem have specific requirements?</p></li></ul><h4><strong>Walk Through Input/Output Examples</strong></h4><p>Once you understand the problem statement and constraints, go over a few input and output examples to make sure you get it.</p><p>Draw them out if it helps, especially for visual data structures like trees or graphs.</p><p>Try to take examples that cover different scenarios of the problem. Think about any  that might come up.</p>","contentLength":4908,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/61c3f6c0-4027-4d37-b4a7-a30fc183fa12_1602x1032.png","enclosureMime":"","commentsUrl":null},{"title":"LibreOffice still kicking at 40, now with browser tricks and real-time collab","url":"https://www.theregister.com/2025/02/13/libreoffice_wasm_zetaoffice/","date":1739465983,"author":"LinuxBender","guid":244,"unread":true,"content":"<p> LibreOffice is a big, mature chunk of code now, but that doesn't make it impossible to teach it impressive new tricks. Some of them could make it more important than ever.</p><p>The open-source office suite had its own <a target=\"_blank\" href=\"https://fosdem.org/2025/schedule/track/libreoffice/\" rel=\"nofollow\">program stream</a> at FOSDEM, including the <a target=\"_blank\" href=\"https://fosdempwa.com/event/5735?year=2025&amp;day=null&amp;test=false\" rel=\"nofollow\">pre-announcement</a> of the new <a target=\"_blank\" href=\"https://wiki.documentfoundation.org/ReleaseNotes/25.2\" rel=\"nofollow\">LibreOffice release 25.2</a>. It has been around in some form since 1985, so this version marks its 40th year. It's middle-aged and, almost inevitably, that means it's big, a bit saggy in places, it definitely has some issues, and it doesn't look as good as it did a couple of decades ago.</p><p>But that doesn't mean it can't be taught interesting things. Even a 40-year-old can acquire new skills and take on entirely new roles.</p><p>It's the same sort of functionality that you get from Google Docs, and indeed this is already possible using the <a target=\"_blank\" href=\"https://www.theregister.com/2022/07/01/collabora_releases_code_2205_webbased/\">Collabora Online web-based version of LibreOffice</a>. The big difference is that such tools run in a browser, so you need to be online. What makes the CRDT implementation different is that this is a local app, working on a local file, but using a network copy to keep changes in sync. The idea is to free you from keeping your apps and data on someone else's computer, without losing the handy collaborative features that web apps bring.</p><p>We also met up with Thorsten for a demo of one of his other babies, <a target=\"_blank\" href=\"https://zetaoffice.net/\" rel=\"nofollow\">ZetaOffice</a>. This is a version of LibreOffice built for the Wasm runtime, which means it can run inside a browser. He demonstrated it to us on an AMD Ryzen-powered developer-spec ThinkPad, but the same binaries could run on any OS and on any CPU, such as ARM hardware. ZetaOffice went into public beta <a target=\"_blank\" href=\"https://blog.allotropia.de/2024/11/08/launching-zetaoffice/\" rel=\"nofollow\">last November</a>, and a first release is looming very soon.</p><p>He showed us it running in multiple modes. You can embed any of the LibreOffice apps into a web page, complete with their normal user interface and so on, much like Google Apps. This enables a website to offer a very rich editing experience of text, spreadsheets, presentations, or anything else. ZetaOffice can also be embedded, scripted, and controlled using the <a target=\"_blank\" href=\"https://github.com/allotropia/zetajs\" rel=\"nofollow\">zetajs wrapper</a>, so you can call it to display embedded rich objects in web pages. We were rather taken by a demo that displayed a moving line chart of ping times to the  website. A script was extracting the times from the ping command, adding lines to a LibreOffice Calc spreadsheet, graphing the last dozen or so lines and embedding the graph in a web page.</p><p>He told us that the Allotropia development team sees ZetaOffice as orthogonal to Collabora Office (or COOL for short). COOL runs in the browser and has simultaneous multiuser editing. ZetaOffice runs in the browser too, but it's perfectly able to run locally and doesn't need an uplink. For now, though, it's single user. The company will make a cross-platform version that is also available for local installation.</p><p>The real power comes from the scriptability and integration with JavaScript, though. We are sure lots of people will find uses for the ability to embed any document LibreOffice can open into any web page, control and automate it from JavaScript, with the choice of a full local user interface – or none, just the content, controlled with a JavaScript UI instead. The code is all under the MIT licence, so it's all-FOSS, and all the work is being upstreamed to LibreOffice itself.</p><p>This is big stuff in more than one way. LibreOffice isn't lightweight anyway. Opening a document in a web page can pull in a gigabyte or so of code, and the memory footprint is a bit more than that. It is still in beta, though, and the team is working hard on modularizing the code into multiple smaller chunks so that it will load much faster and take less memory.</p><p>The potential flexibility here&nbsp;is impressive, though. You could have a portable app on a USB key that was usable on a PC or Mac or Chromebook, regardless of the OS, without installation. Web apps suddenly acquire full rich cross-platform document handling and editing, including, of course, all the main Microsoft files and formats.</p>","contentLength":3989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43038200"},{"title":"Resigning as Asahi Linux project lead","url":"https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/","date":1739460897,"author":"Shank","guid":243,"unread":true,"content":"<p>Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them “jailbreaks” these days) to allow users to run their own unofficial apps on the Nintendo Wii.</p><p>I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other people’s work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didn’t attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.</p><p>When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.</p><p>I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.</p><p>Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which I’ve already spoken at length about and I won’t repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience.</p><p>But then also came the entitled users. This time, it wasn’t about stealing games, it was about features. “When is Thunderbolt coming?” “Asahi is useless to me until I can use monitors over USB-C” “The battery life sucks compared to macOS” (nobody ever complained when compared to x86 laptops…) “I can’t even check my CPU temperature” (yes, I seriously got that one).</p><p>And, of course, “When is M3/M4 support coming?”</p><p>For a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were “alpha” and “unstable” and “not suitable for a daily driver” (despite <a href=\"https://stats.asahilinux.org\">thousands of users</a>, myself included, daily driving it and even using it for servers).</p><p>No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.</p><p>I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.</p><p>Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I won’t go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).</p><p>I did make some progress in 2024, but this left me in a very vulnerable position. I hadn’t gotten nearly as much Asahi work done as I’d liked, and the users weren’t getting any quieter about demanding more features and machine support.</p><p>We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).</p><p>I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. “Full” DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasn’t the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did <a href=\"https://lore.kernel.org/lkml/fda8b831-1ffc-4087-8e7b-d97779b3ecc5@marcan.st/T/#u\">try</a> to bring the subject up on the mailing lists, but it didn’t get much response).</p><p>The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I won’t repeat them in detail here. Suffice it to say, I consider Linus’ handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.</p><p>As you know, this is deeply personal to me, as we’ve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.</p><p>Some might be wondering why we can’t just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But there’s more.</p><p>In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (that’s how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a <a href=\"https://pagure.io/fedora-asahi/mesa-asahi-flatpak\">partial workaround</a> for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.</p><p>I’m not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated <a href=\"https://lore.kernel.org/lkml/20250208204416.GL1130956@mit.edu/\">“We\nare the ‘thin blue line’”</a>, and nobody cared, which just further confirmed to me that I don’t want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to <a href=\"https://lore.kernel.org/lkml/20240828211117.9422-1-wedsonaf@gmail.com/\">quit</a>.</p><p>But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than <a href=\"https://bunny.net/\">bunny.net</a> giving us free CDN credits for the hosting).</p><p>I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say I’m the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they won’t touch the Linux kernel with a 10-foot pole.</p><p>When Apple released the M1, Linus Torvalds <a href=\"https://thenextweb.com/news/linus-torvalds-wants-apples-new-m1-powered-macs-to-run-linux\">wished it could run Linux</a>, but didn’t have much hope it would ever happen. We made it happen, and Linux 5.19 was <a href=\"https://lore.kernel.org/lkml/CAHk-=wgrz5BBk=rCz7W28Fj_o02s0Xi0OEQ3H1uQgOdFvHgx0w@mail.gmail.com/T/#u\">released from an M2 MacBook Air running Asahi Linux</a>. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him <a href=\"https://gist.github.com/marcan/fe70ee6648f3d5ae94eb8332265b8d95\">an invitation</a> to discuss the challenges of kernel contributions and maintenace and see how we could help. He never replied.</p><p>Back in 2011, Con Kolivas <a href=\"https://web.archive.org/web/20110707151924/http://apcmag.com/why_i_quit_kernel_developer_con_kolivas.htm\">left the Linux kernel community</a>. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if you’re not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.</p><p>When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but it’s not fun any more. I have an M3 Pro in a box and I haven’t even turned it on yet. I dread doing the bring-up work. It doesn’t feel worth the trouble.</p><p>I miss having free time where I can relax and not worry about the features we haven’t shipped yet. I miss <a href=\"https://youtube.com/@TsuiokuCircuit\">making music</a>. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we haven’t upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.</p><p>I’m resigning as lead of the Asahi Linux project, effective immediately. The project will <a href=\"https://asahilinux.org/2025/02/passing-the-torch/\">continue on without me</a>, and I’m working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the <a href=\"https://opencollective.com/asahilinux\">Asahi Linux OpenCollective</a> (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).</p><p>I want to thank the entire Asahi Linux team, without whom I would’ve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.</p><p>If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: <a href=\"mailto:marcan@marcan.st\">marcan@marcan.st</a>.</p><p>: A lot of the discussion around this post and the interactions that led to it brings up the term “brigading”. Please read <a href=\"https://hachyderm.io/@chandlerc/114001000657957325\">this excellent Fedi post</a> for a discussion of what is and isn’t brigading.</p>","contentLength":14863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43036904"},{"title":"Is It Time For a Change In GNOME Leadership?","url":"https://tech.slashdot.org/story/25/02/13/0150244/is-it-time-for-a-change-in-gnome-leadership?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739451600,"author":"BeauHD","guid":275,"unread":true,"content":"Longtime Slashdot reader BrendaEM writes: Command-line aside, Cinnamon is the most effective keeper of the Linux desktop flame -- by not abandoning desktop and laptop computers. Yes, there are other desktop GUIs, such as MATE, and the lightweight Xfce, which are valuable options when low overhead is important, such as in LinuxCNC. However, among the general public lies a great expanse of office workers who need a full-featured Linux desktop.\n \nThe programmers who work on GNOME and its family of supporting applications enrich many other desktops do their more than their share. These faithful developers deserve better user-interface leadership. GNOME has tried to steer itself into tablet waters, which is admirable, but GNOME 3.x diminished the desktop experience for both laptop and desktop users. For instance, the moment you design what should be a graphical user interface with words such as \"Activities,\" you ask people to change horses midstream. That is not to say that the command line and GUI cannot coexist -- because they can, as they do in many CAD programs.\n \nI remember a time when GNOME ruled the Linux desktop -- and I can remember when GNOME left those users behind. Perhaps in a future, GNOME could return to the Linux desktop and join forces with Cinnamon -- so that we may once again have the year of the Linux desktop.","contentLength":1346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to add a directory to your PATH","url":"https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/","date":1739449676,"author":"Julia Evans","guid":46,"unread":true,"content":"<p>I was talking to a friend about how to add a directory to your PATH today. It’s\nsomething that feels “obvious” to me since I’ve been using the terminal for a\nlong time, but when I searched for instructions for how to do it, I actually\ncouldn’t find something that explained all of the steps – a lot of them just\nsaid “add this to ”, but what if you’re not using bash? What if your\nbash config is actually in a different file? And how are you supposed to figure\nout which directory to add anyway?</p><p>So I wanted to try to write down some more complete directions and mention some\nof the gotchas I’ve run into over the years.</p><p>Here’s a table of contents:</p><h3>step 1: what shell are you using?</h3><p>If you’re not sure what shell you’re using, here’s a way to find out. Run this:</p><ul><li>if you’re using , it’ll print out </li><li>if you’re using , it’ll print out </li><li>if you’re using , it’ll print out an error like “In fish, please use\n$fish_pid” ( isn’t valid syntax in fish, but in any case the error\nmessage tells you that you’re using fish, which you probably already knew)</li></ul><p>Also bash is the default on Linux and zsh is the default on Mac OS (as of\n2024). I’ll only cover bash, zsh, and fish in these directions.</p><h3>step 2: find your shell’s config file</h3><ul><li>in zsh, it’s probably </li><li>in bash, it might be , but it’s complicated, see the note in the next section</li><li>in fish, it’s probably <code>~/.config/fish/config.fish</code> (you can run  if you want to be 100% sure)</li></ul><h3>a note on bash’s config file</h3><p>Bash has three possible config files: , , and .</p><p>If you’re not sure which one your system is set up to use, I’d recommend\ntesting this way:</p><ol><li>add  to your </li><li>If you see “hi there”, that means  is being used! Hooray!</li><li>Otherwise remove it and try the same thing with </li><li>You can also try  if the first two options don’t work.</li></ol><p>(there are a lot of elaborate flow charts out there that explain how bash\ndecides which config file to use but IMO it’s not worth it and just testing is\nthe fastest way to be sure)</p><h3>step 3: figure out which directory to add</h3><p>Let’s say that you’re trying to install and run a program called \nand it doesn’t work, like this:</p><pre><code>$ npm install -g http-server\n$ http-server\nbash: http-server: command not found\n</code></pre><p>How do you find what directory  is in? Honestly in general this is\nnot that easy – often the answer is something like “it depends on how npm is\nconfigured”. A few ideas:</p><ul><li>Often when setting up a new installer (like , , , etc),\nwhen you first set it up it’ll print out some directions about how to update\nyour PATH. So if you’re paying attention you can get the directions then.</li><li>Sometimes installers will automatically update your shell’s config file\nto update your  for you</li><li>Sometimes just Googling “where does npm install things?” will turn up the\nanswer</li><li>Some tools have a subcommand that tells you where they’re configured to\ninstall things, like:\n<ul><li>Node/npm:  (then append )</li><li>Go:  (then append )</li><li>asdf: <code>asdf info | grep ASDF_DIR</code> (then append  and )</li></ul></li></ul><h3>step 3.1: double check it’s the right directory</h3><p>Once you’ve found a directory you think might be the right one, make sure it’s\nactually correct! For example, I found out that on my machine,  is\nin . I can make sure that it’s the right directory by trying to\nrun the program  in that directory like this:</p><pre><code>$ ~/.npm-global/bin/http-server\nStarting up http-server, serving ./public\n</code></pre><p>It worked! Now that you know what directory you need to add to your ,\nlet’s move to the next step!</p><h3>step 4: edit your shell config</h3><p>Now we have the 2 critical pieces of information we need:</p><ol><li>Which directory you’re trying to add to your PATH (like  )</li><li>Where your shell’s config is (like , , or <code>~/.config/fish/config.fish</code>)</li></ol><p>Now what you need to add depends on your shell:</p><p>Open your shell’s config file, and add a line like this:</p><pre><code>export PATH=$PATH:~/.npm-global/bin/\n</code></pre><p>(obviously replace  with the actual directory you’re trying to add)</p><p>You can do the same thing as in bash, but zsh also has some slightly fancier\nsyntax you can use if you prefer:</p><pre><code>path=(\n  $path\n  ~/.npm-global/bin\n)\n</code></pre><p>In fish, the syntax is different:</p><pre><code>set PATH $PATH ~/.npm-global/bin\n</code></pre><p>(in fish you can also use , some notes on that <a href=\"https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/#a-note-on-fish-add-path\">further down</a>)</p><h3>step 5: restart your shell</h3><p>Now, an extremely important step: updating your shell’s config won’t take\neffect if you don’t restart it!</p><ol><li>open a new terminal (or terminal tab), and maybe close the old one so you don’t get confused</li><li>Run  to start a new shell (or  if you’re using zsh, or  if you’re using fish)</li></ol><p>I’ve found that both of these usually work fine.</p><p>And you should be done! Try running the program you were trying to run and\nhopefully it works now.</p><p>If not, here are a couple of problems that you might run into:</p><h3>problem 1: it ran the wrong program</h3><p>If the wrong  of a is program running, you might need to add the\ndirectory to the  of your PATH instead of the end.</p><p>For example, on my system I have two versions of  installed, which I\ncan see by running :</p><pre><code>$ which -a python3\n/usr/bin/python3\n/opt/homebrew/bin/python3\n</code></pre><p>The one your shell will use is the .</p><p>If you want to use the Homebrew version, you need to add that directory\n() to the  of your PATH instead, by putting this in\nyour shell’s config file (it’s  instead of the usual )</p><pre><code>export PATH=/opt/homebrew/bin/:$PATH\n</code></pre><pre><code>set PATH ~/.cargo/bin $PATH\n</code></pre><h3>problem 2: the program isn’t being run from your shell</h3><p>All of these directions only work if you’re running the program . If you’re running the program from an IDE, from a GUI, in a cron job,\nor some other way, you’ll need to add the directory to your PATH in a different\nway, and the exact details might depend on the situation.</p><ul><li>use the full path to the program you’re running, like <code>/home/bork/bin/my-program</code></li><li>put the full PATH you want as the first line of your crontab (something like\nPATH=/bin:/usr/bin:/usr/local/bin:….). You can get the full PATH you’re\nusing in your shell by running .</li></ul><p>I’m honestly not sure how to handle it in an IDE/GUI because I haven’t run into\nthat in a long time, will add directions here if someone points me in the right\ndirection.</p><h3>problem 3: duplicate  entries making it harder to debug</h3><p>If you edit your path and start a new shell by running  (or , or\n), you’ll often end up with duplicate  entries, because the shell\nkeeps adding new things to your  every time you start your shell.</p><p>Personally I don’t think I’ve run into a situation where this kind of\nduplication breaks anything, but the duplicates can make it harder to debug\nwhat’s going on with your  if you’re trying to understand its contents.</p><p>Some ways you could deal with this:</p><ol><li>If you’re debugging your , open a new terminal to do it in so you get\na “fresh” state. This should avoid the duplication.</li><li>Deduplicate your  at the end of your shell’s config  (for example in\nzsh apparently you can do this with )</li><li>Check that the directory isn’t already in your  when adding it (for\nexample in fish I believe you can do this with <code>fish_add_path --path /some/directory</code>)</li></ol><p>How to deduplicate your  is shell-specific and there isn’t always a\nbuilt in way to do it so you’ll need to look up how to accomplish it in your\nshell.</p><h3>problem 4: losing your history after updating your </h3><p>Here’s a situation that’s easy to get into in bash or zsh:</p><ol><li>Run  to reload your config</li><li>Press the up arrow a couple of times to rerun the failed command (or open a new terminal)</li><li>The failed command isn’t in your history! Why not?</li></ol><p>This happens because in bash, by default, history is not saved until you exit\nthe shell.</p><p>Some options for fixing this:</p><ul><li>Instead of running  to reload your config, run  (or\n in zsh). This will reload the config inside your current\nsession.</li><li>Configure your shell to continuously save your history instead of only saving\nthe history when the shell exits. (How to do this depends on whether you’re\nusing bash or zsh, the history options in zsh are a bit complicated and I’m\nnot exactly sure what the best way is)</li></ul><p>When you install  (Rust’s installer) for the first time, it gives you\nthese instructions for how to set up your PATH, which don’t mention a specific\ndirectory at all.</p><pre><code>This is usually done by running one of the following (note the leading DOT):\n\n. \"$HOME/.cargo/env\"        \t# For sh/bash/zsh/ash/dash/pdksh\nsource \"$HOME/.cargo/env.fish\"  # For fish\n</code></pre><p>The idea is that you add that line to your shell’s config, and their script\nautomatically sets up your  (and potentially other things) for you.</p><p>This is pretty common (for example <a href=\"https://github.com/Homebrew/install/blob/deacfa6a6e62e5f4002baf9e1fac7a96e9aa5d41/install.sh#L1072-L1087\">Homebrew</a> suggests you eval ), and there are\ntwo ways to approach this:</p><ol><li>Just do what the tool suggests (like adding  to your shell’s config)</li><li>Figure out which directories the script they’re telling you to run would add\nto your PATH, and then add those manually. Here’s how I’d do that:\n<ul><li>Run  in my shell (or the fish version if using fish)</li><li>Run <code>echo \"$PATH\" | tr ':' '\\n' | grep cargo</code> to figure out which directories it added</li><li>See that it says  and shorten that to </li><li>Add the directory  to PATH (with the directions in this post)</li></ul></li></ol><p>I don’t think there’s anything wrong with doing what the tool suggests (it\nmight be the “best way”!), but personally I usually use the second approach\nbecause I prefer knowing exactly what configuration I’m changing.</p><p>fish has a handy function called  that you can run to add a directory to your  like this:</p><pre><code>fish_add_path /some/directory\n</code></pre><p>This is cool (it’s such a simple command!) but I’ve stopped using it for a couple of reasons:</p><ol><li>Sometimes  will update the  for every session in the\nfuture (with a “universal variable”) and sometimes it will update the \njust for the current session and it’s hard for me to tell which one it will\ndo. In theory the docs explain this but I could not understand them.</li></ol><p>Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if\nyou there are other major gotchas that have tripped you up when adding a\ndirectory to your PATH, or if you have questions about this post!</p>","contentLength":9872,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GenAI Patterns: Reranker","url":"https://martinfowler.com/articles/gen-ai-patterns/#reranker","date":1739441760,"author":"Martin Fowler","guid":510,"unread":true,"content":"<p>LLMs struggle with large amounts of context. <b>Bharani\n      Subramaniam</b> and I explain how to mitigate this common RAG\n      problem with a <a href=\"https://martinfowler.com/articles/gen-ai-patterns/#reranker\">Reranker</a> which takes the document\n      fragments from the retriever, and ranks them according to their usefulness.</p>","contentLength":253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why young parents should focus on building trust with their kids","url":"https://desunit.com/blog/marshmallow-test-and-parenting/","date":1739430767,"author":"desunit","guid":242,"unread":true,"content":"<p>Today, when my 1.9-year-old daughter tried to grab and eat an unwashed orange, I asked her to give it to me so I could wash and peel it if she could wait for a moment. She did. It got me thinking about the marshmallow experiment, where the idea was that patience equals success. But … it wasn’t the whole story.</p><p>The marshmallow experiment is famous: a little kid in a room staring at a marshmallow. If they wait 15 minutes, they get two marshmallows instead of just one. Some kids would poke the marshmallow, lick it, or just gobble it up. Others found clever ways to distract themselves – singing, closing their eyes, even falling asleep. The results – children who waited supposedly went on to achieve higher scores in school and better life outcomes. The message was clear: if you can delay gratification, you’re set for life.&nbsp;But later studies revealed some serious holes in that conclusion.</p><p>First of all, it’s not just about willpower. A follow-up study showed that kids from stable, reliable homes were much more likely to wait than kids from unpredictable ones. If you’re a kid and the adults in your life constantly break promises, why would you trust them this time? Why wait for the second marshmallow if history tells you it might not show up? Waiting isn’t a character trait; it’s a strategy. And strategies are shaped by experience.</p><p>Kids are like tiny detectives, watching and learning from every single thing we do. My toddler loves routines – the same bedtime story, the same morning/evening ritual. He thrives on knowing what’s next. So when I tell her to wait and then actually deliver, I’m teaching her something important: trust. But if I say “five more minutes” and don’t follow through, I’m sending a different message – that waiting might not be worth it.</p><p>Back to the marshmallow test. Another big revelation came when researchers looked at economic backgrounds. Kids from wealthier families waited longer than kids from low-income families. Not because they had more self-control, but because their environment made waiting feel safer. If you grow up knowing there will always be food on the table, waiting for an extra marshmallow isn’t a big deal. But if your life is more uncertain, grabbing what you can when you can make total sense.&nbsp;It’s a survival skill, not a lack of discipline.</p><p>This reminds me of a moment last week. My daughter asked me to swing her in my arms, but I was busy and promised her we’d do it once I finished my work. She switched to another task and waited. When I finally asked, “Would you like to have fun now?” she cheerfully screamed, “Yes!” Would she have trusted me if I forgot about it? Probably not.</p><p>Kids crave predictability. That’s why they ask for the same bedtime story 300 nights in a row. It’s why they get upset if the routine changes without warning. Predictability equals safety. And safety builds trust. When they trust us, they’re willing to wait, to try, to take risks.</p><p>So, here’s where the marshmallow test gets it wrong: it’s not just about the kid’s ability to wait; it’s about the adult’s ability to create an environment worth waiting in. If we, as parents, and teachers, aren’t consistent, why would a kid believe us when we say, “Just hold on, something better is coming”?</p><p>Another thing I’ve noticed is how much modeling matters. My daughter watches everything I do. If I tell her to wait and then lose my patience two seconds later because the internet is slow, what’s the lesson there? Waiting is for other people? The best way to teach patience is to live it – easier said than done …</p><p>The marshmallow test also doesn’t account for cultural differences. In some cultures, waiting is baked into daily life. Think about Japan, where kids are often taught to wait quietly for meals or gifts. Compare that to the US, where instant gratification is practically a way of life. These cultural norms shape how kids approach situations like the marshmallow test. It’s not just about personality; it’s about the world they live in.</p><p>In 2012, researchers added a modification to the marshmallow experiment. Before the test, kids were split into two groups. One group experienced a broken promise (they were told they’d get crayons to draw with, but never did). The other group experienced a kept promise (they actually got the crayons). Then came the marshmallow test. Which group waited longer? The kids who saw promises kept. The trust made all the difference.</p><p>So what’s the takeaway here? It’s simple, really: as parents, we set the tone. Our actions, promises, and reliability shape how our kids see the world. Are we building an environment where they feel safe enough to wait? Or are we teaching them that they need to grab what they can, when they can?</p><p>The marshmallow test highlights how our actions shape our kids’ lives. Small gestures – like keeping promises, sticking to “just five more minutes,” or reading bedtime stories – build trust and security. These moments might even help them wait for their own second marshmallow – or decide they don’t need to. Ultimately, it’s about the environment we are creating for them. And who knows, maybe one day they will pass their own version of the marshmallow test – not because they have to, but because they know they can.</p>","contentLength":5341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43033463"},{"title":"Sandbox environments: Creating efficient and isolated testing realms","url":"https://www.youtube.com/watch?v=fh7-lQVmX-o","date":1739426433,"author":"CNCF [Cloud Native Computing Foundation]","guid":388,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fh7-lQVmX-o?version=3","enclosureMime":"","commentsUrl":null},{"title":"KitOps: AI Model Packaging Standards","url":"https://www.youtube.com/watch?v=1TD-e_wVe4Q","date":1739426400,"author":"CNCF [Cloud Native Computing Foundation]","guid":387,"unread":true,"content":"<article>Chat with us on Discord:  https://discord.gg/Tapeh8agYy\n\nCheck out our repos:\nKitOps      https://github.com/jozu-ai/kitops\nPyKitOps Python Library  https://github.com/jozu-ai/pykitops\nKitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin</article>","contentLength":253,"flags":null,"enclosureUrl":"https://www.youtube.com/v/1TD-e_wVe4Q?version=3","enclosureMime":"","commentsUrl":null},{"title":"TeX and Typst: Layout Models (2024)","url":"https://laurmaedje.github.io/posts/layout-models/","date":1739421249,"author":"fngjdflmdflg","guid":204,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43032697"},{"title":"Extensible Wasm Applications with Go","url":"https://go.dev/blog/wasmexport","date":1739404800,"author":"Cherry Mui","guid":182,"unread":true,"content":"<p>\n      Cherry Mui\n      13 February 2025\n      </p><p>Go 1.24 enhances its WebAssembly (Wasm) capabilities with the\naddition of the  directive and the ability to build a reactor\nfor WebAssembly System Interface (WASI).\nThese features enable Go developers to export Go functions to Wasm,\nfacilitating better integration with Wasm hosts and expanding the possibilities\nfor Go-based Wasm applications.</p><h2>WebAssembly and the WebAssembly System Interface</h2><p><a href=\"https://webassembly.org/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly (Wasm)</a> is a binary instruction format\nthat was initially created for web browsers, providing the execution of\nhigh-performance, low-level code at speeds approaching native performance.\nSince then, Wasm’s utility has expanded, and it is now used in various\nenvironments beyond the browser.\nNotably, cloud providers offer services that directly execute Wasm\nexecutables, taking advantage of the\n<a href=\"https://wasi.dev/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly System Interface (WASI)</a> system call API.\nWASI allows these executables to interact with system resources.</p><p>Go first added support for compiling to Wasm in the 1.11 release, through the\n port.\nGo 1.21 added a new port targeting the WASI preview 1 syscall API through the\nnew  port.</p><h2>Exporting Go Functions to Wasm with </h2><p>Go 1.24 introduces a new compiler directive, , which allows\ndevelopers to export Go functions to be called from outside of the\nWasm module, typically from a host application that runs the Wasm runtime.\nThis directive instructs the compiler to make the annotated function available\nas a Wasm <a href=\"https://webassembly.github.io/spec/core/valid/modules.html?highlight=export#exports\" rel=\"noreferrer\" target=\"_blank\">export</a>\nin the resulting Wasm binary.</p><p>To use the  directive, simply add it to a function definition:</p><pre><code>//go:wasmexport add\nfunc add(a, b int32) int32 { return a + b }\n</code></pre><p>With this, the Wasm module will have an exported function named  that\ncan be called from the host.</p><p>This is analogous to the <a href=\"https://go.dev/cmd/cgo#hdr-C_references_to_Go\">cgo  directive</a>,\nwhich makes the function available to be called from C,\nthough  uses a different, simpler mechanism.</p><p>A WASI reactor is a WebAssembly module that operates continuously, and\ncan be called upon multiple times to react on events or requests.\nUnlike a “command” module, which terminates after its main function finishes,\na reactor instance remains live after initialization, and its exports remain\naccessible.</p><p>With Go 1.24, one can build a WASI reactor with the  build\nflag.</p><pre><code>$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm\n</code></pre><p>The build flag signals to the linker not to generate the  function\n(the entry point for a command module), and instead generate an\n function, which performs runtime and package initialization,\nalong with any exported functions and their dependencies.\nThe  function must be called before any other exported functions.\nThe  function will not be automatically invoked.</p><p>To use a WASI reactor, the host application first initializes it by calling\n, then simply invoke the exported functions.\nHere is an example using <a href=\"https://wazero.io/\" rel=\"noreferrer\" target=\"_blank\">Wazero</a>, a Go-based Wasm runtime\nimplementation:</p><pre><code>// Create a Wasm runtime, set up WASI.\nr := wazero.NewRuntime(ctx)\ndefer r.Close(ctx)\nwasi_snapshot_preview1.MustInstantiate(ctx, r)\n\n// Configure the module to initialize the reactor.\nconfig := wazero.NewModuleConfig().WithStartFunctions(\"_initialize\")\n\n// Instantiate the module.\nwasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)\n\n// Call the exported function.\nfn := wasmModule.ExportedFunction(\"add\")\nvar a, b int32 = 1, 2\nres, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))\nc := api.DecodeI32(res[0])\nfmt.Printf(\"add(%d, %d) = %d\\n\", a, b, c)\n\n// The instance is still alive. We can call the function again.\nres, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))\nfmt.Printf(\"add(%d, %d) = %d\\n\", b, c, api.DecodeI32(res[0]))\n</code></pre><p>The  directive and the reactor build mode allow applications to\nbe extended by calling into Go-based Wasm code.\nThis is particularly valuable for applications that have adopted Wasm as a\nplugin or extension mechanism with well-defined interfaces.\nBy exporting Go functions, applications can leverage the Go Wasm modules to\nprovide functionality without needing to recompile the entire application.\nFurthermore, building as a reactor ensures that the exported functions can be\ncalled multiple times without requiring reinitialization, making it suitable\nfor long-running applications or services.</p><h2>Supporting rich types between the host and the client</h2><p>Go 1.24 also relaxes the constraints on types that can be used as input and\nresult parameters with  functions.\nFor example, one can pass a bool, a string, a pointer to an , or a\npointer to a struct which embeds  and contains supported\nfield types\n(see the <a href=\"https://go.dev/cmd/compile#hdr-WebAssembly_Directives\">documentation</a> for detail).\nThis allows Go Wasm applications to be written in a more natural and ergonomic\nway, and removes some unnecessary type conversions.</p><p>While Go 1.24 has made significant enhancements to its Wasm capabilities,\nthere are still some notable limitations.</p><p>Wasm is a single-threaded architecture with no parallelism.\nA  function can spawn new goroutines.\nBut if a function creates a background goroutine, it will not continue\nexecuting when the  function returns, until calling back into\nthe Go-based Wasm module.</p><p>While some type restrictions have been relaxed in Go 1.24, there are still\nlimitations on the types that can be used with  and\n functions.\nDue to the unfortunate mismatch between the 64-bit architecture of the client\nand the 32-bit architecture of the host, it is not possible to pass pointers in\nmemory.\nFor example, a  function cannot take a pointer to a struct that\ncontains a pointer-typed field.</p><p>The addition of the ability to build a WASI reactor and export Go functions to\nWasm in Go 1.24 represent a significant step forward for Go’s WebAssembly\ncapabilities.\nThese features empower developers to create more versatile and powerful Go-based\nWasm applications, opening up new possibilities for Go in the Wasm ecosystem.</p>","contentLength":5777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}