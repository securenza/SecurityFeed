{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"HackerNews","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":27,"items":[{"title":"Samsung Q990D unresponsive after 1020 firmware update","url":"https://us.community.samsung.com/t5/Home-Theater/Samsung-Q990D-unresponsive-after-1020-firmware-update/td-p/3168571","date":1741968604,"author":"ftufek","guid":115,"unread":true,"content":"<p>Yes, I have the same issue. Soundbar is not responding to neither remote controller or buttons on the soundbar. Looks like it glitched and there is nothing I can do, even the reset combination of + - for 5 sec doesn’t work</p>","contentLength":224,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43364016"},{"title":"A look at Firefox forks","url":"https://lwn.net/Articles/1012453/","date":1741955104,"author":"sohkamyung","guid":114,"unread":true,"content":"<blockquote><p>\nLWN.net is a subscriber-supported publication; we rely on subscribers\n       to keep the entire operation going.  Please help out by <a href=\"https://lwn.net/Promo/nst-nag4/subscribe\">buying a subscription</a> and keeping LWN on the\n       net.\n</p></blockquote><div>\n           By March 4, 2025</div><p>Mozilla's actions have been rubbing many <a href=\"https://en.wikipedia.org/wiki/Firefox\">Firefox</a> fans the\nwrong way as of late, and inspiring them to look for alternatives.\nThere are many choices for users who are looking for a browser that\nisn't part of the Chrome monoculture but is full-featured and suitable\nfor day-to-day use. For those who are willing to stay in the Firefox\n\"family\" there are a number of good options that have taken vastly\ndifferent approaches. This includes <a href=\"https://www.gnu.org/software/gnuzilla/\">GNU IceCat</a>, <a href=\"https://floorp.app/en\">Floorp</a>, <a href=\"https://librewolf.net/\">LibreWolf</a>, and <a href=\"https://zen-browser.app/\">Zen</a>.</p><p>Mozilla has been disappointing a lot of Firefox users for years,\nbut it seems the pace is accelerating. Its <a href=\"https://lwn.net/Articles/1010922/\">announcement</a> on\nFebruary&nbsp;19 that it needs to \"diversify\" beyond Firefox did not\ninspire confidence, and it annoyed many who would like to see Mozilla\ngo all-in on its flagship browser (and increase its market share)\nrather than chasing AI or dabbling in advertising. But a recent and\nmore alarming example is its introduction of <a href=\"https://lwn.net/Articles/1012430/\">terms of use</a> for the\nbrowser and the removal of its pledge not to sell users' personal\ndata. Though it has <a href=\"https://blog.mozilla.org/en/products/firefox/update-on-terms-of-use/\">backpedaled\nsomewhat</a> since, and rewritten its terms of use, the damage has been\ndone.</p><blockquote>\nLove is a snowmobile racing across the tundra and then suddenly it\nflips over, pinning you underneath. At night, the ice weasels come.\n</blockquote><p>The GNU project also adopted the name IceWeasel for the GNUzilla\nproject—basically Mozilla source code with any <a href=\"https://lists.gnu.org/archive/html/bug-gnuzilla/2006-09/msg00005.html\">non-free\ncode</a>, such as the <a href=\"https://en.wikipedia.org/wiki/Adobe_Flash_Player\">Adobe Flash\nPlayer</a>, stripped out. In&nbsp;2007, Karl Berry <a href=\"https://lists.gnu.org/archive/html/bug-gnuzilla/2007-09/msg00004.html\">announced</a>\nthat GNUzilla would be adopting the name IceCat for its version\n\"<q>not because we have anything against weasels</q>\" but to avoid\nconfusion with Debian's version.</p><p>IceCat has the distinction of being the oldest Firefox fork still\nin development. Ray Dryden <a href=\"https://savannah.nongnu.org/task/?func=detailitem&amp;item_id=4529\">applied</a>\nfor GNUzilla to become part of the GNU Project in August&nbsp;2005,\nand test releases based on Firefox&nbsp;1.5.0 were available later\nthat year. IceCat, as with all of the forks covered in this article,\nis available under the Mozilla Public License (MPL) 2.0. However, the\nscripts and other tools used to create an IceCat release from Firefox\nare licensed under the GPLv3.</p><p>GNUzilla does not distribute binaries of IceCat. The project\nrecommends using <a href=\"https://guix.gnu.org/\">GNU Guix</a> to\ninstall IceCat on x86_64 Linux systems, and also makes its scripts\navailable in its <a href=\"https://savannah.gnu.org/git/?group=gnuzilla\">Git repository</a> to\ncompile IceCat from Firefox's extended-support releases (ESRs). It\nmay, however, also be packaged for a user's favorite Linux\ndistribution. Fedora&nbsp;41, for example, currently has\nIceCat&nbsp;115.20.0esr—which is based on <a href=\"https://www.mozilla.org/en-US/firefox/115.20.0/releasenotes/\">Firefox&nbsp;115.20.0</a>;\nboth were released on February&nbsp;4.</p><p>Current-day IceCat has several changes that distinguish it from\nFirefox. The most immediately obvious is its use of the <a href=\"https://www.gnu.org/software/librejs/\">LibreJS</a> add-on to\nblock \"<q>nonfree nontrivial JavaScript while allowing JavaScript that\nis free and/or trivial</q>\". In practice, this means that a significant \nnumber of sites will not work unless the user adds exceptions for\nthe JavaScript used by the site. Users can choose to add exceptions for\nindividual scripts blocked by LibreJS or to add an exception for the\nentire site. Even LWN, which uses a minimal amount of JavaScript,\nhas scripts that are blocked by LibreJS.</p><p>IceCat includes the <a href=\"https://jshelter.org/\">JShelter</a>\nextension, which attempts to block not just malware, but browser <a href=\"https://en.wikipedia.org/wiki/Device_fingerprint\">fingerprinting</a>\nand user tracking as well. It modifies the JavaScript environment that\nis available to web pages to try to confuse fingerprinters and make it\nmore difficult to carry out attacks using JavaScript. It may block\nAPIs or return fake values to thwart these attempts. Like LibreJS, it\ncan be <a href=\"https://jshelter.org/levels/\">modified or turned off\nentirely</a> for specific sites. There is a <a href=\"https://arxiv.org/abs/2204.01392\">paper</a> from&nbsp;2022 that\nexplains the extension's approach in great detail, and an <a href=\"https://jshelter.org/faq/\">extensive FAQ</a> that may be of use\nin troubleshooting interactions between JShelter and web sites.</p><p>In a similar vein, IceCat includes a fork of\nthe <a href=\"https://addons.mozilla.org/en-US/firefox/addon/tprb/\">Third-party\nRequest Blocker</a> extension that (as the name implies) blocks\nconnections to third-party resources without user consent. It is a\nlittle concerning that the <a href=\"https://gnuzilla.gnu.org/extension.php?id=2632429\">page</a>\ndescribing the extension describes it as \"<q>seemingly maintained by\n'sw'</q>\", and its last update was in March&nbsp;2020. The home page\nlisted for the extension is no longer available. Despite the lag in\ndevelopment, it still seems to be working and blocking plenty of\nthird-party requests. A visit to a site like <a href=\"https://www.theguardian.com/us\">The Guardian</a>, for instance, shows\nseven sites blocked. As the screenshot shows, site layout and images\nare often affected by IceCat's default settings. Usually the sites are\nstill usable, but far less aesthetically pleasing.</p><p>One thing that worked well for me was to enable just enough\nto see the page text and then use the reader view to read a site's\narticles or other content. (Sadly, none of the forks offer a \"browse\neverything in reader view by default\" option.)</p><p>In all, IceCat ships with eight extensions that either attempt to\nenhance user privacy, block non-free software, or unbreak sites that\nare affected by its other extensions. It includes a \"LibreJS/USPS\ncompatibility\" plugin to offer an alternative shipping calculator for\n<a href=\"https://usps.com/\">the US Postal Service</a> site as well as\nan extension to replace JavaScript blocked by LibreJS on the <a href=\"https://en.wikipedia.org/wiki/Library_Genesis\">Library\nGenesis</a> sites.</p><p>The project has an extension-finder service called <a href=\"https://gnuzilla.gnu.org/\">Mozzarella</a>, which (of course)\nonly lists extensions that are free software. However, the extensions\nmay be outdated compared to their counterparts listed in Firefox's\nadd-on catalog. For example, the Privacy Badger extension in the <a href=\"https://gnuzilla.gnu.org/extension.php?id=506646\">Mozzarella\ncatalog</a> was last updated in June&nbsp;2023. The <a href=\"https://addons.mozilla.org/en-US/firefox/addon/privacy-badger17/\">Firefox\ncatalog version</a> was last updated on January&nbsp;29,&nbsp;2025.</p><p>Right now, three people are listed as maintainers for GNUzilla:\nRuben Rodriguez, Amin Bandali, and Mark H. Weaver. The development\nmailing lists are a bit on the quiet side. The <a href=\"https://lists.gnu.org/archive/html/gnuzilla-dev/\">last-archived</a>\nconversation currently for the gnuzilla-dev list is from\nAugust&nbsp;2024. The <a href=\"https://lists.gnu.org/archive/html/bug-gnuzilla/\">bug-gnuzilla</a>\nlist is a little more lively—its last activity was in\nDecember&nbsp;2024.</p><p>IceCat is probably a good choice for folks who are more concerned\nwith the free software ethos and privacy than with\nfunctionality.</p><p>The Floorp project is a much\nnewer entrant. It is developed by a <a href=\"https://docs.floorp.app/docs/other/faq#what-is-ablaze\">community\nof Japanese students</a> called <a href=\"https://ablaze.one/en/\">Ablaze</a>. Development is <a href=\"https://github.com/Floorp-Projects/Floorp\">hosted on GitHub</a>,\nand the project <a href=\"https://github.com/sponsors/Ablaze-MIRAI\">solicits donations</a>\nvia GitHub donations. According to its donations page, donors who\ncontribute at the $100 level may submit ads to feature in the new tab\npage—but the ads, which are displayed as shortcuts with a\n\"sponsored\" label, can be turned off in the settings. I've been unable\nto find any information about the project governance or legal\nstructure of Ablaze.</p><p>Its <a href=\"https://docs.floorp.app/docs/other/contributors/\">contributors</a>\npage lists seven primary maintainers and 39 code contributors, as well as\nmany people who have contributed to its language packs and\ntranslations, or who maintain packages. Floorp does not offer native\npackages for Linux distributions, but it does provide a <a href=\"https://flathub.org/apps/one.ablaze.floorp\">Flatpak</a> via\nFlathub and <a href=\"https://floorp.app/en/download?platform=linux\">precompiled\nreleases for x86_64 and ARM64</a>.</p><p>Originally Floorp was based on Chromium but switched to Firefox in\nearly&nbsp;2022. The first Firefox-derived version was Floorp v7 (<a href=\"https://blog.ablaze.one/786/2021-11-11/\">announcement</a> in\nJapanese), and it was based on the Firefox rapid releases, but the\nproject switched to the ESR releases as their base with v8. The most\nrecent release, version&nbsp;11.23.1, was <a href=\"https://blog.ablaze.one/4678/2025-02-11/\">announced</a> on\nFebruary&nbsp;15, and is based on (according to\n) the Firefox&nbsp;ESR&nbsp;128.8.0 release,\nwhich came out on March&nbsp;4. It would\nbe nice if the project were more explicit in its release notes about which\nversion of Firefox a release was based on. This is not merely for\ncuriosity's sake—it would help users track whether Floorp was\nreceiving the most recent security updates. The project has said that\nit <a href=\"https://blog.ablaze.one/4683/2025-02-20/\">plans</a> to\nmove back to the rapid release versions of Firefox with v12, which is\ncurrently in beta.</p><p>The project promises \"<q>strong tracking protection</q>\" and that\nit does not track users or have any affiliation with advertising\ncompanies. However, the project does not give details on how its\ntracking protection differs from Firefox's. It still uses Google as\nits default search engine and includes the <a href=\"https://www.mozilla.org/en-US/firefox/features/sync/\">Firefox\nbrowser sync</a> feature. It also uses Mozilla's add-ons repository,\nand should work with most Firefox add-ons\nthat are compatible with the corresponding Firefox version.</p><p>Floorp does have a number of interesting features and enhancements\nthat may tempt users. It has a dual-sidebar layout that allows users\nto access bookmarks, history, and other tools on the left-hand side,\nwhile the right-hand has the Web Apps panel. Users can add web sites\nto open in the Web Apps panel, which can be useful while (for example)\ndoing research for an article while keeping a version of the article\nopen in the panel.</p><p>In addition to the Apps panel, Floorp has a split-view feature that\nlets users open two pages side-by-side by selecting a tab and clicking\n\"Split this Tab\". Each split has its own history and URL bar. Floorp's\nlayout is great for wide-screen monitors, and I like the ability to\nopen sites in split view rather than juggling multiple browser\nwindows.</p><p>Another interesting inclusion in Floorp is its <a href=\"https://docs.floorp.app/docs/features/how-to-use-workspaces/\">Workspace\nfeature</a>. This allows users to group tabs by categories like\n\"work\", \"comics\", \"shopping\", or whatever makes sense for the users'\nbrowsing habits. I've found this useful for working on projects and\nstories for LWN—I might have a dozen tabs open for a specific\nstory, which I can group into a single workspace. Workspaces can also\nbe assigned to Firefox's <a href=\"https://support.mozilla.org/en-US/kb/containers#firefox\">multi-account\ncontainers</a>. For example, a user might want to log in to the same\nsite using different accounts—without having to sign in and out\nrepeatedly. Combining the workspace and multi-account containers can\nbe useful in a number of scenarios.</p><p>Firefox's tabs have seen little feature advancement in the past few\nyears. Floorp adds a few much-needed enhancements here, allowing users\nto move the tab bar to the bottom of the window, use a multi-row tab\nbar, and even a vertical tab bar. However, the Floorp implementation\nof the vertical tab bar will go away in v12, now that Mozilla has\n<a href=\"https://www.mozilla.org/en-US/firefox/136.0/releasenotes/\">finally\nadded</a> vertical tabs in Firefox 136.0.</p><p>Overall, Floorp is an interesting project with some nice\nenhancements to the Firefox UI. However, the development roadmap seems\na bit more haphazard than I would like—switching back and forth\nbetween Firefox rapid release and ESRs, for example. That may not\ndissuade other folks, though.</p><p>The LibreWolf project got its start in&nbsp;2020. Its focus is primarily\naround privacy, security, and the removal of \"\"\nfeatures, such as telemetry and DRM, from Firefox. It lists seven core\ncontributors on its home page and points to its <a href=\"https://matrix.to/#/#librewolf:matrix.org\">Matrix room</a> for\ndevelopment discussions. Its development is hosted on <a href=\"https://codeberg.org/librewolf\">Codeberg</a>.</p><p>LibreWolf has the normal configuration options one would expect for\na Firefox fork, but it also has the option of using a special\nconfiguration file called  to set\npreferences that can take effect across multiple profiles rather than\nhaving to tweak the configuration for each profile. It also makes\npreferences easy to back up and move to a new machine. The <a href=\"https://librewolf.net/docs/settings/\">documentation</a> explains\nwhere to find this file, depending on the installation method, and offers\nseveral suggestions for possible preference changes.</p><p>LibreWolf is mostly notable for what it  have\nrather than what it does. That is, it removes other features from\nFirefox that have not been well-received by many users such as <a href=\"https://www.mozilla.org/en-US/firefox/pocket/\">Pocket</a>\nintegration, telemetry, and more. Firefox Sync is disabled by default\nbut it can be enabled in settings.</p><p>LibreWolf does include the <a href=\"https://ublockorigin.com/\">uBlock Origin</a> add-blocker add-on\nas part of its standard installation. It should be noted that uBlock\nOrigin is being <a href=\"https://www.theverge.com/news/622953/google-chrome-extensions-ublock-origin-disabled-manifest-v3\">disabled\nfor Chrome users</a> as Google <a href=\"https://blog.chromium.org/2024/05/manifest-v2-phase-out-begins.html\">phases\nout support</a> for the <a href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/manifest.json\">WebExtension\nAPI</a> V2 in favor of V3, which will curtail features that uBlock\nOrigin and other add-ons require to function. To its credit, Mozilla\nhas <a href=\"https://blog.mozilla.org/en/products/firefox/firefox-manifest-v3-adblockers/\">committed</a>\nto continuing its support for Manifest V2 and V3. LWN <a href=\"https://lwn.net/Articles/879063/\">covered</a> Manifest V3 and\nits impact on content blockers in 2021.</p><p>For the most part, users would be hard-pressed to spot many\ndifferences between LibreWolf and Firefox at first (or second) glance,\nso a screen shot of LibreWolf seemed a bit unnecessary. That approach\nis likely to appeal to many users who are uneasy with things like\ntelemetry and Pocket, but don't want an entirely new browsing\nexperience.</p><p>The Zen browser project is the most recent entrant. Its development\nbegan last year with <a href=\"https://www.reddit.com/r/firefox/comments/1bsm9lu/im_doing_a_new_firefox_based_browser_im_focusing/\">an\nannouncement on Reddit</a>. It is currently in beta, with its most\nrecent version, <a href=\"https://github.com/zen-browser/desktop/releases/tag/1.8.2b\">1.8.2b</a>\nbased on Firefox&nbsp;135.0.1, released on\nFebruary&nbsp;25. Kudos to the Zen project, by the way, for\nproudly including the Firefox version alongside the project version in\nits \"About\" dialog—information that literally every other\nFirefox fork seems intent on hiding. Zen <a href=\"https://zen-browser.app/about/\">lists</a> 12 people in the main\nproject team, and about&nbsp;90 contributors to the\nbrowser. Development for Zen is <a href=\"https://github.com/zen-browser/desktop\">hosted on GitHub</a>,\nand discussion takes place on <a href=\"https://discord.com/invite/zen-browser\">Discord</a> (link goes\nto a Discord invitation).</p><p>Like Floorp, the project <a href=\"https://zen-browser.app/donate/\">solicits donations</a> to\nassist with development, but little information seems to be available\nabout its governance or structure to provide transparency about how\nthe money is spent.</p><p>Unlike the other forks, it is not immediately obvious that Zen is\nan offshoot of Firefox. It does not look at all like the standard\nFirefox interface, even before users start customizing it. Even\nFloorp, which allows significant customization, still bears some\nresemblance to Firefox on first use. Zen sports a tab sidebar on the\nleft that blends the Workspace concept from Floorp and vertical tabs,\nwith a set of default bookmarks (\"Essentials\") as icons at the\ntop. The browser menu is located in the top-left corner, indicated by\na button with three dots. The window title bar is hidden and only\nappears if a user hovers the mouse at the top of the window for a few\nseconds.</p><p>While Zen looks modern and interesting, its sleek user interface\nand configurability comes at the cost of intuitive usability in some\ncases. For example, one might expect that setting Zen to light mode in\nthe \"Language and Appearance\" settings would also change the browser's\ninterface to light mode. It does not, as shown in the\nscreenshot. Instead, a user has to go to the \"Add-ons and Themes\"\nsettings to select a light theme. It would help a great deal if Zen's\n<a href=\"https://docs.zen-browser.app/user-manual/\">user guide</a>\nwere more complete, but it only has a little bit of documentation to\noffer at the moment. To be fair, it is still a beta project, so it may\nbe much improved by the time the Zen browser has its first stable\nrelease. For now, users will need to be ready to dig through Reddit\nand other forums for tips.</p><p>Features like glance, which pre-fetches a link and gives a preview\nof it before opening it in a new tab or window, are useful, but not at\nall obvious how to use, even if one is aware the feature exists. (On\nLinux, activate glance with .) Likewise, Zen's\nsplit-screen mode requires the user to select multiple tabs and then\nright-click to select \"Split Tab\". Rearranging the splits is also not\nintuitive. That said, the additional features are compelling if one is\nwilling to do some searching to figure things out.</p><p>The Zen interface can be customized extensively to suit individual\ntastes via the settings. If those options aren't enough, Zen has its\nown set of add-ons and extensions called <a href=\"https://zen-browser.app/mods/\">Mods</a> to modify the interface\nor add features. This ranges from a <a href=\"https://zen-browser.app/mods/80112b28-39e0-407c-8988-2290bc973b97/\">green-hued theme called Matcha</a> to <a href=\"https://zen-browser.app/mods/ab9b529c-63d6-48c0-a59a-4a407c5c3129/\">tweaks\nto further minimize the sidebar</a>. Most Firefox add-ons should work\nwith Zen as well, though some may clash with its user-interface\nchanges.</p><p>Currently, Zen isn't fully baked enough for me to consider\nswitching to it. Others may be more adventurous in their\nbrowsing habits than I am, though. I can say that it has stabilized\nsignificantly since I first tried it shortly after its first public\nrelease. The project does bear keeping an eye on, and the Mozilla\nfolks could do worse than to copy some of the ideas (and code) that\nthe project is experimenting with.</p><p>The Firefox fork rabbit hole is surprisingly deep. There are a few\nalternatives I chose not to try—but mention here for\ncompleteness—and probably a few that I've missed. The <a href=\"https://basilisk-browser.org/features.html\">Basilisk</a> project\nis a kind of retro-Firefox project that aims to retain technologies that\nFirefox has removed. This includes the deprecated <a href=\"https://en.wikipedia.org/wiki/NPAPI\">Netscape Plugin Application\nProgramming Interface</a> (NPAPI) plugin support, ALSA support on\nLinux, <a href=\"https://en.wikipedia.org/wiki/XUL\">XUL</a> extensions,\nand more.</p><p>The <a href=\"https://www.palemoon.org/\">Pale Moon</a> project is\nanother browser that has forked off of Mozilla Firefox code and no\nlonger tracks it directly. It uses\nthe <a href=\"https://www.palemoon.org/tech/goanna.shtml\">Goanna</a>\nfork of the Gecko rendering engine and still supports NPAPI\nplugins and XUL extensions. The project promises no telemetry or data\ngathering. It offers a somewhat nostalgic <a href=\"https://www.palemoon.org/screenshots.shtml\">look and feel</a>\nthat is similar to Firefox in the mid-2000s.</p><p>For those who pine for the days of the Netscape suite that included\nthe browser, mail client, HTML editor, IRC chat, and more, there is <a href=\"https://www.seamonkey-project.org/\">SeaMonkey</a>. The project\nuses code from Firefox and Thunderbird, though it is not directly\nbased on recent versions. According to its site, it backports security\nfixes from Firefox and Thunderbird ESRs that apply to SeaMonkey. The\nproject also maintains the Composer HTML editor and ChatZilla IRC\nclient that are no longer maintained by Mozilla. SeaMonkey is still\npackaged for a number of Linux distributions, and binaries are\navailable for Linux on x86_64 and x86 as a tarball. It might be a good\noption for users who are still using 32-bit x86 Linux systems.</p><h4>Still dependent on Mozilla</h4><p>Regardless which Firefox fork one chooses, it is important to\nremember the downsides. First and foremost, all of the forks are\ndependent on Mozilla to do the heavy lifting. The bulk of development\nis carried by Mozilla, the direction of Firefox is set by Mozilla, and\nchoosing to run a fork puts the user one step removed from security\nand bug fixes. This does not mean users shouldn't consider one\nof these forks, but they should be aware of the potential\ndownsides.</p><p>There is some precedent for soft forks displacing the original\nupstream. For example, the <a href=\"https://en.wikipedia.org/wiki/Go-oo\">Go-oo</a> fork of\nOpenOffice.org became LibreOffice after Oracle consumed Sun. That fork\nhas clearly overtaken OpenOffice.org in the Linux community as the\ngo-to desktop office suite and its development has <a href=\"https://lwn.net/Articles/729460/\">eclipsed</a> that of\nits counterpart Apache OpenOffice. Go-oo, of course, had corporate\nsupport as well as community support. For a Firefox fork to be truly\nindependent and sustainable, it would need a similar effort behind\nit. Thus far, no such movement has materialized.</p><p>A recent question on the LibreWolf issue tracker drives that point\nhome nicely. User \"kallisti5\" <a href=\"https://codeberg.org/librewolf/issues/issues/2252\">asked</a> if\nLibreWolf was prepared to fork Firefox \"<q>if Mozilla continues\nfarther down this path?</q>\" One of LibreWolf's contributors, \"ohfp\",\nreplied that the project was \"<q>absolutely not prepared to do\nthat</q>\" due to limited time and energy to work on the project as it\nis. \"<q>We would not even remotely be able to fork and maintain a\nbrowser fully, let alone to continually develop and improve\nit.</q>\"</p><p>Another downside to the forks is that there are far fewer eyes on\ntheir code and communities. When Mozilla makes an important move,\nwhether it's positive or negative, users are likely to hear about it\nquickly. As of now, the forks get relatively little attention.</p><p>Folks who want to jump ship from Mozilla's ecosystem entirely,\nwhile still sticking to open source, have some options. Ladybird,\nwhich LWN <a href=\"https://lwn.net/Articles/976822/\">covered</a> in\nJune last year, is an attempt to create a new browser from whole\ncloth. It is an interesting effort, but not ready for day-to-day use\nfor most folks. <a href=\"https://www.qutebrowser.org/\">Qutebrowser</a>, <a href=\"https://nyxt.atlas.engineer/\">Nyxt</a>, and <a href=\"https://www.netsurf-browser.org/\">NetSurf</a> are also worth a\nlook—though they may have some drawbacks for day-to-day use in\nterms of site compatibility and features. We will take a look at some\nof those options soon.</p>","contentLength":19930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43361959"},{"title":"“Normal” engineers are the key to great teams","url":"https://spectrum.ieee.org/10x-engineer","date":1741898147,"author":"jnord","guid":113,"unread":true,"content":"<p>Most of us have encountered a few software engineers who seem practically magician-like, a class apart from the rest of us in their ability to reason about complex mental models, leap to nonobvious yet elegant solutions, or emit waves of high-quality code at unreal velocity. </p><p>I have run into many of these incredible beings over the course of my career. I think their existence is what explains the curious durability of the notion of a “10x engineer,” someone who is 10 times as productive or skilled as their peers. The ideawhich has <a href=\"https://knowyourmeme.com/memes/10x-engineer\" target=\"_blank\">become a meme</a><a href=\"https://dl.acm.org/doi/10.1145/362851.362858\" target=\"_blank\">flimsy, shoddy research</a></p><p>I don’t have a problem with the idea that there are engineers who are 10 times as productive as other engineers. The problems I do have are twofold.</p><h2>Measuring productivity is fraught and imperfect</h2><p>First, how are you measuring productivity? I have a problem with the implication that there is One True Metric of productivity that you can standardize and sort people by. Consider the magnitude of skills and experiences at play: </p><ul><li>What adjacent skills, market segments, and product subject matter expertise are you drawing upon? Design, security, compliance, data visualization, marketing, finance? </li><li>What stage of development? What scale of usage? Are you writing for a <a href=\"https://spectrum.ieee.org/tag/mars\">Mars</a> rover, or shrink-wrapped software you can never change? </li></ul><h2>Engineers don’t own software, teams own software</h2><h2>The best engineering organizations are the ones where normal engineers can do great work</h2><h2>Let’s talk about “normal” engineers</h2><h2>Build sociotechnical systems with “normal people” in mind </h2><h2>Great engineering orgs mint world-class engineers </h2><h2>Don’t hire the “best” people. Hire the right people</h2>","contentLength":1650,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43356995"},{"title":"The Lost Art of Logarithms","url":"https://www.lostartoflogarithms.com/","date":1741892717,"author":"ozanonay","guid":112,"unread":true,"content":"<ul><li>This is a work in progress and nowhere close to completion</li></ul><p>\n                Some paragraphs are coherent; others are not. \n                Sometimes paragraphs are only a phrase or a note to myself.\n                Nothing has been professionally edited. \n            </p><ul><li>It is best viewed on a desktop or laptop computer</li></ul><p>\n                I've been developing the pages in Edge using Visual Studio Code \n                running under Windows 11 on a Microsoft Surface Pro 9.\n                I've also been testing the pages in Chrome on that machine, and   \n                in Safari on a Mac Mini running Sequoia, and \n                in Chrome (version 126, it says) on an Asus Chromebook. \n            </p><p>\n                However, my iPad Mini running iOS 12.5.7 has several \n                problems with these webpages, \n                and the pages often become quite awkward on phones. \n            </p>","contentLength":898,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43356314"},{"title":"OpenAI asks White House for relief from state AI rules","url":"https://finance.yahoo.com/news/openai-asks-white-house-relief-100000706.html","date":1741868429,"author":"jonbaer","guid":111,"unread":true,"content":"<div><p>(Bloomberg) -- OpenAI has asked the Trump administration to help shield artificial intelligence companies from a growing number of proposed state regulations if they voluntarily share their models with the federal government.</p><p>In a 15-page set of policy suggestions released on Thursday, the ChatGPT maker argued that the hundreds of AI-related bills currently pending across the US risk undercutting America’s technological progress at a time when it faces renewed competition from China. OpenAI said the administration should consider providing some relief for AI companies big and small from state rules – if and when enacted – in exchange for voluntary access to models.</p><p>The recommendation was one of several included in OpenAI’s response to a request for public input issued by the White House Office of Science and Technology Policy in February as the administration drafts a new policy to ensure US dominance in AI. President Donald Trump previously rescinded the Biden administration’s sprawling executive order on AI and tasked the science office with developing an AI Action Plan by July.</p><p>To date, there has been a notable absence of federal legislation governing the AI sector. The Trump administration has generally signaled its intention to take a hands-off approach to regulating the technology. But many states are actively weighing new measures on everything from deepfakes to bias in AI systems.</p><p>Chris Lehane, OpenAI’s vice president of global affairs, said in an interview that the US AI Safety Institute – a key government group focused on AI – could act as the main point of contact between the federal government and the private sector. If companies work with the group voluntarily to review models, the government could provide them “with liability protections including preemption from state based regulations that focus on frontier model security,” according to the proposal.</p><p>“Part of the incentive for doing that ought to be that you don’t have to go through the state stuff, which is not going to be anywhere near as good as what the federal level would be,” Lehane said.</p><p>In its policy recommendations, OpenAI also reiterated its call for the government to take steps to support AI infrastructure investments and called for copyright reform, arguing that America’s fair use doctrine is critical to maintaining AI leadership. OpenAI and other AI developers have faced numerous copyright lawsuits over the data used to build their models.</p></div>","contentLength":2483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43352531"},{"title":"Cursor told me I should learn coding instead of asking it to generate it","url":"https://forum.cursor.com/t/cursor-told-me-i-should-learn-coding-instead-of-asking-it-to-generate-it-limit-of-800-locs/61132","date":1741852795,"author":"nomilk","guid":110,"unread":true,"content":"<div itemprop=\"text\"><p>Agree, there are simple things, and there are “golden tokens” and bronce tokens. and depends of your status. the context provided the clean of the code. and  the ussage cursor in that momment like all ais, always explore status of cursor antrophic and openai if you are feeling something are feeling wrong. REstart cursor if you need. Ask for the root and ask for explain each element. to stay secure. and yes they are loops cursor injected errors,  lazyness etc in ALL the ai-s products. so you have to have a feeling when you are gamificated by the model. before and after, sometimes tell to the composer that change the thing, sometimes not. recycle the project using chatgpt plus. can be faster than stay in the same approach. i use a timer to dont loose the track. “One hour for that thing has sense”  there are not magical prompts .  but stay immerse in the flow it is hard, if you are in that momment dont forgent to git push! <img src=\"https://emoji.discourse-cdn.com/apple/slight_smile.png?v=14\" title=\":slight_smile:\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p></div>","contentLength":942,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43351137"},{"title":"Mark Klein, AT&T whistleblower who revealed NSA mass spying, has died","url":"https://www.eff.org/deeplinks/2025/03/memoriam-mark-klein-att-whistleblower-about-nsa-mass-spying","date":1741813508,"author":"leotravis10","guid":109,"unread":true,"content":"<p>EFF is deeply saddened to learn of the passing of Mark Klein, a bona fide hero who risked civil liability and criminal prosecution to help expose a massive spying program that violated the rights of millions of Americans.</p><p>Mark didn’t set out to change the world. For 22 years, he was a telecommunications technician for AT&amp;T, most of that in San Francisco. But he always had a strong sense of right and wrong and a commitment to privacy.</p><p>When the New York Times reported in late 2005 that the NSA was engaging in spying inside the U.S., Mark realized that he had witnessed how it was happening. He also realized that the President was not telling Americans the truth about the program. And, though newly retired, he knew that he had to do something. He showed up at EFF’s front door in early 2006 with a simple question: “Do you folks care about privacy?”&nbsp;</p><p>We did. And what Mark told us changed everything. Through his work, Mark had learned that the National Security Agency (NSA) had installed a secret, secure room at AT&amp;T’s central office in San Francisco, called <a href=\"https://en.wikipedia.org/wiki/Room_641A\">Room 641A</a>. Mark was assigned to connect circuits carrying Internet data to optical “splitters” that sat just outside of the secret NSA room but were hardwired into it. Those splitters—as well as similar ones in cities around the U.S.—<a href=\"https://www.eff.org/files/2014/07/24/backbone-3c-color.jpg\">made a copy</a> of all data going through those circuits and delivered it into the secret room.</p><div><div><div><img src=\"https://www.eff.org/files/2025/03/12/secretroom1_f.jpg\" width=\"500\" height=\"373\" alt=\"Photo of NSA-controlled &quot;secret room&quot; \" title=\"Photo of NSA-controlled &quot;secret room&quot; \"><p>A photo of the NSA-controlled 'secret room' in the AT&amp;T facility in San Francisco (Credit: Mark Klein)</p></div></div></div><p>Mark not only saw how it works, he had the documents to prove it. He brought us over a hundred pages of authenticated AT&amp;T schematic diagrams and tables. Mark also shared this information with major media outlets, numerous Congressional staffers, and at least two senators personally. One, Senator <a href=\"https://www.youtube.com/watch?v=Pylu8ycLfC4\">Chris Dodd</a>, took the floor of the Senate to acknowledge Mark as the great American hero he was.</p><div><div><div><img src=\"https://www.eff.org/files/2025/03/12/mark-klein-washington.jpg\" width=\"1284\" height=\"633\" alt=\"cartoon featuring Mark Klein climbing Capitol Hill\" title=\"cartoon featuring Mark Klein climbing Capitol Hill\"><p>Archival EFF graphic promoting Mark Klein's DC tour</p></div></div></div><p>Mark stood up and told the truth at great personal risk to himself and his family. AT&amp;T threatened to sue him, although it wisely decided not to do so. While we were able to use his evidence to make some change, both EFF and Mark were ultimately let down by Congress and the Courts, which have refused to take the steps necessary to end the mass spying even after Edward Snowden provided even more evidence of it in 2013.&nbsp;</p><p>But Mark certainly inspired all of us at EFF, and he helped inspire and inform hundreds of thousands of ordinary Americans to demand an end to illegal mass surveillance. While we have not yet seen the success in ending the spying that we all have hoped for, his bravery helped to usher numerous reforms so far.</p><p>And the fight is not over. The law, <a href=\"https://www.eff.org/702-spying\">called Section 702</a>, that now authorizes the continued surveillance that Mark first revealed, expires in early 2026. EFF and others will continue to push for continued reforms and, ultimately, for the illegal spying to end entirely.</p><p>Mark’s legacy lives on in our continuing fights to reform surveillance and honor the Fourth Amendment’s promise of protecting personal privacy. We are forever grateful to him for having the courage to stand up and will do our best to honor that legacy by continuing the fight.&nbsp;</p>","contentLength":3237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43347662"},{"title":"Gemini Robotics","url":"https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/","date":1741792149,"author":"meetpateltech","guid":108,"unread":true,"content":"<div><p data-block-key=\"q2c97\">Introducing Gemini Robotics, our Gemini 2.0-based model designed for robotics</p><p data-block-key=\"3djrg\">At Google DeepMind, we've been making progress in how our Gemini models solve complex problems through multimodal reasoning across text, images, audio and video. So far however, those abilities have been largely confined to the digital realm. In order for AI to be useful and helpful to people in the physical realm, they have to demonstrate “embodied” reasoning — the humanlike ability to comprehend and react to the world around us— as well as safely take action to get things done.</p><p data-block-key=\"73qu7\">Today, we are introducing two new AI models, based on Gemini 2.0, which lay the foundation for a new generation of helpful robots.</p><p data-block-key=\"4he0s\">The first is Gemini Robotics, an advanced vision-language-action (VLA) model that was built on Gemini 2.0 with the addition of physical actions as a new output modality for the purpose of directly controlling robots. The second is Gemini Robotics-ER, a Gemini model with advanced spatial understanding, enabling roboticists to run their own programs using Gemini’s embodied reasoning (ER) abilities.</p><p data-block-key=\"dtsn7\">Both of these models enable a variety of robots to perform a wider range of real-world tasks than ever before. As part of our efforts, we’re partnering with Apptronik to build the next generation of humanoid robots with Gemini 2.0. We’re also working with a selected number of trusted testers to guide the future of Gemini Robotics-ER.</p><p data-block-key=\"im3d\">We look forward to exploring our models’ capabilities and continuing to develop them on the path to real-world applications.</p></div><div><h2 data-block-key=\"2q7pg\">Gemini Robotics: Our most advanced vision-language-action model</h2><p data-block-key=\"3d8qf\">To be useful and helpful to people, AI models for robotics need three principal qualities: they have to be general, meaning they’re able to adapt to different situations; they have to be interactive, meaning they can understand and respond quickly to instructions or changes in their environment; and they have to be dexterous, meaning they can do the kinds of things people generally can do with their hands and fingers, like carefully manipulate objects.</p><p data-block-key=\"5bthq\">While our previous work demonstrated progress in these areas, Gemini Robotics represents a substantial step in performance on all three axes, getting us closer to truly general purpose robots.</p><p data-block-key=\"at04h\">Gemini Robotics leverages Gemini's world understanding to generalize to novel situations and solve a wide variety of tasks out of the box, including tasks it has never seen before in training. Gemini Robotics is also adept at dealing with new objects, diverse instructions, and new environments. In <a href=\"https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf\" rel=\"noopener\" target=\"_blank\">our tech report</a>, we show that on average, Gemini Robotics more than doubles performance on a comprehensive generalization benchmark compared to other state-of-the-art vision-language-action models.</p></div><div><p data-block-key=\"5merb\">To operate in our dynamic, physical world, robots must be able to seamlessly interact with people and their surrounding environment, and adapt to changes on the fly.</p><p data-block-key=\"lvkr\">Because it’s built on a foundation of Gemini 2.0, Gemini Robotics is intuitively interactive. It taps into Gemini’s advanced language understanding capabilities and can understand and respond to commands phrased in everyday, conversational language and in different languages.</p><p data-block-key=\"eo201\">It can understand and respond to a much broader set of natural language instructions than our previous models, adapting its behavior to your input. It also continuously monitors its surroundings, detects changes to its environment or instructions, and adjusts its actions accordingly. This kind of control, or “steerability,” can better help people collaborate with robot assistants in a range of settings, from home to the workplace.</p></div><div><p data-block-key=\"ev4ji\">The third key pillar for building a helpful robot is acting with <a href=\"https://deepmind.google/discover/blog/advances-in-robot-dexterity/\" rel=\"noopener\" target=\"_blank\">dexterity</a>. Many everyday tasks that humans perform effortlessly require surprisingly fine motor skills and are still too difficult for robots. By contrast, Gemini Robotics can tackle extremely complex, multi-step tasks that require precise manipulation such as origami folding or packing a snack into a Ziploc bag.</p></div><div><p data-block-key=\"2co1s\">Finally, because robots come in all shapes and sizes, Gemini Robotics was also designed to easily adapt to different robot types. We trained the model primarily on data from the bi-arm robotic platform, <a href=\"https://aloha-2.github.io/\" rel=\"noopener\" target=\"_blank\">ALOHA 2</a>, but we also demonstrated that it could control a bi-arm platform, based on the Franka arms used in many academic labs. Gemini Robotics can even be specialized for more complex embodiments, such as the humanoid Apollo robot developed by Apptronik, with the goal of completing real world tasks.</p></div><div><h2 data-block-key=\"2q7pg\">Enhancing Gemini’s world understanding</h2><p data-block-key=\"f1glf\">Alongside Gemini Robotics, we’re introducing an advanced vision-language model called Gemini Robotics-ER (short for ‘“embodied reasoning”). This model enhances Gemini’s understanding of the world in ways necessary for robotics, focusing especially on spatial reasoning, and allows roboticists to connect it with their existing low level controllers.</p><p data-block-key=\"6k4gb\">Gemini Robotics-ER improves Gemini 2.0’s existing abilities like pointing and 3D detection by a large margin. Combining spatial reasoning and Gemini’s coding abilities, Gemini Robotics-ER can instantiate entirely new capabilities on the fly. For example, when shown a coffee mug, the model can intuit an appropriate two-finger grasp for picking it up by the handle and a safe trajectory for approaching it.</p><p data-block-key=\"1lam2\">Gemini Robotics-ER can perform all the steps necessary to control a robot right out of the box, including perception, state estimation, spatial understanding, planning and code generation. In such an end-to-end setting the model achieves a 2x-3x success rate compared to Gemini 2.0. And where code generation is not sufficient, Gemini Robotics-ER can even tap into the power of in-context learning, following the patterns of a handful of human demonstrations to provide a solution.</p></div><div><h2 data-block-key=\"2q7pg\">Responsibly advancing AI and robotics</h2><p data-block-key=\"16hbn\">As we explore the continuing potential of AI and robotics, we’re taking a layered, <a href=\"https://sites.google.com/corp/view/safe-robots\" rel=\"noopener\" target=\"_blank\">holistic</a> approach to addressing safety in our research, from low-level motor control to high-level semantic understanding.</p><p data-block-key=\"7s8sj\">The physical safety of robots and the people around them is a longstanding, foundational concern in the science of robotics. That's why roboticists have classic safety measures such as avoiding collisions, limiting the magnitude of contact forces, and ensuring the dynamic stability of mobile robots. Gemini Robotics-ER can be interfaced with these ‘low-level’ safety-critical controllers, specific to each particular embodiment. Building on Gemini’s core safety features, we enable Gemini Robotics-ER models to understand whether or not a potential action is safe to perform in a given context, and to generate appropriate responses.</p><p data-block-key=\"3dcnu\">To advance robotics safety research across academia and industry, we are also releasing a new dataset to evaluate and improve semantic safety in embodied AI and robotics. In previous work, we showed how a <a href=\"https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/\" rel=\"noopener\" target=\"_blank\">Robot Constitution</a> inspired by Isaac Asimov’s Three Laws of Robotics could help prompt an LLM to select safer tasks for robots. We have since developed a framework to automatically generate data-driven constitutions - rules expressed directly in natural language – to steer a robot’s behavior. This framework would allow people to create, modify and apply constitutions to develop robots that are safer and more aligned with human values. Finally, the <a href=\"https://asimov-benchmark.github.io/\" rel=\"noopener\" target=\"_blank\">new ASIMOV dataset</a> will help researchers to rigorously measure the safety implications of robotic actions in real-world scenarios.</p><p data-block-key=\"ch213\">To further assess the societal implications of our work, we collaborate with experts in our Responsible Development and Innovation team and as well as our Responsibility and Safety Council, an internal review group committed to ensure we develop AI applications responsibly. We also consult with external specialists on particular challenges and opportunities presented by embodied AI in robotics applications.</p><p data-block-key=\"cis52\">In addition to our partnership with Apptronik, our Gemini Robotics-ER model is also available to trusted testers including Agile Robots, Agility Robots, Boston Dynamics, and Enchanted Tools. We look forward to exploring our models’ capabilities and continuing to develop AI for the next generation of more helpful robots.</p></div><div><p data-block-key=\"duspu\">This work was developed by the Gemini Robotics team. For a full list of authors and acknowledgements please view <a href=\"https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf\" rel=\"noopener\" target=\"_blank\">our technical report</a>.</p></div>","contentLength":8374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43344082"},{"title":"The DuckDB Local UI","url":"https://duckdb.org/2025/03/12/duckdb-ui.html","date":1741784161,"author":"xnx","guid":107,"unread":true,"content":"<div><p><em>TL;DR: The DuckDB team and MotherDuck are excited to announce the release of a local UI for DuckDB shipped as part of the  extension.</em></p></div><p>The DuckDB CLI provides advanced features like interactive multi-line editing, auto-complete, and progress indicators.\nHowever, it can be cumbersome for working with lengthy SQL queries, and its data exploration tools are limited.\nMany of the available third party UIs are great, but selecting, installing, and configuring one is not straightforward.\nUsing DuckDB through a UI should be as simple as using the CLI.\nAnd now it is!</p><p>Starting with <a href=\"https://github.com/duckdb/duckdb/releases/tag/v1.2.1\">DuckDB v1.2.1</a>, a full-featured local web user interface is available out-of-the-box!\nYou can start it from the terminal by launching the DuckDB CLI client with the  argument:</p><p>You can also run the following SQL command from a <a href=\"https://duckdb.org/docs/stable/clients/overview.html\">DuckDB client</a> (e.g., CLI, Python, Java, etc.):</p><p>Both of these approaches install the  extension (if it isn't installed yet),\nthen open the DuckDB UI in your browser:</p><p>The DuckDB UI uses interactive notebooks to define SQL scripts and show the results of queries.\nHowever, its capabilities go far beyond this.\nLet’s go over its main features.</p><blockquote><p>The DuckDB UI runs all your queries locally: your queries and data never leave your computer.\nIf you would like to use <a href=\"https://motherduck.com/\">MotherDuck</a> through the UI, you have to <a href=\"https://duckdb.org/2025/03/12/duckdb-ui.html#motherduck-integration\">opt-in explicitly</a>.</p></blockquote><p>Your attached databases are shown on the left.\nThis list includes in-memory databases plus any files and URLs you’ve loaded.\nYou can explore tables and views by expanding databases and schemas.</p><p>Click on a table or view to show a summary below.\nThe UI shows the number of rows, the name and type of each column, and a profile of the data in each column.</p><p>Select a column to see a more detailed summary of its data.\nYou can use the  button near the top right to inspect the first 100 rows.\nYou can also find the SQL definition of the table or view here.</p><p>You can organize your work into named notebooks.\nEach cell of the notebook can execute one or more SQL statements.\nThe UI supports syntax highlighting and autocomplete to assist with writing your queries.</p><p>You can run the whole cell, or just a selection,\nthen sort, filter, or further transform the results using the provided controls.</p><p>The right panel contains the <a href=\"https://motherduck.com/blog/introducing-column-explorer/\">Column Explorer</a>, which shows a summary of your results.\nYou can dive into each column to gain insights.</p><p>If you would like to connect to <a href=\"https://motherduck.com/\">MotherDuck</a>, you can sign into MotherDuck to persist files and tables to a <a href=\"https://motherduck.com/docs/getting-started/\">cloud data warehouse</a> crafted for using DuckDB at scale and sharing data with your team.</p><p>The DuckDB UI is under active development. Expect additions and improvements!</p><p>Like the DuckDB CLI, the DuckDB UI creates some files in the  directory in your home directory.\nThe UI puts its files in a sub-directory, :</p><ul><li>Your notebooks and some other state are stored in a DuckDB database, .</li><li>When you export data to the clipboard or a file (using the controls below the results), some tiny intermediate files (e.g. ) are generated.\nYour data is cleared from these files after the export is completed, but some near-empty files remain, one per file type.</li></ul><p>Support for the UI is implemented in a <a href=\"https://duckdb.org/docs/stable/extensions/overview.html\">DuckDB extension</a>.\nThe extension embeds a localhost HTTP server, which serves the UI browser application, and also exposes an API for communication with DuckDB.\nIn this way, the UI leverages the native DuckDB instance from which it was started, enabling full access to your local memory, compute, and file system.</p><p>Results are returned in an efficient binary form closely matching DuckDB’s in-memory representation (<a href=\"https://github.com/duckdb/duckdb/blob/v1.2.1/src/include/duckdb/common/types/data_chunk.hpp\">DataChunk</a>).\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\">Server-sent events</a> enable prompt notification of updates such as attaching databases.\nThese techniques and others make for a low-latency experience that keeps you in your flow.</p><p>In this blog post, we presented the new DuckDB UI, a powerful web interface for DuckDB.</p><p>The DuckDB UI shares many of its design principles with the DuckDB database.\nIt’s simple, fast, feature-rich, and portable, and runs locally on your computer.\nThe DuckDB UI extension is also open source: visit the <a href=\"https://github.com/duckdb/duckdb-ui\"> repository</a> if you want to dive in deeper into the extension's code.</p><blockquote><p>The repository does not contain the source code for the frontend, which is currently not available as open-source.\nReleasing it as open-source is under consideration.</p></blockquote>","contentLength":4249,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43342712"},{"title":"Peer-to-peer file transfers in the browser","url":"https://github.com/kern/filepizza","date":1741781323,"author":"keepamovin","guid":106,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43342361"},{"title":"The Startup CTO's Handbook","url":"https://github.com/ZachGoldberg/Startup-CTO-Handbook/blob/main/StartupCTOHandbook.md","date":1741731522,"author":"simonebrunozzi","guid":105,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43337703"},{"title":"A 10x Faster TypeScript","url":"https://devblogs.microsoft.com/typescript/typescript-native-port/","date":1741703543,"author":"DanRosenwasser","guid":104,"unread":true,"content":"<p>Today I’m excited to announce the next steps we’re taking to radically improve TypeScript performance.</p><p>The core value proposition of TypeScript is an excellent developer experience.\nAs your codebase grows, so does the value of TypeScript itself, but in many cases TypeScript has not been able to scale up to the very largest codebases.\nDevelopers working in large projects can experience long load and check times, and have to choose between reasonable editor startup time or getting a complete view of their source code.\nWe know developers love when they can rename variables with confidence, find all references to a particular function, easily navigate their codebase, and do all of those things without delay.\nNew experiences powered by AI benefit from large windows of semantic information that need to be available with tighter latency constraints.\nWe also want fast command-line builds to validate that your entire codebase is in good shape.</p><p>To meet those goals, we’ve begun work on a native port of the TypeScript compiler and tools.\nThe native implementation will <strong>drastically improve editor startup, reduce most build times by 10x, and substantially reduce memory usage</strong>.\nBy porting the current codebase, we expect to be able to preview a native implementation of  capable of command-line typechecking by mid-2025, with a feature-complete solution for project builds and a language service by the end of the year.</p><p>You can , which is offered under the same license as the existing TypeScript codebase.\nCheck the README for instructions on how to build and run  and the language server, and to see a summary of what’s implemented so far.\nWe’ll be posting regular updates as new functionality becomes available for testing.</p><p>Our native implementation is already capable of loading many popular TypeScript projects, including <a href=\"https://github.com/microsoft/TypeScript/tree/main/src/compiler\">the TypeScript compiler itself</a>.\nHere are times to run  on some popular codebases on GitHub of varying sizes:</p><table><thead><tr></tr></thead></table><p>While we’re not yet feature-complete, these numbers are representative of the order of magnitude performance improvement you’ll see checking most codebases.</p><p>We’re incredibly excited about the opportunities that this massive speed boost creates. Features that once seemed out of reach are now within grasp.\nThis native port will be able to provide instant, comprehensive error listings across an entire project, support more advanced refactorings, and enable deeper insights that were previously too expensive to compute.\nThis new foundation goes beyond today’s developer experience and will enable the next generation of AI tools to enhance development, powering new tools that will learn, adapt, and improve the coding experience.</p><p>Most developer time is spent in editors, and it’s where performance is most important.\nWe want editors to load large projects quickly, and respond quickly in all situations.\nModern editors like Visual Studio and Visual Studio Code have excellent performance as long as the underlying language services are also fast.\nWith our native implementation, we’ll be able to provide incredibly fast editor experiences.</p><p>Again using the Visual Studio Code codebase as a benchmark, the current time to load the entire project in the editor on a fast computer is about 9.6 seconds.\nThis drops down to about 1.2 seconds with the native language service, an <strong>8x improvement in project load time</strong> in editor scenarios.\nWhat this translates to is a faster working experience from the time you open your editor to your first keystroke in any TypeScript codebase.\nWe expect all projects to see this level of improvement in load time.</p><p>Overall memory usage also appears to be roughly half of the current implementation, though we haven’t actively investigated optimizing this yet and expect to realize further improvements.\nEditor responsiveness for all language service operations (including completion lists, quick info, go to definition, and find all references) will also see significant speed gains.\nWe’ll also be moving to the Language Server Protocol (LSP), a longstanding infrastructural work item to better align our implementation with other languages.</p><p>Our most recent TypeScript release was TypeScript 5.8, with TypeScript 5.9 coming soon.\nThe JS-based codebase will continue development into the 6.x series, and TypeScript 6.0 will introduce some deprecations and breaking changes to align with the upcoming native codebase.</p><p>When the native codebase has reached sufficient parity with the current TypeScript, we’ll be releasing it as .\nThis is still in development and we’ll be announcing stability and feature milestones as they occur.</p><p>For the sake of clarity, we’ll refer to them simply as TypeScript 6 (JS) and TypeScript 7 (native), since this will be the nomenclature for the foreseeable future.\nYou may also see us refer to “Strada” (the original TypeScript codename) and “Corsa” (the codename for this effort) in internal discussions or code comments.</p><p>While some projects may be able to switch to TypeScript 7 upon release, others may depend on certain API features, legacy configurations, or other constraints that necessitate using TypeScript 6.\nRecognizing TypeScript’s critical role in the JS development ecosystem, we’ll still be maintaining the JS codebase in the 6.x line until TypeScript 7+ reaches sufficient maturity and adoption.</p><p>Our long-term goal is to keep these versions as closely aligned as possible so that you can upgrade to TypeScript 7 as soon as it meets your requirements, or fall back to TypeScript 6 if necessary.</p><p>In the coming months we’ll be sharing more about this exciting effort, including deeper looks into performance, a new compiler API, LSP, and more.\nWe’ve written up some <a href=\"https://github.com/microsoft/typescript-go/discussions/categories/faqs\">FAQs</a> on the GitHub repo to address some questions we expect you might have.\nWe also invite you to join us for an AMA at the <a href=\"https://discord.gg/typescript\">TypeScript Community Discord</a> at  on March 13th.</p><p>A 10x performance improvement represents a massive leap in the TypeScript and JavaScript development experience, so we hope you are as enthusiastic as we are for this effort!</p>","contentLength":6051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43332830"},{"title":"Happy 20th birthday, Y Combinator","url":"https://twitter.com/garrytan/status/1899092996702048709","date":1741702475,"author":"btilly","guid":103,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43332658"},{"title":"RubyLLM: A delightful Ruby way to work with AI","url":"https://github.com/crmne/ruby_llm","date":1741696855,"author":"ksec","guid":102,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43331847"},{"title":"Show HN: Factorio Learning Environment – Agents Build Factories","url":"https://jackhopkins.github.io/factorio-learning-environment/","date":1741694522,"author":"noddybear","guid":101,"unread":true,"content":"<p>I'm Jack, and I'm excited to share a project that has channeled my Factorio addiction recently: the Factorio Learning Environment (FLE).</p><p>FLE is an open-source framework for developing and evaluating LLM agents in Factorio. It provides a controlled environment where AI models can attempt complex automation, resource management, and optimisation tasks in a grounded world with meaningful constraints.</p><p>A critical advantage of Factorio as a benchmark is its unbounded nature. Unlike many evals that are quickly saturated by newer models, Factorio's geometric complexity scaling means it won't be \"solved\" in the next 6 months (or possibly even years). This allows us to meaningfully compare models by the order-of-magnitude of resources they can produce - creating a benchmark with longevity.</p><p>The project began 18 months ago after years of playing Factorio, recognising its potential as an AI research testbed. A few months ago, our team (myself, Akbir, and Mart) came together to create a benchmark that tests agent capabilities in spatial reasoning and long-term planning.</p><p>Two technical innovations drove this project forward: First, we discovered that piping Lua into the Factorio console over TCP enables running (almost) arbitrary code without directly modding the game. Second, we developed a first-class Python API that wraps these Lua programs to provide a clean, type-hinted interface for AI agents to interact with Factorio through familiar programming paradigms.</p><p>Agents interact with FLE through a REPL pattern:\n1. They observe the world (seeing the output of their last action)\n2. Generate Python code to perform their next action\n3. Receive detailed feedback (including exceptions and stdout)</p><p>We provide two main evaluation settings:\n- Lab-play: 24 structured tasks with fixed resources\n- Open-play: An unbounded task of building the largest possible factory on a procedurally generated map</p><p>We found that while LLMs show promising short-horizon skills, they struggle with spatial reasoning in constrained environments. They can discover basic automation strategies (like electric-powered drilling) but fail to achieve more complex automation (like electronic circuit manufacturing). Claude Sonnet 3.5 is currently the best model (by a significant margin).</p><p>You'll need:\n- Factorio (version 1.1.110)\n- Docker\n- Python 3.10+</p><p>The README contains detailed installation instructions and examples of how to run evaluations with different LLM agents.</p><p>We would love to hear your thoughts and see what others can do with this framework!</p>","contentLength":2527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43331582"},{"title":"Show HN: Seven39, a social media app that is only open for 3 hours every evening","url":"https://www.seven39.com/","date":1741655110,"author":"mklyons","guid":100,"unread":true,"content":"<p>Because social media is better when we're all online together.</p><p>No endless scrolling. No FOMO. Just 3 hours of fun every evening.</p><p>The domain was available.</p>","contentLength":152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43328095"},{"title":"uBlock Origin is no longer available on the Chrome Store","url":"https://chromewebstore.google.com/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm?hl=en","date":1741627834,"author":"non-","guid":99,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43322922"},{"title":"Go European: Discover European products and services","url":"https://www.goeuropean.org/","date":1741601747,"author":"doener","guid":98,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43318798"},{"title":"Performance of the Python 3.14 tail-call interpreter","url":"https://blog.nelhage.com/post/cpython-tail-call/","date":1741589067,"author":"signa11","guid":97,"unread":true,"content":"<p>Unfortunately, as I will document in this post, these impressive performance gains turned out to be <strong>primarily due to inadvertently working around a regression in LLVM 19.</strong> When benchmarked against a better baseline (such GCC, clang-18, or LLVM 19 with certain tuning flags), the performance gain drops to 1-5% or so depending on the exact setup.</p><p>When the tail-call interpreter was announced, I was surprised and impressed by the performance improvements, but also confused: I’m not an expert, but I’m passingly-familiar with modern CPU hardware, compilers, and interpreter design, and I couldn’t explain why this change would be so effective. I became curious – and perhaps slightly obsessed – and the reports in this post are the result of a few weeks of off-and-on compiling and benchmarking and disassembly of dozens of different Python binaries, in an attempt to understand what I was seeing.</p><p>At the end, I <a href=\"https://blog.nelhage.com/post/cpython-tail-call/#reflections\">will reflect</a> on this situation as a case study in some of the challenges of benchmarking, performance engineering, and software engineering in general.</p><p>I also want to be clear that I still think the tail-calling interpreter is a great piece of work, as well as a genuine speedup (albeit more modest than initially hoped). I am also optimistic it’s a more robust approach than the older interpreter, in ways I’ll explain in this post. I also really don’t want to blame anyone on the Python team for this error. This sort of confusion turns out to be very common – I’ve certainly misunderstood many a benchmark myself – and I’ll have some reflections on that topic at the end.</p><p>In addition, the impact of the LLVM regression doesn’t seem to have been known prior to this work (and the bug wasn’t fixed as of publishing this post, although it <a href=\"https://github.com/llvm/llvm-project/pull/114990\">since has been</a>); thus, in that sense, the alternative (without this work) probably really was 10-15% slower, for builds using clang-19 or newer. For instance, Simon Willison <a href=\"https://simonwillison.net/2025/Feb/13/python-3140a5/\">reproduced the 10% speedup</a> “in the wild,” as compared to Python 3.13, using builds from <a href=\"https://github.com/astral-sh/python-build-standalone\"></a>.</p><p>Here are my headline results. I benchmarked several builds of the CPython interpreter, using multiple different compilers and different configuration options, on two machines: an Intel server (a <a href=\"https://www.intel.com/content/www/us/en/products/sku/230580/intel-core-i513500-processor-24m-cache-up-to-4-80-ghz/specifications.html\">Raptor Lake i5-13500</a> I maintain in Hetzner), and my Apple M1 Macbook Air. You can reproduce these builds <a href=\"https://github.com/nelhage/cpython-interp-perf/\">using my  configuration</a>, which I found essential for managing so many different moving pieces at once.</p><p>All builds use LTO and PGO. These configurations are:</p><ul><li>: Built using Clang 18.1.8, using computed gotos.</li><li> (Intel only): Built with GCC 14.2.1, using computed gotos.</li><li>: Built using Clang 19.1.7, using computed gotos.</li><li>: Built using Clang 19.1.7, using the new tail-call interpreter.</li><li>: Built using Clang 19.1.7, computed gotos and some  tuning flags which work around the regression.</li></ul><p>I’ve used  as the baseline, and reported the bottom-line “average” reported by /. You can find the complete output files and reports <a href=\"https://github.com/nelhage/cpython-interp-perf/tree/data/\">on github</a>.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>Observe that the tail-call interpreter still exhibits a speedup as compared to clang-18, but that it’s far less dramatic than the slowdown from moving to clang-19. The Python team has also observed larger speedups than I have (after accounting for the bug) on some other platforms.</p><p>You’ll notice I didn’t benchmark the tail-call interpreter on the older Clang release (what would be ). The tail-call interpreter relies on new compiler features which only landed in Clang 19, meaning we can’t test it on earlier versions. This interaction, I think, is a big reason this story was so confusing, and why it took me so many benchmarks to be  I understood the situation.</p><p>A classic bytecode interpreter consists of a  statement inside of a  loop, looking something like so:</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>Most compilers will compile the  into a jump table – they will emit a table containing the address of each  block, index into it with the opcode, and perform an indirect jump.</p><p>It’s <a href=\"https://link.springer.com/content/pdf/10.1007/3-540-44681-8_59.pdf\">long been known</a> that you can speed up a bytecode interpreter of this style by replicating the jump table dispatch into the body of each opcode. That is, instead of ending each opcode with a , each opcode contains a separate instance of the “decode next instruction and index through the jump table” logic.</p><p>Modern C compilers support <a href=\"https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.html#Labels-as-Values\">taking the address of labels</a>, and then using those labels in a “computed goto,” in order to implement this pattern. Thus, many modern bytecode interpreters, including CPython (before the tail-call work), employ an interpreter loop that looks something like:</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>For performance reasons (performance of the compiler, not the generated code), it turns out that Clang and LLVM, internally, actually merges all of the s in the latter code into a <a href=\"https://llvm.org/docs/LangRef.html#indirectbr-instruction\"> LLVM instruction</a>, which each opcode will jump to. That is, the compiler takes our hard work, and deliberately rewrites into a control-flow-graph that looks essentially the same as the -based interpreter!</p><p>Then, during code generation, LLVM performs “tail duplication,” and copies the branch  into each location, restoring the original intent. This dance is documented, at a high level, <a href=\"https://blog.llvm.org/2010/01/address-of-label-and-indirect-branches.html\">in an old LLVM blog post</a> introducing the new implementation.</p><p>The whole reason for the deduplicate-then-copy dance is that, for technical reasons, creating and manipulating the control-flow-graph containing many  instructions can be quite expensive.</p><p>In order to avoid catastrophic slowdowns (or memory usage) in certain cases, LLVM 19 implemented <a href=\"https://github.com/llvm/llvm-project/pull/78582\">some limits on tail-duplication pass</a>, causing it to bail out if duplication would blow up the size of the IR past certain limits.</p><p>Unfortunately, on CPython those limits resulted in Clang <strong>leaving all of the dispatch jumps merged</strong>, and entirely undoing the whole purpose of the computed -based implementation! This bug was <a href=\"https://github.com/llvm/llvm-project/issues/106846\">first identified</a> by another language implementation with a similar interpreter loop, but had not been known (as far as I can find) to affect CPython.</p><p>In addition to the performance impact, we can observe the bug directly by disassembling the resulting object code and counting the number of distinct indirect jumps:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>I am confident that the change to the tail-call duplication logic caused the regression: if <a href=\"https://blog.nelhage.com/post/cpython-tail-call/#the-fix\">you fix it</a>, performance matches clang-18. However, I can’t fully explain the  of the regression.</p><p>Historically, the optimization of replicating the bytecode dispatch into each opcode has been cited to speed up interpreters anywhere from <a href=\"https://github.com/python/cpython/blob/c718c6be0f82af5eb0e57615ce323242155ff014/Misc/HISTORY#L15252-L15255\">20%</a> to <a href=\"https://link.springer.com/content/pdf/10.1007/3-540-44681-8_59.pdf\">100%</a>. However, on modern processors with improved branch predictors, <a href=\"https://inria.hal.science/hal-01100647/document\">more recent work</a> finds a much smaller speedup, on the order of 2-4%.</p><p>We can verify this 2-4% number in practice, because Python still supports the “old-style” interpreter, which uses a single  statement, via a configuration option. Here’s what we see if we benchmark that interpreter (\".nocg\" for “no computed gotos” in the following table):</p><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><p>Notice that \" is only 2% slower than , even though the base  build is 9% slower! I interpret that “2%” as a fairer estimate for the cost/benefit of duplicating opcode dispatch, alone, and I don’t fully understand the other one.</p><p>I haven’t mentioned the  benchmark, which you may notice claims to be  than \" It was at this point that I discovered an additional, and very funny, twist to the story.</p><p>I explained earlier that Clang and LLVM:</p><ol><li>Compiles the  into a jump table and an indirect jump, very similar to the one we create by hand using computed gotos</li><li>Compiles computed gotos into a control-flow graph that closely resembles the classic  graph, which a single instance of the opcode dispatch, and</li><li>Is able to reverse the transformation during codegen in order to duplicate the dispatch</li></ol><p>Those facts taken together, might lead you to ask, “Couldn’t we just start with the -based interpreter, and have the  do tail-duplication, and get the same benefits?”</p><p>clang-18 (or clang-19 with appropriate flags), when presented with the “classic” -based interpreter, <strong>goes ahead and duplicates the dispatch logic into each the body of each opcode anyways</strong>. Here’s another table, showing the same builds with the number of indirect jumps, using the  test from earlier:</p><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><p>Thus, there’s a case to be made that the entire “computed goto” interpreter turns out to be entirely unnecessary complexity (at least for modern Clang). The compiler is perfectly capable of performing the same transformation itself, and (apparently) the computed gotos don’t even suffice to guarantee it!</p><p>That said, I did also test GCC, and GCC (at least as of 14.2.1) does not replicate the , but does implement the desired behavior for when using computed goto. So at least in that case we see the expected behavior.</p><p><a href=\"https://github.com/llvm/llvm-project/pull/114990\">LLVM pull request 114990</a> merged shortly after I published this post, and fixes the regression. I was able to benchmark it before merge and confirm it restores the expected performance.</p><p>For releases before that fix, the <a href=\"https://github.com/llvm/llvm-project/pull/78582\">PR that caused the regression</a> added a tunable option to choose the threshold at which tail-duplication will abort. We can restore similar behavior on clang-19 by simply setting that limit to a very large number.</p><p>I will freely admit that I got nerdsniped quite effectively by this topic, and have gone far deeper than was really necessary. That said, having done so, I think there are a number of interesting lessons and reflections to be taken away, which generalize to software engineering and performance engineering, and I will attempt to extract and meditate upon some of them.</p><p>When optimizing a system, we generally construct some set of benchmarks and benchmarking methodology, and then evaluate proposed changes using those benchmarks.</p><p>Any set of benchmarks or benchmark procedures embeds (often implicitly) what I like to call a “theory of performance.” Your theory of performance is a set of beliefs and assumptions that answer questions like “which variables (may) effect performance, in what ways?” and “what is the relationship between results on benchmarks and the “true” performance in “production”?”</p><p>The benchmarks ran on the tail-call interpreter showed a 10-15% speedup when compared with the old computed-goto interpreter. Those benchmarks were accurate, in that they were accurately measuring (as far as I know) the performance difference between those builds. However, in order to generalize those specific data points into the statement “The tail-call interpreter is 10-15% faster than the computed-goto interpreter, more generally,” or even “The tail-call interpreter will speed up Python by 10-15% for our users,” we need to bring in more assumptions and beliefs about the world. In this case, it turns out the story was more complex, and those broader claims were not true in full generality.</p><p>(Once again, I really don’t want to blame the Python developers! This stuff is  and there are a million ways to get confused or to reach somewhat-incorrect conclusions. I had to do ~three weeks of intense benchmarking and experimenting to reach a better understanding. My point is that this is a very general challenge!)</p><p>This example highlights another recurring challenge, not only in software performance, but in many other domains: “What baseline do you compare against?”</p><p>Any time you propose a new solution or method for some problem, you typically have a way of running your new method, and producing some relevant performance metrics.</p><p>Once you have metrics for  system, however, you need to know what to compare them against, in order to decide if they’re any good! Even if you score well on some absolute scale (assuming there  a sensible absolute scale to evaluate), if your method is worse than an existing solution, it’s probably not that interesting.</p><p>Typically, you want to compare against “the current best-known approach.” But sometimes that can be hard to do! Even if you understand the current approach in theory, you may or may not be an expert in applying it in practice. In the case of software this may mean something like, tuning your operating system or compiler options or other flags. The current-best approach may have published benchmarks, but they’re not always relevant to you; for instance, maybe it was published years ago on older hardware, and so you can’t do an apples-to-apples comparison with the public numbers. Or maybe their tests were run at a scale you can’t afford to replicate.</p><p>I work in machine learning at Anthropic these days, and we see this all the time in ML papers. When a paper comes out claiming some algorithmic improvement or other advance, I’ve noticed that the first detail our researchers ask is often not “What did they do?” but “What baseline did they compare against?” It’s easy to get impressive-looking results if you’re comparing against a poorly-tuned baseline, and that observation turns out to explain a surprising fraction of supposed improvements.</p><h2>On software engineering&nbsp;<a href=\"https://blog.nelhage.com/post/cpython-tail-call/#on-software-engineering\"></a></h2><p>One other highlight, for me, is just how complex and interconnected our software systems are, and how rapidly-moving, and how hard it is to keep track of all the pieces.</p><p>If you’d asked me, a month ago, to estimate the likelihood that an LLVM release caused a 10% performance regression in CPython and that no one noticed for five months, I’d have thought that a pretty unlikely state of affairs! Those are both widely-used projects, both of which care a fair bit about performance, and “surely” someone would have tested and noticed.</p><p>And probably that  situation was quite unlikely! However, with so many different software projects out there, each moving so rapidly and depending on and being used by so many other projects, it becomes practically-inevitable that  regressions “like that one” happen, almost constantly.</p><p>The saga of the computed-goto interpreter illustrates recurring tensions and unresolved questions around optimizers and optimizing compilers, to which we don’t yet have agreed-upon answers as a field.</p><p>We generally expect our compilers to respect the programmer’s intent, and to compile the code that was written in a way that preserves the programmer’s intent.</p><p>We also, however, expect our compilers to optimize our code, and to transform it in potentially-complex-and-unintuitive ways in order to make it run faster.</p><p>These expectations are in tension, and we have a dearth of patterns and idioms to explain to the compiler “why” we wrote code in various ways, and whether we were  trying to trigger a certain output, or make a certain performance-related decision, or not.</p><p>Our compilers typically only  to emit code with “the same behavior” as the code we write; performance is something of a best-effort feature on top of that guarantee.</p><p>Thus, we end up in this odd world where clang-19 compiles the computed-goto interpreter “correctly” – in the sense that the resulting binary produces all the same value we expect – but at the same time it produces an output completely at odds with the intention of the optimization. Moreover, we also see other versions of the compiler applying optimizations to the “naive” -based interpreter, which implement the exact same optimization we “intended” to perform by rewriting the source code.</p><p>In hindsight, it appears that the “computed goto” interpreter, at a source code level, and “replicating the dispatch at a machine-code level” end up being almost-orthogonal notions! We’ve seen examples of every instance of the resulting 2x2 matrix! Because all of those  binaries compute the same values when run, our current tools are essentially unable to talk about the distinctions between them in a coherent way.</p><p>This confusion is one way in which I think the tail-calling interpreter (and the compiler features behind it) represent a genuine, and useful, advance in the state of the art. The tail-call interpreter is built on <a href=\"https://clang.llvm.org/docs/AttributeReference.html#musttail\">the  attribute</a>, which represents a relatively new kind of compiler feature.  does not affect the “observable program behavior,” in the classic sense that compilers think, but is rather a conversation with the ; it requires that the compiler be able to make certain optimizations, and requires that compilation fail if those optimizations don’t happen.</p><p>I’m hopeful this framework will turn out to be a much more robust style for writing performance-sensitive code, especially over time and as compilers evolve. I look forward to continued experiments with features in that category.</p><p>Concretely, I find myself wondering if it would be viable to replace the computed-goto interpreter with something like a (hypothetical) <code>[[clang::musttailduplicate]]</code> attribute on the interpreter  loop. I’m not expert enough in all the relevant IRs and passes to have confidence in this proposal, but perhaps someone with more familiarity can weigh in on the feasibility.</p><p>I want to close with a call-out of how helpful  was for this project. I have been experimenting with nix and NixOS for my personal infrastructure over the last year or so, but they turned out to be a total lifesaver for this investigation.</p><p>In the course of these experiments, I have built and benchmarked  of different Python interpreters, across four different compilers (, , , and ) and using numerous combinations of compiler flags. Managing all of that by hand would have strained my sanity, and I’m  I would have made numerous mistakes during which I mixed up which compiler and which flags went into which build, and so on.</p><p>Using , I was able to keep all of these parallel versions straight, and build them in a reproducible, hermetic style. I was able to write some short abstractions which made them very easy to define, and then know with absolute confidence  any given build in my  store came from, with which compilers and which flags. After a small amount of work to build some helper functions, the core definitions of my build matrix is <a href=\"https://github.com/nelhage/cpython-interp-perf/blob/afd2123bef6ec3fd872d628f2bb519b20684e161/python.nix#L105-L143\">shockingly concise</a>; here’s a taste:</p><div><pre tabindex=\"0\"><code data-lang=\"nixos\"></code></pre></div><p>I was even able to build a custom version of LLVM (with the bugfix patch), and do Python builds using that compiler. Doing so required <a href=\"https://github.com/nelhage/cpython-interp-perf/blob/afd2123bef6ec3fd872d628f2bb519b20684e161/llvm.nix#L14-L25\">all of about 10 lines of code</a>.</p><p>That said, not everything was rosy. For one,  is, by necessity, “weird” in various ways compared to the ways “normal people” use software, and I worry that some of that weirdness may have affected some of my benchmarks or conclusions in ways I didn’t notice. For instance, early on I discovered that nix (by default) builds projects using certain hardening flags that <a href=\"https://github.com/python/cpython/issues/130961\">disproportionately impact the tail-call interpreter</a>. I’ve handled that one, but are there more?</p><p>In addition, Nix is incredibly extensible and customizable, but figuring out how to make a specific customization can be a real uphill battle, and involve a lot of trial and error and source-diving. My patched LLVM build ended up being pretty short and clean, but getting there required me to read a lot of  source code, mixing and matching two under-documented extensibility mechanisms ( and  – not to be confused with , used elsewhere), and one failed attempt which successfully patched , but then silently built a new  against the unpatched version.</p><p>Still,  was clearly enormously helpful here, and on net it definitely made this kind of multi-version exploration and debugging  saner than any other approach I can imagine.</p>","contentLength":19177,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43317592"},{"title":"Tesla created secret team to suppress driving range complaints (2023)","url":"https://www.reuters.com/investigates/special-report/tesla-batteries-range/","date":1741560799,"author":"mathgenius","guid":96,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43314781"},{"title":"It is as if you were on your phone","url":"https://pippinbarr.com/it-is-as-if-you-were-on-your-phone/info/","date":1741527635,"author":"bookofjoe","guid":95,"unread":true,"content":"<p><em>Look at you! On your phone! But you’ve got a secret! And you won’t tell! You’re not on your phone! It is only as if you were on your phone! You’re just pretending to be on your phone! On your phone!</em></p><p><em>It is as if you were on your phone</em> is an  game about an  in which we’re all simultaneously under significant pressure to be on our phones all the time, but also to not be on our phones all the time. Our fingers want to touch the screen, our eyes want to watch the surface, our brains want to be occupied efficiently and always. But it’s also exhausting liking photos, swiping profiles, watching short-form video, and everything else we’re always doing. <em>It is as if you were on your phone</em> presents an alternative:  to be on your phone so that you pass as human, but actually do essentially nothing instead. Follow the prompts and be free.</p><p><em>It is as if you were on your phone</em> was created using <a href=\"https://p5js.org\">p5</a> along with <a href=\"https://hammerjs.github.io/\">Hammer.js</a> for touch gestures.</p>","contentLength":945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43308994"},{"title":"US Ends Support For Ukrainian F-16s","url":"https://ukrainetoday.org/us-ends-support-for-ukrainian-f-16s-but-french-mirages-will-be-salvation-forbes/","date":1741518507,"author":"ctack","guid":94,"unread":true,"content":"<p>The Donald Trump administration has cut off vital support for F-16 jamming capabilities.</p><p>The United States, having decided to end its support for Ukraine, cannot simply turn off the Ukrainian Air Force’s U.S.-designed&nbsp;<a href=\"https://www.unian.net/world/f-16-dlya-ukrainy-aviaekspert-nazval-veroyatnuyu-prichinu-zaderzhki-postavki-belgiyskih-istrebiteley-12937956.html\" target=\"_blank\" rel=\"noreferrer noopener\">F-16</a>&nbsp;fighters . But the Trump administration has cut off vital support for their jamming capabilities, which could deprive the Ukrainian Air Force of a critical air countermeasure.</p><p><a href=\"https://www.forbes.com/sites/davidaxe/2025/03/07/france-to-the-rescue-french-made-mirage-2000-jets-could-become-ukraines-most-important-aerial-radar-jammers/\" rel=\"noreferrer noopener\" target=\"_blank\">However, as Forbes</a>&nbsp;analyst David Ax writes&nbsp;, the Ukrainians are not powerless and can shift the burden of air jamming onto the French Dassault Mirage 2000 fighters.</p><p>As Aks points out, the Ukrainian Air Force is taking full advantage of the ability of F-16s equipped with AN/ALQ-131 pods to flood Russian radar screens with electronic noise. They act as “flying air defenses” with advanced missile warning technology, notes the Conflict Intelligence Team.</p><p>But the Russian Air Force can circumvent the jamming by reprogramming its radars to different frequencies. As Axe points out, while the Biden Air Force was able to keep up with the Russian adaptation by constantly tweaking the AN/ALQ-131 frequencies, under Trump, Ukrainian pilots are not receiving updates, and the programs could soon become obsolete.</p><p>However, France’s Mirage 2000s are equipped with their own powerful jammers, and the Americans are not involved in their programming, the analyst notes.</p><p>The Mirage 2000-5F currently in service with the French Air Force flies with a combination of the Serval radar warning receiver, Sabre jammer and Eclair flare dispenser. This system was cutting edge in the 1980s, but within a generation it was falling behind.</p><p>“Recognizing this weakness and understanding the seriousness of the Russian missile threat over Ukraine, the French Ministry of Defense promised to install new electronic countermeasures on the Mirage 2000s before handing them over to Ukraine. The ministry probably had in mind the predominantly analog Integrated Countermeasures Suite Mark 2 or the all-digital Integrated Countermeasures Suite Mark 3,” Ax notes.</p><p>Either system is an improvement over the older system and a potential replacement for the AN/ALQ-131, as the American pods lag behind the Russian adaptation. The French are firm allies of Ukraine and are willing to reprogram the jammers if necessary.</p><p>In the longer term, the Ukrainians could refit their F-16s with non-American electronic countermeasures, Axe notes. But that could take time and money that Ukraine cannot afford.</p>","contentLength":2470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43307996"},{"title":"My 16-month theanine self-experiment","url":"https://dynomight.net/theanine/","date":1741489692,"author":"dynm","guid":93,"unread":true,"content":"<p>The internet <a href=\"https://en.wikipedia.org/wiki/Theanine\">theanine</a>. This is an amino acid analog that’s naturally found in tea, but now sold as a nutritional supplement for anxiety or mood or memory.</p><p>Biologically speaking, it’s plausible. Theanine is structurally related to the neurotransmitter <a href=\"https://en.wikipedia.org/wiki/Glutamate_(neurotransmitter)\">glutamate</a> (theanine = C₇H₁₄N₂O₃, glutamate = C₅H₈NO₄-). For some reason, everyone is obsessed with stupid flashy dopamine and serotonin, and no one cares about glutamate. But it’s the most common neurotransmitter and theanine is both metabolized into glutamate and seems to itself have various <a href=\"https://en.wikipedia.org/wiki/Theanine#In_vitro\">complicated</a> effects on glutamate receptors.</p><p>Of course, there are lots of supplements that  act on the brain, but are useless when taken orally. That’s because your brain is isolated from your circulatory system by a <a href=\"https://en.wikipedia.org/wiki/Blood%E2%80%93brain_barrier\">thin layer of cells</a> that are extremely picky about what they let through. But it appears that theanine <a href=\"https://doi.org/10.1271/bbb.63.615\"></a> get through these cells and into the brain.</p><p>So that sounds good. But do these low-level effects actually lead to changes in mood in real humans? When I looked into the academic research, I was surprised by how weak it was. Personally, on <a href=\"https://dynomight.net/aspartame/#the-european-food-safety-authority-efsa\">these kinds of issues</a>, I find the European Food Safety Authority to be the single most trustworthy scientific body. They did an <a href=\"https://www.efsa.europa.eu/en/efsajournal/pub/2238\">assessment</a> in 2011 and found:</p><table><tbody><tr><td>Improvement of cognitive function</td><td>cause and effect relationship has not been established</td></tr><tr><td>Alleviation of psychological stress</td><td>cause and effect relationship has not been established</td></tr><tr><td>Maintenance of normal sleep</td><td>cause and effect relationship has not been established</td></tr><tr><td>Reduction of menstrual discomfort</td><td>cause and effect relationship has not been established</td></tr></tbody></table><p><a href=\"https://examine.com/supplements/theanine/\">Examine</a> is an independent website that’s respected for summarizing the scientific literature on health and supplements. They looked into if theanine helped with various things, like alertness, anxiety, and attention. In all cases found  for .</p><p>A <a href=\"https://doi.org/10.1007/s11130-019-00771-5\">2020 review</a> of eight randomized double-blind placebo controlled trials found that theanine  help with stress and anxiety. While this review seems generally good, I found it to be insufficiently paranoid. One study they review found that theanine worked better than <a href=\"https://en.wikipedia.org/wiki/Alprazolam\">alprazolam</a> (xanax) for acute anxiety. The correct response would be, “That’s , and the fact that normal scientific practices could lead to such a conclusion casts doubt on everything.” But the review sort of takes it at value and moves on.</p><p>After 2020, the only major trial I could find was <a href=\"https://doi.org/10.1089/jmf.2020.4803\">this 2021 study</a> that took 52 healthy older Japanese people and gave them theanine (or placebo) for 12 weeks. They tested for improvements in a million different measures of cognitive functioning and mostly found nothing.</p><p>I’ve long found that tea makes me much less nervous than coffee, even with equal caffeine. Many people have suggested theanine as the explanation, but I’m skeptical. Most tea only has ~5 mg of theanine per cup, while when people supplement, they take 100-400 mg. Apparently grassy shade-grown Japanese teas are particularly high in theanine. And I  find those teas particularly calming. But they still only manage ~25 mg per cup. (Maybe it’s because tea is better than coffee?)</p><p>Still, I’ve supplemented theanine on and off for more than 10 years, and it seems helpful. So after seeing the weak scientific evidence, I thought: Why not do a self-experiment?</p><p>Theanine seems ideal because it’s a supplement with short term effects. So you can test it against placebo. (Try that with meditation.) And you can build up a large sample using a single human body without waiting weeks for it to build up in the body before each measurement.</p><p>Everyone agrees theanine is <a href=\"https://www.fda.gov/media/182086/download\">safe</a>. It’s biologically plausible. While academic studies haven’t proven a benefit, they haven’t  one either. Given the vast  evidence, I saw a chance to stick it to the stodgy scientific establishment, to show the power of internet people and give the first rigorous evidence that theanine really works. Stockholm, prepare thyself.</p><p>First, I needed placebos. This was super annoying. The obvious way to create them would be to buy some empty capsules and fill some with theanine and others with some inert substance. But that doesn’t sound fun. Isn’t the whole idea of modernity that we’re supposed to replace labor with capital?</p><p>So I went searching for a pair of capsules I could buy off the shelf, subject to the following constraints:</p><ol><li>Capsule A contains 200 mg of theanine.</li><li>Capsule B contains something with minimal acute effects on anxiety, stress, memory, concentration, etc.</li><li>Capsule B contains something I don’t mind putting into my body.</li><li>Both capsules are exactly the same size and weight.</li><li>Both capsules are almost but not  the same color.</li><li>Both capsules are made by some company with a history of making at least a modest effort to sell supplements that contain what they say they contain, and that don’t have terrifying levels of heavy metals.</li></ol><p>After a ludicrous amount of searching, I found that NOW® sells these veggie capsules:</p><p> 200 mg L-Theanine</p><p> 25 mcg (1,000 IU) Vitamin D</p><p>These are exactly the same size, exactly the same weight, exactly the same texture, and  close in color. They’re so close in color that under warm lighting, they’re indistinguishable. But under cold/blue lighting, the vitamin D capsules are  more yellow. Vitamin D might have some effects on mood, but no one seems to claim that they’re , that you’d feel them within an hour.</p><p>For dosing, I decided to take a capsule whenever I was feeling stressed or anxious. Some people worry this invalidates the results. Not so! I’m still choosing randomly, and this better reflects how people use theanine in practice.</p><p>Theanine is often recommended for reducing anxiety from caffeine. While I didn’t explicitly take caffeine as part of this experiment, I had almost always taken some anyway.</p><p>Statistically, it would have been best to randomize so I had a 50% chance of taking theanine and a 50% chance of taking vitamin D. But I decided that would be annoying, since I was taking these capsules when stressed. So I decided to randomize so I got theanine ⅔ of the time and vitamin D ⅓ of the time.</p><p>Randomization was very easy: I took two theanine capsules and one vitamin D capsule and put them into a little cup. I then closed my eyes, shook the cup around a bit and took one. I then covered the cup with a card.</p><p>This picture shows one vitamin D capsule (top) and two theanine capsules.</p><p>For each trial, I recorded my subjective starting stress level on a scale of 1-5, then set an alarm for an hour, which is <a href=\"https://doi.org/10.3945/jn.112.166371\">enough</a> to reach near-peak concentrations in the blood. After the alarm sounded (or occasionally later, if I missed it) I recorded the end time, my end stress level, and my percentage prediction that what I’d taken was actually theanine. Then, and only then, I looked into the cup. If the two remaining pills were different colors, I’d taken theanine. If not, it was vitamin D.</p><p>After ~14 months, I got frustrated by how slowly data was coming in. This was the first time in my life I’ve had  much chill. At that point, I decided to start taking the capsules once or twice a day, even if I wasn’t stressed. I’ll show the transition point in the graphs below.</p><p>Ultimately, I collected 94 data points, which look like this:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Here are the raw stress levels. Each line line shows one trial, with the start marked with a tiny horizontal bar. Note the clear change when I started dosing daily:</p><p>Alternatively, here’s the  in stress (end - start) as a function of time. If “Δ Stress” is negative, that means stress went down.</p><p>Here are the start and end stress levels for each trial, ignoring time. The dotted line shows equal stress levels, so anything below that line means stress went down:</p><p>Finally, here are the probabilities I gave that each capsule was in fact theanine.</p><p>My stress level  usually go down, at least provided I was stressed at the start. But it went down regardless of if I took theanine or not. And I was  at guessing what I’d taken.</p><p>Why did my stress decrease when I took vitamin D? Maybe it’s the placebo effect. But I suspect it’s mostly reversion to the mean: If you mark down the times in your life when you’re most stressed, on average you’ll be less stressed an hour later. You can see some evidence for this in that stress tended to decrease more when it started at a higher level.</p><p>So, eyeballing the above figures, theanine doesn’t appear to do anything. (We can argue about statistics below.) Why? I think these are the possibilities:</p><ol><li>Theanine works, but I got fake theanine.</li><li>Theanine works, but vitamin D works equally well.</li><li>Theanine works, but I was unlucky.</li><li>Theanine works, but I’m disembodied and unable to report my internal states.</li><li>Theanine works on some people, but not me.</li></ol><p>It’s hard to disprove the idea that theanine works. But I tell you this: I expected it to work. And I  tried. For almost 100 trials over 16 months, I paid attention to what I was feeling and tried to detect  sign that I’d taken theanine, even if it wasn’t a change in stress. I could detect nothing. Even after months of failure, I’d often feel confident that  time I could tell, only to be proven wrong.</p><table><tbody></tbody></table><p>Should I have been surprised by these results? Well, the scientific literature on theanine hasn’t found much of an effect. And the only other  self-experiment on theanine I’ve found is by <a href=\"https://niplav.site/nootropics#LTheanine\">Niplav</a>, who found it did slightly worse than chance and declared it a “hard pass”.</p><p>What about other blinded self-experiments with other substances? They’re surprisingly scarce, but here’s what I could find:</p><p>Stimulants work! But for everything else…</p><p>I particularly encourage you to read the sleep support post. He was confident it worked, he’d recommended it to lots of friends, but it totally failed when put to the test.</p><p>I’ve seen  other self-experiments (including for theanine), but they’re non-blinded and I’d be doing you a disservice if I linked to them. People often mention that  this means the results aren’t scientific, but treat it like a small niggling technicality. It’s not.</p><p>So I propose a new rule: Blind trial or GTFO.</p><p>I know many people reading this probably use and like theanine. Maybe it works for you! But given the weak academic results, and given the fact that I actually did a blinded experiment, I think you now have the burden of proof. Doing this kind of test isn’t hard. If you’re sure theanine (or anything else) works, prove it.</p><h2>Appendix: OK fine let’s argue about statistics</h2><p>Do you demand p-values? Are you outraged I just plotted the data and then started talking about it qualitatively?</p><p>I think faith in statistics follows a U-shaped curve. By default, people don’t trust them. If you learn a  statistics, they seem great. (Particularly if you’re part of a community that’s formed a little cult around one set of statistical practices and convinced each other that they’re more reliable than they are.) But if you learn a  of statistics, then you realize all the assumptions that are needed and all the ways things can go wrong and you become very paranoid.</p><p>If you want p-values, I’ll give you p-values. But first let me point out a problem.</p><p>While I was blinded during each trial, I saw the theanine/D result when I wrote it down. Over time I couldn’t help but notice that my stress dropped even when I took vitamin D, and that I was terrible at predicting what I’d taken. So while this experiment is randomized and blinded, the data isn’t  or . If I did this again, I’d make sure I couldn’t see any outcomes until the end, perhaps by making 100 numbered envelopes, putting three capsules in each, and only looking at what was left at the end.</p><p>But if you want to compute p-values anyway, OK! Here are the basic numbers for the trials when I took theanine:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Stress went down, p &lt; .0000001. But here are the the numbers for vitamin D:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Technically, I did find two significant results. But the second row says that end stress was slightly  with theanine than with vitamin D, and the last row says that I gave slightly  probabilities that I’d taken theanine when I’d actually taken vitamin D.</p><p>Of course, I don’t think this means I’ve proven theanine is harmful. I just think this confirms my general paranoia. To a first approximation, if it ain’t visible in the raw data, I ain’t going.</p><p>Speaking of raw data, you can download mine <a href=\"https://dynomight.net/img/theanine/data.csv\">here</a>.</p>","contentLength":12343,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43305803"},{"title":"Kill your Feeds – Stop letting algorithms dictate what you think","url":"https://usher.dev/posts/2025-03-08-kill-your-feeds/","date":1741457506,"author":"tom_usher","guid":92,"unread":true,"content":"<div data-intro=\"\">\nWe are being boiled like frogs. It happened gradually, one algorithmic tweak at a time. What started as a way to connect with friends has become a system that gives the corporations that run social media control over what we consume and the ability to subtly shape how we think.\n</div><p>We used to control apps like Facebook and Instagram with our own choices. They became daily comforts, making the world seem a little bit smaller and closer by bringing the people that we cared about together in to one place.</p><p>But from the perspective of these companies, that’s a problem. Our personal worlds, our friends, family, and connections, are finite. Once we’ve caught up, we put the app down. That’s bad for business.</p><p>Social media companies need us flicking through their apps as long as they can keep us there. More eyes on ads is more money. So they play the system a bit. You’ve lingered on enough photos of cute puppies, they know what you like.</p><p>Before long those feeds of finite content are replaced by infinite algorithmic content pulled from millions of users trying to optimise their posts to be picked up by the omnipotent algorithms. Algorithms which are completely opaque to us.</p><p>Sci-fi imagines megacorporations controlling our minds with brain implants. Some worry that companies are already listening in. But they don’t have to - they already control our eyes.</p><p>The creators of TikTok, Instagram etc. have gained control over exactly what we see. What we see strongly influences how we think. <a href=\"https://en.m.wikipedia.org/wiki/2021_Facebook_leak\">They know</a> that their feeds make us angry, they know the negative effects on our mental health (particularly that of teens), and they know that they have an influence on our opinion.</p><p>With the power to shape what we see comes the power to shape what we believe. Whether through deliberate manipulation or the slow creep of algorithmic recommendations, engagement is fueled by outrage, and outrage breeds extremism. The result is a feedback loop that isolates users, reinforces beliefs, and deprioritises opposing viewpoints.</p><p>We live in times where being able to form our own opinion is more important than ever. Where knowing how to source and identify truthful information is a critical skill.</p><p>Our reliance on being spoon fed ideas is destroying those abilities, Alec of Technology Connections calls this <a href=\"https://youtu.be/QEJpZjg8GuA\">algorithmic complacency</a>, referencing our increasing inability to look outside our algorithmically created bubble. The social media companies don’t care, the only person who has any interest in fixing this is you.</p><p>It’s time to take back control of how we think. We’ve identified the problem, now it’s time to take action.</p><p>We don’t all have the freedom, interest or willpower to delete social media from our lives entirely. It’s still where our friends are, an occasional distraction from reality and a source of entertainment. You don’t have to become a digital outcast to hold back this influence.</p><ol><li>Go directly to the source - if you like a particular TikTok creator, Facebook page or YouTube channel, skip the feed and go directly to their pages. Consider bookmarking their profiles individually.</li><li>Learn to find information and entertainment without a feed - try to find a creator making videos or writing about a topic of interest without having to stumble across them in a feed.</li><li>Use platforms and platform features that let you control your experience - Instagram’s ‘Following’ feed, YouTube’s Subscriptions page, <a href=\"https://bsky.app\">Bluesky</a>, <a href=\"https://mastodon.social\">Mastodon</a> and <a href=\"https://zapier.com/blog/how-to-use-rss-feeds/\">RSS feeds</a></li><li>Be mindful of engagement traps - recognise how algorithmic feeds are designed to keep you engaged and scrolling. Take a breath and stop the cycle.</li><li>Talk about it - if you’re reading this you a already know this is a problem. Your friends and family may not be aware of how their feeds are manipulating their attention and beliefs. Without intervention, the radicalisation of opinions, and the consequences we’re already seeing, will only escalate.</li></ol><p>The internet should serve you, not the other way around. Take back control. Kill your feeds before they kill your ability to think independently.</p>","contentLength":4057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43302132"},{"title":"Volkswagen reintroducing physical controls for vital functions","url":"https://www.autocar.co.uk/car-news/new-cars/volkswagen-reintroducing-physical-controls-vital-functions","date":1741418749,"author":"zfg","guid":91,"unread":true,"content":"<p>All future <a href=\"https://www.autocar.co.uk/car-review/volkswagen\">Volkswagen</a> models will feature physical controls for the most important functions, design chief Andreas Mindt has said.</p><p>The German firm has been criticised over the past few&nbsp;years for moving many of the vital controls in its cars from physical buttons and dials to the infotainment touchscreen. Volkswagen also introduced haptic ‘sliders’ below the touchscreen for the heating and volume&nbsp;and it started using haptic panels instead of buttons for controls mounted on the steering wheel.</p><p>More recently, the firm has reintroduced physical steering wheel buttons and Mindt said it is committed to reintroducing physical buttons, starting with the production version of the <a href=\"https://www.autocar.co.uk/car-news/new-cars/2025-volkswagen-id-2-will-be-even-better-concept\">ID 2all concept</a> that will arrive next year.</p><p>“From the ID 2all onwards, we will have physical buttons for the five most important functions – the volume, the heating on each side of the car, the fans and the hazard light – below the screen,” said Mindt. “They will be in every car that we make from now on. We understood this.</p><p>“We will never, ever make this mistake any more. On the steering wheel, we will have physical buttons. No guessing any more. There's feedback, it's real, and people love this. Honestly, it's a car. It's not a phone: it's a car.”</p><p>Mindt said VW will continue to offer cars with touchscreens, in part due to new legal requirements that, as in the US, will require all cars to feature a reversing camera.&nbsp;</p><p>“There are a lot of functions you have to deliver in certain areas, so the screen will be big and you will find a lot of HMI [human-machine interface] contents in the depths of the system,” he added. “But the five main things will always be on the first physical layer. That’s very important.”</p>","contentLength":1726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43298271"},{"title":"AI tools are spotting errors in research papers","url":"https://www.nature.com/articles/d41586-025-00648-5","date":1741388098,"author":"kgwgk","guid":90,"unread":true,"content":"<figure><picture><img alt=\"A large stack of papers and folders with coloured tabs.\" loading=\"lazy\" src=\"//media.nature.com/lw767/magazine-assets/d41586-025-00648-5/d41586-025-00648-5_50703894.jpg\"><figcaption></figcaption></picture></figure><p>Late last year, media outlets worldwide warned that black plastic cooking utensils contained worrying levels of cancer-linked flame retardants. The risk was found to be overhyped — a mathematical error in the underlying research suggested a key chemical exceeded the safe limit when in fact it was ten times lower than the limit. Keen-eyed researchers quickly showed that an artificial intelligence (AI) model could have spotted the error in seconds.</p><p>The incident has spurred two projects that use AI to find mistakes in the scientific literature. The Black Spatula Project is an open-source AI tool that has so far analysed around 500 papers for errors. The group, which has around eight active developers and hundreds of volunteer advisers, hasn’t made the errors public yet; instead, it is approaching the affected authors directly, says Joaquin Gulloso, an independent AI researcher based in Cartagena, Colombia, who helps to coordinate the project. “Already, it’s catching many errors,” says Gulloso. “It’s a huge list. It’s just crazy.”</p><p>The other effort is called YesNoError and was inspired by the Black Spatula Project, says founder and AI entrepreneur Matt Schlicht. The initiative, funded by its own dedicated cryptocurrency, has set its sights even higher. “I thought, why don’t we go through, like, all of the papers?” says Schlicht. He says that their AI tool has analysed more than 37,000 papers in two months. Its website flags papers in which it has found flaws – many of which have yet to be verified by a human, although Schlicht says that YesNoError has a plan to eventually do so at scale. Currently, the YesNoError site lists errors which are not real and the initiative has not yet published a full account of how well the tool works.</p><p>Both projects want researchers to use their tools before submitting work to a journal, and journals to use them before they publish, the idea being to avoid mistakes, as well as fraud, making their way into the <a href=\"https://www.nature.com/articles/d41586-025-00026-1\" data-track=\"click\" data-label=\"https://www.nature.com/articles/d41586-025-00026-1\" data-track-category=\"body text link\">scientific literature</a>.</p><p>The projects have tentative support from academic sleuths who work in research integrity. But there are also concerns over the potential risks, including that the tools could be used maliciously and before they are ready. How well the tools can spot mistakes, and whether their claims have been verified, must be made clear, says Michèle Nuijten, a researcher in metascience at Tilburg University in the Netherlands. “If you start pointing fingers at people and then it turns out that there was no mistake, there might be reputational damage,” she says.</p><p>Others add that although there are risks and the projects need to be cautious about what they claim, the goal is the right one. It is much easier to churn out shoddy papers than it is to retract them, says James Heathers, a forensic metascientist at Linnaeus University in Växjö, Sweden. As a first step, AI could be used to triage papers for further scrutiny, says Heathers, who has acted as a consultant for the Black Spatula Project. “It’s early days, but I’m supportive” of the initiatives, he adds.</p><p>Both the Black Spatula Project and YesNoError use large language models (LLMs) to spot a range of errors in papers, including ones of fact as well as in calculations, methodology and referencing.</p><p>The systems first extract information, including tables and images, from the papers. They then craft a set of complex instructions, known as a prompt, which tells a ‘reasoning’ model — a specialist type of LLM — what it is looking at and what kinds of error to hunt for. The model might analyse a paper multiple times, either scanning for different types of error each time, or to cross-check results. The cost of analysing each paper ranges from 15 cents to a few dollars, depending on the length of the paper and the series of prompts used.</p><p>The rate of false positives, instances in which the AI claims an error where there is none, is a major hurdle. Currently, the Black Spatula Project’s system is wrong about an error around 10% of the time, says Gulloso. Each alleged error must be checked with experts in the subject, and finding them is the project’s greatest bottleneck, says Steve Newman, the software engineer and entrepreneur who founded the Black Spatula Project.</p><p>So far, Schlicht’s YesNoError team has quantified the false positives in only around 100 mathematical errors that the AI found in an initial batch of 10,000 papers. Of the 90% of authors who responded to Schlicht, all but one agreed that the error detected was valid, he says. Eventually, YesNoError is planning to work with ResearchHub, a platform which pays PhD scientists in cryptocurrency to carry out peer review. When the AI has checked a paper, YesNoError will trigger a request to verify the results, although this has not yet started.</p>","contentLength":4812,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43295692"},{"title":"Age Verification Laws: A Backdoor to Surveillance","url":"https://www.eff.org/deeplinks/2025/03/first-porn-now-skin-cream-age-verification-bills-are-out-control","date":1741372442,"author":"hn_acker","guid":89,"unread":true,"content":"<h3><b>Age Verification Laws: A Backdoor to Surveillance</b></h3><p><a href=\"https://www.route-fifty.com/digital-government/2025/02/alabama-house-bills-would-limit-social-media-access-minors/403172/?oref=rf-homepage-river\"></a></p><ol><li><a href=\"https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202520260AB728\"></a></li><li><a href=\"https://www.nysenate.gov/legislation/bills/2025/A3323\"></a></li><li><a href=\"https://app.leg.wa.gov/billsummary?BillNumber=5622&amp;Initiative=False&amp;Year=2025\"></a></li></ol><h3><b>The Problem with Age Verification: No Solution Is Safe</b></h3><p><b>no method of age verification is both privacy-protective and entirely accurate</b><a href=\"https://www.eff.org/deeplinks/2024/10/eff-new-york-age-verification-threatens-everyones-speech-and-privacy\"></a></p><p><a href=\"https://www.eff.org/deeplinks/2025/01/face-scans-estimate-our-age-creepy-af-and-harmful\"></a><a href=\"https://www.eff.org/deeplinks/2024/06/hack-age-verification-company-shows-privacy-danger-social-media-laws\"></a></p><p><a href=\"https://www.eff.org/deeplinks/2024/03/analyzing-kosas-constitutional-problems-depth\"></a><a href=\"https://www.eff.org/wp/privacy-first-better-way-address-online-harms\"></a></p>","contentLength":181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43292820"}],"tags":["dev"]}