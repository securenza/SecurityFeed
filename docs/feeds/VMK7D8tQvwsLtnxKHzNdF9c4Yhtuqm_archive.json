{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"HackerNews","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":35,"items":[{"title":"The Worst Programmer I Know (2023)","url":"https://dannorth.net/the-worst-programmer/","date":1742735325,"author":"rbanffy","guid":12808,"unread":true,"content":"<p>The great thing about measuring developer productivity is that you can quickly identify the bad programmers. I want to tell you about the worst programmer I know, and why I fought to keep him in the team.</p><p>A few years ago I wrote a Twitter/X thread about <a href=\"https://twitter.com/tastapod/status/1010461873270153216?s=20\">the best programmer I know</a>, which I should write up as a blog post. It seems only fair to tell you about the worst one too. His name is <a href=\"https://www.linkedin.com/in/timmackinnon/\">Tim Mackinnon</a> and I want you to know how  he is.</p><p>We were working for a well-known software consultancy at a Big Bank that decided to introduce individual performance metrics, “for appraisal and personal development purposes”. This was cascaded through the organisation, and landed in our team in terms of story points delivered. This was after some considered discussion from the department manager, who knew you shouldn’t measure things like lines of code or bugs found, because people can easily game these.</p><p>Instead we would measure stories delivered, or it may have been story points (it turns out it <a href=\"https://www.researchgate.net/publication/4106463_The_Slacker's_Guide_to_Project_Tracking_or_spending_time_on_more_important_things\">doesn’t matter</a>), because these represented business value. We were using something like Jira, and people would put their name against stories, which made it super easy to generate these productivity metrics.</p><p>Which brings me to Tim. Tim’s score was consistently zero. Zero! Not just low, or trending downwards, but literally zero. Week after week, iteration after iteration. Zero points for Tim.</p><p>Well Tim clearly had to go. This was the manager’s conclusion, and he asked me to make the necessary arrangements to have Tim removed and replaced by someone who actually delivered, you know, stories.</p><p>And I flatly refused. It wasn’t even a hard decision for me, I just said no.</p><p>You see, the reason that Tim’s productivity score was zero, was that <em>he never signed up for any stories</em>. Instead he would spend his day pairing with different teammates. With less experienced developers he would patiently let them drive whilst nudging them towards a solution. He would not crowd them or railroad them, but let them take the time to learn whilst carefully crafting moments of insight and learning, often as <a href=\"https://en.wikipedia.org/wiki/Socratic_questioning\">Socratic questions</a>, what ifs, how elses.</p><p>With seniors it was more like co-creating or sparring; bringing different worldviews to bear on a problem, to produce something better than either of us would have thought of on our own. Tim is a heck of a programmer, and you always learn something pairing with him.</p><p>Tim wasn’t delivering software; Tim was delivering a team that was delivering software. The entire team became more effective, more productive, more aligned, more idiomatic, more , because Tim was in the team.</p><p>I explained all this to the manager and invited him to come by and observe us working from time to time. Whenever he popped by, he would see Tim sitting with someone different, working on “their” thing, and you could be sure that the quality of that thing would be significantly better, and the time to value significantly lower—yes, you can have better and faster and cheaper, it just takes discipline—than when Tim wasn’t pairing with people.</p><p>In the end we kept Tim, and we quietly dropped the individual productivity metrics in favour of team accountability, where we tracked—and celebrated—the business impact we were delivering to the organisation as a high-performing unit.</p><p>Measure productivity by all means—I’m all for accountability—ideally as tangible business impact expressed in dollars saved, generated, or protected. This is usually hard, so proxy business metrics are fine too.</p><p>Just don’t try to measure the individual contribution of a unit in a complex adaptive system, because the premise of the question is flawed.</p><p>DORA metrics, for example, are about how the system of work works, whether as Westrum culture indicators or flow of technical change into production. They measure the engine, not the contribution of individual pistons, because that <a href=\"https://en.wikipedia.org/wiki/Chewbacca_defense\">makes no sense</a>.</p><p>Also, if you ever get the chance to work with Tim Mackinnon, you should do that.</p>","contentLength":3994,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43452649"},{"title":"Landrun: Sandbox any Linux process using Landlock, no root or containers","url":"https://github.com/Zouuup/landrun","date":1742651819,"author":"Zoup","guid":12314,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43445662"},{"title":"Monster Cables picked the wrong guy to threaten (2008)","url":"https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/","date":1742603437,"author":"wallflower","guid":10748,"unread":true,"content":"<blockquote><p><em>… Once I have received the above materials and explanations from you, I will undertake to analyze this information and let you know whether we are willing to accede to any of the demands made in your letter. <strong>If my analysis shows that there is any reasonable likelihood that we have infringed in any way any of Monster Cable’s intellectual property rights, we will of course take any and all action necessary to resolve the situation. </strong> If I do not hear from you within the next fourteen days, or if I do hear from you but do not receive </em><em>all of the information requested above, I will assume that you have abandoned these claims and closed your file.</em></p><p><em> As for your requests for information, or for action, directed to me: I would remind you that it is you, not I, who are making claims; and it is you, not I, who must substantiate those claims.  You have not done so.</em></p><p><em> I have seen Monster Cable take untenable IP positions in various different scenarios in the past, and am generally familiar with what seems to be Monster Cable’s </em><em>modus operandi in these matters.  I therefore think that it is important that, before closing, I make you aware of a few points.</em></p><p><em> After graduating from the University of Pennsylvania Law School in 1985, I spent nineteen years in litigation practice, with a focus upon federal litigation involving large damages and complex issues.  My first seven years were spent primarily on the defense side, where <strong>I developed an intense frustration with insurance carriers who would settle meritless claims for nuisance value when the better long-term view would have been to fight against vexatious litigation as a matter of principle.</strong> In plaintiffs’ practice, likewise, I was always a strong advocate of standing upon principle and taking cases all the way to judgment, even when substantial offers of settlement were on the table.  I am “uncompromising” in the most literal sense of the word.  If Monster Cable proceeds with litigation against me I will pursue the same merits-driven approach; I do not compromise with bullies and <strong>I would rather spend fifty thousand dollars on defense than give you a dollar of unmerited settlement funds.</strong> As for signing a licensing agreement for intellectual property which I have not infringed: that will not happen, under any circumstances, whether it makes economic sense or not.</em></p><p><em> I say this because my observation has been that Monster Cable typically operates in a hit-and-run fashion.  Your client threatens litigation, expecting the victim to panic and plead for mercy; and what follows is a quickie negotiation session that ends with payment and a licensing agreement.  Your client then uses this collection of licensing agreements to convince others under similar threat to accede to its demands.  Let me be clear about this: <strong>there are only two ways for you to get anything out of me.  You will either need to (1) convince me that I have infringed, or (2) obtain a final judgment to that effect from a court of competent jurisdiction. </strong>It may be that my inability to see the pragmatic value of settling frivolous claims is a deep character flaw, and I am sure a few of the insurance carriers for whom I have done work have seen it that way; but it is how I have done business for the last quarter-century and you are not going to change my mind.  If you sue me, the case will go to judgment, and I will hold the court’s attention upon the merits of your claims–or, to speak more precisely, the absence of merit from your claims–from start to finish. <strong>Not only am I unintimidated by litigation; I sometimes rather miss it.</strong></em></p></blockquote><p>I can relate to Denke’s final comment quoted above ….  I wonder what the attendant publicity is doing for his sales.</p>","contentLength":3715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43442178"},{"title":"France rejects backdoor mandate","url":"https://www.eff.org/deeplinks/2025/03/win-encryption-france-rejects-backdoor-mandate","date":1742589311,"author":"hn_acker","guid":7969,"unread":true,"content":"<p><a href=\"https://www.lemonde.fr/societe/article/2025/03/21/l-assemblee-vote-pour-le-maintien-de-la-confidentialite-des-messageries-cryptees-lors-d-une-nuit-agitee_6584121_3224.html\"></a></p><p><a href=\"https://www.eff.org/deeplinks/2019/01/give-ghost-backdoor-another-name\"></a><a href=\"https://www.justsecurity.org/64968/why-the-ghost-keys-solution-to-encryption-is-no-solution/\"></a><a href=\"https://www.internetsociety.org/resources/doc/2020/fact-sheet-ghost-proposals/\"></a></p><p><a href=\"https://www.laquadrature.net/en/warondrugslaw/\"></a></p><p><a href=\"https://www.eff.org/deeplinks/2024/03/european-court-human-rights-confirms-undermining-encryption-violates-fundamental\"></a></p><p><a href=\"https://www.eff.org/deeplinks/2024/12/defending-encryption-us-and-abroad\"></a></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43440513"},{"title":"IronRDP: a Rust implementation of Microsoft's RDP protocol","url":"https://github.com/Devolutions/IronRDP","date":1742571327,"author":"mikece","guid":10747,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43436894"},{"title":"Career Development: What It Means to Be a Manager, Director, or VP (2015)","url":"https://kellblog.com/2015/03/08/career-development-what-it-really-means-to-be-a-manager-director-or-vp/","date":1742554181,"author":"AnhTho_FR","guid":10746,"unread":true,"content":"<p>It’s no secret that I’m not a fan of big-company HR practices.&nbsp; I’m more of the <a href=\"http://www.amazon.com/First-Break-All-Rules-Differently-ebook/dp/B00HL2S4LW\">First Break all the Rules</a> type.&nbsp; Despite my general skepticism of many standard practices, we still do annual performance reviews at my company, though I’m thinking seriously of dropping them.&nbsp; (See <a href=\"http://www.amazon.com/Get-Rid-Performance-Review-Managing/dp/B004X8WB48\">Get Rid of the Performance Review</a>.)</p><p>Another practice I’m not hugely fond of is “leveling” — the creation of a set of granular levels to classify jobs across the organization.&nbsp; Leveling typically results in something that looks like this:</p><p>While I am a huge fan of compensation benchmarking (i.e., figuring out what someone is worth in the market before they do by getting another job), I think classical leveling has a number of problems:</p><ul><li>It’s futile to level across functions. Yes, you might discover that a Senior FPA Analyst II earns the same as a Product Marketing Director I, but why does that matter?&nbsp; It’s a coincidence.&nbsp; It’s like saying with $3.65 I can buy either a grande non-fat latte or a head of organic lettuce.&nbsp; What matters is the fair price of each of those goods in the market — not they that happen to have the same price.&nbsp; So I object to the whole notion of levels across the organization. &nbsp;It’s not canonical; it’s coincidence.</li></ul><ul><li>Most leveling systems are too granular, with the levels separated by arbitrary characterizations. It’s makework.&nbsp; It’s fake science.&nbsp; It’s bureaucratic and encourages a non-thinking “climb the ladder” approach to career development.&nbsp; (“Hey, let’s develop you to go from somewhat-independent to rather-independent this year.”)</li></ul><ul><li>It conflates career development and salary negotiation. It encourages a mindset of saying, “what must I do to make L10” when you want to say, “I want a $10K raise.”&nbsp; I can’t tell you the number of times people have asked me for “development” or “leveling” conversations where I get excited and start talking about learning, skills gaps, and such and it’s clear all they wanted to talk about was salary.&nbsp; Disappointing.</li></ul><p>That said, I do believe there are three meaningful levels in management and it’s important to understand the differences among them.&nbsp; I can’t tell you the number of times someone has sincerely asked me, “what does it take to be a director?” or, “how can I develop myself into a VP?”</p><p>It’s a hard question.&nbsp; You can turn to the leveling system for an answer, but it’s not in there.&nbsp; For years, in fact, I’ve struggled to find what I consider to be a good answer to the question.</p><p>I’m not talking about Senior VP vs. Executive VP or Director vs. Senior Director.&nbsp; I view such adjectives as window dressing or <a href=\"https://www.englishclub.com/ref/esl/Idioms/E/earn_your_stripes_162.htm\">stripes</a>:&nbsp; important recognition along the way, but nothing that fundamentally changes one’s level.</p><p>I’m not talking about how many people you manage.&nbsp; In call centers, a director might manage 500 people.&nbsp; In startups, a VP might manage zero.</p><p>I am talking about one of three levels at which people operate:&nbsp; manager, director, and vice president.&nbsp; Here are my definitions:</p><ul><li><strong>Managers are paid to drive results with some support</strong>. They have experience in the function, can take responsibility, but are still learning the job and will have questions and need support.&nbsp; They can execute the tactical plan for a project but typically can’t make it.</li></ul><ul><li><strong>Directors are paid to drive results with little or no supervision</strong> (“set and forget”). Directors know how to do the job.&nbsp; They can make a project’s tactical plan in their sleep.&nbsp; They can work across the organization to get it done.&nbsp; I love strong directors.&nbsp; They get shit done.</li></ul><ul><li><strong>VPs are paid to make the plan</strong>. Say you run marketing.&nbsp; Your job is to understand the company’s business situation, make a plan to address it, build consensus to get approval of that plan, and then go execute it.</li></ul><p>The biggest single development issue I’ve seen over the years is that many VPs still think like directors. [1]</p><p>Say the plan didn’t work.&nbsp;&nbsp; “But, we executed the plan we agreed to,” they might say, hoping to play a get-out-of-jail-free card with the CEO (which is about to boomerang).</p><p>Of course, the VP got approval to execute the plan. &nbsp;Otherwise, you’d be having a different conversation, one about termination for insubordination.</p><p>But the plan didn’t work.&nbsp; Because directors are primarily execution engines, they can successfully play this card.&nbsp; Fair enough.&nbsp; Good directors challenge their plans to make them better.&nbsp; But they can still play the approval&nbsp;card successfully because their primary duty is to execute the plan, not make it.</p><p>VP’s, however, cannot play the approval card. &nbsp;The VP’s job is to get the right answer. &nbsp;They are the functional expert.&nbsp; No one on the team knows their function better than they do.&nbsp; And even if someone did, they are still playing the VP of function role and it’s their job – and no one else’s — to get the right answer.</p><p>Now, you might be thinking, “glad I don’t work for Dave” right now — he’s putting failure of a plan to which he and the team agreed on the back of the VP.&nbsp; And I am.</p><p>But it’s the same standard to which the CEO is held.&nbsp; If the CEO makes a plan, gets it approved by the board, and executes it well but it doesn’t work, they cannot tell the board “but, but, it’s the plan we agreed to.”&nbsp; Most CEOs wouldn’t even dream of saying that.&nbsp; It’s because CEOs understand they are held accountable not for effort or activity, but results.</p><p>Part of truly operating at the VP level is to internalize this fact.&nbsp; You are accountable for results.&nbsp; Make a plan that you believe in.&nbsp; Because if the plan doesn’t work, you can’t hide behind approval.&nbsp; Your job was to make a plan that worked.&nbsp; If the risk of dying on a hill is inevitable, you may as well die on your own hill, and not someone else’s.</p><p>Paraphrasing the ancient <a href=\"https://www.youtube.com/watch?v=OHug0AIhVoQ\">Fram oil filter commercial</a>, I call this “you can fire me now or fire me later” principle.&nbsp; An executive should never sign up for a plan they don’t believe in.&nbsp; They should risk being fired now for refusing to sign up for the plan (e.g., challenging assumptions, delivering bad news) as opposed to halfheartedly executing a plan they don’t believe in and almost certainly getting fired for its failure later. &nbsp;The former is a far better way to go than the latter.</p><p>This is important not only because it prepares the VP to one day become a&nbsp; CEO, but also because it empowers the VP in making their plan.&nbsp; If this my plan, if I am to be judged on its success or failure, if I am not able to use approval as a get-out-of-jail-free card, then is it the right plan?</p><p>That’s the thinking I want to stimulate. &nbsp;That’s how great VPs think.</p><p>[1] Since big companies throw around the VP title pretty casually, this post is arguing that many of those VPs are actually directors in thinking and accountability. &nbsp;This may be one reason why big company VPs have trouble adapting to the e-staff of startups.</p>","contentLength":6966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43434093"},{"title":"Boycott IETF 127","url":"https://boycott-ietf127.org/","date":1742530592,"author":"randompeach","guid":10745,"unread":true,"content":"<div><p><b>Ellen Emilia Anna Zscheile</b></p></div><div><p><b>Björne Söderberg Laanemäe</b></p></div><div><p><b>Samuel Eduardo Aleman Espinoza</b></p></div>","contentLength":84,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43431780"},{"title":"The Burnout Machine","url":"https://unionize.fyi/","date":1742495057,"author":"flxfxp","guid":6448,"unread":true,"content":"<p><i>Originally written by Biozombie, published in 2600 Hacker Quarterly, Autumn 2024</i></p><p>\n            Let’s get real for a minute: the tech industry loves to sell us on the myth of the \"dream\n            job.\"\n            You\n            know\n            the\n            pitch - beanbags in the office, free kombucha on tap, and \"Agile\" processes that are supposed to\n            make\n            everything\n            more flexible, more efficient. But the reality? It’s a meat grinder that chews up developers,\n            sysadmins,\n            and\n            infosec pros and spits them out the other side - burnt out, disillusioned, and disposable.\n        </p><p>\n            We’re living in a world where billion dollar tech companies expect us to live and breathe code,\n            demanding\n            80\n            hour weeks under the guise of \"passion.\" And what do we get in return? Burnout, anxiety, and the\n            constant\n            threat\n            of layoffs. It’s time to face facts: this industry is not your friend. It’s a machine, and\n            unless we\n            start\n            organizing, it’s going to keep grinding us down. It’s time to talk about unionizing tech jobs.\n        </p><p>\n            Remember when Agile was supposed to save us all? Flexible sprints, self-organizing teams - yeah, right. In\n            practice, Agile has been twisted into a tool for management to push us harder and faster. They say\n            it’s\n            about\n            \"responding to change over following a plan,\" but let’s be honest - it’s about dangling\n            more\n            carrots\n            and keeping\n            us on a treadmill that never stops. The sprint becomes a marathon, and we’re the ones paying the\n            price.\n\n        </p><p>\n            And then there’s burnout. We’re in an industry where burnout isn’t just common -\n            it’s\n            expected. If you’re not\n            pulling all-nighters, you’re \"not committed.\" If you’re not answering Slack messages at\n            midnight,\n            you’re \"not a\n            team player.\" This culture is toxic, and it’s only getting worse. The relentless churn of\n            projects,\n            the\n            constant\n            pressure to innovate, and the ever-present threat of obsolescence create a perfect storm of stress. And\n            what’s\n            the industry’s solution? A mindfulness app and a lecture on work-life balance. Give me a break.\n        </p><p>\n            Let’s talk about job security - because there isn’t any. The tech industry loves to hype itself\n            as a\n            meritocracy, where the best and the brightest rise to the top. But in reality, it’s a meat market. As\n            soon\n            as\n            you’re not \"on the cutting edge,\" you’re out. Outsourcing, contract work, gig economy\n            bullshit -\n            it’s all\n            designed to keep us insecure, to keep us grinding away at the next big thing with no guarantee that\n            we’ll\n            have a\n            job next week, next month, or next year.\n        </p><p>\n            Companies love to brag about their innovation, but the real innovation is finding new ways to make us\n            disposable. Permanent employment? That’s for suckers. Why pay benefits and offer job security when\n            they\n            can\n            churn through contractors and freelancers like cheap code? And don’t get me started on those\n            non-compete\n            clauses\n            - designed to keep you locked down and terrified to make a move that might actually be good for your career.\n        </p><p>\n            And let’s not forget the ethical side of this equation. We’re being asked to build the future,\n            to\n            develop AI,\n            blockchain, and all the other buzzword technologies that are supposed to change the world. But at what cost?\n            How\n            many of us have been forced to work on projects that make us sick to our stomachs - surveillance tech, data\n            mining tools, algorithms that reinforce social biases - because we don’t have the power to say no?\n        </p><p>\n            That’s the kicker. We’re the ones building the damn future, but we have no say in how it’s\n            built. We don’t get\n            to decide whether our code is used for good or for evil. And as long as we’re isolated, as long as\n            we’re afraid\n            to speak up because we might lose our jobs, nothing will change.\n        </p><p>\n            This industry isn’t going to fix itself. The billionaires at the top aren’t going to suddenly\n            grow a\n            conscience,\n            and they aren’t going to give us the power to push back. That power has to come from us - from\n            organizing,\n            from\n            resisting, from breaking - unless we organize, unless we unionize.\n        </p><p>\n            Unionizing isn’t just about getting better pay or benefits (though we desperately need both).\n            It’s\n            about taking\n            back some control. It’s about having a say in how we work, what we work on, and how we’re\n            treated.\n            It’s about\n            saying no to the endless churn, the burnout culture, the gig economy bullshit.\n        </p><p>\n            And don’t let anyone tell you it’s impossible. The Alphabet Workers Union at Google?\n            They’re\n            showing us it can\n            be done. They’re standing up to one of the biggest companies in the world and saying,\n            \"Enough.\" We\n            need\n            more of\n            that. We need to take that energy and spread it across the industry - across all the companies that are\n            profiting off our sweat and tears.\n        </p><p>\n            Hackers, we’ve always been about more than just code. We’ve been about freedom - freedom of\n            information, freedom\n            from control. Unionizing is the next logical step. It’s about taking the hacker ethos into the\n            workplace,\n            about\n            organizing to protect ourselves and each other.\n        </p><ul><li> Talk to your coworkers. Break the silence. The first step to\n                organizing is\n                realizing you’re not alone.\n            </li><li><strong>Support Existing Efforts:</strong> If you’re in a company where union efforts are already\n                underway, get\n                involved.\n                If not, start thinking about how you can start one.\n            </li><li> We’re hackers - we know how to communicate securely, how to\n                organize without\n                being\n                detected. Let’s use those skills to build something real, something that can stand up to the\n                powers that\n                be.</li><li> Let’s make sure that any union platform we build isn’t\n                just about wages and\n                hours, but\n                about ethics too. We need to have a say in what we’re building and how it’s used.</li></ul><p>\n            The tech industry is a runaway train, and if we don’t do something soon, we’re going to get run\n            over. The\n            burnout, the job insecurity, the nightmares - it’s all going to keep getting worse unless we take a\n            stand.\n            Unionizing isn’t just a nice idea - it’s a necessity.\n        </p><p>\n            So let’s do what hackers do best: let’s disrupt. Let’s take the tools they’ve given\n            us, the skills we’ve\n            honed, and use them to build something better. Let’s unionize. Let’s take back our industry,\n            take back our\n            jobs, and take back our futures.\n        </p><p>\n            The future of tech is being written right now, and it’s up to us to decide what kind of story it will\n            be.\n            Let’s make it a story we can be proud of.\n        </p>","contentLength":7983,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43427002"},{"title":"OpenAI Audio Models","url":"https://www.openai.fm/","date":1742491080,"author":"KuzeyAbi","guid":10744,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43426022"},{"title":"Claude can now search the web","url":"https://www.anthropic.com/news/web-search","date":1742489472,"author":"meetpateltech","guid":6324,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43425655"},{"title":"FOSS infrastructure is under attack by AI companies","url":"https://thelibre.news/foss-infrastructure-is-under-attack-by-ai-companies/","date":1742475049,"author":"todsacerdoti","guid":6174,"unread":true,"content":"<p>Three days ago, Drew DeVault - founder and CEO of SourceHut - published a blogpost called, \"Please stop externalizing your costs directly into my face\", where he complained that LLM companies were crawling data without respecting robosts.txt and causing severe outages to SourceHut.</p><p>I went, \"Interesting!\", and moved on.</p><p>Then, yesterday morning, KDE GitLab infrastructure was overwhelmed by another AI crawler, with IPs from an Alibaba range; this caused GitLab to be temporarily inaccessible by KDE developers.</p><p>I then discovered that, one week ago, an Anime girl started appearing on the GNOME GitLab instance, as the page was loaded. It turns out that it's the default loading page for Anubis, a proof-of-work challenger that blocks AI scrapers that are causing outages.</p><p>By now, it should be pretty clear that this is no coincidence. AI scrapers are getting more and more aggressive, and - since FOSS software relies on public collaboration, whereas private companies don't have that requirement - this is putting some extra burden on Open Source communities.</p><p>So let's try to get more details – going back to Drew's blogpost. According to Drew, LLM crawlers don't respect robots.txt requirements and include expensive endpoints like git blame, every page of every git log, and every commit in your repository. They do so using random User-Agents from tens of thousands of IP addresses, each one making no more than one HTTP request, trying to blend in with user traffic.</p><p>Due to this, it's hard to come off with a good set of mitigations. Drew says that several high-priority tasks have been delayed for weeks or months due to these interruptions, users have been occasionally affected (because it's hard to distinguish bots and humans), and - of course - this causes occasional outages of SourceHut.</p><p>Drew here does not distinguish between which AI companies are more or less respectful of robots.txt files, or more accurate in their user agent reporting; we'll be able to look more into that later.</p><p>Finally, Drew points out that this is not some isolated issue. He says, </p><blockquote>All of my sysadmin friends are dealing with the same problems, [and] every time I sit down for beers or dinner to socialize with sysadmin friends it's not long before we're complaining about the bots. [...] The desperation in these conversations is palpable.</blockquote><p>Which brings me back to yesterday's KDE GitLab issues. According to Ben, part of the KDE sysadmin team, all of the IPs that were performing this DDoS were claiming to be MS Edge, and were due to Chinese AI companies; he mentions that Western LLM operators, such as OpenAI and Anthropic, were at least setting a proper UA - again, more on this later.</p><p>The solution - for now - was to ban the version of Edge that the bots were claiming to be, though it's hard to believe that this will be a definitive solution; these bots do seem keen on changing user agents to try to blend in as much as possible.</p><p>Indeed, GNOME has been experiencing issues since a last November; as a temporary solution they had rate-limited non-logged in users from seeing merge requests and commits, which obviously also caused issues for real human guests.</p><p>The solution the eventually settled to was switching to Anubis. This is a page that presents a challenge to the browser, which then has to spend time doing some math and presenting the solution back to the server. If it's right, you get access to the website.</p><p>According to the developer, this project is \"a bit of a nuclear response, but AI scraper bots scraping so aggressively have forced my hand. I hate that I have to do this, but this is what we get for the modern Internet because bots don't conform to standards like robots.txt, even when they claim to\".</p><p>However, this is also causing user issues. When a lot of people open the link from the same place, it might happen that they get served some higher-difficulty exercise that will take some time to complete; there's one user reporting one minute delay, and another - from his phone - having to wait around two minutes.</p><p>Why? Well, a GitLab link was pasted in a chatroom! Similarly, the same happened when the Triple Buffering GNOME merge request was posted to Hacker News, and thus received a lot of attention over there. As the developer said, it's a nuclear option for crawlers, but it also has human consequences.</p><p>Over Mastodon, one GNOME sysadmin, Bart Piotrowski, kindly shared some numbers to let people fully understand the scope of the problem. According to him, in around two hours and a half they received 81k total requests, and out of those only 3% passed Anubi's proof of work, hinting at 97% of the traffic being bots – an insane number!</p><p>That said, at least  worked. Other organizations are having a harder time dealing with these scrapers.</p><p>As an example, here's Jonathan Corbet, who runs the FOSS news source LWN, warns users that the website might be \"occasionally sluggish\"… due to DDoS from AI scraper bots. He claims that \"only a small fraction of our traffic is serving actual human readers\", and at some point, the bots \"decides to hit us from hundreds of IP addresses at once. [..] They don't identify themselves as bots, and robots.txt is the only thing they don't read off the site\".</p><p>Many expressed solidarity, including Kevin Fenzi, sysadmin for the Fedora project. They've also been having issues with AI scrapers: firstly, one month ago they had to fight to get pagure.io to stay alive:</p><p>However, things got  over time, so they had to block a bunch of subnets, which has also impacted many real users. Out of desperation, at one point Kevin decided to ban the entire country of Brazil to get things to work again; to my understanding, this ban is still in effect, and it's not so clear where a longer-term solution might be found.</p><p>And, as Neal Gompa points out, even this blocking an entire country only gets you so far, and apparently the Fedora infrastructure has been \"regularly down for weeks\" because of AI scrapers.</p><p>Another project that's been hit by this issue  is Inkscape. According to Martin Owens, it's not \"the usual Chinese DDoS from last year, but from a pile of companies that started ignoring our spider conf and started spoofing their browser info. I now have a Prodigius block list. If you happen to work for a big company doing AI, you may not get our website anymore\".</p><p>And, well, Martin is not the only developer who has built a \"prodigious block list\". Even BigGrizzly from Frama software was flooded by a bad LLM crawler, and built a list of 460K IPs with spoofed user agents to ban; he's offering to share the list around.</p><p>One more comprehensive attempt at this is the \"ai.robots.txt\" project, an open list of web crawlers associated with AI companies. They offer a robots.txt that implements the Robots Exclusion Protocol and a .htaccess file that will return an error page when getting a request from any AI crawler in their list.</p><p>We can get some more numbers about the crawlers if we go a few months back. Here's a post by Dennis Schubert about the Diaspora (an Open Source decentralized social network) infrastructure, where he says that \"looking at the traffic logs made him impressively angry\".</p><p>In the blogpost, he claims that one fourth of his entire web traffic is due to bots with an OpenAI user agent, 15% is due to Amazon, 4.3% is due to Anthropic, and so on. Overall, we're talking about 70% of the entire requests being from AI companies.</p><blockquote>they don’t just crawl a page once and then move on. Oh, no, they come back every 6 hours because lol why not. They also don’t give a single flying fuck about , because why should they. [...] If you try to rate-limit them, they’ll just switch to other IPs all the time. If you try to block them by User Agent string, they’ll just switch to a non-bot UA string (no, really). This is literally a DDoS on the entire internet.</blockquote><p>A similar number is given by the Read the Docs project. In a blogpost called, \"AI crawlers need to be more respectful\", they claim that blocking  AI crawlers immediately decreased their traffic by 75%, going from 800GB/day to 200GB/day. This made the project save up around $1500 a month.</p><p>The rest of the article is pretty impressive too; they talk about crawlers downloading tens of terabytes of data within a few days, or more. It's hard to block them entirely, since they use various different IPs.</p><p>I do wonder how much of this is scraping for training data, and how much instead is the \"search\" function that most LLMs provide; nonetheless, according to Schubert, \"normal\" crawlers such as Google's and Bing's only add up to a fraction of a single percentage point, which hints at the fact that other companies are indeed abusing their web powers.</p><p>But it's not just scrapers, or I would've titled this \"AI scrapers\", not \"AI companies\". Another issue that Open Source community have been fighting with is AI-generated bug reports, as an example.</p><p>This was first reported by Daniel Stenberg of the Curl project, in a blogpost titled \"The I in LLM stands for Intelligence\". Curl offers a bug bounty project, but lately, they've noticed that many bug reports are generated by AI. These look credible and take up a lot of developer time to check, but they also contain the typical hallucinations you'd expect from AIs. </p><p>It's pretty crazy to have to go through your own code because a bug report confidently tells you there's some critical security issue to fix, and … not finding it, because the whole issue is just AI hallucination.</p><p>A similar issue was reported by Seth Larson, who's on the security report triage team for CPython, pip, urllib3, Requests, and more. He says,</p><blockquote>Recently I've noticed an uptick in extremely low-quality, spammy, and LLM-hallucinated security reports to open source projects. The issue is in the age of LLMs, these reports appear at first-glance to be potentially legitimate and thus require time to refute.</blockquote><p>This is a pretty big issue. As he points out, responding to security reports is expensive, and responding to invented but credible bug reports causes some significant additional burden on maintainers, which might drive them out of the Open Source world.</p><p>The article ends with a request: please, do not use AI or LLM systems for detecting vulnerabilities. He says, \"These systems today cannot understand code, finding security vulnerabilities requires understanding code AND understanding human-level concepts like intent, common usage, and context.\".</p><p>Again, I want to point out that these issues impact disproportionately on the FOSS world; not only do Open Source projects often have less resources compared to commercial products, but - being community-driven projects - much more of their infrastructure is public and thus susceptible to both crawlers and AI-generated bug reports or issues. </p>","contentLength":10751,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43422413"},{"title":"The Frontend Treadmill","url":"https://polotek.net/posts/the-frontend-treadmill/","date":1742473531,"author":"Kerrick","guid":7968,"unread":true,"content":"<p>A lot of frontend teams are very convinced that rewriting their frontend will lead to the promised land. And I am the bearer of bad tidings.</p><p>If you are building a product that you hope has longevity, your frontend framework is the least interesting technical decision for you to make. And all of the time you spend arguing about it is wasted energy.</p><p>If your product is still around in 5 years, you’re doing great and you should feel successful. But guess what? Whatever framework you choose will be obsolete in 5 years. That’s just how the frontend community has been operating, and I don’t expect it to change soon. Even the popular frameworks that are still around are completely different. Because change is the name of the game. So they’re gonna rewrite their shit too and just give it a new version number.</p><p>Product teams that are smart are getting off the treadmill. Whatever framework you currently have, start investing in getting to know it deeply. Learn the tools until they are not an impediment to your progress. That’s the only option. Replacing it with a shiny new tool is a trap.</p><p>I also wanna give a piece of candid advice to engineers who are searching for jobs. If you feel strongly about what framework you want to use, please make that a criteria for your job search. Please stop walking into teams and derailing everything by trying to convince them to switch from framework X to your framework of choice. It’s really annoying and tremendously costly.</p><p>I always have to start with the cynical take. It’s just how I am. But I do want to talk about what I think should be happening instead.</p><p>Companies that want to reduce the cost of their frontend tech becoming obsoleted so often should be looking to get back to fundamentals. Your teams should be working closer to the web platform with a lot less complex abstractions. We need to relearn what the web is capable of and go back to that.</p><p>Let’s be clear, I’m not suggesting this is strictly better and the answer to all of your problems. I’m suggesting this as an intentional business tradeoff that I think provides more value and is less costly in the long run. I believe if you stick closer to core web technologies, you’ll be better able to hire capable engineers in the future without them convincing you they can’t do work without rewriting millions of lines of code.</p><p>And if you’re an engineer, you will be able to retain much higher market value over time if you dig into and understand core web technologies. I was here before react, and I’ll be here after it dies. You may trade some job marketability today. But it does a lot more for career longevity than trying to learn every new thing that gets popular. And you see how quickly they discarded us when the market turned anyway. Knowing certain tech won’t save you from those realities.</p><p>I couldn’t speak this candidly about this stuff when I held a management role. People can’t help but question my motivations and whatever agenda I may be pushing. Either that or I get into a lot of trouble with my internal team because they think I’m talking about them. But this is just what I’ve seen play out after doing this for 20+ years. And I feel like we need to be able to speak plainly.</p><p>This has been brewing in my head for a long time. The frontend ecosystem is kind of broken right now. And it’s frustrating to me for a few different reasons. New developers are having an extremely hard time learning enough skills to be gainfully employed. They are drowning in this complex garbage and feeling really disheartened. As a result, companies are finding it more difficult to do basic hiring. The bar is so high just to get a regular dev job. And everybody loses.</p><p>What’s even worse is that I believe a lot of this energy is wasted. People that are learning the current tech ecosystem are absolutely not learning web fundamentals. They are too abstracted away. And when the stack changes again, these folks are going to be at a serious disadvantage when they have to adapt away from what they learned. It’s a deep disservice to people’s professional careers, and it’s going to cause a lot of heartache later.</p><p>On a more personal note, this is frustrating to me because I think it’s a big part of why we’re seeing the web stagnate so much. I still run into lots of devs who are creative and enthusiastic about building cool things. They just can’t. They are trying and failing because the tools being recommended to them are just not approachable enough. And at the same time, they’re being convinced that learning fundamentals is a waste of time because it’s so different from what everybody is talking about.</p><p>I guess I want to close by stating my biases. I’m a web guy. I’ve been bullish on the web for 20+ years, and I will continue to be. I think it is an extremely capable and unique platform for delivering software. And it has only gotten better over time while retaining an incredible level of backwards compatibility. The underlying tools we have are dope now. But our current framework layer is working against the grain instead of embracing the platform.</p>","contentLength":5133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43422162"},{"title":"The Pain That Is GitHub Actions","url":"https://www.feldera.com/blog/the-pain-that-is-github-actions","date":1742441851,"author":"qianli_cs","guid":10743,"unread":true,"content":"<p>For the past two weeks, I’ve been spending most of my time rewriting our CI scripts in GitHub Actions. This is the third time we’ve had to redo our CI setup—first GitHub Actions, then <a href=\"https://earthly.dev/\">Earthly</a> (which we moved away from because it was discontinued), and now, reluctantly, back to GitHub Actions.</p><p>Our CI is complex: merge queues, multiple runners (self-hosted, <a href=\"https://blacksmith.sh/\">blacksmith.sh</a>, GitHub-hosted), Rust builds, Docker images, and heavy integration tests. Every PR we merge burns through an hour of CI time, running across multiple parallel runners.</p><p>There are a few things we'd like to have (which we deem as \"good software practice\") but it's nothing unheard of:</p><ol><li>Everything that goes into `main` must pass all tests.</li><li>Trivial mistakes (formatting, unused deps, lint issues) should be fixed automatically, not cause failures.</li><li>The artifacts we test with in CI should be the exact ones we release.</li><li>CI should complete quickly (to keep developers happy).</li></ol><p>GitHub Actions technically allows all of this—but setting it up is a frustrating mess, full of hidden gotchas, inconsistent behavior, and a debugging experience that makes me question my choices.</p><h3>Strange Way to Enforce Status Checks with Merge Queue</h3><p>The key to enforcing a clean  branch is GitHub’s <a href=\"https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/configuring-pull-request-merges/managing-a-merge-queue\">merge queue</a>, which rebases a PR onto  before running CI. Sounds great. But here’s the fun part:</p><ul><li>We need CI to run  entering the queue to auto-fix trivial issues.</li><li>We need CI to run  inside the queue to verify the final merge.</li><li>GitHub Actions makes it weirdly hard to require both runs to pass.</li></ul><p>The solution? <strong>Name the jobs identically</strong> in both phases. That’s it. GitHub treats them as the same check, so they both need to succeed. Solved by reading this answer in a <a href=\"https://stackoverflow.com/questions/76655935/when-does-a-github-workflow-trigger-for-merge-group-and-is-it-restricted-by-bran/78030618#78030618\">Stack Overflow post</a> after a few hours of debugging. Any other way you try to do this leads to either status checks being awaited before you put something in the queue (so it never starts the job) or worse, things just get merged even if the job you'd like to pass in the merge queue fails.</p><p>A few days ago, someone <a href=\"https://news.ycombinator.com/item?id=43368870\">compromised a popular GitHub Action</a>. The response? \"Just pin your dependencies to a hash.\" Except as comments also pointed out, almost no one does.</p><p>Even setting aside supply chain attacks, GitHub’s security model is a confusing maze to me: My point of view is that if I can't understand a security model easily it's probably doomed to fail or break at some point. Disclaimer: I'm writing this as a github actions user with only a vague understanding of it so I'd be delighted to hear that it is not just \"things piled on top of things until it's safe\", which is my current impression. I do understand very well that the problem of having secure CI for distributed source control is complicated.</p><p>In github, there is a \"default\" token called . The way it works is that it gets initialized with some default permissions. You can set that default in the settings of your repository (under Actions -&gt; General -&gt; Workflow Permissions). Here is what the github documentation says about it:</p><blockquote><p>If the default permissions for the GITHUB_TOKEN are restrictive, you may have to elevate the permissions to allow some actions and commands to run successfully. If the default permissions are permissive, you can edit the workflow file to remove some permissions from the GITHUB_TOKEN.</p></blockquote><p>Removing permission that aren't necessary sounds nice (though I do think a better \"default\" would be to start with  and require the user to add whatever is needed). Unfortunately, there are <a href=\"https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#permissions\">many of them</a> and it's hardly clear for all of them what they are protecting if you're not a github expert.</p><p>Your workflow permissions also don’t really depend on the action itself. Here is an example of such an instance, I'm using <code>softprops/action-gh-release</code> to automatically create a new release on github</p><div><div><pre><code></code></pre></div></div><p>Why do I need a custom token? Because without it, the release completes, but doesn’t trigger our post-release workflow. The sad part is that you don't get any indication about it until you eventually <a href=\"https://github.com/softprops/action-gh-release/issues/59\">find an issue</a> where someone had the same problem and that leads you in the right direction.</p><p>You can also elevate permissions in your workflow yaml file. That seems like a strange thing to do inside the code you're trying to protect. At least there are some limitations according to the github docs:</p><p>You can use the  key to add and remove read permissions for forked repositories, but typically you can't grant write access. The exception to this behavior is where an admin user has selected the <strong>Send write tokens to workflows from pull requests</strong> option in the GitHub Actions settings. For more information, see <a href=\"https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/enabling-features-for-your-repository/managing-github-actions-settings-for-a-repository#enabling-workflows-for-private-repository-forks\">Managing GitHub Actions settings for a repository</a>.</p><p>This is just one of many instances which I believe is the root of what makes the github actions security model so obscure: there are too many pitfalls accompanied by exceptions that you have to account for. Clearly the system is very powerful and allows you to do many things but it also expands the attack surface for breaking things.</p><p>As far as I can tell I'm not alone in this. Another instance of the same problem I ran into is when I read <a href=\"https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/about-self-hosted-runners\">this paragraph</a> where they recommend that you don't use self-hosted runners in public repositories:</p><blockquote><p>We recommend that you only use self-hosted runners with private repositories. This is because forks of your public repository can potentially run dangerous code on your self-hosted runner machine by creating a pull request that executes the code in a workflow.</p></blockquote><p>However, github also has a setting for self-hosted runners where pull-requests from external collaborators need to be approved before running. A practical question that comes up for me is \"are self-hosted runners in combination with this setting safe\"? I do believe so, but github documentation doesn't say and there is no consensus in the rest of the internet about it. It's hard to be 100% confident given how much complexity there is. Even github documentation writers don't seem to understand their security model anymore.</p><h3>Docker and Github Actions, an Unholy Combination</h3><p>If you thought GitHub Actions was bad, try mixing in Docker.</p><p>GitHub lets you <a href=\"https://docs.github.com/en/actions/writing-workflows/choosing-where-your-workflow-runs/running-jobs-in-a-container\">run jobs inside a container</a>. This is great in theory—you can prepackage dependencies into a dev container instead of installing them every run. In practice:</p><ol><li>File permissions break constantly. A container builds files as one user, but GitHub runners may use another (different uid and gid) to run it. So it may be unable to either access the files in the container or in the github workspace and temporary host directories that get mounted.</li><li>The  directory moves. Your dev container may install tools into , but inside GitHub Actions, it’s suddenly . Tools that rely on files in  may no longer find them.</li><li>Any action that interacts with the host system might break now. For example, I use <a href=\"https://docs.blacksmith.sh/github-actions-runners/sticky-disks\">blacksmith’s sticky disk action</a> to mount an NVMe drive for caching (since GitHub caches are limited to 10GB). It didn’t work inside a container until <a href=\"https://docs.blacksmith.sh/github-actions-runners/sticky-disks#using-sticky-disks-inside-a-container\">they made a fix for me</a> (thanks to Aditya Jayaprakash from blacksmith.sh for the one day turn-around time on this!).</li></ol><p>Meanwhile, the  field itself has weird limitations. Want to override the entrypoint? Nope. Want to run  steps inside a container and others outside? Nope.</p><h3>Developing Workflows with YAML</h3><p>All of this logic you end up writing in YAML can unfortunately get complicated pretty quickly and you're bound to make mistakes. I was using RustRover as my IDE when writing the YAML which had some linter checks for github YAML built-in and it helped a lot. I still found myself wishing for much better static checking for all of this. It doesn't help that you can't really try any of this locally (I know of <a href=\"https://github.com/nektos/act\">act</a> but it only supports a small subset of the things you're trying to do in CI). I found that the best way to debug CI is to create an identical repo to the one you're trying to make changes for and do <code>git commit -a -m \"wip\" &amp;&amp; git push test-ci branch</code> until CI works as expected.</p><p>Since I didn't want to run the whole CI pipeline every time I made a change, I tried to keep individual workflows small and have them push artifacts at the end of their steps, then subsequent workflows could download the artifacts and re-use them instead of rebuilding everything from scratch. This lets you test workflows in isolation because you can just download artifacts from a previous run until it works (of course when downloading from a previous run one needs to provide a token to the download-artifact action, but this can just be the default token. Why does it still need to be provided then is yet another unsolved mystery...).</p><p>The main workflow file then becomes a chain of invoking other YAML files:</p><div><div><pre><code>  invoke-build-rust:  \n  invoke-build-java:  \n  invoke-tests-unit:  \n  invoke-tests-adapter:  \n  invoke-build-docker:  \n  invoke-tests-integration:  \n  invoke-tests-java:  \n</code></pre></div></div><p>Notice the  added to some jobs. Another gotcha that took me too long to figure out. Every time when I ran the entire CI pipeline things wouldn't work but when I ran the steps individually they would work just fine (that's because when you call a workflow from another workflow secrets aren't shared by default).</p><p>There are many more gotchas I wanted to write about, but the post is already quite long. Overall, I'm still happy with <a href=\"https://github.com/feldera/feldera/tree/main/.github/workflows\">our new CI scripts</a> because it reduced the time to merge significantly for us. I just wish the process to get there would be less time-consuming and that it would be easier to debug when things go wrong. I guess, I'm hoping for some innovation here.</p>","contentLength":9480,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43419701"},{"title":"AI Blindspots – Blindspots in LLMs I've noticed while AI coding","url":"https://ezyang.github.io/ai-blindspots/","date":1742402912,"author":"rahimnathwani","guid":10742,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43414393"},{"title":"How fast the days are getting longer (2023)","url":"https://joe-antognini.github.io/astronomy/daylight","date":1742400800,"author":"antognini","guid":10741,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43413935"},{"title":"fd: A simple, fast and user-friendly alternative to 'find'","url":"https://github.com/sharkdp/fd","date":1742384657,"author":"tosh","guid":6089,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43410692"},{"title":"I'm the Canadian who was detained by ICE for two weeks","url":"https://www.theguardian.com/us-news/2025/mar/19/canadian-detained-us-immigration-jasmine-mooney","date":1742383272,"author":"n1b0m","guid":5927,"unread":true,"content":"<p>here was no explanation, no warning. One minute, I was in an immigration office talking to an officer about my work visa, which had been approved months before and allowed me, a Canadian, to work in the US. The next, I was told to put my hands against the wall, and patted down like a criminal before being <a href=\"https://www.theguardian.com/us-news/2025/mar/18/canadian-actor-jasmine-mooney-detained-mexico-border\" data-link-name=\"in body link\">sent to an Ice detention center</a> without the chance to talk to a lawyer.</p><p>I grew up in Whitehorse, Yukon, a small town in the northernmost part of <a href=\"https://www.theguardian.com/world/canada\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">Canada</a>. I always knew I wanted to do something bigger with my life. I left home early and moved to Vancouver, British Columbia, where I built a career spanning multiple industries – acting in film and television, owning bars and restaurants, flipping condos and managing Airbnbs.</p><p>In my 30s, I found my true passion working in the health and wellness industry. I was given the opportunity to help launch an American brand of health tonics called Holy! Water – a job that would involve moving to the US.</p><p>I was granted my trade Nafta work visa, which allows Canadian and Mexican citizens to work in the US in specific professional occupations, on my second attempt. It goes without saying, then, that I have no criminal record. I also love the US and consider myself to be a kind, hard-working person.</p><p>I started working in California and travelled back and forth between Canada and the US multiple times without any complications – until one day, upon returning to the US, a border officer questioned me about my initial visa denial and subsequent visa approval. He asked why I had gone to the San Diego border the second time to apply. I explained that that was where my lawyer’s offices were, and that he had wanted to accompany me to ensure there were no issues.</p><p>After a long interrogation, the officer told me it seemed “shady” and that my visa hadn’t been properly processed. He claimed I also couldn’t work for a company in the US that made use of hemp – one of the beverage ingredients. He revoked my visa, and told me I could still work for the company from Canada, but if I wanted to return to the US, I would need to reapply.</p><p>I was devastated; I had just started building a life in California. I stayed in Canada for the next few months, and was eventually offered a similar position with a different health and wellness brand.</p><p>I restarted the visa process and returned to the same immigration office at the San Diego border, since they had processed my visa before and I was familiar with it. Hours passed, with many confused opinions about my case. The officer I spoke to was kind but told me that, due to my previous issues, I needed to apply for my visa through the consulate. I told her I hadn’t been aware I needed to apply that way, but had no problem doing it.</p><p>Then she said something strange: “You didn’t do anything wrong. You are not in trouble, you are not a criminal.”</p><p>I remember thinking: <em>Why would she say that? Of course I’m not a criminal!</em></p><p>She then told me they had to send me back to Canada. That didn’t concern me; I assumed I would simply book a flight home. But as I sat searching for flights, a man approached me.</p><p>There was no explanation, no warning. He led me to a room, took my belongings from my hands and ordered me to put my hands against the wall. A woman immediately began patting me down. The commands came rapid-fire, one after another, too fast to process.</p><p>They took my shoes and pulled out my shoelaces.</p><p>“What are you doing? What is happening?” I asked.</p><p>“You are being detained.”</p><p>“I don’t understand. What does that mean? For how long?”</p><p>That would be the response to nearly every question I would ask over the next two weeks: “I don’t know.”</p><p>They brought me downstairs for a series of interviews and medical questions, searched my bags and told me I had to get rid of half my belongings because I couldn’t take everything with me.</p><p>“Take everything with me where?” I asked.</p><p>A woman asked me for the name of someone they could contact on my behalf. In moments like this, you realize you don’t actually know anyone’s phone number anymore. By some miracle, I had recently memorized my best friend Britt’s number because I had been putting my grocery points on her account.</p><p>I gave them her phone number.</p><p>They handed me a mat and a folded-up sheet of aluminum foil.</p><p>I was taken to a tiny, freezing cement cell with bright fluorescent lights and a toilet. There were five other women lying on their mats with the aluminum sheets wrapped over them, looking like dead bodies. The guard locked the door behind me.</p><p>For two days, we remained in that cell, only leaving briefly for food. The lights never turned off, we never knew what time it was and no one answered our questions. No one in the cell spoke English, so I either tried to sleep or meditate to keep from having a breakdown. I didn’t trust the food, so I fasted, assuming I wouldn’t be there long.</p><p>On the third day, I was finally allowed to make a phone call. I called Britt and told her that I didn’t understand what was happening, that no one would tell me when I was going home, and that she was my only contact.</p><p>They gave me a stack of paperwork to sign and told me I was being given a five-year ban unless I applied for re-entry through the consulate. The officer also said it didn’t matter whether I signed the papers or not; it was happening regardless.</p><p>I was so delirious that I just signed. I told them I would pay for my flight home and asked when I could leave.</p><p>Then they moved me to another cell – this time with no mat or blanket. I sat on the freezing cement floor for hours. That’s when I realized they were processing me into real jail: the Otay Mesa Detention Center.</p><p>I was told to shower, given a jail uniform, fingerprinted and interviewed. I begged for information.</p><p>“How long will I be here?”</p><p>“I don’t know your case,” the man said. “Could be days. Could be weeks. But I’m telling you right now – you need to mentally prepare yourself for months.”</p><p>I felt like I was going to throw up.</p><p>I was taken to the nurse’s office for a medical check. She asked what had happened to me. She had never seen a Canadian there before. When I told her my story, she grabbed my hand and said: “Do you believe in God?”</p><p>I told her I had only recently found God, but that I now believed in God more than anything.</p><p>“I believe God brought you here for a reason,” she said. “I know it feels like your life is in a million pieces, but you will be OK. Through this, I think you are going to find a way to help others.”</p><p>At the time, I didn’t know what that meant. She asked if she could pray for me. I held her hands and wept.</p><p>I felt like I had been sent an angel.</p><p>I was then placed in a real jail unit: two levels of cells surrounding a common area, just like in the movies. I was put in a tiny cell alone with a bunk bed and a toilet.</p><p>The best part: there were blankets. After three days without one, I wrapped myself in mine and finally felt some comfort.</p><p>For the first day, I didn’t leave my cell. I continued fasting, terrified that the food might make me sick. The only available water came from the tap attached to the toilet in our cells or a sink in the common area, neither of which felt safe to drink.</p><p>Eventually, I forced myself to step out, meet the guards and learn the rules. One of them told me: “No fighting.”</p><p>“I’m a lover, not a fighter,” I joked. He laughed.</p><p>I asked if there had ever been a fight here.</p><p>“In this unit? No,” he said. “No one in this unit has a criminal record.”</p><p>That’s when I started meeting the other women.</p><p>That’s when I started hearing their stories.</p><p>And that’s when I made a decision: I would never allow myself to feel sorry for my situation again. No matter how hard this was, I had to be grateful. Because every woman I met was in an even more difficult position than mine.</p><p>There were around 140 of us in our unit. Many women had lived and worked in the US legally for years but had overstayed their visas – often after reapplying and being denied. They had all been detained without warning.</p><p>If someone is a criminal, I agree they should be taken off the streets. But not one of these women had a criminal record. These women acknowledged that they shouldn’t have overstayed and took responsibility for their actions. But their frustration wasn’t about being held accountable; it was about the endless, bureaucratic limbo they had been trapped in.</p><p>The real issue was how long it took to get out of the system, with no clear answers, no timeline and no way to move forward. Once deported, many have no choice but to abandon everything they own because the cost of shipping their belongings back is too high.</p><p>I met a woman who had been on a road trip with her husband. She said they had 10-year work visas. While driving near the San Diego border, they mistakenly got into a lane leading to Mexico. They stopped and told the agent they didn’t have their passports on them, expecting to be redirected. Instead, they were detained. They are both pastors.</p><p>I met a family of three who had been living in the US for 11 years with work authorizations. They paid taxes and were waiting for their green cards. Every year, the mother had to undergo a background check, but this time, she was told to bring her whole family. When they arrived, they were taken into custody and told their status would now be processed from within the detention center.</p><p>Another woman from Canada had been living in the US with her husband who was detained after a traffic stop. She admitted she had overstayed her visa and accepted that she would be deported. But she had been stuck in the system for almost six weeks because she hadn’t had her passport. Who runs casual errands with their passport?</p><p>One woman had a 10-year visa. When it expired, she moved back to her home country, Venezuela. She admitted she had overstayed by one month before leaving. Later, she returned for a vacation and entered the US without issue. But when she took a domestic flight from Miami to Los Angeles, she was picked up by Ice and detained. She couldn’t be deported because Venezuela wasn’t accepting deportees. She didn’t know when she was getting out.</p><p>There was a girl from India who had overstayed her student visa for three days before heading back home. She then came back to the US on a new, valid visa to finish her master’s degree and was handed over to Ice due to the three days she had overstayed on her previous visa.</p><p>There were women who had been picked up off the street, from outside their workplaces, from their homes. All of these women told me that they had been detained for time spans ranging from a few weeks to 10 months. One woman’s daughter was outside the detention center protesting for her release.</p><p>That night, the pastor invited me to a service she was holding. A girl who spoke English translated for me as the women took turns sharing their prayers – prayers for their sick parents, for the children they hadn’t seen in weeks, for the loved ones they had been torn away from.</p><p>Then, unexpectedly, they asked if they could pray for me. I was new here, and they wanted to welcome me. They formed a circle around me, took my hands and prayed. I had never felt so much love, energy and compassion from a group of strangers in my life. Everyone was crying.</p><p>At 3am the next day, I was woken up in my cell.</p><p>“Pack your bag. You’re leaving.”</p><p>I jolted upright. “I get to go home?”</p><p>The officer shrugged. “I don’t know where you’re going.”</p><p>Of course. No one ever knew anything.</p><p>I grabbed my things and went downstairs, where 10 other women stood in silence, tears streaming down their faces. But these weren’t happy tears. That was the moment I learned the term “transferred”.</p><p>For many of these women, detention centers had become a twisted version of home. They had formed bonds, established routines and found slivers of comfort in the friendships they had built. Now, without warning, they were being torn apart and sent somewhere new. Watching them say goodbye, clinging to each other, was gut-wrenching.</p><p>I had no idea what was waiting for me next. In hindsight, that was probably for the best.</p><p>Our next stop was Arizona, the San Luis Regional Detention Center. The transfer process lasted 24 hours, a sleepless, grueling ordeal. This time, men were transported with us. Roughly 50 of us were crammed into a prison bus for the next five hours, packed together – women in the front, men in the back. We were bound in chains that wrapped tightly around our waists, with our cuffed hands secured to our bodies and shackles restraining our feet, forcing every movement into a slow, clinking struggle.</p><p>When we arrived at our next destination, we were forced to go through the entire intake process all over again, with medical exams, fingerprinting – and pregnancy tests; they lined us up in a filthy cell, squatting over a communal toilet, holding Dixie cups of urine while the nurse dropped pregnancy tests in each of our cups. It was disgusting.</p><p>We sat in freezing-cold jail cells for hours, waiting for everyone to be processed. Across the room, one of the women suddenly spotted her husband. They had both been detained and were now seeing each other for the first time in weeks.</p><p>The look on her face – pure love, relief and longing – was something I’ll never forget.</p><p>We were beyond exhausted. I felt like I was hallucinating.</p><p>The guard tossed us each a blanket: “Find a bed.”</p><p>There were no pillows. The room was ice cold, and one blanket wasn’t enough. Around me, women lay curled into themselves, heads covered, looking like a room full of corpses. This place made the last jail feel like the Four Seasons.</p><p>I kept telling myself: <em>Do not let this break you.</em></p><p>Thirty of us shared one room. We were given one Styrofoam cup for water and one plastic spoon that we had to reuse for every meal. I eventually had to start trying to eat and, sure enough, I got sick. None of the uniforms fit, and everyone had men’s shoes on. The towels they gave us to shower were hand towels. They wouldn’t give us more blankets. The fluorescent lights shined on us 24/7.</p><p>Everything felt like it was meant to break you. Nothing was explained to us. I wasn’t given a phone call. We were locked in a room, no daylight, with no idea when we would get out.</p><p>I tried to stay calm as every fiber of my being raged towards panic mode. I didn’t know how I would tell Britt where I was. Then, as if sent from God, one of the women showed me a tablet attached to the wall where I could send emails. I only remembered my CEO’s email from memory. I typed out a message, praying he would see it.</p><p>Through him, I was able to connect with Britt. She told me that they were working around the clock trying to get me out. But no one had any answers; the system made it next to impossible. I told her about the conditions in this new place, and that was when we decided to go to the media.</p><p>She started working with a reporter and asked whether I would be able to call her so she could loop him in. The international phone account that Britt had previously tried to set up for me wasn’t working, so one of the other women offered to let me use her phone account to make the call.</p><p>We were all in this together.</p><p>With nothing to do in my cell but talk, I made new friends – women who had risked everything for the chance at a better life for themselves and their families.</p><p>Through them, I learned the harsh reality of seeking asylum. Showing me their physical scars, they explained how they had paid smugglers anywhere from $20,000 to $60,000 to reach the US border, enduring brutal jungles and horrendous conditions.</p><p>One woman had been offered asylum in Mexico within two weeks but had been encouraged to keep going to the US. Now, she was stuck, living in a nightmare, separated from her young children for months. She sobbed, telling me how she felt like the worst mother in the world.</p><p>Many of these women were highly educated and spoke multiple languages. Yet, they had been advised to pretend they didn’t speak English because it would supposedly increase their chances of asylum.</p><p>Some believed they were being used as examples, as warnings to others not to try to come.</p><p>Women were starting to panic in this new facility, and knowing I was most likely the first person to get out, they wrote letters and messages for me to send to their families.</p><p>It felt like we had all been kidnapped, thrown into some sort of sick psychological experiment meant to strip us of every ounce of strength and dignity.</p><p>We were from different countries, spoke different languages and practiced different religions. Yet, in this place, none of that mattered. Everyone took care of each other. Everyone shared food. Everyone held each other when someone broke down. Everyone fought to keep each other’s hope alive.</p><p>I got a message from Britt. My story had started to blow up in the media.</p><p>Almost immediately after, I was told I was being released.</p><p>My Ice agent, who had never spoken to me, told my lawyer I could have left sooner if I had signed a withdrawal form, and that they hadn’t known I would pay for my own flight home.</p><p>From the moment I arrived, I begged every officer I saw to let me pay for my own ticket home. Not a single one of them ever spoke to me about my case.</p><p>To put things into perspective: I had a Canadian passport, lawyers, resources, media attention, friends, family and even politicians advocating for me. Yet, I was still detained for nearly two weeks.</p><p>Imagine what this system is like for every other person in there.</p><p>A small group of us were transferred back to San Diego at 2am – one last road trip, once again shackled in chains. I was then taken to the airport, where two officers were waiting for me. The media was there, so the officers snuck me in through a side door, trying to avoid anyone seeing me in restraints. I was beyond grateful that, at the very least, I didn’t have to walk through the airport in chains.</p><p>To my surprise, the officers escorting me were incredibly kind, and even funny. It was the first time I had laughed in weeks.</p><p>I asked if I could put my shoelaces back on.</p><p>“Yes,” one of them said with a grin. “But you better not run.”</p><p>“Yeah,” the other added. “Or we’ll have to tackle you in the airport. That’ll really make the headlines.”</p><p>I laughed, then told them I had spent a lot of time observing the guards during my detention and I couldn’t believe how often I saw humans treating other humans with such disregard. “But don’t worry,” I joked. “You two get five stars.”</p><p>When I finally landed in Canada, my mom and two best friends were waiting for me. So was the media. I spoke to them briefly, numb and delusional from exhaustion.</p><p>It was surreal listening to my friends recount everything they had done to get me out: working with lawyers, reaching out to the media, making endless calls to detention centers, desperately trying to get through to Ice or anyone who could help. They said the entire system felt rigged, designed to make it nearly impossible for anyone to get out.</p><p>The reality became clear: Ice detention isn’t just a bureaucratic nightmare. It’s a business. These facilities are privately owned and run for profit.</p><p>Companies like CoreCivic and GEO Group receive <a href=\"https://www.opensecrets.org/news/2022/06/private-prison-industry-shifts-focus-to-immigrant-detention-centers-funding-immigration-hawks/\" data-link-name=\"in body link\">government funding</a> based on the number of people they detain, which is why they <a href=\"https://theappeal.org/geo-group-earnings-mass-deportations/\" data-link-name=\"in body link\">lobby</a> for stricter immigration policies. It’s a <a href=\"https://investors.geogroup.com/news-releases/news-release-details/geo-group-awarded-15-year-contract-us-immigration-and-customs\" data-link-name=\"in body link\">lucrative business</a>: CoreCivic made over <a href=\"https://ir.corecivic.com/static-files/d3f1752e-87b8-4256-99ed-f3803c5817f8\" data-link-name=\"in body link\">$560m</a> from Ice contracts in a single year. In 2024, GEO Group made more than <a href=\"https://www.usaspending.gov/recipient/9b308edb-a62c-659b-704b-ef4e5cf3f795-P/2024\" data-link-name=\"in body link\">$763m</a> from Ice contracts.</p><p>The more detainees, the more money they make. It stands to reason that these companies have no incentive to release people quickly. What I had experienced was finally starting to make sense.</p><p>This is not just my story. It is the story of thousands and thousands of people still trapped in a system that profits from their suffering. I am writing in the hope that someone out there – someone with the power to change any of this – can help do something.</p><p>The strength I witnessed in those women, the love they gave despite their suffering, is what gives me faith. Faith that no matter how flawed the system, how cruel the circumstances, humanity will always shine through.</p><p>Even in the darkest places, within the most broken systems, humanity persists. Sometimes, it reveals itself in the smallest, most unexpected acts of kindness: a shared meal, a whispered prayer, a hand reaching out in the dark. We are defined by the love we extend, the courage we summon and the truths we are willing to tell.</p>","contentLength":20550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43410548"},{"title":"The Lost Art of Research as Leisure","url":"https://kasurian.com/p/research-as-leisure","date":1742378955,"author":"altilunium","guid":10740,"unread":true,"content":"<p><a href=\"https://www.washingtonpost.com/archive/entertainment/books/1979/12/30/papyrus-to-paperbacks-the-world-that-books-made/43c411da-2bf7-4e5f-8869-caaac5422e9e/\" rel=\"\">Books are the carriers of civilisation</a></p><p>Linking civilisation and human culture to books, reading and writing is not unique to Tuchman.</p><p><a href=\"https://www.amazon.com/exec/obidos/ASIN/037575766X/braipick-20\" rel=\"\">the seal of all the admirable inventions of mankind</a></p><p>A few generations later, Henry David Thoreau, writing in the seclusion of Walden Pond, wrote that “books are the treasured wealth of the field and the fit inheritance of generations and culture.”</p><p><a href=\"https://www.youtube.com/watch?v=MVu4duLOF6Y\" rel=\"\">marvelled</a></p><p>Tuchman’s platitude, then, has persisted through the centuries: Books carry civilisation. Not because they are inherently sacred objects of inherently sacred knowledge, but because reading and writing assemble and shape culture. And without culture, there is no civilisation.</p><p>In Arabic, the root word for civilisation — ح-ض-ر: to be present, to settle, to remain — expresses a profound shift from wandering to dwelling. For Islam, that shift began with a search at the boundary between city and desert.</p><p><em>Read in the Name of your Lord who created.</em></p><p><em>Read in the Name of your Lord who created.</em></p><p>To command an unlettered man to Read unsettles the essential pillar that reading is largely, or exclusively, the one dimensional act of decoding printed symbols. The Arabic word, “Iqra,” often translated as to “read” contains a curious ambiguity — it simultaneously means “to read” and “to recite.” To recite is to engage in a primarily oral act, externally expressive. To read is to engage in something more private and solitary, internally reflective.</p><p><em>Pleasures of Reading in the Age of Distractions</em></p><p>If reading does not flow outward to build and contribute to the living networks of human knowledge, the divine command to Read feels hamstrung, unfulfilled.</p><p>Reading alone however — even with its duality — is not enough. The Quran’s command to read has a direction.</p><p><em>Read in the Name of Your Lord who Created. Created humans from a clinging clot. Read! And your Lord is the Most Generous, Who taught by the pen—taught humanity what they knew not.</em></p><p>The command to Read in the name of our Creator confers — as Rebecca Elson put in “We Astronomers,” a poem about resisting disenchantment — a “responsibility to awe.” The Quranic Read could be interpreted as a responsibility to awe. It is an invitation to learn with both disciplined inquiry, and receptive wonder.</p><p>Over the last century, our responsibility to awe has been a source of anxiety.</p><p>In 1926 — the year the radio, a dizzying new addition to the American home, brought the World Series to living rooms across the country; the year Bell Telephone perfected transcontinental calls from New York to San Francisco for $18; the year the Orpheum Theatre opened in Los Angeles, its legendary neon sign still shining today—Virginia Woolf worried about the future of reading.</p><p><a href=\"https://newrepublic.com/article/120389/movies-reality\" rel=\"\">edition</a></p><p>In the New Yorker’s “Talk of the Town,” White reflected on the Rollins College President’s prediction that “in fifty years, only five percent of the people in the country will be reading. White writes, “to us, it would seem that even if only one person out of a hundred and fifty million should continue as a reader, he would be the one worth saving, the nuclear around which to found a university. This ‘impossible person,” this “Last Reader,” is the “queen bee,” from whom a “new race of men, linked perfectly with the long past by the broken chain of their intellect, to carry on the community.” He concludes that it is more likely the race will perpetuate “through audiovisual devices, which ask no discipline of the mind, and which are already giving the room the languor of an opium parlor.”</p><p>45 years later, in 1996 — the year Fox News launched on satellite television; the year Dolly was cloned; the year of the “miniature telephone”; of dial-up, the “mouse” and “keyboard”, “www” and “@”; the last year before Amazon would change the Internet — Susan Sontag worries about the future of reading.</p><p><em>nothing less than the death of inwardness</em></p><p><em>Amusing Ourselves to Death,</em><a href=\"https://monoskop.org/images/d/db/Ong_Walter_J_Orality_and_Literacy_2nd_ed.pdf\" rel=\"\">neither in “orality” nor “literacy”</a></p><p>Woolf, White and Sontag foresaw the corrosive, savage effect of the “audio-visual” on the human brain and soul. They did not worry about the disappearance of books, but about the cultural collapse that would occur when reading shifts from an immersive, contemplative act to something passive, fragmented and superficial. The death of reading was not a loss of books, but a loss of culture.</p><p>These fears have not been unfounded. Today, we find ourselves in precisely the cultural crisis that Woolf, White, and Sontag anticipated—not a world without books, but a world where fragmented attention and superficial engagement have eroded the foundations of shared meaning and cultural coherence.</p><p><a href=\"https://www.amazon.com/Liquid-Modernity-Zygmunt-Bauman/dp/0745624103\" rel=\"\">liquid</a></p><p><em>The Disappearance of Rituals</em></p><p>For T.S. Eliot, writing in post-World War II England, “culture” is a mutually dependent hierarchy of three “senses”— the individual, the group, and society — that manifest in creating “the pattern of society as a whole.” With the fragmentation of any one sense from the other - the individual from the group, the group from society — “higher civilisation is unlikely to be found.”</p><p>In this fragmented landscape, we need not just diagnoses but prescriptions. How might we rebuild the foundations of culture when our very modes of attention have been compromised? The answer may lie in recovering an ancient understanding of leisure—not as idleness, but as a form of directed contemplation.</p><p>Josef Piper, writing at the same time as Eliot, but in a defeated and fragmented Germany, declares leisure the basis of culture. By “leisure,” Pieper does not mean idleness, but the more ancient type of leisure — leisure as the Greek σχολή (scholē), or school.</p><p>Pieper’s leisure is a contemplative one—it is, in essence, a style of unconstrained research. Such leisure is not merely, or singularly, the pursuit of knowledge “for its own sake,” nor is it simply “reading for pleasure.” The leisure that forms the basis of culture is a directed and intentional curiosity — it is the practice of formulating questions and seeking answers with a disposition towards wonder, not rigid certainty. Where free time is not used for research — for developing questions, and investigating the answers with an explorer’s spirit — cultural coherence crumbles. For Pieper, without leisure as letters, or “research as leisure,” there is no pattern from which higher civilisation is found.</p><p>Taken together, Eliot and Pieper offer complementary architectures of culture: Eliot describes the external pattern of culture, while Pieper describes the internal condition—leisure—that nourishes and regenerates that pattern. Without the structural coherence Eliot describes, culture risks disintegration; without the contemplative leisure Pieper champions, that structure is hollowed.</p><p>Looking into the abyss of the uncanny valley, leisure as letters unlocks a new cultural imagination. The formal but playful exchange of ideas, driven by intentional curiosity, creates a new culture.</p><p>What does cultural recovery look like in practice? Assembling the pattern of society as a whole begins with a shift in perspective: seeing reading, and inquiry, not as a burdensome or academically cloistered act, but as an act of playful and intentional curiosity.</p><p>For some, the compulsion to read manifests as a productivity hack, or as the passive consumption of viral self-help books and novels. These readers treat reading not as a tool to discern the reality around them, but as an obligation to signal productive virtue or as mere entertainment, no different from a reality TV show.</p><p>For others, many of whom are voracious readers, the compulsion to read manifests as an exercise in confirmation bias: collecting fragments of ideas that validate existing worldview. These readers treat reading not as an invitation to increased depth, but as an opportunity to superficially appropriate concepts that comfortably align with their existing beliefs. The result is an intellectual ventriloquism that stunts curiosity.</p><p>For the academy, “research” is a term of art. For our purposes, research is not a rarefied academic exercise. It is a fundamentally human activity, an adventure, a craft, a conviviality that assembles culture.</p><p>Non-experts can, and should, aspire to expertise.</p><p>Having the library of Alexandria in our pockets has dulled, rather than heightened, our senses. Despite unprecedented access to information, there is a sluggish incuriosity, a giving of the self to the algorithm that feeds us information, rather than allows us to search for it.</p><p>Yet curiosity, at its core, is simple: it is observation, attention, and the persistent asking of why, and how. Curiosity is to stand before the Creator in quiet submission that within each question is a universe of more questions.</p><p>Cultivating curiosity is as simple as picking up a magazine, coming across an essay on bird migration and wanting to learn more. It is as simple as taking a walk, noticing the sidewalk or street beneath you, the buildings, trees, plant and animal life around you, and wondering how and why it all got there.</p><p>It was during my own walks that I began to question the suburban landscape around me: How did these houses get built, why these houses, and why this style, why are the streets so wide? How did this neighborhood come to be? What I initially saw as a monotonous sprawl became a mysterious puzzle, an adventure in understanding how urban planning, architecture, land use, economics and technology touched my life.</p><p>Curiosity without direction is mere distraction. For our purposes, curiosity must crystallise into a question. Passive curiosity must be transformed into an active search for truth. For the leisurely researcher, developing a question is fun because it is anti-disciplinary, multidisciplinary and free to flourish without the arcane rules of the academy.</p><p><em>which is not in itself bad</em></p><p>My own questions about the suburbs were bad — broad, vague, sprawling. My anchor question “how did the suburbs come to be?” became “how did zoning create the modern suburbs?” and from there, “what is the history of zoning?” to “how did the mall create the modern suburbs?” to “why are there parking minimums” and “are the suburbs actually rational?” and on and on and on.</p><p>With practice, I learned a simple formula: A good question is specific enough to guide research, but open enough to allow for discovery. Discovery is essential for the blossoming of thoughts because it is where multidisciplinarity, connections and new questions emerge.</p><p>Once a question takes shape, it needs substance to grow. Gathering evidence is where most researchers get stuck.</p><p>First, our information ecosystem has created collectors, rather than readers, out of us. Collecting PDFs, books, and lists of books is a uniquely thrilling pleasure but it can hamstring us. The most challenging part of gathering evidence is organising it.</p><p>Second, for the leisurely researcher, self-study must include the discipline’s foundational texts. Understanding those texts, and how they’ve shaped what we think the subject, is crucial because it is where a healthy dose of disagreeableness and scepticism can thrive. By understanding the rules that govern a discipline’s thinking, the leisurely researcher is empowered to thoughtfully question them, explore paths conventional wisdom has overlooked, and develop new answers.</p><p>Finally, there is always more to read. That’s ok.</p><p>Research has to culminate, even if its culmination is more questions to research. The culmination does not have to be ground-breaking, but it does have to exist. A resolution must materialise in tangible form, as an essay, a video, a social media thread, or even a letter to a friend.</p><p>What distinguishes research as leisure from idle browsing is precisely this movement toward creation. However modest, your answer must contribute to the conversation rather than merely consuming or repeating what others have said.</p><p>Ideally, like the duality of the Quran’s Read, the culmination of research is social, conversational and communal. Fundamental to the art of research as leisure is the creation of formal and informal ‘communities of knowledge’ in which well-researched ideas are communicated in a written form, and presented for wider debate.</p><p>Today, these communities are everywhere. Substack, YouTube, Discord, Twitter. They exist in small-scale book clubs, writing circles, and informal discussion groups that meet in living rooms and coffee shops across the world. Through these communities, much like the Bloomsbury Group, the Inklings, Gertrude Stein’s Salon, or the Vienna Circle, we nurture the living networks through which ideas are tested, refined, cross-pollinated, and passed along.</p><p>In doing so, we gradually reassemble the pattern of society as a whole that makes higher culture, and civilisation, possible.</p><p>By embracing a culture of formal and informal expertise, a culture of research as leisure, we can restore our sense of wonder, and with it, the capacity to inform, negotiate and transcend the orthodoxies of our time.</p><ul><li><p><em>Lost in Thought: The Hidden Pleasures of an Intellectual Life</em></p></li><li><p><em>The Pleasures of Reading in the Age of Distraction</em></p></li><li><p><em>Slow Reading in a Hurried Age</em></p></li><li><p><em>Notes Toward a Definition of Culture </em></p></li><li><p><em>Leisure: The Basis of Culture</em></p></li><li><p><em>The Disappearance of Rituals</em></p></li><li><p><em>How Romantics and Victorians Organized Information</em></p></li><li><p><em>Dialogue Concerning the Two Chief World Systems: Ptolemaic and Copernican</em></p></li><li><p><em>Amusing Ourselves to Death</em></p></li><li><p><em>The Anatomy of Influence: Literature as a Way of Life</em></p></li><li><p><em>The Study: The Inner Life of Renaissance Libraries</em></p></li><li><p><em>The Sociological Imagination</em></p></li></ul>","contentLength":13651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43410061"},{"title":"Crew-9 Returns to Earth","url":"https://www.spacex.com/launches/mission/?missionId=crew-9-return","date":1742362372,"author":"saikatsg","guid":10739,"unread":true,"content":"<p>\n        On its flight to the International Space Station, Dragon executes a series of burns that position the vehicle\n        progressively closer to the station before it performs final docking maneuvers, followed by pressurization of\n        the vestibule, hatch opening, and crew ingress.\n      </p><p>\n        On its flight to the International Space Station, Dragon executed a series of burns that positioned the vehicle\n        progressively closer to the station before it performed final docking maneuvers, followed by pressurization of\n        the vestibule, hatch opening, and crew ingress.\n      </p>","contentLength":601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43408540"},{"title":"Make Ubuntu packages 90% faster by rebuilding them","url":"https://gist.github.com/jwbee/7e8b27e298de8bbbf8abfa4c232db097","date":1742342117,"author":"jeffbee","guid":10738,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43406710"},{"title":"Turkish university annuls Erdogan rival's degree, preventing run for president","url":"https://www.reuters.com/world/asia-pacific/istanbul-university-annuls-istanbul-mayor-imamoglus-diploma-over-irregularities-2025-03-18/","date":1742329882,"author":"perihelions","guid":10737,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43404679"},{"title":"US appeals court rules AI generated art cannot be copyrighted","url":"https://www.reuters.com/world/us/us-appeals-court-rejects-copyrights-ai-generated-art-lacking-human-creator-2025-03-18/","date":1742321853,"author":"rvz","guid":4665,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43402790"},{"title":"Amazon to kill off local Alexa processing, all voice requests shipped to cloud","url":"https://www.theregister.com/2025/03/17/amazon_kills_on_device_alexa/","date":1742318866,"author":"johnshades","guid":10736,"unread":true,"content":"<p>Come March 28, those who opted to have their voice commands for Amazon's AI assistant Alexa processed locally on their Echo devices will lose that option, with all spoken requests pushed to the cloud for analysis.</p><p>Amazon hasn't formally announced the change, and the <a href=\"https://www.amazon.com/gp/help/customer/display.html?nodeId=GQXLLWHBCVL6L5QD\" rel=\"nofollow\">help page</a> for the feature still makes no mention of the March 28 deprecation. But the internet souk confirmed to  that emails to users about the update, which caused a stir on social media over the weekend, are indeed legit.</p><p>\"We are reaching out to let you know that the Alexa feature 'Do Not Send Voice Recordings' that you enabled on your supported Echo device(s) will no longer be available beginning March 28, 2025,\" a copy of the email sent to Echo users relayed to  read.</p><p>\"As we continue to expand Alexa's capabilities with generative AI features that rely on the processing power of Amazon's secure cloud, we have decided to no longer support this feature.\"</p><h3>All your phrase are belong to us</h3><p>So there it is, apparently: Alexa's latest generative AI tricks are too demanding for the hardware on the handful of Echo devices that support local processing — the 4th-gen Echo Dot, Echo Show 10, and Show 15. Less powerful Echo gadgets don't have any option for processing locally. Therefore, all spoken Alexa requests are going up into Amazon systems for remote processing.</p><p>Privacy-conscious users who enabled the \"Do Not Send Voice Recordings\" setting won't get a say; it's being disabled automatically. Not that many people bothered enabling the local option in the first place, Amazon claims.</p><p>\"If you do not take action, your Alexa Settings will automatically be updated to 'Don't save recordings,'\" Amazon told affected users.</p><p>\"Starting on March 28, your voice recordings will be sent to and processed in the cloud, and they will be deleted after Alexa processes your requests,\" the email continued. \"If your voice recordings setting is updated to 'Don't save recordings,' voice ID will not work and you will not be able to create a voice ID for individual users to access more personalized features.\"</p><p>In other words, unless you let Amazon store your recordings, you're stuck with a feature-limited Alexa. And all voice commands are going up regardless.</p><h3>Private, for a given definition of privacy</h3><p>While Echo owners may not be happy about about losing the option for on-device audio processing, the soon-to-be-scrapped feature wasn't exactly airtight with its privacy protection to begin with.</p><p>Another <a href=\"https://www.amazon.com/gp/help/customer/display.html?nodeId=G4X4X8C26Z73P8SQ\" rel=\"nofollow\">Amazon help page</a> that gives more detail on the Do Not Send Voice Recordings option notes that even when audio recordings stay local, a text transcript of each request still gets shipped off to Amazon's cloud for processing anyway. Those transcripts are stored right alongside voice recordings and don't auto-delete — you have to manually purge them via your Voice History, assuming you knew they existed in the first place.</p><p>Amazon customers shouldn't be surprised, though: The tech titan's approach to privacy has long raised eyebrows, particularly when it comes to Alexa and the other gadgets it plants in homes to gain an audio and visual foothold.</p><p>Studies have <a href=\"https://www.theregister.com/2022/04/27/amazon_audio_data/\">claimed</a> that Amazon uses Alexa voice interaction data to help target ads — both on Echo devices and across the web. Third-party apps available for Alexa-enabled devices don't offer much comfort either, at times <a href=\"https://www.theregister.com/2020/07/29/amazon_google_voice_apps/\">lacking</a> clear privacy policies or adequate safeguards on how user data is handled.</p><p>Then there's last year's drama in America surrounding Ring cameras, when the FTC claimed the super-corp's lax security controls allowed Amazon employees and contractors <a href=\"https://www.theregister.com/2024/04/25/ring_ftc_settlement/\">to access</a> customers' private video feeds. The agency also <a href=\"https://www.theregister.com/2023/06/01/ftc_alexa_ring_amazon_settlement/\">alleged</a> that Amazon unlawfully retained Alexa voice recordings of children indefinitely, violating child privacy laws.&nbsp;</p><p>Amazon, naturally, denies that eliminating the on-device processing feature will impede user privacy.&nbsp;</p><p>\"The Alexa experience is designed to protect our customers' privacy and keep their data secure, and that's not changing,\" an Amazon spokesperson told us. \"We're focusing on the privacy tools and controls that our customers use most and work well with generative AI experiences that rely on the processing power of Amazon's secure cloud.\"</p><p>It's those generative AI updates, revealed in late February alongside the <a href=\"https://www.aboutamazon.com/news/devices/new-alexa-generative-artificial-intelligence\" rel=\"nofollow\">launch</a> of Alexa+, that appear to be driving the change. The three devices mentioned by Amazon as supporting local voice processing (Dot 4, Show 10, Show 15) are all in the <a href=\"https://www.amazon.com/dp/B0DCCNHWV5?ref=ods_surl_xaa_us#:~:text=What%20devices%20can%20customers%20use%20Alexa%2B%20on%3F\" rel=\"nofollow\">lineup</a> of devices supported by Alexa+, which Amazon is no doubt keen to push users to adopt.&nbsp;</p><p>Unlike the classic Alexa, which Amazon said will continue to be available, generative AI through Alexa+ - and the fresh stream of user data required to fuel them - will only be available to Amazon Prime subscribers, or anyone willing to shell out $19.99 per month without Prime. Whether you're sticking with Alexa or using Alexa+, commands are processed remotely.</p><p>Amazon told us customers will still have plenty of privacy options available, \"including the option to not save their voice recordings at all.\" That feature, as we noted above, means losing out on many essential Alexa features, like the voice assistant being able to recognize an individual speaker and respond based on their preferences, which for many multi-user households is an essential feature.&nbsp;</p><p>\"We'll continue learning from customer feedback, and building privacy features on their behalf,\" Amazon said.&nbsp;®</p><p><em> Under the direction of President Donald Trump and his US government standards body NIST, scientists linked to the federally funded Artificial Intelligence Safety Institute have been told to talk less about “AI safety,” “responsible AI,” and “AI fairness,” and focus more on “reducing ideological bias, to enable human flourishing and economic competitiveness,” in machine-learning research, WiReD magazine <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.wired.com/story/ai-safety-institute-new-directive-america-first/\">reports</a>.</em></p>","contentLength":5858,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43402115"},{"title":"Show HN: I made a tool to port tweets to Bluesky mantaining their original date","url":"https://bluemigrate.com/","date":1742317659,"author":"nols05","guid":10735,"unread":true,"content":"<div><div><img alt=\"BlueMigrate\" loading=\"lazy\" width=\"40\" height=\"40\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=https%3A%2F%2Fcdn.bsky.app%2Fimg%2Favatar%2Fplain%2Fdid%3Aplc%3Aiu4nttjmi4kwd3trqkeq2dku%2Fbafkreidlks5quizuwgy64ksfqlprps7wwb5cqgtjswvufyzyaog3ukx3sy%40jpeg&amp;w=48&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.bsky.app%2Fimg%2Favatar%2Fplain%2Fdid%3Aplc%3Aiu4nttjmi4kwd3trqkeq2dku%2Fbafkreidlks5quizuwgy64ksfqlprps7wwb5cqgtjswvufyzyaog3ukx3sy%40jpeg&amp;w=96&amp;q=75 2x\" src=\"https://bluemigrate.com/_next/image?url=https%3A%2F%2Fcdn.bsky.app%2Fimg%2Favatar%2Fplain%2Fdid%3Aplc%3Aiu4nttjmi4kwd3trqkeq2dku%2Fbafkreidlks5quizuwgy64ksfqlprps7wwb5cqgtjswvufyzyaog3ukx3sy%40jpeg&amp;w=96&amp;q=75\"></div><p>Import your tweets to Bluesky in a few clicks 🦋\n\n👉 https://bluemigrate.com</p></div>","contentLength":80,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43401855"},{"title":"Apple restricts Pebble from being awesome with iPhones","url":"https://ericmigi.com/blog/apple-restricts-pebble-from-being-awesome-with-iphones/","date":1742315001,"author":"griffinli","guid":3313,"unread":true,"content":"<p>During Pebble v1, I learned how much harder it is to build a great smartwatch experience on iPhone than it is on Android. It sounds like things have actually gotten worse over the last 8 years. </p><p>I want to set expectations accordingly. We will build a good app for iOS, but be prepared - there is no way for us to support all the functionality that Apple Watch has access to. It’s impossible for a 3rd party smartwatch to send text messages, or perform actions on notifications (like dismissing, muting, replying) and many, many other things.</p><p>Here are the things that are harder or impossible for 3rd party smartwatches (ie non Apple Watches) to do on iPhone:</p><ul><li>There’s no way for a smartwatch to send text messages or iMessages.</li><li>You can’t reply to notifications or take ‘actions’ like marking something as done.</li><li>It’s very difficult to enable other iOS apps to work with Pebble. Basically iOS does not have the concept of ‘interprocess communication’(IPC) like on Android. What we did before was publish an SDK that other apps (like Strava) could integrate to make their own BLE connection to Pebble. It was a clunky quasi-solution that other apps didn’t like, because it was hard to test (among other things)</li><li>If you (accidentally) close our iOS app, then your watch can’t talk to app or internet</li><li>Impossible for watch to detect if you are using your phone, so your watch will buzz and display a notification even if you are staring at your iPhone</li><li>You can’t easily side load apps onto an iPhone. That means we have to publish the app on the iPhone appstore. This is a gigantic pain because Apple. Every update comes with the risk that a random app reviewer could make up some BS excuse and block the update.</li><li>Because of iOS Appstore rules, it would be hard for us to enable 3rd party watchface/app developers to charge for their work (ie we can’t easily make an appstore within our app)</li><li>Getting a <a href=\"https://pebble.github.io/rockyjs/\">Javascript engine to run in PebbleOS</a> forced us to go through many hoops due to iOS — creating a compiler inside the Pebble iPhone app that in itself needed to be written in (cross-compiled to) JS to work with Apple's restriction on downloadable code can only be JS</li><li>As a Pebble watch/app developer, using the iOS app as relay to the watch sucks since the \"developer mode\" terminates every few minutes</li></ul><p>As an aside, back at Pebble, we went to <a href=\"https://www.theverge.com/2015/11/23/9788146/pebble-time-quick-message-reply\">crazy lengths</a> to find a way to let Pebble users to send text messages from Pebble. Our bizdev team did an impressive custom SMS-over-IP deal with AT&amp;T to enable this, but the end result was a pretty rough user experience (messages sent from Pebble didn’t appear in the Messages app on iPhone).</p><p> - it sounds like the situation has actually . This 2024 <a href=\"https://s3.amazonaws.com/jnswire/jns-media/7d/b2/15969695/showtemp_83.pdf\">class action lawsuit</a> against Apple states that: </p><ul><li>You must set notifications to display full content previews on your lockscreen for them to also be sent to a 3rd party watch (new restriction added in iOS 13).</li><li>Apple closed off the ability of smartwatches after Pebble to negotiate with carriers to\nprovide messaging services, and now requires users to turn off iMessage (disabling iOS’s core messaging platform) if they want to take advantage of such contracts between a third-party smartwatch maker and cellular carriers.</li></ul><h3><strong>Why are things so much harder on iOS?</strong></h3><p>Well, mostly because Apple systematically makes it nearly impossible for 3rd party wearable developers to build a smartwatch experience comparable to Apple Watch experience.</p><p>Apple claims their restrictions on competitors are only about security, privacy, crafting a better experience etc etc. At least that’s what they tell you as they tuck you into bed. I personally don’t agree - they’re clearly using their market power to lock consumers into their walled ecosystem. This causes there to be less competition, which increases prices and reduces innovation. DOJ seems to agree. For now at least…Tim Apple <a href=\"https://www.axios.com/2025/01/03/tim-cook-apple-donate-1-million-trump-inauguration\">paid $1m</a> to <a href=\"https://www.tuaw.com/2025/01/22/tech-leaders-attend-trumps-inauguration-with-tim-cook/\">sit near Trump at the inauguration</a>, so who knows how long until Trump tells DOJ to drop the case. There’s also an Apple Watch class-action lawsuit working its way through the system.</p><h3><strong>But we’re going to try anyways</strong></h3><p>The problem is that 40% of everyone who signed up on <a href=\"http://repebble.com/\">rePebble.com</a> still uses an iPhone. So we’re going to make a damn iOS app. I guess we’re gluttons for punishment. Just understand a few things:</p><ul><li>Our watch will&nbsp;&nbsp;appear to have less developed functionality on iOS than Android. This is Apple’s fault, not ours.</li><li>Some features will appear first on our Android app, and then eventually we’ll add them to the iOS app. This is because the majority of our development team uses Android phones, and generally we’re building things for ourselves, so naturally Android comes first.</li><li>I don’t want to see any tweets or blog posts or complaints or whatever later on about this. I’m publishing this now so you can make an informed decision about whether to buy a new watch or not. If you’re worried about this, the easiest solution is to .</li></ul><p>Apple will never change their ways unless you, the Pebble-curious iPhone user, complain loudly or switch to Android. Which is also hard because Apple tries it’s best to lock you into their platform.</p><p><strong>Are you an iPhone user who wants to use our watches?</strong> Start by posting a comment below this post. Hopefully there will be a lot - let’s show them that people actually want this to improve.</p><p>If you live in the US, tell your elected representatives to support legislation like ACCESS Act and AICO.</p><p>If you live in Europe, thank you for voting for representatives who passed the DMA. We will be <a href=\"https://developer.apple.com/support/ios-interoperability/\">petitioning Apple</a> under DMA Article 6 to request interoperability with Apple Watch APIs. </p>","contentLength":5614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43401245"},{"title":"Two new PebbleOS watches","url":"https://ericmigi.com/blog/introducing-two-new-pebbleos-watches/","date":1742313567,"author":"griffinli","guid":4664,"unread":true,"content":"<p>We’re excited to announce two new smartwatches that run open source PebbleOS and are compatible with thousands of your beloved Pebble apps. </p><ul><li> has an ultra crisp black and white display, polycarbonate frame, costs $149 and starts shipping in July.</li><li> has a larger 64-colour display, metal frame, costs $225 and starts shipping in December.</li></ul><p>Both are available in limited quantities, with worldwide shipping. Prices are in USD. Pre-ordering is the only way to get one - they will not be sold in stores. Pre-order today at <a href=\"http://store.repebble.com/\">store.rePebble.com</a>!</p><h2>Why are we making new Pebble-like smartwatches?</h2><p>Pretty simple - because we want one! No company has made a perfect smartwatch for people like <a href=\"https://www.youtube.com/watch?v=lKie-vgUGdI\">us</a>, so we’re going to make the exact smartwatch we want. Read the <a href=\"https://ericmigi.com/blog/why-were-bringing-pebble-back\">full story on my blog</a>, but it comes down to 5 key features:</p><ul><li>Simple and beautiful design</li></ul><p>No smartwatch on the market since Pebble offers this combination of features…until today!</p><p>I think you might recognize this one 😉 It’s almost exactly a Pebble 2, upgraded with modern chips and new tricks. Duo is short for ‘Do-over’.</p><p><em><strong>Similar to Pebble 2, it features</strong></em></p><ul><li>Ultra crisp 1.26” black and white e-paper display</li><li>Runs 10,000+ Pebble apps and watchfaces</li><li>Lightweight polycarbonate frame in two colour options - White or Black</li><li>Water resistant (targeting IPX8)</li></ul><ul><li>30 day battery life (up from 7)</li><li>Linear resonance actuator (quieter and stronger than vibrating motor)</li><li>More reliable buttons (up to 30% longer lifetime in testing)</li><li>Barometer and compass sensors</li></ul><p>Since this watch will look and feel just like a Pebble 2, you can refamiliarize yourself with it via<a href=\"https://www.youtube.com/watch?v=KQh1b_srGM4\"> videos</a>, or<a href=\"https://www.starkinsider.com/2016/10/pebble-2-review-smartwatch-perfected.html?utm_source=chatgpt.com\"> reviews</a>. For people interested in hacking on PebbleOS firmware, we’re offering an optional JTAG connector. I recommend buying 2 units if you want to hack, just in case!</p><p>This is my dream watch. It’s everything Pebble Time 2 was going to be and more! </p><ul><li>64-colour 1.5” e-paper display. Same display as Pebble Time 2 - much more room for text and details (53% bigger and 88% more pixels)</li><li>Runs 10,000+ Pebble apps and watchfaces</li><li>Metal frame and buttons (Black/White and likely a 3rd colour option as well)</li><li>30 day battery life (estimate)</li><li>Flat glass lens (less glare and reflections than Pebble Time family curved lens)</li><li>Water resistant (targeting IPX8)</li><li>Linear resonance actuator (vibrator)</li><li>Standard 22mm watch strap</li></ul><p>The industrial design is closely based on Pebble 2, which I really love. It’s slightly bigger to accommodate the larger display. Both the frame and buttons are made of metal (most likely CNC milled aluminum). More details, including final colour options, will be shared later this year. </p><p>Left: Core 2 Duo - Right: Core Time 2</p><table><tbody><tr></tr><tr></tr><tr></tr><tr><td>6-axis IMU, compass, barometer</td></tr><tr></tr><tr><td><em><strong>Linear resonance actuator (vibrator)</strong></em></td></tr><tr></tr><tr></tr><tr></tr><tr><td>Heart rate, step and sleep tracking</td></tr><tr></tr></tbody></table><p>Each watch runs open source&nbsp;<a href=\"https://github.com/pebble-dev/pebble-firmware\">PebbleOS</a>. This enables all the baseline Pebble features like receiving notifications, timeline, watchfaces, alarms, timers, calendar, music control, basic fitness tracking, etc. </p><p>The really fun part is that most of the existing 10,000+ PebbleOS watchfaces and apps will immediately work on these new watches, though some may try to access web services that no longer exist. Browse the full appstore on <a href=\"https://apps.rebble.io/en_US/watchfaces\">apps.rebble.io</a>.</p><p>Existing apps/faces will show up with a border on Core Time 2 until developers update them, since it has a larger display (200x228 vs 144x168 pixels). Read more about on the <a href=\"https://developer.rebble.io/developer.pebble.com/blog/2016/10/11/Emery-SDK-Beta/index.html\">old Pebble dev blog</a>.</p><p>We will publish a companion mobile app for Android and iOS. My friend and past Pebble colleague, Steve, recently joined us to lead this effort. He’s joining crc32, long-time <a href=\"https://github.com/pebble-dev/mobile-app/actions/runs/13609363513\">Cobble</a> developer, who has been working with me since last summer. We’ll also be working on an updated SDK for creating new PebbleOS watchfaces or apps.</p><p>These watches will be sold exclusively through <a href=\"http://store.repebble.com/\">store.rePebble.com</a>. Due to limited supply of display inventory, both watches will be manufactured in limited quantities. I highly recommend placing a pre-order - we will be manufacturing fewer watches than the number of people who have signed up on rePebble already! A pre-order secures your watch but gives you flexibility if you change your mind - you can get a refund at any time up until your watch ships. </p><p>Left: Core 2 Duo running PebbleOS - Right: Engineering samples</p><p>Schedule-wise,  is quite far along. We’ve already produced dozens of Core 2 Duo watches for testing and development. We’ve tested and confirmed our <a href=\"https://i.imgur.com/HwdTncx.mp4\">button improvements</a>. PebbleOS has been compiled for the new architecture and runs on the watch. All firmware development is open source - you can follow the fun on <a href=\"https://github.com/pebble-dev/pebble-firmware/commits/main/\">Github</a> and <a href=\"https://discord.gg/aRUAYFN\">Discord</a>. Our current schedule calls for shipments to begin in July. </p><p>How are we so far along, given that PebbleOS was only open-sourced in January? Two things helped: a) I took a monetary risk and began product development a bit earlier 😉,&nbsp;and b) we found a supplier who still had inventory of some Pebble 2 components. </p><p>Core Time 2 display lighting up! </p><p>For , we’ve finished component selection, initial industrial and mechanical design, and found sources for long lead time components. Now, we’re in the middle of creating the first prototypes. </p><p>The grand irony of hardware development is that software development is usually the slowest part of the project. Not this time! We’re extraordinarily thankful to Google for open sourcing PebbleOS, which gave us a massive boost.</p><p>We’ll share more details (like Core Time 2 frame colour options) later this year, as we get closer to mass production. Our current schedule shows shipments beginning in December. Follow along on this <a href=\"https://ericmigi.com/\">blog</a>, via <a href=\"https://repebble.com/signup\">email</a>, on <a href=\"https://bsky.app/profile/ericmigi.com\">Bluesky</a> or <a href=\"https://twitter.com/ericmigi\">Twitter</a>.</p><h3>Why were these specific specifications selected?</h3><p>Building a smartwatch is an exhausting (😂) exercise of constraint maximization. Think of it as linear algebra - we’re solving multiple equations with multiple unknown variables. The primary constraint is display selection. This choice governs the size and shape of the physical design, as well as being the component with the most power drain and biggest cost. Other variables are Bluetooth chip (governs: cost, software compatibility, engineering time), factory selection (cost, quality, speed, risk), sensors (power, cost, software), battery (size, battery life, cost) and more!</p><p>Despite what Pebble’s second Kickstarter branding proclaimed, solving this inherently requires compromise. It’s a fine line to walk and generally the data is not 100% known upfront, so it inevitably requires some degree of trusting your gut. </p><p>: we’re adding a touchscreen to Core Time 2. Why? Very specifically, I want to add the concept of ‘complications’ to watchfaces and widgets. Like on Apple Watch, these complications/widgets will show glanceable information like weather, next calendar event, step count, etc. The touch screen adds the ability to tap on the complication and directly open the associated app. This is much faster to use than opening an app via the button menu, and saves your quick launch (long-press on the buttons) for other apps. The touchscreen may be used for other interactions, like swiping to rapidly scroll down a list, but that will be lower priority. </p><p>Core Time 2 will be the first PebbleOS watch to ship with a touchscreen, but not the first that we designed! In 2015, Pebble designed it into a watch called Cutts (or C2) and did some (very) preliminary software development work to integrate a touchscreen into PebbleOS.</p><p>: I’m really excited that battery life will be increased from 7 days to 1 month! This is due to massive improvements in Bluetooth chip power efficiency over the last 10 years. </p><p>: we’re adding this primarily for potential use in apps that benefit from audio output, like a ChatGPT or other AI agent app. It can’t easily be used for making voice calls, since the watches will not support Bluetooth Classic (required for headset profiles), but theoretically someone could write a custom voice calling client (eg SIP or something).</p><p>: neither watch will support smartstraps. Sorry. Most people don’t even remember this feature even existed, which is kinda the answer to why it will not be supported. RIP.</p><h3>You shouldn’t get one if…</h3><p><strong>You need a perfectly polished smartwatch.</strong> This project is a labour of love rather than a startup trying to sell millions of watches. There may be some rough edges (literally). Things will get delayed. Some features will not be ready at launch. Things could break. Things could not last as long as you’d like. The only thing we can guarantee is that it will be awesome and a lot of fun! Every time you look down at your watch, you will smile 🙂</p><p><strong>You’re looking for a fitness or sports watch</strong>. That’s not what we’re making. From what we hear, Garmin watches are great for runners/cyclists/triathletes!   </p><p><strong>You’re comparing this to an Apple Watch</strong>. There is NO way for a 3rd party smartwatch to compete with Apple Watch. Apple restricts 3rd parties in major ways - read <a href=\"https://ericmigi.com/blog/apple-restricts-pebble-from-being-awesome-with-iphones\">my blog post</a> for more information. For example, 3rd party watches on iOS cannot send replies to notifications.   </p><p>These watches are not made for everyone. We want to be upfront with you about what to expect. </p><blockquote><p>Watch images above feature impeccably designed watchfaces from <a href=\"https://ttmm.is/pebble/\">TTMM</a>, including one of my all-time favourites - <a href=\"https://apps.rebble.io/en_US/application/57812aa56c21044501000ed5?query=ttmm&amp;section=watchfaces\">TTMMBRN</a>. Thank you Albert!</p></blockquote>","contentLength":9212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43400989"},{"title":"Google to buy Wiz for $32B","url":"https://www.reuters.com/technology/cybersecurity/google-agrees-buy-cybersecurity-startup-wiz-32-bln-ft-reports-2025-03-18/","date":1742300309,"author":"uncertainrhymes","guid":10734,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43398518"},{"title":"Moving away from US cloud services","url":"https://martijnhols.nl/blog/moving-away-from-us-cloud-services","date":1742285377,"author":"MartijnHols","guid":10733,"unread":true,"content":"<p>For years, using US clouds in the EU has been questionable. <strong>Time and time again, data-sharing agreements between the EU and the US get busted</strong>, showing there's just no legal compatibility between EU privacy rights and US spying laws. Every few years, it's <a href=\"https://en.wikipedia.org/wiki/PRISM\">revealed</a> that the US is <a href=\"https://www.wired.com/story/congress-spy-powers-fisa-ndaa-trump-702/\">spying more</a> than expected. While everyone in the EU kinda knows storing personal data on US clouds is , they figure that since everyone is doing it, even if it's not legal, then <a href=\"https://blog.iusmentis.com/2025/01/17/mag-ik-ondertussen-data-in-de-cloud-zetten-als-die-fysiek-in-europa-blijft/\">at least you'd be wrong with everyone else, so that makes it less bad</a>. And soon, it may become <a href=\"https://noyb.eu/en/us-cloud-soon-illegal-trump-punches-first-hole-eu-us-data-deal\">fully illegal</a>.</p><p>This sets a dangerous precedent. It increasingly appears that the US government will use tech companies as a weapon.</p><img alt=\"Taking the exit with the US cloud\" loading=\"lazy\" width=\"2750\" height=\"1350\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcover.38be4c3b.png&amp;w=3840&amp;q=75 1x\" src=\"https://martijnhols.nl/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcover.38be4c3b.png&amp;w=3840&amp;q=75\"><p>All things considered, <strong>it's high time to move away from US cloud services</strong>. In this article, I'll break down how I'm migrating away, what I switched to, and the challenges along the way.</p><p>Before starting, I made an inventory of the US cloud services I depend on. I decided to focus on cloud services (and not software and hardware), as that's where the biggest risks are. My  dependency list roughly in order of reliance:</p><ul><li>Microsoft Office 365</li></ul><p>This list is focused on cloud services I use professionally (for my business), but I plan to migrate personal services as well (privately I also rely on WhatsApp for communication and iCloud for backups and notes).</p><div><div><p>While they're technically not cloud services, I also depend on online\nplatforms like Hacker News, Reddit,\n<a href=\"https://www.linkedin.com/in/martijnhols/\">LinkedIn</a>,\n<a href=\"https://x.com/MartijnHols\">Twitter</a> and\n<a href=\"https://bsky.app/profile/martijnhols.nl\">BlueSky</a> and WhatsApp. I use social\nmedia like Hacker News and Reddit to reach people and stay informed, and\nLinkedIn, Twitter and BlueSky are ways for people to find and follow me. These\nplatforms are only as good as the people on them, making most of them\nirreplaceable. Not because of their infrastructure, but because of their\ncommunities.</p></div></div><h2>Replacing the dependencies</h2><p>With my list handy, I replaced each dependency one by one. A useful tool for finding alternatives is the <a href=\"https://european-alternatives.eu/\">European Alternatives</a> website.</p><h3>Goodbye Microsoft Office 365</h3><p>Microsoft 365 was my most important dependency. It has all of my mail, including history that I'm legally required to save, my calendar, and most of my important files, which I'm legally required to keep long-term. Losing access to these services isn't an option.</p><p>On top of that, I've had long-standing usability issues with Microsoft 365. Particularly OneDrive for Mac, which was constantly draining my battery and overheating my CPU, making the switch even more appealing.</p><p>I went looking for an all-in-one solution; mail, calendar, and storage, mostly because it's simpler, but I also don't want to pay for everything separately. In the end, I settled on <a href=\"https://proton.me/\">Proton</a>. I had heard of Proton Mail before as a privacy-focused highly-encrypted mail provider, but never really looked into it as I had no real need for that, and I figured it would come at the cost of usability and price. But <strong>I was seriously impressed by how good Proton software is.</strong> Their apps are really . I actually have a better experience than I did using Microsoft's. I'm impressed by how they made everything work so well without sacrificing privacy in any way.</p><p>The <a href=\"https://proton.me/business/plans\">Proton Business Suite</a>, which includes 1 TB for Mail, Calendar, VPN, Pass and Drive, is only 12.99 per month (the non-business variant is 9.99). And just like Microsoft 365, it has full-fledged support for teams.</p><p>Another thing that impressed me about Proton is <a href=\"https://www.washingtonpost.com/technology/2023/06/27/gmail-switch-proton-email/\">how easy</a> switching from Microsoft 365 was. Their <a href=\"https://proton.me/easyswitch\">Easy Switch</a> tool, handled all the data importing for me in just a few clicks. All I had to do manually was update the DNS for my domains. And as a bonus, Proton allows setting up 15 custom email domains, so I can cancel all of my separate <a href=\"https://www.transip.nl/email-hosting/\">email hosting</a> and centralize all my mailboxes.</p><p>Yet another bonus: the Proton Business Plan includes a VPN. I only need one very occasionally and my needs were never enough to subscribe to one, so this is a nice extra perk.</p><p>I was very hesitant to switch password managers. I'd used Bitwarden daily for over 4 years, and despite its usability issues, it had become a core part of my toolkit.</p><p>But as I really wanted to ditch US cloud services, I looked into replacing it. After all, even though my vaults are encrypted, they're still hosted by Bitwarden and they can restrict my access as they see fit.</p><p>The obvious place to start was <a href=\"https://proton.me/pass\">Proton Pass</a>, since it came  with the Proton Business Suite. When I saw <a href=\"https://www.youtube.com/watch?v=CBdDYurOMyg\">1Password users switching</a>, I figured it had to be good. Turns out, it really is! <strong>It has all of the features I used in Bitwarden, and more.</strong> The main new things I appreciate are sharing a password via a link, and the return of the login dropdown in login forms. Oh, I missed you so.</p><p>Migration was even easier than mail; just export in Bitwarden and import in Proton Pass, and you're done. Even TOTP codes and secure notes transfer over seamlessly.</p><p>GitHub only hosts my repositories and CI; I don't store any personal data on GitHub, nor do I use it to process it. I have copies of my repositories on my computer, plus occasional backups. The only real risk is my Mac's SSD self-destructing at the exact same time; then I'd lose a few days of work.</p><p>Still, I rely on GitHub heavily for my projects, so migrating will take time. I'll find a new home for my repositories eventually, but considering the impact, this will probably be the last thing I migrate, so I haven't really explored alternatives yet.</p><h3>Escaping Google search (almost)</h3><p>If you've tried alternative search engines, you know: Google search is really, really good. It's very hard to find an alternative that doesn't make you want to switch back.</p><p>Startpage is a Dutch-owned search engine that promises \"uncompromising\" privacy. Despite that, <strong>I was really surprised by how good its search results are.</strong> Turns out that's because it's actually a Google proxy. Bummer.</p><p>It's unclear what Startpage's reliance on Google means for privacy, but it's still much better than using Google directly. It doesn't eliminate the US cloud dependency though. Unfortunately, fully European alternatives that don't make me want to switch back are scarce.</p><h3>Disconnecting Cloudflare/Google DNS</h3><p>The promise of optimal DNS performance attracted me to the DNS servers of Cloudflare and Google, but this too comes with privacy issues and reliance on US infrastructure. As part of this de-US-ing, I've opted to replace those with <a href=\"https://www.quad9.net/\">Quad9</a>. Quad9 is a free, Swiss-based DNS service focused on  and privacy.</p><div><div><p>This section only covers Cloudflare's DNS servers (1.1.1.1), not its\nnameservers or CDN. If you're looking to move away from Cloudflare's CDN,\ncheck out <a href=\"https://jonathan-frere.com/posts/switching-to-bunny-cdn\">Switching to Bunny\nCDN</a> by Jonathan\nFrere for a relevant experience.</p></div></div><p>Docker Hub (a registry for ) is essential to my ; it stores the private images that my servers <a href=\"https://github.com/containrrr/watchtower\">automatically pull</a> to install new versions of my apps.</p><p>I'm quite eager to move away from Docker Hub. I haven't been a fan of Docker Inc. for a long time; they make it really obvious their focus is entirely on enterprise customers. Docker started going downhill when they <a href=\"https://reddit.com/r/docker/comments/85w2vd/docker_cloud_is_shutting_down/\">shut down Docker Cloud</a>. Later, they even canceled my subscription based on their \"fair use\" clause because they misinterpreted their logs (they were counting <a href=\"https://github.com/containrrr/watchtower/discussions/668\">HTTP HEAD requests</a> as image pulls).</p><p>Since my needs are simple, I initially considered self-hosting a Docker registry with a European S3-compatible backend, looking at  and <a href=\"https://www.scaleway.com/en/object-storage/\">Scaleway Object Storage</a>. But the self-hosted Docker registries seemed to require too much complexity and it just didn't seem worth the overhead.</p><p>Instead, I went with <a href=\"https://www.scaleway.com/en/container-registry/\">Scaleway Container Registry</a>. It's fully managed, straightforward to use, and keeps my images in the EU. In the end it took me less than two hours to setup and will probably cost me . Can't beat that.</p><p>For NPM (a Node package registry), a key challenge is the reliance on public . These are downloaded and installed in every CI build and every time someone clones the project. While these are heavily cached, NPM becoming unavailable would break most tooling.</p><p>Unfortunately I couldn't find any public European NPM mirrors (let me know if I missed any). I think this is because most large companies host their own private mirrors. I've used <a href=\"https://verdaccio.org/\">Verdaccio</a> before as a private registry and cache, and setting it up should be fairly easy with a Docker image.</p><p>However, this requires setting up persistent storage first. I'll sort this out shortly after publishing this article. Once that's ready, I'll set up Verdaccio. That way, if NPM becomes unavailable, I'll still have access to the versions I've been using, and others can use that registry to get started.</p><p><strong>Migrating away from US cloud services was easier than I expected.</strong> While there are still some challenges to tackle, migrating my most important dependency (Microsoft 365) took just an afternoon. This is mostly thanks to Proton, which turned out to be a surprisingly good alternative to Microsoft 365.</p><p>Replacing Docker Hub required more research, mainly because I hadn't set up a custom Docker registry or persistent storage before – something I should have tackled ages ago.</p><p>Other services, such as GitHub, will take longer. But it's on my list and seems within reach. It even seems fun to explore European alternatives. Or maybe I'll self-host something.</p><p>Google Search, on the other hand, still feels unbeatable, as its best replacement still uses it under the hood. , but for now using a more privacy-focused proxy seems to be the only viable option.</p><p>If you're thinking about reducing your reliance on US cloud services, now is a good time to make it so. The risks, both in terms of privacy and control over your infrastructure, are getting harder to ignore. As my experience shows, some migrations may be a lot easier than you'd expect.</p><p>At the very least, think twice before signing up for  US services. Consider European services instead.</p><p>If you've gone through a similar transition, I'd love to hear what worked (or didn't work) for you.</p>","contentLength":9852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43396795"},{"title":"Block YouTube ads on AppleTV by decrypting and stripping ads from Profobuf (2022)","url":"https://ericdraken.com/pfsense-decrypt-ad-traffic/","date":1742284790,"author":"udev4096","guid":4663,"unread":true,"content":"<div><div><p>I discovered that putting a man-in-the-middle proxy between my Apple TV and the world lets me decrypt HTTPS traffic. From there, I can read the Protocol Buffer data Google uses to populate YouTube with ads. It is too CPU-intensive to decode Protobuf on the fly, so instead, I found a flaw in the Protobuf format which allows me to reliably change one byte to obliterate ads.</p><p>What follows is a reference guide for setting up a bare-metal network router to block malicious ads, obnoxious ads, tracking, clickbait, crypto-jackers, scam popups, Windows spying on you, etc. using blocklists to protect all networked devices.</p></div></div><div><div> Let’s build a cryptographically-strong router with FreeBSD and pfSense to completely block YouTube ads using a flaw in the Google Protocol Buffer format to completely block pre-roll, mid-roll, and end-roll YouTube ads on Apple TV and iPhones, network-wide.</div></div><div><div> I want to support content creators, so to be fair, after a few months of blocking YouTube ads, I am now paying for YouTube Premium; Just because I can break something, doesn’t mean I need to.</div></div><h3>Part 1 – Setup pfSense on Bare-Metal</h3><h3>Part 2 – Isolate Network LANs</h3><h3>Part 3 – Setup DNS Adblocking</h3><h3>Part 4 – Trick the YouTube Ad Algorithm</h3><h3>Part 5 – Decrypt HTTPS Traffic</h3><h3>Part 6 – Intercept Apple TV and iOS YouTube Ads</h3><h3>Part 7 – Reverse-Engineer Protobuf Messages</h3><h2>Why block Malicious Ads and Behaviour Tracking?</h2><p>You are a valuable commodity that is bought and sold without your knowledge or consent. You will be tricked with clickbait, distracted with large ads, and enticed to leave the site you are on at every opportunity. Plus, everything you do online is being monitored so your habits and searches can be remarketed and sold over and over again for years.</p><p> – Knowing what you like to watch and read, what phone you have, what you watch on Netflix, what you shop for, what you ask Alexa about, yout taste in music, etc. is  valuable to advertisers. Spying on people is such a big problem that Europe passed the <a href=\"https://en.wikipedia.org/wiki/General_Data_Protection_Regulation\" target=\"_blank\" rel=\"nofollow\">GDPR law</a> so every site you visit asks if you are okay with cookies (and we blindly click “ok” to hide the banner). We must wrestle back privacy ourselves.</p><p> – If privacy doesn’t concern you, how about this: it is well-known that between 25% and 40% of network traffic is ads, tracking, JavaScript to load trackers (<a href=\"https://github.com/fingerprintjs/fingerprintjs\" target=\"_blank\" rel=\"nofollow\">fingerprint.js</a>, <a href=\"https://www.npmjs.com/package/@analytics/google-tag-manager\" target=\"_blank\" rel=\"nofollow\">googletagmanager.js</a>), websocket traffic to collect how you scroll and what you type (<a href=\"https://www.hotjar.com/\" target=\"_blank\" rel=\"nofollow\">Hotjar</a>), and the like. Do you have a 100 Mbps internet connection? Consider it 60 Mbps!</p><p> – Then there is clickbait. “You won’t believe what Tom Cruise did. He…” and you may want to click. Then you are in the spider’s web. How about fake news? Or articles that don’t say “sponsored” in size-8 font, but now say “underscored” to be clever. What is even real anymore? As soon as you click on clickbait, you may end up on a page with a dozen more ads that aren’t approved by Google but lead to a dark world of maliciousness. <strong>Clickbait is so incredibly profitable to scammers.</strong></p><p> – Some websites will load crypto-mining JavaScript (e.g. <a href=\"https://www.fortinet.com/blog/threat-research/the-growing-trend-of-coin-miner-javascript-infection\" target=\"_blank\" rel=\"nofollow\">CoinHive.js</a>) so while you read, they overheat and abuse your computer to try to make a few pennies. Some sites will load JavaScript that tries to steal from your crypto wallet or trick you into transferring cryptocurrency.</p><div><div> It is highly lucrative yet detrimental to you to track and trick you, and only  can do something about it.</div></div><p>A virtual machine, Docker image, or Raspberry Pi are not performant enough to protect a whole SMB network; We need dedicated hardware with a cryptographic instruction set so that its only function is to route, decrypt, and monitor packets in and out. Here is what I used.</p><ul><li>A mini PC with the AES-NI instruction set (e.g. J4125)</li><li>Several gigabytes of DDR4 RAM (e.g. 32 GiB)</li><li>A decent mSATA SSD drive (e.g. 128 GiB)</li><li>A USB drive to transfer pfSense</li></ul><p>I’ve ordered a mini J4125 PC from AliExpress, ordered 32 GB of DDR4 RAM and a 128 GB mSATA from Amazon, and will assemble them for the first time now.</p><div><div> Out of caution, I searched diligently for a barebones mini PC that did not include RAM or an SSD; there is nothing stopping an overseas seller from including some generic RAM and SSD but charging Samsung prices.</div></div><div><div> 128 GB of disk space on a router? Yup. That should be plenty of space to hold logs and not wear down the SSD too quickly,  to allow beautiful packet capture (and maybe an edge cache for NPM and Docker?).</div></div><p>A beautiful box, isn’t it? It only has 3 LAN ports, but it can be extended with network switches.</p><h2>Install pfSense on Bare Metal</h2><p>I’ve never used <a href=\"https://www.pfsense.org/getting-started/\" target=\"_blank\" rel=\"nofollow\">pfSense</a> before, so we will explore this together. The compressed image is about 360 MB and can be flashed to a USB drive with an AppImage binary of Etcher (very cool). Decisions, decisions: VGA install or serial? Let’s serial into the new router. Why not?</p><p>Well, that looks painful. It would also be a whole production to serial into the box in case of an emergency because the serial port is inside, and there isn’t even an RS232 or JTAG connector – just some narrow header pins. Yikes. Let’s go with VGA and plug a keyboard into the USB port – get ready to navigate with arrows and tabs.</p><p>I’ll follow this guide on <a href=\"https://www.youtube.com/watch?v=9kSZ1oM-4ZM\" target=\"_blank\" rel=\"nofollow\">YouTube</a>. I’ll pass on encrypting the disk since I would like to avoid entering a passphrase each time the mini PC reboots. A stripe disk is fine since there is only one disk. I have no idea what to expect yet, so I will pass on dropping to a shell for a more advanced configuration.</p><p>I ejected the USB containing the boot image (important) and rebooted the little box. It played a melody on the internal speaker (there is an internal buzzer and thankfully it isn’t very loud).</p><p>Do I need to have a LAN connection already, or can I just start the thing? I’ll just start pfSense and let it complain to me if it wants… and according to the YouTube <a href=\"https://youtu.be/9kSZ1oM-4ZM?t=610\" target=\"_blank\" rel=\"nofollow\">tutorial</a>, I should guess which port is LAN 1. I’ll do that now.</p><p>I figured out that I should set the LAN 1 to a static IP address that is not in my existing router’s DHCP range, so I went with . Now I can access an admin web portal (/). Hooray.</p><p>Yikes, the mini PC beeped at me and informed me that ‘admin’ has logged in. That startled me a bit, but hey that is pretty neat.</p><h2>Enable the AES-NI Cryptographic Instruction</h2><p>I played around with the wizard, used defaults, and got to the web configurator. The first thing that caught my eye was <code>AES-NI CPU Crypto: Yes (inactive)</code>. I went out of my way to get a mini PC with AES-NI. What gives?</p><p>Ah, this needs to be enabled in System &gt; Advanced &gt; Miscellaneous. Why not auto-detect this and use the best option? I’m glad I spotted that, or else this mini PC might as well be a Celeron J1900 of yesteryear.</p><p>Having 32 GiB of RAM, let’s take advantage of that and use a generous amount of RAM for  and , and since hopefully this 128 GiB SSD has wear levelling, let’s take a RAM Disk backup every hour.</p><p>Reboot!  is now active.</p><p>This dashboard is pretty slick. I’m just discovering that there are widgets that can be added to the Dashboard, including S.M.A.R.T to alert us if the SSD is going bad. Nice.</p><p>Hang on, when I added the Services Status widget, something called  shows up. What is that? Research shows it’s a daemon for hardware smart keys that we can probably do without(?). It can be disabled in the  file like so:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Wait. After some time went by, I noticed the router slowed down, fatally.</p><div><div> Do NOT try to disable the Smart Card Service as it is needed by IPsec; if you start experimenting with an IPsec VPN tunnel and the  daemon is disabled, then your hard disk will fill up with logs and your CPU will run hot.</div></div><h2>Adblocking with pfBlockerNG</h2><p>This unboxing and setup has been fun, but I’d like to block all the bad traffic on my network. I’ve been using a workhorse of a DNS-level adblocker called <a href=\"https://ericdraken.com/block-malicious-ads-with-pi-hole/\" target=\"_blank\">Pi-Hole</a> on a… yes, Pi, but it would be nice if I can reclaim that wee bit of hardware for something else and use a comparable add-on module in pfSense. Let’s explore that now.</p><blockquote><p>pfBlockerNG is a very powerful package for pfSense® which provides advertisement and malicious content blocking along with geo-blocking capabilities.</p></blockquote><p>Question: Do I install the first  or the  which feels like a developer version? I’m a software developer, so this is for me, but am I a pfSense developer? No. Maybe it will show me advanced logs or I can mess about with LUA? Let’s Google this.</p><p>From <a href=\"https://forum.netgate.com/topic/156604/pfblockerng-vs-pfblockerng-devel\" target=\"_blank\" rel=\"nofollow\">here</a>, random people are saying to install the development version. Another <a href=\"https://linuxincluded.com/block-ads-malvertising-on-pfsense-using-pfblockerng-dnsbl/\" target=\"_blank\" rel=\"nofollow\">blogger</a> advocates using the dev version as well. Meh, I guess we can install , , and Python 3.8. This doesn’t  like a development version since it has exciting dependencies.</p><p>That was painless and only added an extra 20 MiB. It seems a lot of the dependencies are part of pfSense already. The knight at the end of Raiders would say that I have chosen wisely (hey, why did Indy age like a normal person up to Indy 4 if he drank the immortality water that the thousand-year-old knight also drank?).</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>There are a lot of options in step three. This is not like Pi-hole at all. I’m going to <a href=\"https://www.youtube.com/watch?v=xizAeAqYde4\" target=\"_blank\" rel=\"nofollow\">come back to this</a> and set up my network instead so I accomplish retiring my Nighthawk R700 or giving it new life as a Wi-Fi AP.</p><div><div> If the  service won’t start or the status tab states , try deleting the empty file  (<a href=\"https://www.reddit.com/r/pfBlockerNG/comments/9x5sid/pfb_dnsbl_service_will_not_start/\" target=\"_blank\" rel=\"nofollow\">ref</a>).</div></div><h2>Isolate LANs for Security</h2><p>An opportunity has presented itself: I can create real networks on each of the three router Gigabit ports (not VLANs), and should I do so? Yes, yes I should. I would like a dedicated hardware network for all my home-phoning spy devices (Alexas and Apple TV) so they don’t flood my network with metrics info and “sure I’m muted and not listening to you” audio payloads back to their HQs.</p><p>I can see it now: A Wi-Fi AP on a hardware LAN that is isolated from everything else and dedicated to these gadgets,  runs through the adblocker  traps hard-coded DNS queries to  and  and others (I’ll have to explore this) so YouTube on my TV doesn’t sneakily bypass  any DNS-level blocker. It’s so Utopian an outcome I may not be able to sleep.</p><p>I’ve decided that my bottom-shelf TP-Link wireless router that is so old that  might as well be “A.D. 1200” is going to be my Wi-Fi AP for those IoT spy devices.</p><p>In sum, there will be a dedicated hardware LAN</p><ul><li>with a wireless AP (AC1200) for Amazon/Apple gadgets and the TV.</li><li>with a wired switch for all the beefy computers and clusters in my lab.</li><li>with another wireless AP (R7000) just for iPhones and watches.</li></ul><p>As an aside, since doing an Offensive Security hacking course in my spare time, and I rare-earth-magnet-strongly suggest isolating Wi-Fi devices from any critical LAN segments connected to devices that touch daily banking or stock trading (or crypto wallets).</p><h2>Class B IPv4 172.31.1.0/24 Network for Untrusted Devices</h2><p>The class B IPv4 range 172.16/16 is a valid range of private IP addresses. I’m not uncomfortable with Alexa and Apple TV being even on the same class network as my main LAN segment, so I will banish them to the class B private network at the hardware level, and my more trusted LANs will be on the traditional class C network (192.168/16). This helps mitigate any misconfigured  rules by naturally having no routes between the two networks.</p><p>Be sure to enable the DHCP resolver on the physical NIC that will connect smart devices (which mainly just tell me the weather and creepily listen to me sleep).</p><p>From this point, DHCP works on this new network, but by default, it assigns IP addresses but does  routing. All traffic is blocked by default.</p><p>We need to manually add rules so traffic on the physical NICs goe somewhere.</p><p>There is a logging message. Let me reproduce it below.</p><blockquote><p>Hint: the firewall has limited local log space. Don’t turn on logging for everything.</p></blockquote><p>I read that to mean, “Congratulations on not cheaping out on your SSD. Now go forth and log everything, my son.”</p><p>I’m not a new-age, fancy-jazz, coloured-light- or smart-plug-controlling guy who forgot how to turn on a light without his phone, so I do not need to have smart devices on the same network as my phone (why create dozens of wireless attack vectors into your home?). I’m classicly trained to actuate an electromechanical current interrupter on the wall and light let there be.</p><h2>Setup the Untrusted Wi-Fi AP</h2><p>How do I reach the admin UI of AC1200 Wi-Fi AP now? I factory reset it, plugged the WAN NIC into the ETH3 NIC of the pfSense router, but both devices are just blinking at me.</p><p>I suppose I can just Wi-Fi into the factory-reset AC1200. Yikes, 2016 was a bad year for responsive web UIs I take it. This is horrible; I’ll pull out a netbook for this. One sec.</p><p>It seems the Archer C5 has no AP Mode. This is my problem, not yours, but I’m still going to vent.</p><p>Oh, and the “refresh” icon on the top of the DCHP Leases page in pfSense is not “refresh”, but “reload service”. Whoops.</p><p>Well, I bricked the AC1200 router. I will have to run an Ethernet cable manually… but, wait, my thin notebook has no Ethernet ports and needs a USB-NIC adapter. Happy Friday.</p><div><div> Connect LAN to LAN, not the AP’s WAN to pfSense’s LAN unless you want to do double NATing.</div></div><p>There were shenanigans, but I set the LAN IP of the AC1200 to , the ETH3 NIC IP of the pfSense router to , and set the pfSense DHCP service on ETH3 to assign addresses . What  was setting the AC1200 to  as it was unreachable (reason unknown). Oh yes, I had to turn off firewally things and , and basically drop the horsepower of this TP-Link router down to that of a potato battery. The above settings allow me to access the AC1200 remotely now.</p><p>The other video ran its course, so I started following this <a href=\"https://www.youtube.com/watch?v=FPgPHJvLmh0\" target=\"_blank\" rel=\"nofollow\">YouTube video</a> (set the speed to 1.5x).</p><p>One more thing: I installed the  package for pfSense and scanned the AC1200 router, and found some sneaky ports open.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Port  is a print server port that I’ve now closed. However, the Archer C5 AC1200 is vulnerable to all kinds of Kali <a href=\"https://www.hackingtutorials.org/wifi-hacking-tutorials/tp-link-archer-c5-router-hacking/\" target=\"_blank\" rel=\"nofollow\">mischief</a> so it was wise to put it on its own network. I’m not sure how to close port 22 and the  service on it the AC1200 because the stock firmware is ancient and crippled, so I’ll just have to block port 22 on the whole LAN segment.</p><p>I’ve also taken care of disallowing private networks to ingress on the WAN (see the next section to set up DMZ).</p><h2>Unable to Reach 172.31.1.x from 192.168.10.x</h2><p>Ping and Traceroute are aiding me in my efforts to connect to the AC1200 Wi-Fi AP from my  LAN. I went ahead and added the subnet to the Symantec Firewall rules just in case (Symantec has its place now and then, but yes, definitely have available PC CPU horsepower to spare).</p><p>Now, it seems ICMP packets are no longer blocked between networks, but I still cannot ping the AP web management UI even though I can see the pings in the traffic logs.</p><p>I’ve even added an “any to any” firewall rule on the Untrusted network. No change.</p><p>Let’s try a stealth scan instead: <code>sudo nmap -sS -v 172.31.1.*</code>.</p><p>Nope, pfSense doesn’t like that at all. And, the whole network stopped working. Nice security! Also, dang.</p><p>The good news is that I’ve isolated the packet malaise to the TP-Link AC1200 box itself. I suspect that I need to add  to forward packets with no addresses in them, but I’d need root access to the AC1200. Let’s burn it to the ground and rebuild from its sprinkler-soaked ashes.</p><h2>Replace Stock Firmware on the AC1200 Wi-Fi Access Point</h2><p>Of course, I cannot actually stop Untrusted LAN devices from reaching the AC1200 as they all exist downstream from the pfSense box.</p><p>DD-WRT open-source router firmware, meet my ancient Archer C5 and do your thing.</p><p>The Archer C5 did not accept the DD-WRT firmware. Hmm… how about OpenWRT?</p><p>The Archer C5 did not accept the OpenWRT firmware either. What the actual facepalm (WTAF)?</p><p> My hardware is revision 2 using the Broadcom chipsets which are notoriously difficult networking chips.</p><div><div> Devices with Broadcom Wi-Fi chipsets have limited OpenWrt supportability (due to limited FLOSS driver availability for Broadcom chips). (REF: OpenWRT.org)</div></div><p>Alright, so OpenWRT, DD-WRT, and Tomato projects have no firmware for this AC1200 with unpopular Broadcom chipsets. Into the refuse bin it goes.</p><h2>Archer C5 v2 into the Refuse Bin, R7000 as the New Wi-Fi AP</h2><p>I’ve dismantled the AC1200 so I do not forget why I threw it out. It’s too bad because it’s so pretty on the inside, and they always say, “It is what is inside that counts… except if you are a router with Broadcom chips.”</p><p>The R7000 is factory reset, and here is the first problem:</p><div><div> On factory reset, the Nighthawk R7000 is pretty uptight about the format of the password. One rule is that no more than two identical consecutive characters are allowed. Well, thanks Netgear for pretty much providing a Regex to password crackers. Let’s disable all those rules with a few keystrokes to delete the JavaScript “blocking” the form submission. Now my admin password does not conform to the Regex and is super long. Muhahaha to Netgear password crackers.</div></div><p>The R7000 is in AP mode, but I can still access the pfSense web management page from the Untrusted network. Let’s lock down the web UI in pfSense under Firewall Rules.</p><h2>Set up the Trusted Wireless Network</h2><p>The Untrusted network is now looking good. It’s time to make the  R7000 Nighthawk I have into a Wi-Fi AP as well so my phone and watch have a safe place to connect to, as well as a laptop when I want to RDP into my wired machines from the kitchen. I was saving that for a honeypot AP, but I can come back to that later.</p><p>Let’s see if I can Wi-Fi into the Wireless LAN’s R7000…</p><div><div> Remember to physically unplug the pfSense upstream router from the R7000 because the R7000 is too helpful and will enter into AP mode by sensing any upstream routers, then you cannot get into the web UI anymore.</div></div><p>Since only my trusted devices should be on the Wireless LAN, I’ll turn off 2.4 GHz wi-fi because anything recent and wireless should support 5 GHz. That means those pesky AliExpress Pineapple wi-fi password stealers on the cheap side only use 2.4 GHz, so a neighbour is going to have to put in some effort to snoop on my network. Plus, 5 GHz is blocked more easily by walls and concrete, so I prefer it for averting medium-range snooping. But, I so am going to set up a honeypot and to brake check my faith in humanity.</p><p>It is normally straightforward to put a Wi-Fi router into AP mode by disabling WAN and DHCP.</p><h2>Network Devices Interconnectivity Check</h2><p>Do all my dozens of computers, laptops, Pis, clusters, NAS drives, and the like still connect as before? Most important is my web-scraping bot in a hardened, RAIDed, dedicated machine with its own UPS. But alas, I cannot SSH into it even though the SSH handshake packets make it to the hefty box.</p><p>Could this be our old frienemy IPv4 forwarding being disabled? Possibly. I’m able to SSH into the machine from my iPhone (seriously) when on the same network.</p><p>Nope. Adding  in the right place with a restart did not yield joy.</p><p>According to  (to tail  logs), UFW (Uncomplicated Firewall) is not blocking ICMP requests or TCP requests on port 22. When I do something nutty like try to SSH on, say, port 23, then I can see the UFW block logs in . Confirmed: Packets can reach that machine.</p><p>Running <code>tcpdump src 192.168.10.100</code> where the IP is from the Trusted network on the target machine shows it  responding to pings. I’m even getting replies to SSH handshake requests. So now we know that  packets are being dropped. Interesting! Aside:  is awesome.</p><p>Let’s follow the trail. Digging a little deeper I see replies to ICMP and SSH handshakes are being sent to some IP over HTTPS that I do not recognize. Bizzare. When I run the usual  tools I see that <a href=\"https://ericdraken.com/ssh-using-real-ip-bypass-vpn/\" target=\"_blank\">replies are going over a VPN</a> that I completely forgot about. Ha. Replies to a different subnet are egressing over the VPN, but cannot return properly. Neat.</p><p>Now that I remember what I did in 2019, I re-added NAT alias rules, and it’s showtime again.</p><h2>Windows File Sharing Gotchas</h2><p>Your path may be smoother, but I’ve always seem to make the Trench Run instead of remote-piloting a handful of lead-filled X-Wings at light speed right  the Death Star’s reactor to make it go boom: the easy way.</p><p>I’ve added some rules to allow Static DHCP devices to talk to each other – Windows devices – but by default, the Private Network in the Windows Defender uses the local subnet as the rule scope. That means different subnets are isolated. We can’t just relax the pfSense DHCP subnet mask to say  because it conflicts with another subnet. Instead, just to get file sharing working, I relax the  in Advanced Settings like below. Be sure to modify In and Out for SMB and ICMP.</p><p>Again, please add whatever subnets you desire instead of .</p><h2>Public Service Announcement: Edge Browser</h2><p>Why does the Microsoft Edge browser start automatically and run in the background, and why can’t I kill it when I ? If you’ve asked yourself this, you’re not alone. It turns out Edge starts up when you log in and it keeps running in the background. Here is the fix:</p><p>I suggest downloading Winaero Tweaker and applying registry tweaks to cut down on the Redmond Spy Machine.</p><h2>Block Clickbait, Endless Ads, and Dangerous Sites</h2><p>Thanks to web-browser and DNS-level adblockers (i.e. Pi-hole), it’s commonplace to block bad sites, crypto-miners, fingerprinters, trackers, remarketers, banners, pop-ups, fake tech-support scam alerts, and all manner of unscrupulousness designed to take advantage of you. Let’s take pfBlockerNG on pfSense for spin.</p><p> If you have multiple network interfaces (the mini PC has four), then you need to enable the Permit Firewall Rules for multiple interfaces and select them.</p><p>Would you like to have discretion over blocklists? Let’s add a DNS blocklist related to gambling and reload pfBlockerNG to see if a poker site is blocked on the Trusted LAN.</p><p>If you would prefer the connection to just close instead of rendering a PHP page, create a new PHP script with the following code and select it in the pfBlockerNG settings page:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><h2>Intercept All DNS Requests, Even to Hardcoded DNS Servers</h2><p>Let’s make sure all clients behind the pfSense router use the local Unbound DNS server so pfBlockerNG can act on them. We do not want apps and home assistants to bypass our DNS server, so we have to add some NAT rules.</p><p>First, we have to block DNS over TLS (for now) and only allow local DNS requests (note the rule order):</p><p>Here is a NAT rule for one interface. I started by making a rule for each interface except WAN (obviously) like this below.</p><div><div> NAT reflection should be disabled so the wild Internet cannot access our DNS server.</div></div><p>To make life simpler, I made a firewall alias of all non-WAN interfaces called . Covering IPv4 and IPV6 to redirect local DNS queries on port 53 to localhost are the following redirect rules:</p><p>Let’s also log trapped DNS requests. Head to the  page, click “Display Custom Options”, and add the lines:</p><p>Well, hello there, Microsoft Windows. What are you up to trying to reach Google Tag Manager? Naughty OS. That request is now black-holed to a non-existent IP at .</p><p>Let’s turn our attention to the TV and see how it fares under DNS interception.</p><h2>How to Restrict Apple TV and iPhone YouTube Ads?</h2><div><div> Regarding YouTube, YouTube has been showing 7-second and 15-second ads, twice, back-to-back, nearly every few minutes. Why are the ads incessant and so long? I do not mind the occasional ad, similar to live TV, but these frequent ads would warrant FTC complaints it they were on live TV.</div></div><p>YouTube is tricky because ads are also videos that come from the same domain, so domain-name blockers like pfBlockerNG cannot act on them. The best pfBlockerNG and Pi-hole can do is block  only after you watch an ad video and click on the ad.</p><p>Many people opt to use a web browser like Firefox or Chrome with <a href=\"https://chrome.google.com/webstore/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm?hl=en\" target=\"_blank\" rel=\"nofollow\">uBlock Origin</a> that acts on JavaScript as a workaround. It might be enough to watch YouTube on a web browser and stream that to a smart TV. However, we cannot restrict ads on the iPhone (without jailbreaking and compromising it).</p><p>What are our options? How can we safely restrict YouTube ads on all network devices?</p><h2>Trick the YouTube Ad Algorithm Instead</h2><div><div> Among friends, let’s say that English-speaking countries get ads for the most ridiculous things because their residents are assumed to have disposable income. Can we instead make YouTube think we are an undesirable advertising target?</div></div><p>What do ads in other parts of the world look like? Are those living in Antarctica or <a href=\"https://xkcd.com/713/\" target=\"_blank\" rel=\"nofollow\">Low Earth Orbit</a> getting a lot of ads too?</p><p>What would happen if we leverage the capabilities of this pfSense router to route YouTube Location Tracking information through a VPN that terminates in some remote part of the world with fewer YouTube viewers per capita? In other words, let’s make ourselves undesirable to advertisers and see if we get fewer ads.</p><h2>Research into YouTube Advertizing Spend</h2><p>Let’s also check some <a href=\"https://medium.com/@ChannelMeter/youtubes-top-countries-47b0d26dded\" target=\"_blank\" rel=\"nofollow\">YouTube statistics</a> about viewers by country for insights. Thinking about following some Reddit advice and VPN’ing into India? Think again.</p><p>That was 2019. This is <a href=\"https://backlinko.com/youtube-users\" target=\"_blank\" rel=\"nofollow\">2020</a>:</p><p>I’m not a digital advertiser, but I can see that people in the UK and Canada watch a large number of videos per sitting. If I  an advertiser though, I’d pump those two countries with video ad after video ad because, statistically, those residents will take the eyeball kicking. All things being equal, I definitely need a VPN to terminate outside of Canada, the UK, and the United States (English-speaking countries) to enjoy YouTube more.</p><p>Does age play a factor? Who  advertisers want? I want to be that guy on paper.</p><div><div> Let’s trick YouTube into believing I am a 70-year-old male living in Italy. Yes, that should definitely cut down on the Nespresso and Starbucks ads, at least.</div></div><p>How then to convince YouTube that I am a retired Sicilian living on a small chain island? I embellished that last part. Seventy and in Italy is sufficient.</p><p>Let’s do this. In the YouTube account…</p><p>It is doubtful that this is all it takes for our goal. Let’s find a VPN exit point in Italy.</p><p>Nice. <a href=\"https://ericdraken.com/out/nordvpn\" target=\"_blank\">NordVPN</a> has about 60 servers in Italy (that’s an affiliate link by the way).</p><h2>Selectively Route Apple TV Over the VPN</h2><p>Let’s go through some tutorials to set up  in pfSense. Just kidding! We’re going to use WireGuard – we have the Intel AES-NI crypto instruction set because we didn’t go cheap and get a yesteryear J1900 mini PC that sellers are trying to offload.</p><p>I’ll now install the FreeBSD WireGuard package.</p><p>Next, add a tunnel and enable it. According to this <a href=\"https://www.reddit.com/r/PFSENSE/comments/m0989o/nordvpn_wireguard_setup_works/\" target=\"_blank\" rel=\"nofollow\">thread</a> and this <a href=\"https://www.reddit.com/r/PFSENSE/comments/m0989o/nordvpn_wireguard_setup_works/\" target=\"_blank\" rel=\"nofollow\">thread</a> on Reddit, we need to get some information for WireGuard and NordLynx from a sacrificial Linux VM to transpose the settings (i.e. private key) to the pfSense router. No problem.</p><p>Run <code>sudo wg showconf nordlynx</code> to see your private key needed by the pfSense tunnel config.</p><p>Here are various screenshots that show the steps in more detail.</p><div><div> Enter  and then  as the subnet mask. Do not go for  as there is a glitch or bug in the UI or whathaveyou. The result will still be .</div></div><p>That should be enough to allow Diagnostics to  Italy.</p><p>Now that the easy part is out of the way, let’s set some Policy rules to send the Apple TV traffic over the VPN to Italy as a baseline test.</p><blockquote><p>Traffic from LAN to WAN is processed as described in the following more detailed example.</p><ul><li>Port forwards or 1:1 NAT on the LAN interface (e.g. proxy or DNS redirects)</li><li>Firewall rules for the LAN interface:<ul><li>Floating rules inbound on LAN</li><li>Rules for interface groups including the LAN interface</li></ul></li><li>1:1 NAT or Outbound NAT rules on WAN</li><li>Floating rules that match outbound on WAN</li></ul></blockquote><p>I’ll make an alias, for now, to hold some clients that have static DHCP entries and hostnames I gave them in pfSense.</p><p>Floating rules  have high precedence, so I’ll add some rules below the automatic pfBlockerNG rules that were created, and I’ll add a nice little blue separator while I’m here.</p><p>And here is that rule as a very long screenshot:</p><p>Apply. Wait. Let’s try it out using one of my notebooks connected to the Untrusted network.</p><p>Google is in Italian. Very cool. Now for the Apple TV.</p><p>Winner winner, chicken diner. All my YouTube is in Italian. I get some ads, not as many, but because Italians speak slowly and with a kind of sexy accent I do not mind the ads for Nutella at all.</p><p>With this technique, I no longer feel manipulated by non-English ads. I have personalized ads , but given my new status as a retired gentleman I should turn that back on to scare away advertising dollars, er, euros. I wonder if Netflix and Amazon Prime behave any differently…</p><p>Dang. Netflix is having problems. Amazon Prime is even worse. It looks like some CSS or font files are blocked as well, and the thumbnails aren’t loading. It’s time to move to Phase Two: Tunnel only YouTube traffic over the VPN.</p><div><div> Do not try to send all the Apple TV traffic over a VPN because Netflix, Prime, and others are wise to VPN providers and have gotten great at geofencing.</div></div><h2>Selectively Route Apple TV YouTube Traffic Over the VPN</h2><p>Let’s start by adding Firewall Policy rules to send the most common YouTube domains over the VPN.</p><p>As I’m about to add the rules, my hands hover over the keyboard not knowing what domains to tunnel. They need to be FQDN (fully-qualified domain names, no wildcards). Let’s open up a Chromium-based browser and see what traffic it generates in DevTools.</p><p>Here are some candidate FQDNs to add:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>But wait, I hear you ask, why  and ? This is a preventative measure just in case one of those domains is geo-jacked (Geo-IP LowJacking). I wouldn’t put it past Google engineers to geo-jack the fonts domains like , but I’ll take a chance they don’t in the interest of scaling to billions of page views efficiently.</p><p>Here are my new rules where I chain two of them using a tag so I can limit YouTube tunnelling to only the same untrusted machines (including Apple TV).</p><p>And with that, YouTube thinks I’m in Milan, Netflix and Prime Video think I am still in Canada, and the ads… oh the ads… they are few and far between, and when they do come on, they are just a treat to listen to in that slow, lack-of-harsh-aspirants-or-yelling of a beautiful language Italian is.</p><h2>Gotcha: DNS Race Condition</h2><p>A day has gone by and I’ve noticed that I only get Nutella and Ferrero Roche ads in the middle of videos, not at the start. Odd. I did some research and this is what I found:</p><p>This means that the hostnames are resolved to IP addresses  and those IPs are used in my VPN tunnelling policy rules.</p><blockquote><p>A hostname entry in a host or network type alias is <strong>periodically resolved and updated by the firewall every few minutes</strong>. The default interval is 300 seconds (5 minutes), and can be changed by adjusting the value of Aliases Hostnames Resolve Interval on System &gt; Advanced, Firewall &amp; NAT tab. – pfSense</p></blockquote><p>Ah-ha, so I suspect there is a DNS race condition. Let me explain:</p><p>This happens if, say, the Alias Daemon updates the IPs of the FQDNs. Then, I turn on the Apple TV for the first time all day. Since the usual TTL (time-to-live) of DNS queries is 1440 seconds (30 minutes), all the YouTube DNS entries will be cache misses and will need to be updated. At this point, the IPs from the second DNS queries may be from a pool and are not guaranteed to be the same that the Alias Daemon has. When the Alias Daemon checks again in five minutes, it may resolve the FQDNs to yet different IPs!</p><p>Let’s solve this by overwriting whatever TTL (time-to-live) YouTube has in its DNS entries:</p><p>And with that, no more DNS lookup race condition.</p><h2>Gotcha: Authentication Trouble, Forbidden 403 Error</h2><p>Sometimes videos will not play. For security, YouTube embeds your IP in the  request. I’ve known about this since my post about <a href=\"https://ericdraken.com/download-youtube-videos-with-php/\" target=\"_blank\">Download YouTube 4K Videos with PHP</a> back in 2016. The new problem is that various JavaScript and “are you human?” assets are tunnelled over VPN, but those darn domains like <code>r5---sn-hpa7kn76.googlevideo.com</code> are not tunnelled and thus come from the wrong IP. Queue the  error.</p><p>Let’s fail fast with a quick experiment: I’ve gotten the IP of the above second-level domain name (SLD), added it manually to the list of domains/IPs to VPN tunnel, applied the change, and refreshed YouTube:</p><p>Excellent. Now, we just need a way to tunnel that wildcard  domain. Unfortunately, the NAT and Firewall rules work with IPs, not <a href=\"https://www.reddit.com/r/PFSENSE/comments/7wwnun/wildcard_domains_in_aliases/\" target=\"_blank\" rel=\"nofollow\">wildcard domain names</a>. Can we predict or enumerate these domains?</p><p>Here is a Wireshark capture of DNS requests to  to show that the SLDs (second-level domains) are not eyeballably predictable:</p><p>Let’s drop into a web browser with adblocking disabled and walk the HAR waterfall of my interaction with YouTube that led to ads showing up.</p><p>What are GET requests like</p><p><code>GET https://r7---sn-uxa0n-t8ge.googlevideo.com/generate_204</code></p><p>doing, exactly? I’ll give this problem some thought offline.</p><h2>Gotcha: YouTube is Now Showing UK Ads, Not Italian Ads</h2><p>Before I could even solve the previous gotcha, British ads started showing up with the same frequency as if we did nothing. Ads from the UK are even more incessant than those from Canada, trailing behind the USA and India according to my earlier stats. It would be a complete failure if we get UK ads. Why does this happen suddenly? I’ve opened a fresh browser in a VM and tunnelled all traffic through Italy. The only leak I can find is when I query  on my Italian tunnel and see a UK address in the ASN. Could this small leak be our undoing?</p><p>Even with my browser’s language set to  and location data off, this is the only leak I can spot. Then, in addition to a VPN exiting in Italy, it has to be one that doesn’t leak  (Autonomous System Numbers – used for automated routing) that gives up a different country. Dang, Google, you’re good. I’m going to have to bring my A+ game to this one.</p><h2>Find a VPN Exit Node with no ASN Leak</h2><p>By visiting <code>https://nordvpn.com/servers/tools/</code>, I can see the VPN endpoint nodes in Italy. There are many Wireguard endpoints with <a href=\"https://ericdraken.com/out/nordvpn\" target=\"_blank\">NordVPN</a>. Just to move things forward for this exercise, I’ll add an OpenVPN tunnel in pfSense and connect to several VPN nodes and examine the ASNs. It’s better than nothing, and more importantly, I’d like to eliminate the  as the leak of GeoIP information. Here is the <a href=\"https://support.nordvpn.com/Connectivity/Router/1626958942/pfSense-2-5-Setup-with-NordVPN.htm\" target=\"_blank\" rel=\"nofollow\">guide</a> I used.</p><p>Through trial and error, I found a VPN node that is registered to an ISP in Italy as found in the  and  info.</p><h2>Hijack Google Video DNS Queries</h2><p>To make any of this work, I need a technique to route the wildcard  domain through the VPN.</p><div><div> Suppose I write a plugin for pfSense that periodically s the DNS query log, keeps track of the  queries, and adds them to a unique list of aliases for Google Video domains; if backed by an LRU eviction policy, this could keep working indefinitely. However, if each video uses a unique, mangled domain, then this does not work unless I hit refresh on every single video.</div></div><p>On the other hand, if I “hold up” the DNS query for the  domains, add the IPs to some alias list,  allow the DNS response to finish the round trip, we may be in business!</p><p>Where to even start? Here are some <a href=\"https://github.com/NLnetLabs/unbound/tree/master/pythonmod/examples\" target=\"_blank\" rel=\"nofollow\">Python example scripts</a> just to get some inspiration. A quick, mental reverse-engineering of a handful of scripts reveals that there are some event hooks available. Nice.</p><p>Among friends, let’s say that I can build up the pool of Google video IPs in real-time. How then to add these IPs programmatically to the firewall alias list for YouTube  restarting the firewall? One person actually hacked the PHP scripts in pfSense. Tempting, but I’ll do more research. Another person created a <a href=\"https://github.com/jaredhendrickson13/pfsense-api\" target=\"_blank\" rel=\"nofollow\">REST API for pfSense</a>. Jackpot!</p><div><div> We need to add IPs to the firewall policy rule to route YouTube videos over a VPN to avoid incessant and obnoxious North-American ads, but the IPs keep changing due to changing, mangled second-level domain names (SLDs). Using Python 3 and a REST API, we will monitor the appropriate DNS queries, note the IP(s) of the response, hold the response, add the IP(s) to the VPN tunnelling policy rule, then release the DNS query response.</div></div><h2>Research Python Methods to Hijack DNS Requests</h2><p>Why this approach? It’s future-proof, modular, elegant, maintainable, automated, and it lends itself to a future decision tree that could truly restrict YouTube ads outright.</p><p>First, I will enable SSHd in pfSense and take a peek around.</p><p>Let’s take this opportunity to make a disk backup.  or “duh” shows that only 800 MiB is in use on the SSD. Let’s  the whole box from our local machine in about four minutes.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><div><div> To verify the owners and permissions are set in the extended attributes locally, run<code>getfattr -d -m ^ -R -- ~/.pfsense-backup</code></div></div><p>Now that we have a pfSense backup (I’m told just backing up  works too), let’s install the <a href=\"https://github.com/jaredhendrickson13/pfsense-api\" target=\"_blank\" rel=\"nofollow\">REST API</a>.</p><p>This part had me confused. You see, I was looking at the bottom of the screen wondering how the heck I could copy a truncated hash as a token. After a few tries, I noticed the green message at the top that I had been trained to ignore. It has the token.</p><p>Next, with the API credentials set up, let’s try out the API:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><h3>Explore the Unbound Python Module</h3><p>Running  shows that the current version of Python is 3.8.</p><p>As for the Unbound DNS Resolver, I had some luck tinkering in  and writing simple Python 3.8 code to log DNS query messages. We now have both parts needed to dynamically update the firewall aliases and tunnel all YouTube traffic once and for all.</p><p>If you are looking for Python module docs for Unbound, here they are:</p><p>Run these commands to quickly get the documentation.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><div><div> The example code is from Python 2.4, so be prepared to run Black and PyCharm code formatting, or run . Also, the most important part of this whole exercise (getting the IPs from the DNS reply) is missing, so here is the hint: . Don’t forget to manually hack the byte strings to pull out the proper IP addresses in binary form, first.</div></div><p>Now we have Python docs and access to all the capabilities. Excellent.</p><p>Next, take a backup of your OS or VM and install  and  wherever, <code>./configure --with-pythonmodule</code>, , fix some errors in the Unbound code,  again, then you’ll have the generated python module () in order to remove all the missing-method red error lines in PyCharm.</p><h2>Smoke Test: A Python DNS-Hijacking Script</h2><p>Here is a smoke test of the ability to hijack  DNS requests with reply IPs that the script has caught in just a few minutes (the timestamps are just to maintain a crude LRU cache):</p><p>Duplicate IP addresses are possible, and that is fine. I let the smoke test run overnight. Here is the PoC (proof of concept) script I ran as the Unbound Python module script.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>When I woke up, the Unbound DNS resolver service . Here are the logs:</p><div><div> Capturing all the IPs from the DNS queries to  and  puts pfSense into a crawl as all the rules need to be reloaded on each addition.</div></div><div><div> Research and install a Squid-like proxy, create a fake-but-trusted CA certificate, host it, install it in a browser as a PoC, decode TLS traffic, and victory dance.</div></div><p>Actually, it is  illegal to <a href=\"https://github.com/NSSpiral/Blackb0x\" target=\"_blank\" rel=\"nofollow\">jailbreak most Apple TV boxes</a>, so we could break in, add a root certificate valid for the pfSense box, MITM traffic from the Apple TV, and then Microsoft Bob is your uncle. That works because the pfSense box as the gateway can decrypt Apple TV traffic, inspect the request headers for the offending ad , block the request, and re-encrypt other valid requests to Mountainview, California.</p><p>But, then my iPhone would still show ads because it is harder to jailbreak, plus banking apps may detect this and not work anymore. Jailbreaking is too extreme, anyway.</p><div><div> I used a jailbroken iPhone all the time in Japan because of a quirky cellphone law. You see, because of icky perverts who like to take photos inappropriately on elevators and escalators, Japan passed a law that made the camera shutter sound mandatory on all photos.Super unfortunate was that taking a  of a web page also made the same loud, unmuteable shutter sound. Imagine you are on a train and you screenshot a Google map, it makes that loud shutter noise, and then you get dirty looks from the train riders. Yeah, I had to jailbreak and zero out the camera sound file.</div></div><p>Let’s see what it takes to spy on the HTTPS traffic from the Apple TV and iPhone to see if we can block ad URLs that way.</p><h2>Install a Fake-but-Trusted CA Cert on Apple TV and iPhone?</h2><p>Not wanting to jailbreak and add self-signed certs to Apple TV and iPhone, how hard would it be instead to add fake-but-trusted Certificate Authority (CA) certificates to each device?</p><p>The ‘A’ in CA means there is no one higher to vet such a certificate. The ‘A’ is so powerful, that back in 2001 only a Windows patch was able to revoke some <a href=\"https://en.wikipedia.org/wiki/Verisign#2001:_Code_signing_certificate_mistake\" target=\"_blank\" rel=\"nofollow\">dangerous Verisign certificates</a>. As a thought experiment, new CAs must come into existence from time to time. Let’s Encrypt is relatively new, for example. There should then be an in-warranty way to get a fake, trusted CA cert into an Apple TV and iPhone. If that is possible, then an entire world of MITM spycraft is available to decrypt TLS packets in the clear and use good ‘ol URL blocking on requests like</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Let’s see how easy this would be.</p><p>In fact, there are many, many CAs. Here is a quick  in pfSense:</p><h2>Experiment with Squid and SquidGuard</h2><p>I’m aware of <a href=\"https://mitmproxy.org/\" target=\"_blank\" rel=\"nofollow\">mitmproxy</a>, but it needs to be side-channel installed onto the pfSense router. Let’s see if the  proxy that is available as a pfSense package can do what we need. First, I will take a bare-metal backup again so I can roll back in case  is better.</p><p>I’ve installed those packages, and naturally, there are more buttons and options than in a space shuttle. I’ll find a <a href=\"https://turbofuture.com/internet/Intercepting-HTTPS-Traffic-Using-the-Squid-Proxy-in-pfSense\" target=\"_blank\" rel=\"nofollow\">guide</a>.</p><p>I’ve followed the steps in the guide, however, since I have a large SSD and generous RAM, I’ve made a dedicated folder  (and ) with 8 GiB of cache and a juicy allowance on the per-item cache size which should also help with Docker and NPM speed-up. Two birds, one stone. With Transparent HTTPS support, this should be pretty rad.</p><div><div> If web traffic slows down while using Squid, here are some System Tunables that can make Squid faster (<a href=\"https://forum.netgate.com/topic/85937/pfsense-2-2-3-internet-is-very-slow-via-squid3/12\" target=\"_blank\" rel=\"nofollow\">ref</a>):<p><code>kern.ipc.nmbclusters 32768</code></p><p>Also, for local disk cache,  is asynchronous  (great for Docker too) and uses POSIX-threads to avoid blocking the main Squid process on disk-I/O.</p></div></div><p>We can actually generate a CA cert in pfSense itself.</p><p>Now, how to get it into the Apple TV and iPhone? It should be hosted somewhere, right? How about on the router?</p><h2>Self-Host the MITM CA Certificate</h2><p>Self-hosting with a single command is ridiculously easy. From the SSH shell into pfSense, I can create a web folder and server like so:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>When I visit  I should get a blank page with “Hello”. From here, clients behind the pfSense router can temporarily access static documents.</p><p>To make like easier, here is a PHP script to cause the MITM cert to download.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>As another smoke test, I’ll add the MITM CA to Chrome (manually) and enable the SSL Filtering. The defaults are fine in Squid. Here is the log file when I visit :</p><p>However, on every other browser and machine there are HTTPS errors like so:</p><div><div> If you get locked out of pfSense with a TLS error, you may have to disable Remote Cert Checks as the pfSense web configurator uses a self-signed certificate. Or else, you can bypass the proxy for the pfSense UI under <em>Bypass Proxy for These Destination IPs</em> with <code>pfsense; pfsense.localdomain</code>.</div></div><h2>Abandoning Squid: Too Slow, Too Heavy</h2><p>After a day of painfully setting up Squid and SquidGuard and adding blacklists and even manual regex for things like , I’m having nothing but issues with Squid. Here are the top pain points:</p><ul><li>It’s slow. It’s really slow.</li><li>The ACL (Access Control List) settings are cumbersome.</li><li>There is an issue with  (<a href=\"https://forum.netgate.com/topic/141472/https-filter-with-https-http\" target=\"_blank\" rel=\"nofollow\">ref</a>).</li><li>The SquidGuard URL filter takes eons to update a list.</li><li>The Squid UI is unbelievably lacking.</li></ul><p>Squid makes me sad. I don’t get sad, but Squid makes me sad with its promise and ultimate letdown. I’ve now obliterated Squid and restored the router from the  backup I made earlier. Here is a handy little script to show a diff of what has been added by Squid and related packages.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>The output is something like this under the  option:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><h2>Install MITMProxy in a FreeBSD Jail</h2><p>Even though written in Python, I’ll give <a href=\"https://mitmproxy.org/\" target=\"_blank\" rel=\"nofollow\">mitmproxy</a> a try next; at the very least it can be purpose-built to block YouTube ads with its rich API and Python-hook extensibility. It was a coin toss between  and  – a Metasploit hack tool – to achieve on-the-fly TLS interception, but the former can be scripted with Python and has a satisfying UI. Let’s go.</p><div><div> Please read the whole section before trying any commands because I backtracked a bit but want to explain why.</div></div><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>You’ll notice that there are only three binaries about 24 MiB each. As I understand it, they have a self-contained Python 3 environment and frozen dependencies. I’d like to jail these binaries because, well, because. First, let’s see if there is a vulnerability report for  at <a href=\"https://vuxml.freebsd.org/freebsd/index.html\" target=\"_blank\" rel=\"nofollow\">vuxml.freebsd.org</a>. Nothing. How about at <a href=\"https://www.exploit-db.com/\" target=\"_blank\" rel=\"nofollow\">Exploit-DB</a>? Nothing again. Good.</p><p>First, what version of FreeBSD is this pfSense install?</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Now, according to this <a href=\"https://blog.viktorpetersson.com/2018/01/27/jails-on-pfsense.html\" target=\"_blank\" rel=\"nofollow\">guide</a>, I’ll need to set up jails myself as they are disabled in a default pfSense installation. Not knowing FreeBSD at all before today, I had to hack around to find a URL to download the  package manually. After another bare-metal backup, here are the steps I took:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>We need to do some hacking to get  working on pfSense’s take on FreeBSD because  is missing completely. What I’ve done is copy the  binaries  a jail (via ) back to the root system.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Let’s set up a jail for .</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p> We must enable raw sockets in this jail to allow transparent proxy mode to work. If not, MITMProxy will report errors like “Transparent mode failure: FileNotFoundError(2, ‘No such file or directory’)” or “Cannot open connection, no hostname given.” This is because raw sockets are inaccessible and server information is unavailable. We can easily edit the  config file per jail like so:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p><strong></strong> MITMProxy calls <code>sudo -n /sbin/pfctl -s state</code> but there is no  in . Run  inside the jail.</p><div><div> If you are unsuccessful when you run  inside the jail, you may get an error like this: “ssend socket: Operation not permitted”. If you are successful, then  works as it needs access to raw sockets.</div></div><p>Now we can copy over the  binaries and take them for a spin.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Things are getting tricky with this next part. Running any of the binaries above results in:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>So, there is no  folder nor any similar dynamic linker that I could find. I tried this, however:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Apparently, there is a  that can solve this for us (unavailable on pfSense), however, this is getting ridiculous! Let’s try a new tactic. Since we are in a jail, we are not bound to the crippled (read: secured) pfSense environment. Maybe we can install the  package normally in a jail?</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>And, Bingo was his name-o. After this, simply running  in the jailed console opens the MITMProxy UI. Nice. Note, this version  be one or two minor versions behind the master branch. Let’s clean up with  and do another bare-metal backup.</p><p>This is getting exciting. First, in pfSense, add a virtual IP for  attached to . Then, add a NAT rule to temporarily forward port  to  to access the proxy from the LANs.</p><p>If not in the jail console, I’ll run</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>and add the proxy setting  to my sacrificial notebook (that is auto-wiped daily). When the browser opens, we can already see colourful log entries in the MITMProxy UI.</p><p>The next step is to get the auto-generated CA PEM file used by MITMProxy (<code>~/.mitmproxy/mitmproxy-ca-cert.pem</code>). Since any CA cert here is snake oil, I’ll use the provided one. TLS traffic from my devices is safe as long as I use my own proxies.</p><p>Let’s put our experience from our previous attempt at self-hosting a CA into action. However, there is no PHP in the jail, so we can use a Python 3 web server instead.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><div><div> MITMProxy conveniently has onboarding settings to serve the same CA cert, as we did manually, just by visiting .</div></div><p>After installing the CA in the Trusted Root Store on my clean notebook (and rebooting), I am treated to this display:</p><p>Let’s see if we can get this cert on my iPhone.</p><p>This is incredibly exciting. Can we LoJack the Apple TV box next?</p><p>But wait, the router is slowing down.  is burning up the CPU… on idle.</p><p>Of course: Python is a single-threaded paradigm with the GIL (Global Interpreter Lock) ensuring threads do not actually run concurrently – unless they are blocking on I/O, which is the case here(?). Except, most of the CPU work is to generate TLS certs on the fly for each request. Yikes. Running  forgoes the UI and extreme logging. The extreme logging of all the headers and full responses heavily slows down , but  by default only logs entries like classic Apache logs – much kinder on the CPU.</p><div><div> Some advanced, high-security web servers have trouble with the MITMProxy certificates due to <a href=\"https://security.stackexchange.com/a/29990/114882\" target=\"_blank\" rel=\"nofollow\">Certificate Pinning</a> – this is a technique where the server or the client know the fingerprint of the expected certificate in advance so it cannot be forged. A workaround is to use the  option to let them bypass the proxy.</div></div><p>For my fun, I’ll go with this CLI command:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>While on YouTube, we can see the page ads clear as day with their unencrypted headers; can a simple regex now block them? They are exposed, and afraid, and their days have run out.</p><p>We can even see details about each request. For example, all the SAN info is laid out for this wide-reaching certificate. There are curiously a lot of  domains covered by this cert.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Shortly, I’ll write a Python script to block YouTube  URLs.</p><h2>Patch MITMProxy Source Code for Server SNI Interrogation</h2><p>This step may be optional for most, but as a reminder to myself, to make  work better in Transparent Proxy Mode, the SNI of the server request needs to be checked against the list of regular expressions or else only the server’s IP is used for matching in many cases. Here is a quick patch I made that can be applied directly in the jail shell (or just type a few lines manually) for  version 7.0.4:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td><div><div>Index:venv/lib/python3.8/site-packages/mitmproxy/addons/next_layer.py</div><div>Subsystem:com.intellij.openapi.diff.impl.patch.CharsetEP</div><div>===================================================================</div><div>diff--gita/usr/local/lib/python3.8/site-packages/mitmproxy/addons/next_layer.pyb/usr/local/lib/python3.8/site-packages/mitmproxy/addons/next_layer.py</div></div></td></tr></tbody></table></div></div><p>With the above patch, I can now reliably intercept a few hosts and let all others pass through.</p><h2>Smoke Test: Intercept YouTube Ads with MITMProxy</h2><p>After reading the docs and navigating the  source code in the PyCharm IDE, I’ve written a little script to block ads and tracking URLs coming from YouTube from my clean notebook. I won’t reproduce the code just yet because it didn’t succeed in blocking ads as hoped, so instead, I’ll spend the time investigating why.</p><p>Here are the smoke test filters I used where for a given top-level domain, URLs with the following partial strings are blocked:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>My initial results on blocking are positive. Everything I wanted to be blocked is faithfully blocked. Note, the  entries are due to my script, and the  failures are due to pfBlockerNG black-holing the request.</p><p>Even in the DevTools network panel, the requests are truly blocked.</p><p>Then how come I am still seeing ads? I’ve disabled HTTP/2 so that subsequent requests on the same channel don’t slide by. Mind you, sometimes the ads skip on their own, or fail to play, but they still show up. Interesting. Could YouTube be using WebSockets? I need some inspiration, so I’ll look at uBlock Origin’s regex filters for some ideas.</p><div><div> If you see the error , then the DNS blocker (i.e. pfBlockerNG) is breaking the upstream TLS handshake for a given domain. Either whitelist it in pfBlockerNG (so the request goes through), or intercept it and block the connection in . This error happens to black-holed domains when the upstream TLS cert cannot be sniffed. The cleanest strategy is to use .</div></div><h2>Examine uBlock Origin Regex Patterns for Inspiration</h2><p>Here are some of the regex/filters that uBlock Origin uses on YouTube.</p><p>At first blush, it seems that a community of like-minded individuals is playing whack-a-mole with YouTube’s HTML and JavaScript. This has got me thinking: How does a video know to play an ad with JavaScript?</p><p>How does YouTube know if the ad converts? They must target ads for individuals, so a given video must receive some unique information about an ad, such as the click link and alt text. WebSockets would be a pain to maintain, especially with all the mobile clients. They must be using stateless JSON to relay that pertinent information in an innocuous URL request that has no telltale signs of ad-ness. Let’s hunt for this info in the JSON replies captured by .</p><p>Snap, Crackle, and Pop. We have a new plan: surgically alter the JSON response body to eliminate or Byzantine-up the ad information.</p><h2>Surgically Alter the JSON Response to Remove Ads</h2><p>After a bit more playful exploration, a trove of blocklorne URLs is right there in the JSON payload. In fact, most of what I am trying to block shows up right here:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>However, YouTube has bobby-trapped their UI and there is more than one way their obfuscated JavaScript code can pull down the ad details.</p><p><strong>Let’s blow it all away right now.</strong></p><p>After a lot of fun taking apart the YouTube UI and HTTP workflow, taking into account cookies and naughty service workers, I am successfully able to strip away all the pre-roll, post-roll, mid-video, and, well, all the video ads. Here is a screenshot from  showing how select REST queries are intercepted, decrypted, modified, put back into the response, and the headers updated (content length, etc.).</p><p>With this new ability, we could even inject JavaScript into the main YouTube web page and subvert their JavaScript in a sort of ECMAScript arms race, possibly even leveraging some of the filters from uBlock Origin. However, we can hang our hats on this accomplishment for today.</p><div><div> We can strip out ads from the JSON payload for YouTube web ads using a router.</div></div><h2>The iOS YouTube App Uses Protobuf, not JSON</h2><p>I can see very similar data in the Protocol Buffer (Protobuf) version of the same API calls as the web version to that of the YouTube iOS app. That complicates things, somewhat: We cannot lean on JSONPath to hunt down advertisement sections of JSON because with Protobuf the keys are just numbers that can even change.</p><div><div> YouTube compiles a large list of all the ads you are going to see and sends that to you in a sneaky payload. In fact, it is easier to visualize this when reading Protobuf. If you manage to exhaust that list, then another large list will be coming your way.</div></div><p>I can see strings like “Telus” and “Samsung TV” and “Boxing Week” and “Buy now”. Remember when YouTube was a fun place? A fable about a Golden Goose comes to mind, Alphabet.</p><p>As a consequence of being able to see unencrypted traffic from my iPhone, I’m taken aback by the sheer amount of tracking information laid bare; It’s like I have electrodes on my head and chest while I’m running on a treadmill and a bunch of scientists in white lab coats with clipboards are standing shoulder-to-shoulder recording everything about my internals.</p><div><div> Your apps are tracking you like crazy: what you do, how long you dwell, when you leave a given app, and so much more. The URL <code>https://play.googleapis.com/log/batch</code> shows up a lot in my logs.</div></div><p>The next question is: Does the iOS app protocol behave like the web app?</p><h2>Timing Analysis to Detect Ad Videos?</h2><p>The iOS network traffic is not like the web traffic; Google has teams and teams of engineers dedicated to making sure blocking their ads isn’t computationally feasible. Daunted but undeterred, I was staring at network requests to let my mind zone out and wander when I noticed a pattern I had not noticed before.</p><p>For the  version of YouTube, I can eyeball which URLs are ads and which are the videos I want to watch. Take a look:</p><p>How am I able to eyeball which video URLs are ads in this chaos?</p><p>Take a look at the query parameter . For the web version, a chunk of the video I want is fetched from the 0th byte, then immediately another video is fetched with a  starting again at the 0th byte. Both happen near-simultaneously – faster than a human can click on a new video. It turns out this, as well as examining the  parameter for the length of the full video (short videos are likely ads), can reasonably allow us to detect and doctor ad videos.</p><p>However, the iOS YouTube protocol does  use the  query parameter or even the  header; video chunks use a counter like  and  etc. We must reverse engineer the Protobuf responses.</p><h2>Decode the YouTube Protobuf Responses</h2><p>Here are some decoded Protobuf log files I created then opened in the PyCharm IDE.</p><p>After logging decoded Protobuf messages to disk for offline analysis, I did notice something that piqued my interest.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td><div><div>entitlement</div><div>entitlement</div></div></td></tr></tbody></table></div></div><p>I wonder what would happen if I were to, say, toggle those? This is tantalizing, but it is cheating, and hence no fun. Back to heuristics.</p><div><div> As with JSON, can I blow away the Protobuf sections that serve up ads? Could I instead detect the ad videos in the payload, then dynamically modify their responses to be, say, a cached 0.01s video file? The 30s ~ 300s of unskippable ads could be over in the blink of an eye without blocking all those URLs.</div></div><p>Let’s start by blocking the ads as intended.</p><p>The Protobuf responses are a hot mess of bytes, but there are human-readable URLs that can be grepped.</p><p>You’d think a simple LRU cache that blocks soon-encountered ad URLs could be the way to go, but, alas, the ad URLs do not quite match the URLs sent over the wire. Also, who is to say that YouTube won’t randomize the position of query-string parameters one day? We need an  lookup of flagged ad URLs that are polymorphic (and group homomorphic) to live ad URLs.</p><p>It might be tempting to split a query string into a sorted dictionary and reassemble it, but we have no way of knowing what the query string boundary is. Plus, a live ad URL could add a key and disrupt the sorting.</p><p>Addionally, I’ve encountered URLs like this that purposely try to obfuscate the query params:</p><blockquote><p>https://r4—sn-vgqsrns6.googlevideo.com/videoplayback/expire/1640607416<p>/ei/WFrJYdWnFfyTsfIP4s2BsAk</p>/id/o-AE7swWOPOwXu3GyRght/requiressl/yesmm/31,26/…</p></blockquote><p>Notice how  is just ?</p><p>I propose heuristically scanning for query and path parameters of ad URLs with high entropy and using those as keys (fingerprints). For example, in</p><blockquote><p>https://.googlevideo.com/initplayback?source=youtube&amp;orc=1&amp;oeis=1&amp;c=IOS&amp;oss=1&amp;oda=1&amp;oad=5500&amp;ovd=5500&amp;oaad=11000&amp;oavd=11000<p>&amp;ocs=700&amp;oputc=1&amp;oses=1&amp;ofpcc=1&amp;osbr=1&amp;osnz=1&amp;msp=1&amp;odeak=1&amp;odepv=1</p>&amp;osfc=1&amp;id=&amp;ip=&amp;initcwndbps=&amp;mt=</p></blockquote><p>One could note the following candidates in descending order of length:</p><ul></ul><p>Any or all of them could be lookup keys each pointing to the same dictionary of deconstructed query parameters. A lookup of a live URL would involve the same process of finding the highest entropy parameters and checking the URL dictionary for a match. The cache data structure can even be multi-level with the root keys being just the length of the high-entropy strings.</p><div><div> Even with the ability to block polymorphic URLs, the video ads are still indistinguishable from content video without context from the Protobuf structure.</div></div><h2>Smoke Test: Intercept and Decode Protobuf in Python</h2><div><div> Decoding ~500 kiB of raw Protobuf in pure Python is painfully slow.</div></div><p>Decoding ~500 kiB of Protobuf in pure Python, especially the decoding step of converting it to over 1 MiB of human-readable text to parse the ad URLs, takes more time than the connection timeout most of the time. I’ll run some benchmarks using pure Python vs. the native C++ library.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>If you caught that, it takes about  in Python, and  in C++! In this Never Ending Story, we have to find a way to parse the raw Protobuf payloads in Python using the C++ library . In the interest of time, I’ll use  and communicate with the C++  binary directly (since raw decoding is not supported in Python anyway).</p><h2>Fuzzing the YouTube Video Ad Responses</h2><p>How about fuzzing the ad video responses? Now being able to isolate ad videos, as a smoke test, I sent back  responses with empty bodies and the iOS app went bananas; it was as if there is an infinite loop with no delay just hammering YouTube’s own servers trying to get the next part of the video in panic mode. I felt bad for their servers, so I stopped. Then, what would a happy-path response payload look like?</p><p>Try as I might, when I send back empty s, s, s, truncate response bodies, or just null-out part of the ad video, the iOS app crawls then crashes spectacularly with a dying breath of a messed up iOS UI. I now block some error reporting endpoint at  that indicates a “dev assertion failed” so I don’t make some overworked QA pull out their hair.</p><div><div> We’ve learned that blocking ad URLs causes the app to deploy countermeasures and even when defeated, the app hangs forever on the ad screen. We’ve also learned that fuzzing ad videos often causes the app to crash – there is even session meta data in the video response chunks.</div></div><p>Let’s go back to what worked with JSON and obliterate the section of the Protobuf responses that contain the array of ad details.</p><h2>Enter Burp Suite Tools for Penetration Testing</h2><p>There is a library for <a href=\"https://portswigger.net/burp\" target=\"_blank\" rel=\"nofollow\">Burp Suite</a> called  (get the <a href=\"https://github.com/nccgroup/blackboxprotobuf/tree/master/lib\" target=\"_blank\" rel=\"nofollow\">original Burp Suite version</a>,  the PyPi fork, unless you like infinite recursion bugs) that is designed to decode raw Protobuf wire messages, inject something naughty, then re-encode them again to see how a Protobuf endpoint behaves. We are going to have so much fun together in this next section.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>You may encounter a small world of pain because some forks of blackboxprotobuf will cause a stack overflow due to deep recursion. You can see this by adding <code>sys.setrecursionlimit(200)</code>.</p><p>Compiling the original library source code for Burp Suite and using the C++ bindings will allow us to transcode ~500 kiB of raw Protobuf bytes in just a few seconds.</p><div><div> At the top of your import chain before you import , add<div data-nosnippet=\"data-nosnippet\" data-settings=\" no-popup minimize scroll-mouseover\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>to use the C++  implementation whenever possible.</p></div></div><p>It is now possible to generate a best-guess  schema with a single function:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>The schema isn’t perfect, and it is huge and deeply nested, and takes forever to pretty-print, and is probably wrong, but is just good enough to pull out the ad details like so (Protobuf to JSON in this sample):</p><p>The Python schema is huge and looks like this for about 250,000 more charcters:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Reverse engineering the Protobuf schema sounds good on paper, but our target is spectacularly complex and a moving target.</p><h2>Exfil the Proto Schemas from the App, Cleanly?</h2><p>As fun as it to reverse the Protobuf and generate a best-guess schema, wouldn’t it be more ninja-like to exfil the actual, working  or schema files from the smartphone app? Let’s pull out the Protobuf schemas from the Android version of the YouTube app and see if the schemas are the same or compatible.</p><p>This is what I tried , but it went nowhere with the <a href=\"https://github.com/marin-m/pbtk\" target=\"_blank\" rel=\"nofollow\">Protobuf Toolkit</a> (PBTK). I reproduce it here so I remember what I tried:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>After installing Qt dependencies (pronounced “cute”), I was treated to a GUI.</p><p>Next, I got the most recent release of a 100 MiB Android APK file from <a href=\"https://apkpure.com\" target=\"_blank\" rel=\"nofollow\">apkpure.com</a>.</p><p>Excited in vain, the most PBTK could get was a 59-byte proto file. Another tool called <a href=\"https://ibotpeaches.github.io/Apktool/install/\" target=\"_blank\" rel=\"nofollow\">Apktool</a> also looked promising, but the best it can do is  bytecode, not decompile it – this may be good enough for Pen Testers, however.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>You can see that Google went out of its way to complicate reverse engineering.</p><p>Google thoughtfully did leave some hints.</p><p>Upon deeper inspection, the Protobuf classes are right here, in Java, decorated with getters and setters. Since we are using Python, and we cannot get the true schema files, I will leave this approach for now.</p><h2>Hardcore Deep-Dive into Protobuf and Wire Format</h2><p>After gazing into a sea of decrypted network traffic again, then triggering errors and assertion fails on my iPhone with Protobuf fuzzing, and taking a peek at the error logs being phoned home, I’ve noticed that ads register for “slots” in a given video. They can register for pre-roll, mid-roll, end-roll, full-page, and ad pods (back-to-back ads). Blocking an ad URL causes an error along the lines of “some ad that doesn’t exist booked a slot” and UI panic sets in.</p><p>I’m back. The Wire Format is surprisingly elegant, except for <a href=\"https://developers.google.com/protocol-buffers/docs/encoding#signed_integers\" target=\"_blank\" rel=\"nofollow\">ZigZag encoding</a>. Through trial and error, editing out chunks of Protobuf with a hex editor is just a no-go.</p><p>While computationally expensive, decoding, editing, and re-encoding without the original schema leads to a modified encoding. This is likely because we cannot detect if ZigZag encoding is being used, or if a number is an , , , , etc., plus the order of object fields is normally non-deterministic. Here is some <a href=\"https://developers.google.com/protocol-buffers/docs/encoding#implications\" target=\"_blank\" rel=\"nofollow\">Protobuf trivia</a> on the matter:</p><h2>Exploit a Protobuf Flaw to Easily Remove All Ads by Changing One Byte</h2><p>Casually poring over the C++ source code, an interesting comment in the Protobuf code caught my eye:</p><blockquote><p> is used to keep track of fields that were seen when parsing a protocol message but whose field numbers or types are unrecognized. This most frequently occurs when new fields are added to a message type and then messages containing those fields are read by old software that was compiled before the new types were added. (<a href=\"https://developers.google.com/protocol-buffers/docs/proto3#unknowns\" target=\"_blank\" rel=\"nofollow\">ref</a>)</p></blockquote><p>Yes, what to do with unknown fields? What to do indeed. And, how easy would it be to say, change a  field key to, say,  thus making an entire substructure of advertisement and tracking information suddenly unavailable? Tantalizing.</p><p>And, if we can calculate the field tags in bytes with bit-twiddling, then can we use a simple regex to AMF the section of ads in  time?</p><p>As a motivating example, I’d like to find the field key  which is not as simple as searching for . Here is an implementation of a tag-scanning algorithm so you can see the bit-twiddling:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>We know the wire type is  (length-delimited nested string/message), and one target field  is . When bit-twiddled, we get the target </p><p>where the final  happens to mean  (the wire type) in hex. In binary, this is:</p><p><code>10101010 11111111 10111000 10111100 00000001</code></p><p>Let’s lose the MSB from each byte as per the var-length wire format:</p><p><code>.0101010 .1111111 .0111000 .0111100 .0000001</code></p><p>Then we shift and add only the first four bytes since the LSB is first:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Finally, we shift out the number of wire type bits (3) to get back the field key:</p><p><code>395198378 &gt;&gt; 3 = 49399797</code></p><p>And that, folks, is a taste of how Wire Format works.</p><p>Fantastic. Now, all we have to do is scan the Protobuf bytes for classic ad URL signatures like  to bound our field search, then move backward from there until we find the target(s) field tags and thus field keys we would like to denature (e.g.  –&gt; ).</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Notice how the Protobuf response payload is ? As I said, Google makes it computationally expensive to decode, alter, and re-encode without the C++ source proto files, but a quick linear scan takes no effort at all.</p><p>Just a quick note, there is more than one field tag, but not all of them represent ads. That is why we need to backtrack from the  markers.</p><h2>Smoke Test: Remove Ads from Protobuf in O(n)-Time</h2><p> In one pass with no additional memory, I’m able to scan a huge 1.8 MiB chunk of jibberish-looking Protobuf data, and in the screenshot below only at the 30,593 byte (of 1.8 MiB) is our target found, and then backtracking ~600 characters yields our target field key to denature. Not only is this amazing, but I don’t even need to block  or URLs with  in them; Those requests are never made in the first place, anymore.</p><h2>Analysis of this Successful Adblocking Technique</h2><p>By taking advantage of a feature (flaw?) in Protobuf that allows it to be backward compatible with schema changes, along with the fact that Protobuf is very sensitive to byte changes due to its compact nature, we can change a single byte in a critical location and tell Protobuf that an entire section of deeply-nested data is from a future schema version and it should be ignored.</p><p>Google returns huge responses in Protobuf (e.g. 1.8 MiB) – including even the layout of the iOS app – so only C++/Swift is fast enough to understand it all before the connection times out. I’ve shown that Python is several orders of magnitude too slow in decoding these Protobuf payloads, so connections do time out waiting on Python. With web-based JSON, the whole payload needs to be parsed, edited, and re-serialized; With my Protobuf technique, it takes microseconds thanks to a single linear scan and then ultra-quick backtracking. This technique is suitable for real-time adblocking without blocklists.</p><p>All those  and  URLs on Apple devices originate from the Protobuf payload. This means they all go away for free – we don’t need to block them. In fact, the YouTube app is zippier because fewer connections are made to ad URLs in the first place. This means we can avoid keeping a blocklist of YouTube ad URLs and stay on the sidelines of the whack-a-mole fun. Ads do not register for video location “slots” on the Apple devices and the content just plays.</p><p>This is a heuristic technique that looks for two strings:  and some calculated field tag nearby, so this technique is designed to be future-proof.</p><p>Even if Google changes the field tag (and breaks millions of apps and Apple TVs before they upgrade), it’s an academic exercise to enhance the following script to discover the new field tag(s) automatically.</p><h3>Should Google be Worried?</h3><p>This is a <strong>highly-specialized technique</strong> to block Apple-device YouTube ads (or Instagram, Whatsapp, Facebook, etc. tracker blocking). The CPU requirements to decrypt and re-encrypt HTTPS traffic greatly exceed those available to Raspberry Pis. Even if some company takes my script and considers making and selling a NIC dongle, it would likely not be powerful enough. An <a href=\"https://www.nvidia.com/en-us/shield/shield-tv/\" target=\"_blank\" rel=\"nofollow\">Nvidia Shield</a> could handle it, but if you already have Android devices, then just hack the binaries; My technique is for Apple device owners where we don’t want to compromise the OS so that further reduces the audience of this technique.</p><h2>The MITMProxy YouTube Adblocking Script</h2><p>Here is the MITMProxy addon script that serves as a proof-of-concept to block YouTube ads on networked Apple devices. The script can be run as follows (note the prerequisites in the script and be sure to install them first). Name it  and run the following command:</p><p><code>mitmdump --listen-port 8080 --listen-host 127.0.0.1 -s \"youtube.py\"</code></p><p>Here is the script, including a fairness function to allow ads 5% of the time:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>This script happens to work in Python for a TLS-decrypting man-in-the-middle proxy written in Python. As a working proof-of-concept, it’s pretty rad. Of course, it can be rewritten in Rust or Go or anything but single-threaded Python, but as an intellectual exercise to defeat ads that are served from the same domain as content, it’s elegant.</p><p>It’s unknown if CAD  $11.99/mo ($13.43/mo with tax) is even reasonable: Do I personally incur CAD $11.99 of cost to advertisers each month?</p><p>Since ads are auctioned, the CPV (cost-per-view) varies. Also, many ad campaigns have a capped daily budget, so theoretically there should be fewer ads in the evenings as budgets run out during the day.</p><p>I watched YouTube on and off for a day on a clean notebook computer with private browsing. My history showed that I only “watched” 10 videos:</p><ul><li>I fast-forwarded through a few of them to get past the “like and subscribe” runtime padding.</li><li>I jumped to the end of one just to get to the “top three” from a “top twenty” list.</li><li>Two were low quality so I left early.</li><li>The rest were music videos.</li></ul><p>In all, for watching parts of 10 videos, I was exposed to 8 ads, and only two were skippable (which I skipped).</p><p>Let’s use USD $0.15 as a CPV. In one day, let’s say, I incurred 8 x $0.15, or $1.20 to advertisers. Extrapolated to one month, that is roughly USD $36/mo. Do I really cost advertisers USD $36/mo for very casual YouTube viewing? That sounds terrible for advertisers.</p><h3>CPV from US Advertising Spend Divided by Total Views</h3><p>From <a href=\"https://www.statista.com/statistics/289658/youtube-global-net-advertising-revenues/\" target=\"_blank\" rel=\"nofollow\">Statistica</a>, in 2019, US YouTube advertisers spent $15.1 billion dollars. Also in 2019, US residents had 916 billion views (<a href=\"https://ericdraken.com/pfsense-decrypt-ad-traffic/#ad-spend\">ref</a>). That works out to an average of $15.1B / 916B, or USD $0.0165 per view. Then for me, that is only USD 13 cents. Extrapolated to one month, I theoreticaly cost advertisers only USD $3.96/mo.</p><h3>Is YouTube Premium Worth It?</h3><p>When I allowed ads for my experiment, I hit the hardware mute button. I also looked away because I have several computers with a lot going on. Ad spend is wasted on me, but I still want to support content creators. For me, CAD $13.48/mo is more than I incur on actual ads and more than I pay for a Netflix subscription. The only way to justify the cost is to have YouTube playing constantly in the background on a TV.</p><p>However, I truly enjoy a handful of creators, so I may start watching them in the background on non-stop play. Let’s give the three-month YouTube Premium trial a chance, and I will still be monitoring what they track about me.</p><p>Recently I learned that due to abuses of the <a href=\"https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_Act\" target=\"_blank\" rel=\"nofollow\">DMCA Act of 1998</a>, YouTube content creators who make reaction videos and “easter egg” videos may have their videos claimed by big companies like Sony and Viacom. That means that from when a claim is made, all ad revenue goes to those big companies, and not even to the creators. That means in all likelihood I unknowingly may not even be supporting my favourite YouTube creators.</p><div><div> Many fair-use and video-game-commentary videos may have automated copyright claims against them, meaning that ad revenue goes to big companies with deep legal pockets and your favourite creators may get nothing, so more and more creators leave YouTube for Twitch.</div></div><h2>Summary of Accomplishments</h2><p>I rarely give up, so this is an example of going into an extreme problem-solving mode to solve a fun problem loosely using cryptography and reverse engineering. In the end, a single byte turned it all around, so it was all worth it to come to an elegant and satisfying solution.</p><div><div> We were able to set up a hardware router from scratch, segment LANs into trusted and untrusted zones, set up traditional DNS adblocking, add a transparent MITM proxy, and ultimately block YouTube ads on networked Apple devices.<p>Note: This was a hard problem – now solved – so I am paying for YouTube Premium to give the CPU a rest.</p></div></div>","contentLength":73881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43396735"},{"title":"GIMP 3.0","url":"https://testing.gimp.org/news/2025/03/16/gimp-3-0-released/","date":1742253953,"author":"wicket","guid":1137,"unread":true,"content":"<p>At long last, the <strong>first release of  3.0 is here</strong>! This is the end result of\nseven years of hard work by volunteer developers, designers, artists, and community members (for reference,  2.10 was first <a href=\"https://www.gimp.org/news/2018/04/27/gimp-2-10-0-released/\">published in 2018</a>\nand the initial development version of  3.0 was <a href=\"https://www.gimp.org/news/2020/11/06/gimp-2-99-2-released/\">released in 2020</a>). With  3.0 you can do more than ever before, more easily, more&nbsp;quickly!</p><p>While we can’t cover every single change in  from 2.10, we want to highlight some of the biggest ones as you start exploring this new&nbsp;release.</p><ul><li><p>Need to tweak a filter you applied hours ago?\nNew in  3.0 is non-destructive editing for\nmost commonly-used filters. See the changes in\nreal time with on-canvas&nbsp;preview.</p></li><li><p>Exchange files with more applications, including\n files as well as better  export and\nmany new&nbsp;formats.</p></li><li><p>Don’t know how big to make your drawing?\nSimply set your paint tool to expand layers automatically as&nbsp;needed.</p></li><li><p>Making pro-quality text got easier, too. Style your text,\napply outlines, shadows, bevels, and more, and you can\nstill edit your text, change font and size,\nand even tweak the style&nbsp;settings.</p></li><li><p>Organizing your layers has become much easier with the ability to\n  select multiple items at once, move them or transform them all&nbsp;together!</p></li><li><p>Color Management was again improved, as our long-term project to make\n   an advanced image editor for all&nbsp;usages.</p></li><li><p>Updated graphical toolkit () for modern desktop&nbsp;usage.</p></li></ul><p>We’ve prepared <a href=\"https://testing.gimp.org/release-notes/gimp-3.0.html\">release notes</a> to go over all the changes, improvements, new features, and more. And if you’d like even more details, you can peruse the <a href=\"https://gitlab.gnome.org/GNOME/gimp/-/blob/master/NEWS\"></a> changelog for all 2.99 and 3.0 &nbsp;releases.</p><p>But to see it for yourself, you can get  3.0 directly from our <a href=\"https://www.gimp.org/downloads/\">Downloads page</a> and try it&nbsp;out!</p><h2>Other Releases in GIMPVerse</h2><p>To accompany our release of  3.0.0, packagers should also be aware\nthat we&nbsp;released:</p><p>We also advise all packagers to use the latest  version: .\nIt contains bug fixes for major issues (ranging from crashes to input\ndevices’ grab issues,  glitches with interfaces in \nlanguages, and&nbsp;more…).</p><p> 3.0 is a new milestone. The application is in active\ndevelopment and if you think this is awesome, wait until\nyou see our plans for the&nbsp;future!</p>","contentLength":2145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43393822"},{"title":"Deep Learning Is Not So Mysterious or Different","url":"https://arxiv.org/abs/2503.02113","date":1742230022,"author":"wuubuu","guid":10732,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43390400"},{"title":"Rippling sues Deel over spying","url":"https://twitter.com/parkerconrad/status/1901615179718406276","date":1742216632,"author":"amacneil","guid":10731,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43388133"},{"title":"The Alexa feature \"do not send voice recordings\" you enabled no longer available","url":"https://discuss.systems/@dev/114161826926246661","date":1742186509,"author":"luu","guid":942,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43385268"},{"title":"Launching RDAP; sunsetting WHOIS","url":"https://www.icann.org/en/announcements/details/icann-update-launching-rdap-sunsetting-whois-27-01-2025-en","date":1742172528,"author":"radeeyate","guid":909,"unread":true,"content":"<p>As of 28 January 2025, the Registration Data Access Protocol (RDAP) will be the definitive source for delivering generic top-level domain name (gTLD) registration information in place of sunsetted WHOIS services. RDAP offers several advantages over WHOIS including support for internationalization, secure access to data, authoritative service discovery, and the ability to provide differentiated access to registration data. RDAP was developed by the Internet Engineering Task Force.</p><p>RDAP has been offered by ICANN-accredited registrars and gTLDs since 2019.</p><p>To request access to nonpublic gTLD registration data, use the Registration Data Request Service (<a href=\"https://rdrs.icann.org/\">RDRS</a>) for participating registrars or contact the sponsoring registrar directly to determine their disclosure process. Please make sure you have first checked that the data is unavailable through the ICANN Lookup tool. The RDRS is intended for use by those with a legitimate interest in nonpublic data like law enforcement, intellectual property professionals, consumer protection advocates, cybersecurity professionals, and government officials.</p>","contentLength":1102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43384069"},{"title":"Docs – Open source alternative to Notion or Outline","url":"https://github.com/suitenumerique/docs","date":1742125132,"author":"maelito","guid":150,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43378239"}],"tags":["dev"]}