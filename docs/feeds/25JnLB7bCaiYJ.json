{"id":"25JnLB7bCaiYJ","title":"Tech News","displayTitle":"Tech News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":89,"items":[{"title":"Honor says its ‘Robot phone’ with moving camera can dance to music","url":"https://techcrunch.com/2026/03/01/honor-says-its-robot-phone-with-moving-camera-can-dance-to-music/","date":1772379000,"author":"Ivan Mehta","guid":155234,"unread":true,"content":"<article>Honor first teased its “Robot phone” with a movable camera arm earlier this year. Ahead of the Mobile World Congress (MWC) in Barcelona, the Chinese company provided more details about the device, including how the robot can respond to different situations without commands. The company said that it is planning to launch this device in […]</article>","contentLength":346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Honor launches its new slim foldable Magic V6 with a 6,600 mAh battery","url":"https://techcrunch.com/2026/03/01/honor-launches-its-new-slim-foldable-magic-v6-with-a-6600-mah-battery/","date":1772377989,"author":"Ivan Mehta","guid":155233,"unread":true,"content":"<article>Honor also previewed battery tech that could take foldable batteries over 7,000 mAh mark</article>","contentLength":88,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SaaS in, SaaS out: Here’s what’s driving the SaaSpocalypse","url":"https://techcrunch.com/2026/03/01/saas-in-saas-out-heres-whats-driving-the-saaspocalypse/","date":1772373600,"author":"Dominic-Madori Davis","guid":155232,"unread":true,"content":"<article>What's behind the SaaSpocalypse? It simply seems a new supreme has risen. </article>","contentLength":74,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Some Linux LTS Kernels Will Be Supported Even Longer, Announces Greg Kroah-Hartman","url":"https://linux.slashdot.org/story/26/03/01/0429234/some-linux-lts-kernels-will-be-supported-even-longer-announces-greg-kroah-hartman?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772364840,"author":"EditorDavid","guid":155189,"unread":true,"content":"An anonymous reader shared this report from the blogIt's FOSS:\n\nGreg Kroah-Hartman has updated the projected end-of-life (EOL) dates for several active longterm support kernels via a commit. The provided reasoning? It was done \"based on lots of discussions with different companies and groups and the other stable kernel maintainer.\" The other maintainer is Sasha Levin, who co-maintains these Linux kernel releases alongside Greg. Now, the updated support schedule for the currently active LTS kernels looks like this: \n — Linux 6.6 now EOLs Dec 2027 (was Dec 2026), giving it a 4-year support window. \n\n — Linux 6.12 now EOLs Dec 2028 (was Dec 2026), also a 4-year window. \n\n — Linux 6.18 now EOLs Dec 2028 (was Dec 2027), at least 3 years of support. \n\nWorth noting above is that Linux 5.10 and 5.15 are both hitting EOL this year in December, so if your distro is still running either of these, now is a good time to start thinking about a move.\n","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthropic's Claude Leaps to #2 on Apple's 'Top Apps' Chart After Pentagon Controversy","url":"https://slashdot.org/story/26/02/28/2046221/anthropics-claude-leaps-to-2-on-apples-top-apps-chart-after-pentagon-controversy?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772354040,"author":"EditorDavid","guid":155166,"unread":true,"content":"Anthropic's Claude AI assistant \"jumped to the No. 2 slot on Apple's chart of top U.S. free apps late on Friday,\" reports CNBC:\n\n\nThe rise in popularity suggests that Anthropic is benefiting from its presence in news headlines, stemming from its refusal to have its models used for mass domestic surveillance or for fully autonomous weapons... OpenAI's ChatGPT sat at No. 1 on the App Store rankings on Saturday, while Google's Gemini was at No. 3... On Jan. 30, [Claude] was ranked No. 131 in the U.S., and it bounced between the top 20 and the top 50 for much of February, according to data from analytics company Sensor Tower... [And Friday night, for 85.3 million followers] pop singer Katy Perry posted a screenshot of Anthropic's Pro subscription for consumers, with a heart superimposed over it. \n\nFriday Anthropic posted \"We are deeply grateful to our users, and to the industry peers, policymakers, veterans, and members of the public who have voiced their support in recent days. Thank you. \"","contentLength":1002,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The TechBeat: The State of The Noonion: Blogging Our Way Through the AI Boom (3/1/2026)","url":"https://hackernoon.com/3-1-2026-techbeat?source=rss","date":1772349077,"author":"Techbeat","guid":155188,"unread":true,"content":"<p>By <a href=\"https://hackernoon.com/u/mexcmedia\">@mexcmedia</a> [ 2 Min read ] \n MEXC COO Vugar Usi explains why retail-first exchanges are winning in crypto’s 2026 reset, leveraging zero-fee trading and user trust. <a href=\"https://hackernoon.com/navigating-cryptos-2026-reset-why-retail-first-exchanges-are-winning\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/crafinsstudio\">@crafinsstudio</a> [ 20 Min read ] \n I tested eight piano apps on two pianos for three weeks. Here's what I'd actually recommend. <a href=\"https://hackernoon.com/best-piano-learning-apps-in-2026-an-in-depth-comparison-of-music-education-technology\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/lomitpatel\">@lomitpatel</a> [ 5 Min read ] \n How CMOs win CFO buy-in using incrementality, trust, AI, and capital allocation to drive margin expansion and revenue durability. <a href=\"https://hackernoon.com/how-cmos-win-cfo-buy-in-at-scale\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/qatech\">@qatech</a> [ 8 Min read ] \n Manual testing can't keep up with modern development. See how QA.tech's AI testing automation catches bugs on every PR -- no Playwright or Cypress scripts to ma <a href=\"https://hackernoon.com/the-ai-builder-stack-linear-cursor-vercel-and-qatech\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/saumyatyagi\">@saumyatyagi</a> [ 15 Min read ] \n Most teams plateau at \"AI writes code, a human reviews it.\" This article presents the Dark Factory Pattern — a four-phase architecture using holdout scenarios a <a href=\"https://hackernoon.com/the-dark-factory-pattern-moving-from-ai-assisted-to-fully-autonomous-coding\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/scylladb\">@scylladb</a> [ 5 Min read ] \n Blitz migrated from Postgres and Elixir to Rust and ScyllaDB, cutting latency, costs, and 100+ cores down to four cloud nodes. <a href=\"https://hackernoon.com/rust-rewrite-postgres-exit-blitz-revamps-its-league-of-legends-backend\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/noonion\">@noonion</a> [ 13 Min read ] \n HackerNoon’s 2016–2026 evolution: $727k Q4 revenue, 62% Business Blogging CAGR, 4.4M monthly pageviews, and resilient, AI-aware publishing. <a href=\"https://hackernoon.com/the-state-of-the-noonion-blogging-our-way-through-the-ai-boom\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/melissaindia\">@melissaindia</a> [ 4 Min read ] \n Learn 6 proven strategies to secure executive buy-in for Master Data Management by aligning MDM with ROI, risk reduction, and business goals. <a href=\"https://hackernoon.com/selling-master-data-management-to-leadership-6-proven-strategies\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/confluent\">@confluent</a> [ 5 Min read ] \n Learn how Python developers build real-time AI agents using MCP, Kafka, and Flink—modern agentic workflows explained on HackerNoon. <a href=\"https://hackernoon.com/how-python-devs-can-build-ai-agents-using-mcp-kafka-and-flink\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/chris127\">@chris127</a> [ 8 Min read ] \n Stablecoins aren't just \"crypto dollars\"—they're experiments in digital money stability. Each type offers different trade-offs, learn more about them here <a href=\"https://hackernoon.com/a-comprehensive-guide-to-stablecoins-types-risks-and-the-future-of-digital-money\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mexcmedia\">@mexcmedia</a> [ 2 Min read ] \n MEXC ranks No. 1 globally in XAUT perpetual volume, hitting $3.43B as tokenized gold demand rises amid record spot gold prices in 2026. <a href=\"https://hackernoon.com/mexc-ranks-no-1-in-xaut-perpetual-volume-globally-demonstrating-strong-liquidity-and-user-activity\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/scylladb\">@scylladb</a> [ 4 Min read ] \n Discover how Yieldmo migrated from DynamoDB to ScyllaDB to cut database costs, achieve multicloud flexibility, and deliver ads in single-digit millisecond laten <a href=\"https://hackernoon.com/how-yieldmo-cut-database-costs-and-cloud-dependencies\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/opensourcetheworld\">@opensourcetheworld</a> [ 7 Min read ] \n I replaced $1,200/year in cloud subscriptions with one home server. Here's the setup, costs, apps, Bitcoin node, local AI, and what I'd do differently.  <a href=\"https://hackernoon.com/i-replaced-$1200year-in-cloud-subscriptions-with-a-single-home-server-heres-what-i-learned\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/khamisihamisi\">@khamisihamisi</a> [ 4 Min read ] \n Western tech is built in environments of abundance. In emerging markets, these assumptions often fail quickly. <a href=\"https://hackernoon.com/building-for-emerging-markets-what-western-startups-miss\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/davidiyanu\">@davidiyanu</a> [ 8 Min read ] \n Cloud cost and system reliability are the same problem viewed through different instruments.  <a href=\"https://hackernoon.com/when-cloud-bills-crash-the-system-cost-as-a-reliability-issue\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/thomascherickal\">@thomascherickal</a> [ 51 Min read ] \n Google Antigravity is not just for coding. It is for your entire computer. Stop scrolling - everything you do on a computer has just been automated. <a href=\"https://hackernoon.com/google-antigravity-the-disruptor-that-just-changed-the-computing-world-forever\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/johnpphd\">@johnpphd</a> [ 4 Min read ] \n How precompiling context for AI agents beats context stuffing. Lessons from building 100+ specialized agents for a web3 application. <a href=\"https://hackernoon.com/lessons-from-building-a-100-agent-swarm-in-web3\">Read More.</a></p><p>](https://hackernoon.com/the-complete-guide-to-ai-agent-memory-files-claudemd-agentsmd-and-beyond)** <img src=\"https://cdn.hackernoon.com/images/mpDOI8AQeYeu5cc9VGleWjM9xvB2-mu0383b.png\" alt=\"\">\n By <a href=\"https://hackernoon.com/u/paoloap\">@paoloap</a> [ 7 Min read ] \n Learn how CLAUDE.md, AGENTS.md, and AI memory files work. Covers file hierarchy, auto-memory, @imports, and which files you actually need for your setup. <a href=\"https://hackernoon.com/the-complete-guide-to-ai-agent-memory-files-claudemd-agentsmd-and-beyond\">Read More.</a></p>","contentLength":3343,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Silicon Valley's Ideas Mocked Over Penchant for Favoring Young Entrepreneurs with 'Agency'","url":"https://slashdot.org/story/26/03/01/011246/silicon-valleys-ideas-mocked-over-penchant-for-favoring-young-entrepreneurs-with-agency?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772343240,"author":"EditorDavid","guid":155119,"unread":true,"content":"In a 9,000-word expose, a writer for Harper's visited San Francisco's young entrepreneurs in September to mockingly profile \"tech's new generation and the end of thinking.\" \nThere's Cluely founder Roy Lee. (\"His grand contribution to the world was a piece of software that told people what to do.\") And the Rationalist movement's Scott Alexander, who \"would probably have a very easy time starting a suicide cult...\"\n\nAlexander's relationship with the AI industry is a strange one. \"In theory, we think they're potentially destroying the world and are evil and we hate them,\" he told me. In practice, though, the entire industry is essentially an outgrowth of his blog's comment section... \"Many of them were specifically thinking, I don't trust anybody else with superintelligence, so I'm going to create it and do it well.\" Somehow, a movement that believes AI is incredibly dangerous and needs to be pursued carefully ended up generating a breakneck artificial arms race. \n\nThere's a fascinating story about teenaged founder Eric Zhu (who only recently turned 18):\n\nClients wanted to take calls during work hours, so he would speak to them from his school bathroom. \"I convinced my counselor that I had prostate issues... I would buy hall passes from drug dealers to get out of class, to have business meetings.\" Soon he was taking Zoom calls with a U.S. senator to discuss tech regulation... Next, he built his own venture-capital fund, managing $20 million. At one point cops raided the bathroom looking for drug dealers while Eric was busy talking with an investor. Eventually, the school got sick of Eric's misuse of the facilities and kicked him out. He moved to San Francisco. \n\nEric made all of this sound incredibly easy. You hang out in some Discord servers, make a few connections with the right people; next thing you know, you're a millionaire... Eric didn't think there was anything particularly special about himself. Why did he, unlike any of his classmates, start a $20 million VC fund? \"I think I was just bored. Honestly, I was really bored.\" Did he think anyone could do what he did? \"Yeah, I think anyone genuinely can.\" \n\nThe article concludes Silicon Valley's investors are rewarding young people with \"agency\". Although \"As far as I could tell, being a highly agentic individual had less to do with actually doing things and more to do with constantly chasing attention online.\" Like X.com user Donald Boat, who successfully baited Sam Altman into buying him a gaming PC in \"a brutally simplified miniature of the entire VC economy.\" (After which \"People were giving him stuff for no reason except that Altman had already done it, and they didn't want to be left out of the trend.\")\n\nShortly before I arrived at the Cheesecake Factory, [Donald Boat] texted to let me know that he'd been drinking all day, so when I met him I thought he was irretrievably wasted. In fact, it turned out, he was just like that all the time... He seemed to have a constant roster of projects on the go. He'd sent me occasional photos of his exploits. He went down to L.A. to see Oasis and ended up in a poker game with a group of weapons manufacturers. \"I made a bunch of jokes about sending all their poker money to China,\" he said, \"and they were not pleased....\" \n\n\"I don't use that computer and I think video games are a waste of time. I spent all the money I made from going viral on Oasis tickets.\" As far as he was concerned, the fact that tech people were tripping over themselves to take part in his stunt just confirmed his generally low impression of them. \"They have too much money and nothing going on...\" Ever since his big viral moment, he'd been suddenly inundated with messages from startup drones who'd decided that his clout might be useful to them. One had offered to fly him out to the French Riviera. \n\nThe author's conclusion? \"It did not seem like a good idea to me that some of the richest people in the world were no longer rewarding people for having any particular skills, but simply for having agency.\"","contentLength":4037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rebuild Your Life in 180 Days: The No-Excuses Blueprint","url":"https://hackernoon.com/rebuild-your-life-in-180-days-the-no-excuses-blueprint?source=rss","date":1772343004,"author":"BenoitMalige","guid":155187,"unread":true,"content":"<p>\\\n\\\nIf you apply what’s on this email to the T, you can rebuild yourself in six months. Bold statement? Yes.</p><p>But give me a few minutes and I’ll gift you the EXACT blueprint for:</p><ul><li>A different baseline of energy.</li><li>A different standard for what you tolerate.</li><li>A different life altogether.</li></ul><p>And this is not manifestation. This is&nbsp;, and this is what we do here.</p><p>We will work on: identity, body, skills, environment, mind, and social circle. \\n Run all six or don’t waste your time. Half-transformations are just elaborate procrastination.</p><p>180 days. For a completely new life. This is important. This is doable. All you have to do is apply this.</p><p>\\\nIf you don’t change your identity, you’ll drag the same problems into your “new” life like a moldy suitcase. Most people fail not here because they try to bolt new habits onto an old self-image.</p><p>&nbsp;and&nbsp;&nbsp;transformation begins when you burn the blueprint of the person you’ve been and draw a new one from scratch. Humans behave in a way that matches the story they believe about themselves, even when the story sucks.</p><p>If you think of yourself as someone who&nbsp;to get in shape, you’ll sabotage yourself like clockwork. \\n If you think of yourself as someone who&nbsp;in shape, your decisions start matching that identity automatically.</p><p>before strategy. \\n before habits. \\n &nbsp;before everything.</p><p>Everything (how you eat, how you talk, how you sit, how you dress) has to come from the new identity. If it doesn’t align, it dies.</p><p>When your actions don’t match the person you claim to be, your brain rings the alarm. Everyone around you feels it too. Nothing smells worse than someone pretending to have standards they don’t enforce.</p><p>Stop being the person who “intends.” \\n <strong>Become the person who “does.”</strong></p><p>Before you rebuild, you rip out the rotten floorboards:</p><ul></ul><p>Each one gets one question:</p><p><strong>Does this serve the future I’m building?</strong></p><p>If it’s not a hell yes, it’s a surgical no.</p><p>Delete the apps. \\n Cut the friends. \\n Change the job. \\n Burn the costume.</p><p>Yes, it hurts. \\n No, it won’t kill you. \\n .</p><h3><strong>Treat your new identity like religion</strong></h3><p>Rituals. Symbols. Structure.</p><p>Morning routines, nighttime reflections, clothes that match your new standard, reminders on your wall, habits you don’t negotiate.</p><p>Your&nbsp;&nbsp;won’t die quietly. It will bribe you with nostalgia, craving, laziness, and bullshit stories about balance.</p><p>Expect relapse thoughts. \\n Prepare counters. \\n .</p><ol><li>Choose your archetype — Write your identity profile: habits, values, style, energy, boundaries, even flaws.</li><li>Cut contradictions — If your environment belongs to the old you,&nbsp;</li><li>Act as if from day one — No “warming up.” You switch&nbsp;</li><li>Document the proof — Log every moment you acted like the new identity.</li><li>Protect the signal — Avoid people, environments, or content that drag you back.</li><li>Accelerate the feedback loop — Put yourself in rooms where the new identity is&nbsp;&nbsp;to belong.</li></ol><p>Once the identity locks in, your reality rearranges itself around you.</p><p><em>(Your body is the receipt for your discipline.)</em></p><p>You can talk about transformation all day. \\n Your body is the part you can’t fake.</p><p>When you walk into a room with a completely different body:</p><ul><li>People treat you differently</li><li>You treat yourself differently</li></ul><p>This isn’t about six-pack obsession. \\n This is about building a body that proves you finish what you start.</p><p>Every rep becomes a vote. Every walk reinforces grit. Every choice cements identity.</p><p>Most people fail because they try to “fit fitness in.” \\n You don’t fit transformation into your life. \\n You build your damn life around it.</p><p>Discipline in the kitchen → discipline everywhere. \\n Sloppiness in the body → sloppiness in ambition.</p><p>“How you do one thing is how you do everything” is cliché because it’s true.</p><p>One day, I decided to work out every single day. It’s a non negotiable. This is what I do, but you don’t have to go that extreme.</p><p>1. Train 4x/week minimum — Heavy lifts. Push/pull/legs/full-body. \\n Bonus: Add two long incline walks weekly.</p><p>2. Eat like an adult, not a toddler</p><ul><li>Same meals every day (or close)</li></ul><p>Alcohol, weed, binge nights. Anything that unravels discipline. Cut it.</p><p>4. Walk 10,000 steps a day </p><p>Rain or shine. Inside or outside. No excuses.</p><p>5. Sleep like it’s a performance drug </p><p>Because It is. \\n 7.5 hours minimum. \\n No screens 1–2 hours before bed. \\n No caffeine after 2 p.m.</p><p>If you’re not measuring, you’re guessing. \\n And guessing is how you stay average.</p><p>The gap is where normal people quit and transformed people are born.</p><p>You lift when you’re tired. \\n You walk when it rains. \\n You prep meals when everyone else is ordering Uber Eats.</p><p>That’s what creates the gulf between you and the old you.</p><p><em>(You don’t need more confidence. You need skills that print confidence on demand.)</em></p><p>\\\nOnce the body and identity are locked in, you weaponize them.</p><p>Power in the modern world = skills. \\n Stackable, monetizable, rare skills.</p><p>Skills put you in rooms the old you couldn’t even pronounce.</p><p>Most people “learn” the slow way by dabbling, exploring, taking courses and doing nothing with them.</p><p>You? \\n You learn like your life depends on it. There is a full chapter dedicated to that in. Use it.</p><p>The old you takes 6 months to start something. \\n The new you learns a high-income skill in 2 weeks and gets paid by week 4.</p><p>It’s not intelligence. \\n It’s intensity.</p><ul></ul><p>Pick ONE. \\n Master it. \\n Stack the others later.</p><h3><strong>The 90-day mastery protocol</strong></h3><ol><li><p>Choose one skill — Eliminate everything else.</p></li><li><p>7-day deep dive — Saturate your brain. \\n 6+ hours/day. (you’ll find the time) \\n Books, videos, podcasts, notes.</p></li><li><p>Build one real project — Landing page, video, funnel, outreach sequence. Something you can show.</p></li><li><p>Get feedback fast — Ask someone 10 steps ahead to tear it apart. (ChatGPT can do that very well if you ask it nicely).</p></li><li><p>50 videos \\n 100 tweets \\n 100 cold emails \\n 10 funnels \\n Whatever matches your skill. VOLUME is king.</p></li><li><p>Get paid ASAP — Even $39 counts. \\n Once someone pays you, you’re in business.</p></li></ol><p>Repeat until you’re dangerous.</p><p><em>(Willpower is overrated. Your environment is the real puppet master.)</em></p><p>\\\nYour environment will beat your discipline over time. \\n Always.</p><p>You can have the perfect mindset. \\n You can read the books. \\n You can “be motivated.”</p><p>But if you live in the same messy room, around the same lazy friends, with the same digital junk food…</p><p>You will snap back to baseline.</p><ul><li>Delete apps that hijack focus.</li><li>Unfollow accounts that normalize mediocrity.</li><li>Unfollow friends that don’t have what you want.</li><li>Clear your space of old-self objects and clutter.</li><li>Remove junk food, trash habits, and triggers.</li></ul><h3><strong>Make good habits frictionless</strong></h3><ul></ul><ul><li>Logged out of Netflix (have someone else change the password for you)</li><li>No snacks in the house. Seriously.</li></ul><h3><strong>Your circle counts as environment</strong></h3><p>If the people around you crawl, you won’t sprint.</p><p>You don’t need dramatic exits, just become harder to reach. \\n Distance does the work for you.</p><p>And sometimes? You literally need to move. \\n New city. \\n New apartment. \\n New country.</p><p>Fresh soil grows different roots. Don’t be scared of change.</p><ul><li>Create sacred zones (work, training, rest)</li></ul><p>When your environment stops tolerating the old you, the old you suffocates.</p><p><em>(If your mind is brittle, your success has an expiration date.)</em></p><p>You can have the body, the skills, the money.. but if your mind collapses under stress, criticism, or uncertainty, you’re toast.</p><p>Real resilience isn’t “staying positive.” \\n That’s .</p><p><strong>Resilience is taking hits without turning them into excuses.</strong></p><ul><li>You become mentally strong by doing hard things on purpose.</li><li>You don’t react to every feeling.</li><li>The gap between impulse and action is where adulthood starts.</li><li>Most burned-out people aren’t doing too much — they’re doing too little of what matters.</li></ul><p>When weakness shows up, you don’t negotiate with it. \\n You kill it.</p><p>You need a sentence that snaps you back into execution instantly.</p><p>Mine used to be: \\n <strong>“Stop bullshitting yourself. Move.”</strong></p><ul><li>3–5 non-negotiables daily</li><li>Weekly voluntary hardship (fasting, cold, public speaking, etc.)</li><li>Cut mental junk food (fear-driven news, gossip, chaos content)</li><li>Reinforce identity nightly</li></ul><p>When your mind becomes unshakeable, your life becomes predictable in the best way.</p><p><em>(Your circle is the hidden thermostat of your life.)</em></p><p>Every relationship is either a plus-one or a minus-one. \\n There is no neutral.</p><p>Someone is either feeding your fire or smothering it.</p><p>You become unrecognizable when you stop asking, \\n “Do I like this person?” \\n and start asking, \\n “Do they make me better?”</p><ul><li>Score each person (+1 / 0 / -1)</li><li>Replace with higher-caliber people</li><li>Hold boundaries like your life depends on it</li></ul><p>Your social ecosystem becomes a force multiplier. \\n When everyone around you is winning, discipline stops feeling like effort, it becomes the baseline.</p><p>\\\n<strong>The part everyone skips and wonders why nothing changes.</strong></p><ol><li>Pick a start date within 72 hours. No “next Monday” bullshit.</li><li>Run all six pillars in parallel. This is not a buffet. \\n You don’t pick favorites.</li><li>Measure your progress daily. Body, skills, environment, mindset, social shifts.&nbsp;.</li><li>Audit every 2 weeks. What works stays. \\n What stalls gets replaced.</li><li>Treat this like a mission, not a vibe. You’re not here to worship the process. \\n You’re here to become unrecognizable.</li></ol>","contentLength":9235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sam Altman Answers Questions on X.com About Pentagon Deal, Threats to Anthropic","url":"https://news.slashdot.org/story/26/03/01/0233230/sam-altman-answers-questions-on-xcom-about-pentagon-deal-threats-to-anthropic?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772332740,"author":"EditorDavid","guid":155100,"unread":true,"content":"Saturday afternoon Sam Altman announced he'd start answering questions on X.com about OpenAI's work with America's Department of War — and all the developments over the past few days. (After that department's negotions had failed with Anthropic, they announced they'd stop using Anthropic's technology and threatened to designate it a \"Supply-Chain Risk to National Security\". Then they'd reached a deal for OpenAI's technology — though Altman says it includes OpenAI's own similar prohibitions against using their products for domestic mass surveillance and requiring \"human responsibility\" for the use of force in autonomous weapon systems.) \nAltman said Saturday that enforcing that \"Supply-Chain Risk\" designation on Anthropic \"would be very bad for our industry and our country, and obviously their company. We said [that] to the Department of War before and after. We said that part of the reason we were willing to do this quickly was in the hopes of de-esclation.... We should all care very much about the precedent... To say it very clearly: I think this is a very bad decision from the Department of War and I hope they reverse it. If we take heat for strongly criticizing it, so be it.\" \n\n\nAltman also said that for a long time, OpenAI was planning to do \"non-classified work only,\" but this week found the Department of War \"flexible on what we needed...\"\n\n Sam Altman: The reason for rushing is an attempt to de-escalate the situation. I think the current path things are on is dangerous for Anthropic, healthy competition, and the U.S. We negotiated to make sure similar terms would be offered to all other AI labs. \n\nI know what it's like to feel backed into a corner, and I think it's worth some empathy to the Department of War. They are... a very dedicated group of people with, as I mentioned, an extremely important mission. I cannot imagine doing their work. Our industry tells them \"The technology we are building is going to be the high order bit in geopolitical conflict. China is rushing ahead. You are very behind.\" And then we say \"But we won't help you, and we think you are kind of evil.\" I don't think I'd react great in that situation. I do not believe unelected leaders of private companies should have as much power as our democratically elected government. But I do think we need to help them. \n\n\n\nQuestion: Are you worried at all about the potential for things to go really south during a possible dispute over what's legal or not later on and be deemed a supply chain risk...? \n\n\n\nSam Altman: Yes, I am. If we have to take on that fight we will, but it clearly exposes us to some risk. I am still very hopeful this is going to get resolved, and part of why we wanted to act fast was to help increase the chances of that... \n\n\nQuestion: Why the rush to sign the deal ? Obviously the optics don't look great. \n\n\nSam Altman: It was definitely rushed, and the optics don't look good. We really wanted to de-escalate things, and we thought the deal on offer was good. \nIf we are right and this does lead to a de-escalation between the Department of War and the industry, we will look like geniuses, and a company that took on a lot of pain to do things to help the industry. If not, we will continue to be characterized as as rushed and uncareful. I don't where it's going to land, but I have already seen promising signs. I think a good relationship between the government and the companies developing this technology is critical over the next couple of years... \n\n\n\nQuestion: What was the core difference why you think the Department of War accepted OpenAI but not Anthropic? \n\n\nSam Altman: [...] We believe in a layered approach to safety--building a safety stack, deploying FDEs [embedded Forward Deployed Engineers] and having our safety and alignment researcher involved, deploying via cloud, working directly with the Department of War. Anthropic seemed more focused on specific prohibitions in the contract, rather than citing applicable laws, which we felt comfortable with. We feel that it it's very important to build safe system, and although documents are also important, I'd clearly rather rely on technical safeguards if I only had to pick one... \n\n\n\n\nI think Anthropic may have wanted more operational control than we did... \n\n\n\nQuestion: Were the terms that you accepted the same ones Anthropic rejected? \n\n\nSam Altman: No, we had some different ones. But our terms would now be available to them (and others) if they wanted. \n\n\n\nQuestion: Will you turn off the tool if they violate the rules? \n\n\n\nSam Altman: Yes, we will turn it off in that very unlikely event, but we believe the U.S. government is an institution that does its best to follow law and policy. What we won't do is turn it off because we disagree with a particular (legal military) decision. We trust their authority.\n\n \n\nQuestions were also answered by OpenAI's head of National Security Partnerships (who at one point posted that they'd managed the White House response to the Snowden disclosures and helped write the post-Snowden policies constraining surveillance during the Obama years.) And they stressed that with OpenAI's deal with Department of War, \"We control how we train the models and what types of requests the models refuse.\"\n\n\n\n\nQuestion: Are employees allowed to opt out of working on Department of War-related projects? \n\n\nAnswer: We won't ask employees to support Department of War-related projects if they don't want to. \n\n\n\nQuestion: How much is the deal worth? \n\n\nAnswer: It's a few million $, completely inconsequential compared to our $20B+ in revenue, and definitely not worth the cost of a PR blowup. We're doing it because it's the right thing to do for the country, at great cost to ourselves, not because of revenue impact... \n\n\n\n\nQuestion: Can you explicitly state which specific technical safeguard OpenAI has that allowed you to sign what Anthropic called a 'threat to democratic values'? \n\n\nAnswer: We think the deal we made has more guardrails than any previous agreement for classified AI deployments, including Anthropic's. Other AI labs (including Anthropic) have reduced or removed their safety guardrails and relied primarily on usage policies as their primary safeguards in national security deployments. Usage policies, on their own, are not a guarantee of anything. Any responsible deployment of AI in classified environments should involve layered safeguards including a prudent safety stack, limits on deployment architecture, and the direct involvement of AI experts in consequential AI use cases. These are the terms we negotiated in our contract. \n\nThey also detailed OpenAI's position on LinkedIn:\n\nDeployment architecture matters more than contract language. Our contract limits our deployment to cloud API. Autonomous systems require inference at the edge. By limiting our deployment to cloud API, we can ensure that our models cannot be integrated directly into weapons systems, sensors, or other operational hardware... \n\n\n\nInstead of hoping contract language will be enough, our contract allows us to embed forward deployed engineers, commits to giving us visibility into how models are being used, and we have the ability to iterate on safety safeguards over time. If our team sees that our models aren't refusing queries they should, or there's more operational risk than we expected, our contract allows us to make modifications at our discretion. This gives us far more influence over outcomes (and insight into possible abuse) than a static contract provision ever could. \n\n\n\nU.S. law already constrains the worst outcomes. We accepted the \"all lawful uses\" language proposed by the Department, but required them to define the laws that constrained them on surveillance and autonomy directly in the contract. And because laws can change, having this codified in the contract protects against changes in law or policy that we can't anticipate.","contentLength":7920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The trap Anthropic built for itself","url":"https://techcrunch.com/2026/02/28/the-trap-anthropic-built-for-itself/","date":1772323738,"author":"Connie Loizos","guid":155094,"unread":true,"content":"<article>Anthropic, OpenAI, Google DeepMind and others have long promised to govern themselves responsibly. Now, in the absence of rules, there's not a lot to protect them.</article>","contentLength":163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Duolingo Users Grow, But Users Disliked Increased Ads and Subscription Pushes. Stock Plummets Again","url":"https://slashdot.org/story/26/02/28/2321238/duolingo-users-grow-but-users-disliked-increased-ads-and-subscription-pushes-stock-plummets-again?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772321100,"author":"EditorDavid","guid":155088,"unread":true,"content":"Friday was \"a horrible day\" for investors in Duolingo, reports Fast Company. But Friday's one-day 14% drop is just part of a longer story. \n\nSince last May, Duolingo's stock has dropped 81%. Yes, the company faced a social media backlash that month after its CEO promised they'd become an \"AI-first\" company (favoring AI over human contractors). And yes, Duolingo did double its language offerings using generative AI. But more importantly, that summer OpenAI showed how easy it was to just roll your own language-learning tool from a short prompt in a GPT-5 demo, while Google built an AI-powered language-learning tool into its Translate app. \n\n\nAnd yet, Friday Duolingo's shares dropped another 14%, after announcing good fourth quarter results but an unpopular direction for its future. Fast Company reports:\n\n\nOn the surface, many of the company's most critical metrics saw decent gains for the quarter, including: \n — Daily Active Users: 52.7 million (up 30% year-over-year) \n — Paid Subscribers: 12.2 million (up 28% year-over-year) \n — Revenue: $282.9 million (up 35% year-over-year) \n — Total bookings: $336.8 million (up 24% year-over-year) \n\nThe company also reported its full-year 2025 financials, revealing that for the first time in its history, it crossed the $1 billion revenue mark for a fiscal year. \n\nBut the Motley Fool explains that Duolingo's higher ad loads and repeated pushes for subscription plans \"generated revenues in the short term, but made the Duolingo platform less engaging. Ergo, user growth decelerated while revenues rose.\" Thursday Duolingo announced a big change to address that, including moving more features into lower-priced tiers. Barron's reports:\n\nD.A. Davidson analyst Wyatt Swanson, who rates Duolingo stock at Neutral, posited that the push to monetize \"led to disgruntled users and a meaningful negative impact to 'word-of-mouth' marketing.\" Duolingo has guided for bookings growth between 10% and 12% in 2026, compared with the 20% rate the company would have expected to see \"if we operated like we have in past years....\"\nIf stock reaction is any indication, investors are concerned about Duolingo's new focus.","contentLength":2171,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Duolingo Grows, But Users Disliked Increased Ads and Subscription Pushes. Stock Plummets Again","url":"https://slashdot.org/story/26/02/28/2321238/duolingo-grows-but-users-disliked-increased-ads-and-subscription-pushes-stock-plummets-again?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772321100,"author":"EditorDavid","guid":155099,"unread":true,"content":"Friday was \"a horrible day\" for investors in Duolingo, reports Fast Company. But Friday's one-day 14% drop is just part of a longer story. \n\nSince last May, Duolingo's stock has dropped 81%. Yes, the company faced a social media backlash that month after its CEO promised they'd become an \"AI-first\" company (favoring AI over human contractors). And yes, Duolingo did double its language offerings using generative AI. But more importantly, that summer OpenAI showed how easy it was to just roll your own language-learning tool from a short prompt in a GPT-5 demo, while Google built an AI-powered language-learning tool into its Translate app. \n\n\nAnd yet, Friday Duolingo's shares dropped another 14%, after announcing good fourth quarter results but an unpopular direction for its future. Fast Company reports:\n\n\nOn the surface, many of the company's most critical metrics saw decent gains for the quarter, including: \n — Daily Active Users: 52.7 million (up 30% year-over-year) \n — Paid Subscribers: 12.2 million (up 28% year-over-year) \n — Revenue: $282.9 million (up 35% year-over-year) \n — Total bookings: $336.8 million (up 24% year-over-year) \n\nThe company also reported its full-year 2025 financials, revealing that for the first time in its history, it crossed the $1 billion revenue mark for a fiscal year. \n\nBut the Motley Fool explains that Duolingo's higher ad loads and repeated pushes for subscription plans \"generated revenues in the short term, but made the Duolingo platform less engaging. Ergo, user growth decelerated while revenues rose.\" Thursday Duolingo announced a big change to address that, including moving more features into lower-priced tiers. Barron's reports:\n\nD.A. Davidson analyst Wyatt Swanson, who rates Duolingo stock at Neutral, posited that the push to monetize \"led to disgruntled users and a meaningful negative impact to 'word-of-mouth' marketing.\" Duolingo has guided for bookings growth between 10% and 12% in 2026, compared with the 20% rate the company would have expected to see \"if we operated like we have in past years....\"\nIf stock reaction is any indication, investors are concerned about Duolingo's new focus.","contentLength":2171,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why did Netflix back down from its deal to acquire Warner Bros.?","url":"https://techcrunch.com/2026/02/28/why-did-netflix-back-down-from-its-deal-to-acquire-warner-bros/","date":1772316468,"author":"Anthony Ha","guid":155075,"unread":true,"content":"<article>Netflix's co-CEO reportedly told Trump, \"I took your advice.\"</article>","contentLength":61,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New 'Star Wars' Movies Are Coming to Theatres. But Will Audiences?","url":"https://entertainment.slashdot.org/story/26/02/28/0514259/new-star-wars-movies-are-coming-to-theatres-but-will-audiences?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772314440,"author":"EditorDavid","guid":155078,"unread":true,"content":"\"The drought of upcoming Star Wars movies is coming to an end soon,\" writes Cinemablend. In May the The Mandalorian and Grogu opens, and one year later there's the release of the Ryan Gosling-led Star Wars: Starfighter. \n\nBut \"there are some insiders who already believe that Starfighter will be a bigger hit than The Mandalorian and Grogu...\"\n\nAccording to unnamed sources who spoke with Variety, there's a \"sense\" that Star Wars: Starfighter, which is directed by Deadpool &amp; Wolverine's Shawn Levy, will be a more satisfying viewing experience. These same sources are allegedly impressed by the early footage they've seen of Ryan Gosling's performance and also suggested that Levy has \"recaptured the franchise's spirit of fun.\" Furthermore, the article states that there's concern that because The Mandalorian and Grogu is spinning out of a streaming-exclusive series, it might not have as much appeal to people who aren't already fans of The Mandalorian... Star Wars: Starfighter, on the other hand, will be accessible to everyone equally. It's set five years after The Rise of Skywalker, which is an unexplored period for the Star Wars franchise onscreen. It's also expected that most, if not all of its featured characters will be brand-new, so no knowledge of past adventures is required. \nSlashdot reader gaiageek reminds us that 2027 will also see a special 50-year anniversary event in movie in theatres: a \"newly restored\" version of the original 1977 Star Wars.","contentLength":1473,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What to know about the landmark Warner Bros. Discovery sale","url":"https://techcrunch.com/2026/02/28/warner-bros-netflix-paramount-acquisition-timeline-wbd/","date":1772314086,"author":"Lauren Forristal","guid":155067,"unread":true,"content":"<article>Learn more about Paramount's planned acquisition of Warner Bros. Discovery — a historic Hollywood megadeal valued at $111 billion — as it continues to develop.</article>","contentLength":163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthropic’s Claude rises to No. 1 in the App Store following Pentagon dispute","url":"https://techcrunch.com/2026/03/01/anthropics-claude-rises-to-no-2-in-the-app-store-following-pentagon-dispute/","date":1772312706,"author":"Anthony Ha","guid":155066,"unread":true,"content":"<article>Anthropic’s chatbot Claude seems to have benefited from the attention around the company’s fraught negotiations with the Pentagon.</article>","contentLength":134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The billion-dollar infrastructure deals powering the AI boom","url":"https://techcrunch.com/2026/02/28/billion-dollar-infrastructure-deals-ai-boom-data-centers-openai-oracle-nvidia-microsoft-google-meta/","date":1772311315,"author":"Russell Brandom","guid":155065,"unread":true,"content":"<article>Here's everything we know about the biggest AI infrastructure projects, including major spending from Meta, Oracle, Microsoft, Google, and OpenAI.</article>","contentLength":146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US Threatens Anthropic with 'Supply-Chain Risk' Designation. OpenAI Signs New War Department Deal","url":"https://tech.slashdot.org/story/26/02/28/2028232/us-threatens-anthropic-with-supply-chain-risk-designation-openai-signs-new-war-department-deal?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772310840,"author":"EditorDavid","guid":155071,"unread":true,"content":"It started Friday when all U.S. federal agencies were ordered to \"immediately cease\" using Anthropic's AI technology after contract negotiations stalled when Anthropic requested prohibitions against mass domestic surveillance or fully autonomous weapons. But later Friday there were even more repercussions... \n\n\nIn a post to his 1.1 million followers on X.com, U.S. Secretary of War Pete Hegseth criticized Anthropic for what he called \"a master class in arrogance and betrayal as well as a textbook case of how not to do business with the United States Government or the Pentagon.\"\n\nOur position has never wavered and will never waver: the Department of War must have full, unrestricted access to Anthropic's models for every LAWFUL purpose in defense of the Republic... Cloaked in the sanctimonious rhetoric of \"effective altruism,\" [Anthropic and CEO Dario Amodei] have attempted to strong-arm the United States military into submission — a cowardly act of corporate virtue-signaling that places Silicon Valley ideology above American lives. The Terms of Service of Anthropic's defective altruism will never outweigh the safety, the readiness, or the lives of American troops on the battlefield. Their true objective is unmistakable: to seize veto power over the operational decisions of the United States military. That is unacceptable... \n\nIn conjunction with the President's directive for the Federal Government to cease all use of Anthropic's technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic... America's warfighters will never be held hostage by the ideological whims of Big Tech. This decision is final. \n\nMeanwhile, Anthrophic said on Friday that \"no amount of intimidation or punishment from the Department of War will change our position.\" (And \"We will challenge any supply chain risk designation in court.\")\nDesignating Anthropic as a supply chain risk would be an unprecedented action — one historically reserved for US adversaries, never before publicly applied to an American company. We are deeply saddened by these developments. As the first frontier AI company to deploy models in the US government's classified networks, Anthropic has supported American warfighters since June 2024 and has every intention of continuing to do so. We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government... Secretary Hegseth has implied this designation would restrict anyone who does business with the military from doing business with Anthropic. The Secretary does not have the statutory authority to back up this statement. \n\nAnthropic also defended the two exceptions they'd requested that had stalled contract negotiations. \"[W]e do not believe that today's frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger America's warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights.\" \n\n\nAlso Friday, OpenAI announced that \"we reached an agreement with the Department of War to deploy our models in their classified network.\"\n\nOpenAI CEO Sam Altman emphasized that the agreement retains and confirms OpenAI's own prohibitions against using their products for domestic mass surveillance — and requires \"human responsibility\" for the use of force including for autonomous weapon systems. \"The Department of War agrees with these principles, reflects them in law and policy, and we put them into our agreement. We also will build technical safeguards to ensure our models behave as they should, which the Department of War also wanted. \"\n\nWe are asking the Department of War to offer these same terms to all AI companies, which in our opinion we think everyone should be willing to accept. We have expressed our strong desire to see things de-escalate away from legal and governmental actions and towards reasonable agreements. We remain committed to serve all of humanity as best we can. The world is a complicated, messy, and sometimes dangerous place.\n","contentLength":4349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Antarctica's Massive Neutrino Observatory Gets an Upgrade","url":"https://science.slashdot.org/story/26/02/28/0632201/antarcticas-massive-neutrino-observatory-gets-an-upgrade?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772307240,"author":"EditorDavid","guid":155058,"unread":true,"content":"There's already 5,000 sensors embedded in Antarctica's ice to look for evidence of neutrinos, reports the Washington Post. But in November scientists drilled six new holes at least a mile and a half deep and installed cables with hundreds more light detectors — an upgrade to the massive 15-year-old IceCube Neutrino Observatory to detect the charged particles produced by lower-energy neutrinos interacting with matter:\n\n\n\nWhen they do, the neutrinos produce charged particles that travel through the ice at nearly the speed of light, creating a blue glow called Cherenkov radiation... \"Within the first couple years, we should be making much better measurements,\" [said Erin O'Sullivan, an associate professor of physics at Uppsala University in Sweden and a spokesperson for the project.] \"There's hope to expand the detector, by an order of magnitude in volume, so the important thing there is we're not just seeing a few neutrino point sources, but we're starting to be a true telescope. ... That's really the dream.\" \n\nThe scientists spent seven years planning the upgrade, according to the article. \"To drill holes a mile and a half deep takes about 30 hours, and 18 more hours to return to the surface,\" the article points out. \"Then, the race begins because almost immediately, the hole starts to shrink as the water refreezes.\" (\"If it takes too much time, the principal investigator says, \"the instruments don't fit in anymore!\")","contentLength":1442,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Simplest Way to Understand How LLMs Actually Work!","url":"https://hackernoon.com/the-simplest-way-to-understand-how-llms-actually-work?source=rss","date":1772305206,"author":"Amit Juneja","guid":155087,"unread":true,"content":"<p>The magic of transformers lies in their attention mechanism. But what does that actually mean?</p><p>\\\nHere's a simplified explanation to build intuition.</p><p>Consider: \"What is the capital of France?\"</p><p>As humans, we parse this as:</p><ul><li>\"What\" signals a question</li><li>\"is\" indicates the current timeframe</li><li>\"capital\" means the main city</li><li>\"France\" is the country for which I want the capital</li></ul><p>We process it instantly. But for a computer? Different story.</p><h2>THE ATTENTION MECHANISM: Q, K, V</h2><p>Transformers use a clever trick: for every word (technically tokens), the model creates three different representations:</p><p><strong>Query (Q) - \"What information am I looking for?\"</strong></p><p>For the word \"capital,\" the query is something like: \"What kind of entity am I describing?\"</p><p><strong>Key (K) - \"What information can I provide?\"</strong></p><p>Every word gets a key that describes what it offers. For the word \"capital,\" the key is something like: \"I'm a noun describing geographic/political entities.\"</p><p><strong>Value (V) - \"Here's my actual meaning.\"</strong></p><p>The word \"capital\" has the semantic meaning \"main city, governmental center, and administrative importance.\"</p><p>The model compares the query from one word against the keys of all other words. This produces .</p><p>Here is what happens when the word \"capital\", with its query of \"What kind of entity am I describing?\", checks against the keys of all the other words:</p><ul><li>\"France\" responds with its key → </li><li>\"What\" responds with </li><li>\"is\" responds with </li></ul><p>Higher scores contribute more to the final understanding. So after this, the representation of \"capital\" is enriched with strong context from \"France.\"</p><p>This doesn't happen just once. Transformers use  running in parallel, like several people reading the same sentence, each noticing different patterns. One might focus on grammar, another on meaning, another on long-range dependencies.</p><p>In another head, the word \"capital\" could be querying for the timeframe. In this case, the word \"is\" will give a high score for the current time.</p><p>All these attention scores combined give a rich context to each word. So the word \"capital\" knows that it is a question, it is for the current timeframe, and it is about \"France.\"</p><p>After each attention layer, information flows through a Feed Forward Network. This is where the answers start to form. This network processes the context-enriched representations, helping build toward output predictions like 'Paris.'</p><p>The combination of attention + FFN, repeated across layers, gives transformers their power.</p><p>Unlike older models that processed words one at a time, transformers:</p><ul><li>Look at the entire sentence at once</li><li>Let every word \"attend to\" every other word</li><li>Capture relationships between distant words</li><li>Build understanding through multiple layers</li></ul><p>That's transformer attention in action.</p><p>*This explanation simplifies many technical details to focus on core concepts. For a deeper dive, check out \"Attention Is All You Need\" by Vaswani et al.*</p>","contentLength":2839,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"'World's Largest Battery' Soon At Google Data Center: 100-Hour Iron-Air Storage","url":"https://hardware.slashdot.org/story/26/02/28/0446211/worlds-largest-battery-soon-at-google-data-center-100-hour-iron-air-storage?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772303640,"author":"EditorDavid","guid":155019,"unread":true,"content":" Interesting Engineering reports:\n\nUS tech giant Google announced on Tuesday that it will build a new data center in Pine Island, Minnesota. The new facility will be powered by 1.9 gigawatts (GW) of clean energy from wind and solar, coupled with a 300-megawatt battery, claimed to be the 'world's largest', with a 30-gigawatt-hour (GWh) capacity and 100-hour duration... The planned battery would dwarf a 19 GW lithium-ion project in the UAE... \n\nForm Energy's batteries work very differently from most large batteries today. Instead of using lithium like the batteries in electric cars, they store electricity by making iron rust and then reversing the rusting process to release the energy when needed... Form's iron-air batteries are heavier and less efficient than their counterparts; they can only return about 50% to 70% of the energy used to charge them, while lithium-ion batteries return more than 90%. However, Form's batteries have one distinct advantage. They are cheaper than lithium-ion batteries, costing about $20 per kilowatt-hour of storage, which is almost three times as cheap... It will store 150 MWh of electricity and can supply to the grid for up to 100 hours, delivering about 1.5 MW at peak output.\n \nThanks to long-time Slashdot reader schwit1 for sharing the article.","contentLength":1295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"After US-Israel Attacks, 90 Million Iranians Lose Internet Connectivity","url":"https://news.slashdot.org/story/26/02/28/1733240/after-us-israel-attacks-90-million-iranians-lose-internet-connectivity?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772300100,"author":"EditorDavid","guid":155018,"unread":true,"content":"CNN reports that images from Iran's capital \"have shown cars jammed along Tehran's street, with heavy traffic on major roads after today's wave of attacks by the US and Israel.\" And though Iran has a population of 93 million, the attacks suddenly plunged Iran into \"a near-total internet blackout with national connectivity at 4% of ordinary levels,\" according to internet monitoring experts at NetBlocks. \n\nCNN reports:\n\nSince Iran's brutal crackdown earlier this year, the regime has made progress to allow only a subset of people with security clearance to access the international web, experts said. After previous internet shutdowns, some platforms never returned. The Iranian government blocked Instagram after the internet shutdown and protests in 2022, and the popular messaging app Telegram following protests in 2018. \n\n\nThe International Atomic Energy Agency announced an hour ago that they're \"closely monitoring developments\" — keeping in contact with countries in the region and so far seeing \"no evidence of any radiological impact.\" They're also urging \"restraint to avoid any nuclear safety risks to people in the region.\" \n\nUPDATE (1 PM PST):\nQatar, Bahrain and Kuwait \"are shifting to remote learning starting Sunday until further notice following Iranâ(TM)s retaliatory strikes on Saturday,\" reports CNN.","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"America's Teenagers Say AI Cheating Has Become a Regular Feature of Student Life","url":"https://news.slashdot.org/story/26/02/28/0541228/americas-teenagers-say-ai-cheating-has-become-a-regular-feature-of-student-life?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772296440,"author":"EditorDavid","guid":154993,"unread":true,"content":"Tuesday Pew Research announced their newest findings: that 54% of America's teens use AI help with schoolwork:\nOne-in-five teens living in households making less than $30,000 a year say they do all or most of their schoolwork with AI chatbots' help. A similar share of those in households making $30,000 to just under $75,000 annually say this. Fewer teens living in higher-earning households (7%) say the same.\" \n\n\"The survey did not ask students whether they had used chatbots to write essays or generate other assignments...\" notes the New York Times. \"But nearly 60% of teenagers told Pew that students at their school used chatbots to cheat 'very often' or 'somewhat often.'\" Agreeing with that are the Pew Researchers themselves. \"Our survey shows that many teens think cheating with AI has become a regular feature of student life.\" \n\nOne worried teenager still told the researchers that AI \"makes people lazy and takes away jobs.\" But another teenager told the researchers that \"Everyone's going to have to know how to use AI or they'll be left behind.\" \n\nThanks to long-time Slashdot reader theodp for sharing the article.","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI’s Sam Altman announces Pentagon deal with ‘technical safeguards’","url":"https://techcrunch.com/2026/02/28/openais-sam-altman-announces-pentagon-deal-with-technical-safeguards/","date":1772295456,"author":"Anthony Ha","guid":154979,"unread":true,"content":"<article>OpenAI's CEO claims its new defense contract includes protections addressing the same issues that became a flashpoint for Anthropic.</article>","contentLength":132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Researchers Measure, Detect and Benchmark AI Manipulation","url":"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss","date":1772295326,"author":"Tencent","guid":155015,"unread":true,"content":"<ol><li>Enes Altuncu, ea483@kent.ac.uk (University of Kent, UK)</li><li>Virginia N. L. Franqueira, V.Franqueira@kent.ac.uk (University of Kent, UK)</li><li>Shujun Li, S.J.Li@kent.ac.uk (University of Kent, UK)</li></ol><p>Recent advancements in AI, especially deep learning, have contributed to a significant increase in the creation of new realistic-looking synthetic media (video, image, and audio) and manipulation of existing media, which has led to the creation of the new term “deepfake”. Based on both the research literature and resources in English and in Chinese, this paper gives a comprehensive overview of deepfake, covering multiple important aspects of this emerging concept, including 1) different definitions, 2) commonly used performance metrics and standards, and 3) deepfake-related datasets, challenges, competitions and benchmarks. In addition, the paper also reports a meta-review of 12 selected deepfake-related survey papers published in 2020 and 2021, focusing not only on the mentioned aspects, but also on the analysis of key challenges and recommendations. We believe that this paper is the most comprehensive review of deepfake in terms of aspects covered, and the first one covering both the English and Chinese literature and sources.</p><p>: Deepfake, Survey, Definition, Datasets, Benchmarks, Challenges, Competitions, Standards, Performance Metrics.</p><p>Recent advancements in AI and machine learning have increased the capability to produce more realistic media, e.g., video, image, and audio. Especially, state-of-the-art deep learning methods enabled the generation of “deepfakes”, manipulated or synthetic media the realness of which are not easily recognisable by the human eye. Although deepfake is a relatively new phenomenon (having first appeared at the end of 2017), its growth has been remarkable. According to the 2019 and 2020 Deeptrace reports on the state of deepfake [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark139\">2</a>], the number of deepfake videos in the English-speaking internet grew from 7,964 (December 2018) to 14,678 (July 2019) to 85,047 (December 2020), representing a 968% increase from 2018 to 2020.</p><p>In this work, we review existing deepfake-related research ecosystem in terms of various aspects, including performance metrics and standards, datasets, challenges, competitions, and benchmarks. Furthermore, we provide a meta-review of 12 selected deepfake-related survey papers which covers several additional aspects other than the mentioned ones in a systematic manner, such as performance comparison, key challenges, and recommendations.</p><p>Despite being a hugely popular term, there is a lack of consensus on the definition of “deepfake” and the boundary between deepfakes and non-deepfakes is not clear cut. For this survey, we adopt a relatively more inclusive approach to cover all forms of manipulated or synthetic media that are considered deepfakes in a broader sense. We also cover closely related topics including biometrics and multimedia forensics, since deepfakes are often used to launch presentation attacks against biometrics-based authentication systems and detection of deepfakes can be considered part of multimedia forensics. A more detailed discussion on different definitions of “deepfake” is given next.</p><h2>1.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Definitions of the Term Deepfake</h2><p>As its name implies, the term “deepfake” is derived from the combination of “deep” (referring to  (DL)) and “fake”. It is normally used to refer to manipulation of existing media (image, video and/or audio) or generation of new (synthetic) media using DL-based approaches. The most commonly discussed deepfake data are fake face images, fake speech forgeries, and fake videos that combine both fake images and fake speech forgeries. While having “fake” in the word indicates manipulated or synthesised media, there are plenty of benign applications of the deepfake technology, e.g., for entertainment and creative arts. With this respect, another term “deep synthesis” has been proposed as a more neutral-sounding alternative [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark197\">60</a>]. This new term, however, has not been widely adopted.</p><p>In addition to the lack of a universal definition, as mentioned already, the boundary between deepfakes and non-deep fakes is actually not a clear cut. There are at least two important aspects we should consider, one on detection of and the other on creation of deepfakes.</p><p>First, detection of deepfakes often follows very similar approaches to detection of traditional fakes generated without using DL techniques. Advanced detection methods have also started leveraging DL to improve their performance, but they do not necessarily need to know how a target media is created (deep or not). To some extent, one could argue that detecting deepfakes does not involve developing deepfake-specific methods (even though some researchers choose to do so), but a more robust and universal detector that can handle any (deep or not) fake media. This can be seen for two closely related topics: biometrics and multimedia forensics. For biometrics, there is a trend of using deep learning techniques to generate fake biometric signals (e.g., face images and videos) for biometric spoofing or presentation attacks. For multimedia forensics, deepfake-based forgeries have become a new threat to the traditional problem of “forgery detection”. For both topics, detection of biometric spoofing and multimedia forgeries have evolved to consider both deep and non-deep fakes.</p><p>Second, one may argue that the word “deep” in “deepfake” does not necessarily refer to the use of “deep learning”, but any “deep” (i.e., sophisticated) technology that creates a very believable fake media. For instance, Brady [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark146\">9</a>] considered deepfake as audio-visual manipulation using “a spectrum of technical sophistication … and techniques”. They also introduced two new terms,  and , referring to “low level manipulation of audio-visual media created with (easily) accessible software [or no software] to speed, slow, restage or re-contextualise content”. This broader understanding of “deepfake” has also been adopted by law makers for new legislations combating malicious deepfakes. For instance, the following two United States acts define “deepfakes” as follows:</p><ul><li>2018 Malicious Deep Fake Prohibition Act<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark1\">1</a>:</li></ul><p>§1041.(b).(2): “<em>the term ‘deep fake’ means an audiovisual record created or altered in a manner that the record would falsely appear to a reasonable observer to be an authentic record of the actual speech or conduct of an individual.</em>”</p><ul><li>2019 DEEP FAKES Accountability Act<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark2\">2</a>:</li></ul><p>§1041.(n).(3): “<em>The term ‘deep fake’ means any video recording, motion-picture film, sound recording, electronic image, or photograph, or any technological representation of speech or conduct substantially derivative thereof—</em></p><p><em>(A)&nbsp; which appears to authentically depict any speech or conduct of a person who did not in fact engage in such speech or conduct; and</em></p><p><em>(B)&nbsp; the production of which was substantially dependent upon technical means, rather than the ability of another person to physically or verbally impersonate such person.</em>”</p><p>As we can see from the above legal definitions of “deepfake”, the use of DL as a technology is not mentioned at all. The focus here is on “authenticity”, “impersonation” and (any) “technical means”.</p><h2>1.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Scope and Contribution</h2><p>Based on the above discussion on definitions of deepfake, we can see it is not always straightforward or meaningful to differentiate deepfakes from non-deep fakes. In addition, for our focus on performance evaluation and comparison, the boundary between deepfakes and non-deep fakes is even more blurred. This is because DL is just a special (deeper) form of machine learning (ML), and as a result, DL and non-deep ML methods share many common concepts, metrics and procedures.</p><p>Despite the fact that deepfake may be understood in a much broader sense, in this work, we have a sufficiently narrower focus to avoid covering too many topics. We, therefore, decided to define the scope of this survey as follows:</p><ul><li>For metrics and standards, we chose to include all commonly used ones for evaluating general ML methods and those specifically defined for evaluating deepfake creation or detection methods.</li><li>For datasets, challenges, competitions and benchmarks, we considered those related to fake media covered in the deepfake-related survey papers and those with an explicit mention of the term “deepfake” or a comparable term.</li><li>For the meta-review, we considered only survey papers whose authors explicitly referred to the term “deepfakes” in the meta data (title, abstract and keywords).</li></ul><p>Research papers covered in this survey (i.e., the deepfake-related survey papers) were identified via systematic searches on the scientific databases, Scopus and China Online Journals (COJ)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark4\">3</a>. The following search queries were used to perform the searches on Scopus and COJ, respectively:</p><p>(deepfake* OR deep-fake* OR “deep fake*”) AND (review OR survey OR overview OR systemati* OR SoK)</p><p>(deepfake OR 深度伪造) AND (综述 OR 进展)</p><p>The searches returned 41 survey papers in English and 15 survey papers in Chinese. Out of these papers, eight published in English and four published in Chinese were selected for consideration.</p><p>Deepfake-related challenges, competitions and benchmarks were identified via multiple sources: the survey papers selected, research papers from the co-authors’ personal collections, Google Web searches, and manual inspection of websites of major AI-related conferences held in 2020 and 2021 (where such challenges and competitions are routinely organised). The inspected conferences include those listed in the ACL (Association for Computational Linguistics) Anthology<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark5\">4</a>, ICCV, CVPR, AAAI, ICML, ICLR, KDD, SIGIR, WWW, and many others. In addition, a comprehensive list of datasets was compiled based on the selected survey papers and the identified challenges, competitions, and benchmarks. Relevant standards were identified mainly via research papers covered in this survey, the co-authors’ personal knowledge, and Google Web searches. For performance metrics, we covered those commonly used based on relevant standards, the survey papers, and the identified challenges, competitions, and benchmarks.</p><p>In this survey, we focus on performance evaluation and comparison of deepfake generation and detection methods. The metrics used for such performance evaluations are at the core of our discussions. In this section, we review the performance metrics that are commonly used to evaluate deepfake generation and detection algorithms. Note that all metrics covered in this section are also commonly used for evaluating performance of similar systems that are not for generating or detecting deepfakes. Therefore, this section can be seen as a very brief tutorial on general performance metrics.</p><p>In the last subsection, we also briefly discuss how the related performance metrics are covered in formal standards. By “formal standards”, we refer to standards defined following a formal procedure, often by one or more established standardisation bodies such as the International Organization for Standardization (ISO)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark6\">5</a> and the International Electrotechnical Commission (IEC)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark7\">6</a>. Note that we consider a broad range of documents defined to be standards by standardisation bodies, e.g., International Telecommunication Union (ITU)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark8\">7</a> recommendations and ISO technical reports (TRs).</p><h2>3.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The Confusion Matrix</h2><p>Deepfake detection is primarily a binary classification problem. A binary classifier takes an input that is  or  and outputs a binary value denoting it to be  or . For example, a deepfake detection system will take a suspected image as the input that may be  or  and output  or .</p><p>A fundamental tool used in evaluating a binary classifier is the  that summarises the success and failure of the classification model. On one axis are the two  values and on the other axis are the two  values. The classification is  (true positive and true negative) when the actual and the predicted values match. It is  (false positive and false negative) when the actual and predicted values do not match. Table <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark9\">1</a> shows the confusion matrix for a binary deepfake classifier (detector). The two cells in green, TP (the number of ) and TN (the number of ), indicate correct prediction results, and the two cells in red, FN (the number of ) and FP (the number of ), indicate two different types of errors when making incorrect prediction results.</p><p>\\\nTable 1: Confusion matrix for a binary classifier for detecting deepfake.</p><p>|    | fake (predicted) | real (predicted) |\n|----|----|----|\n| fake (actual) | TP | FN |\n| real (actual) | FP | TN |</p><h2>3.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Precision and Recall</h2><p>Based on the four fundamental values introduced in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark3\">3.1</a>, i.e., TP, TN, FP and FN, we define two important performance metrics for a binary classifier –  and .</p><p>Precision of a binary classifier is defined as the fraction of  samples among all the . In the confusion matrix, it is the fraction of true samples in the first column. It can be formally defined as Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark10\">1</a>).</p><p>When the “natural” ratio between positive and negative samples is significantly different from the test set, it is often useful to adjust the weight of the false positives, which leads to the  (wP) defined in Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark11\">2</a>), where  0 is a weight determined by the ratio between the negative and positive samples.</p><p>Recall of a binary classifier is the fraction of  samples among the  samples, as shown in Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark12\">3</a>). In the confusion matrix, it is the fraction of true samples in the first row.</p><p>Let us consider an example binary classifier that predicts if an image from a database containing both deepfake and real (authentic) images is fake or not. Precision of the classifier is the fraction of correctly classified images among all images classified as deepfake. On the other hand, recall is the fraction of deepfake images identified by the classifier, among all deepfake images in the database.</p><h2>3.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; True and False Positive Rates</h2><p>Focusing on predicted positive samples, we can also define two metrics:  (TPR), also called  (CDR), as the fraction of the predicted positive samples among the actually positive samples and  (FPR), also called  (FAR), as the fraction of the predicted positive samples among the actually negative samples, as shown in Eqs. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark13\">4</a>) and</p><p>(<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark14\">5</a>). In the confusion matrix, TPR is the fraction of predicted positive samples in the first row and FPR is the fraction of predicted positive samples in the second row. Note that TPR is basically a different name for  (Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark12\">3</a>)).</p><h2>3.4&nbsp;&nbsp;&nbsp;&nbsp; True and False Negative Rates</h2><p>Similar to true and false positive rates, we can define two other rates focusing on negative predicted results:  (TNR) indicating the fraction of the predicted negative samples among the actually negative samples, and  (FNR) indicating the fraction of the predicted negative samples among the actually positive samples, as shown in Eqs. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark15\">6</a>) and (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark16\">7</a>).</p><h2>3.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sensitivity and Specificity</h2><p>In some applications of binary classifiers, especially in biology and medicine, the TPR and the TNR are more commonly used, and they are often called  (TPR) and  (TNR). The focus of these two terms is on the two types of correctness of the predicted results. These are less used in deepfake-related research, hence, we will not refer to them in the remainder of this paper.</p><p>Focusing on error rates means that we need to consider the FPR and the FNR. These two rates normally conflict with each other so that reducing one rate normally leads to an increase in the other. Therefore, rather than trying to reduce both error rates at the same time, which is normally impossible, the more realistic task in practical applications is to find the right balance so that they are both below an acceptable threshold.</p><p>In some applications, such as biometrics, people are particularly interested in establishing the so-called  (EER) or  (CER), the point where the FPR and the FNR are equal. The EER/CER is not necessarily a good metric for some applications, especially when the two types of errors are of different levels of importance, e.g., for detecting critical deepfakes (e.g., fake news that can influence how people cast their votes) we can often tolerate more false positives (false alarms) than false negatives (missed alarms).</p><h2>3.7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Accuracy and F-Score</h2><p>In addition to the EER/CER, there are also other metrics that try to reflect both types of errors, in order to give a more balanced indication of the overall performance of a binary classifier. The two most commonly used are  and  (also called ). Both metrics can be defined based on the four fundamental values (TP, TN, FP, and FN).</p><p>Accuracy of a binary classifier is defined as the fraction of  samples (true positives and true negatives) among the total number of samples that have been classified, as shown in Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark17\">8</a>).</p><p>The F-score of a binary classifier is actually a family of metrics. Its general form can be described based on a parameter  as defined in Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark18\">9</a>).</p><p>The most widely used edition of all F-scores is the so-called , which is effectively the F-score with  = 1. More precisely, it is defined as shown in Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark19\">10</a>).</p><h2>3.8&nbsp;&nbsp;&nbsp;&nbsp; Receiver Operating Characteristic Curve and Area Under Curve</h2><p><strong>Receiver operating characteristic</strong> (ROC) curves are commonly used to measure the performance of binary classifiers that output a score (or probability) of prediction.</p><p>Consider the following. Let  be the set of all test samples and let the output scores  () (for all  ∈ ) lie in the interval [] on the real line. Let  ∈ [] be a prediction threshold for the model, and assume that the classifiers works as follows for all  ∈ :</p><p>\\\nIt is easy to see that, for  = , all the samples will be classified as positive, leading to FN = TN = 0 so TPR = FPR = 1; while for  = , all the samples will be classified as negative, leading to FP = TP = 0 so TPR = FPR = 0. For other threshold values between  and , the values of TPR and FPR will normally be between 0 and 1. By changing  from  to  continuously, we can normally get a continuous curve that describes how the TPR and FPR values change from (0,0) to (1,1) on the 2D plane. This curve is the ROC curve of the binary classifier.</p><p>For a random classifier, assuming that  () distributes uniformly on [] for the test set, we can mathematically derive its ROC curve being the TPR = FPR line, whose area under the ROC curve (AUC) is 0.5. For a binary classifier that performs better than a random predictor, we can also mathematically prove that its AUC is always higher than 0.5, with 1 being the best possible value. Note that no binary classifier can have an AUC below 0.5, since one can simply flip the prediction result to get a better predictor with an AUC of 1 − AUC. The relationship between the ROC and the AUC is graphically illustrated in Figure <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark20\">1</a>.</p><p>Another widely used performance metric for binary classifiers that can return a probability score for the predicted label is . For a binary classification with a true label  ∈ {0*,* 1} and an estimated probability  = Pr( = 1), the log loss per sample is the negative log-likelihood of the classifier given the true label, defined as shown in Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark21\">12</a>).</p><p>Given a testing set with  samples, the log loss score of a binary classifier can be calculated using Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark22\">13</a>), where  is 1 if the -th sample is true and 0 if false, and ˆ is the predicted probability of  = 1.</p><h2>3.10&nbsp;&nbsp;&nbsp;&nbsp; Extension to Multi-class Classifiers</h2><p>All metrics that are defined based on the four basic values TP, TN, FP and FN can be easily extended to <strong>multi-class classification</strong> by considering the prediction to be true or false individually with respect to each class. For example, if the system is classifying animals (cats, dogs, horses, lions, tigers, etc.), then a true positive prediction of an image to be of a cat, would simultaneously be true negative predictions for the remaining classes (dogs, horses, lions, tigers, etc.). If an image of a cat is incorrectly predicted to be that of a dog, it would be a false negative with respect to a cat, a false positive with respect to a dog, and a true negative with respect to all other classes.</p><h2>3.11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Perceptual Quality Assessment (PQA) Metrics</h2><p>By definition, the main goal of deepfakes is to make it hard or impossible for human consumers (listeners or viewers) to distinguish fake media from real media. Therefore, when evaluating the quality of deepfake media, the quality perceived by human consumers of the media is key. This calls for subjective assessment of the perceptual quality of the deepfake media as the “gold standard”. The most widely used subjective perceptual quality assessment (PQA) metric for audio-visual signals is  (MOS), which has been widely used by the signal processing and multimedia communication communities, including digital TV and other multimedia-related consumer applications. As its name implies, MOS is calculated by averaging the subjective scores given by a number of human judges, normally following a numerical scale between 1 and 5 or between 0 and 100. MOS has been used in some deepfake-related challenges (see Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark113\">5.2</a>) and also for evaluating and comparing the quality (realness/naturalness) of deepfake datasets (see Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark64\">4.6</a>).</p><p>As a general subjective PQA metric, MOS has been standardised by the ITU<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark23\">8</a>. There are also ITU standards defining more specific subjective Video Quality Assessment (VQA) metrics and the standard procedures one should follow to conduct VQA user studies, e.g., ITU-T Recommendation P.910 “Subjective video quality assessment methods for multimedia applications”<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark24\">9</a>. Note that the ITU standards focus more on traditional perceptual quality, i.e., how good a signal looks or sounds, even if it looks or sounds not real (e.g., too smooth). On the other hand, for deepfakes, the focus is rather different because what matters is the realness and naturalness of the created media, i.e., how real and natural it looks or sounds, even if it is of low quality. To some extent, we can also consider realness and naturalness as a special aspect of perceptual quality.</p><p>One major problem of subjective PQA metrics like MOS is the need to recruit human judges and to have a well-controlled physical testing environment and protocol, which are not easy for many applications. To help reduce the efforts and costs of conducting PQA-related user studies, various objective PQA metrics have been proposed, where the term “objective” refers to the fact that such metrics are human-free, i.e., automatically calculated following a computational algorithm or process. Depending on whether a reference exists, such objective PQA metrics can be largely split into three categories: full-reference (FR) metrics (when the original “perfect-quality” signal is available as the reference), reduced-reference (RR) metrics (when some features of the original “perfect-quality” signal are available as the reference), and no-reference (NR) metrics (when the original signal is unavailable or such an original signal does not exist). For deepfakes, normally NR or RR metrics are more meaningful because the “fake” part of the word means that part of the whole data does not exist in the real world, hence a full reference cannot be obtained. RR metrics are still relevant because deepfakes are often produced for a target’s specific attributes (e.g., face and voice), where the reduced reference will be such attributes. NR metrics will be useful to estimate the realness and naturalness of a deepfake, simulating how a human judge would rate it in a controlled subjective PQA user study.</p><p>PQA is a very active research area and many PQA metrics have been proposed, some of which have been widely used in real-world products and services, e.g.,  (MSE), <strong>peak signal-to-noise ratio</strong> (PSNR) and <strong>structural similarity index measure</strong> (SSIM) for FR PQA of digitalimages and videos defined as in Eqs. (14), (15), and (16), respectively, where X = {xi} n i is the reference (the original signal), Y = {yi} n i is the signal whose visual quality is assessed, n is the number of pixels in X and Y , L is the maximum possible pixel value of X and Y (e.g., 255 for 8-bit gray-scale images), c1 = (k1L) 2 and c2 = (k2L) 2 ) are two stabilising parameters (k1 = 0.01 and k2 = 0.03 by default). For more about PQA metrics for different types of multimedia signals, we refer readers to some relevant surveys [3, 51, 72].</p><h2>3.12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; More about Standards</h2><p>Many of the basic performance metrics described in this section have been widely used by deepfake researchers as de facto standards, e.g., EER, log loss and MOS have been widely used in deepfake-related challenges (see Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark74\">5</a>). Also, the combination of precision, recall and F1-score has been widely used to assess performance of binary classifiers. While there have been a number of ITU standards on PQA to date, there does not seem to be many standardisation efforts on the performance metrics for evaluation of binary classifiers. This was the case until at least 2017, when ISO and IEC jointly set up the ISO/IEC JTC 1/SC 42<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark28\">10</a>, a standardisation subcommittee (SC) focusing on AI under ISO/IEC JTC 1<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark29\">11</a>, the joint technical committee for standardising “information technology”.</p><p>One recent effort that ISO/IEC JTC 1/SC 42 made is to produce the ISO/IEC TR 24029-1:2021 “Artificial Intelligence (AI) – Assessment of the robustness of neural networks – Part 1: Overview”<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark30\">12</a>, a technical report (TR) that systematically covers many commonly used performance assessment concepts, methods and metrics. Although the technical report has “neural networks” in its title, most performance assessment concepts, methods and metrics included are common ones for all supervised machine learning models.</p><p>In terms of performance metrics, two other ongoing work items of the ISO/IEC JTC 1/SC 42 that deserve attention are as follows:</p><ul><li>ISO/IEC DTS (Draft Technical Specification) 4213 “Information technology – Artificial Intelligence – Assessment of machine learning classification performance”<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark31\">13</a></li><li>ISO/IEC AWI (Approved Work Item) TS (Technical Specifications) 5471 “Artificial intelligence – Quality evaluation guidelines for AI systems”<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark32\">14</a></li></ul><p>While the ISO/IEC JTC 1/SC 42 was created very recently, another standardisation subcommittee under ISO/IEC JTC1 has a much longer history of nearly 20 years: the ISO/IEC JTC 1/SC 37<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark33\">15</a> that focuses on biometrics-related technology. This standardisation subcommittee is highly relevant for deepfake since deepfake faces can be used to spoof biometrics-based user authentication systems. In this context, the following three standards are of particular relevance:</p><p><strong>ISO/IEC 19795-1:2021 “Information technology – Biometric performance testing and reporting – Part 1: Principles and framework”</strong><a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark34\">16</a>: This standard covers general metrics about evaluating biometric systems. Two major metrics in this context are  (FAR) and  (FRR), which refer to the standard FPR and FNR, respectively. This standard also deprecates the use of single-number metrics including the EER and AUC (which were widely used in biometrics-related research in the past).</p><p><strong>ISO/IEC 30107-1:2016 “Information technology – Biometric presentation attack detec-tion – Part 1: Framework”</strong><a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark36\">17</a>: This standard defines a general framework about <strong>presentation attack detection</strong> (PAD) mechanisms, where the term “” refers to the “<em>presentation of an artefact or of human characteristics to a biometric capture subsystem in a fashion intended to in-terfere with system policy</em>”. It focuses on biometric recognition systems, where a PAD mechanism is a binary classifier trying to predict presentation attacks (also called attack presentations, e.g., fake faces) as positive and bona fide (real) presentations as negative.</p><p><strong>ISO/IEC 30107-3:2017 “Information technology – Biometric presentation attack detection – Part 3: Testing and reporting”</strong><a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark37\">18</a>: This standard defines a number of special performance metrics for evaluating PAD mechanisms standardised in the ISO/IEC 30107-1:2016. Three such metrics look at error rates: <strong>attack presentation classification error rate</strong> (APCER) referring to the standard FPR, <strong>normal/bona fide presentation classification error rate</strong> (NPCER/BPCER) referring to the standard FNR, and <strong>average classification error rate</strong> (ACER) that is defined as the average of the APCER and the NPCER/BPCER. Such metrics have been used in biometrics-related challenges such as Face Anti-spoofing (Presentation Attack Detection) Challenges<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark38\">19</a>. When deepfake images or videos are used to spoof a biometric system, such standardised metrics will become relevant.</p><p>This section provided a comprehensive summary of performance metrics used for evaluating and bench-marking binary classifiers. It is rare that all such metrics are used for a specific application. Instead, one or several are chosen based on specific needs. For a deepfake detection system as a binary classifier, many researchers have chosen to use overall metrics such as accuracy, AUC, EER and log loss, but the combination of precision, recall and F1-score is also common. Some deepfake-related challenges and competitions have introduced their own specific metrics, some of which will be described in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark74\">5</a>. The use of different performance metrics can make comparison of different reported results more difficult, so we hope the expected new ISO/IEC standard particularly ISO/IEC 4213 will help.</p><p>It is worth mentioning that, in addition to evaluating performance of deepfake detectors, the introduced performance metrics for evaluating binary classifiers can also be used to evaluate performance of deepfake generation methods by considering how deepfake detectors fail. For instance, organisers of the Voice Conversion Challenge 2018 and 2020 used this approach to benchmark how well voice conversion (VC) systems can generate high-quality fake speech samples.</p><p>Another point we would like to mention is that for deepfake videos there are two levels of performance metrics: those at the frame level (metrics of each frame), and those at the video level (metrics for the whole video). Generally speaking, the latter can be obtained by averaging the former for all frames, potentially following an adaptive weighting scheme, so that more important (key) frames will be counted more.</p><p>In this section, we cover all deepfake-related datasets we identified from the meta-review of deepfake-related survey papers, deepfake-related challenges, competitions and benchmarks covered, one online collection of deepfake-related datasets on GitHub<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark39\">20</a>, and the co-authors’ personal collections. Table <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark40\">2</a> shows basic information about these datasets. We explain them in four categories: deepfake image datasets, deepfake video datasets, deepfake audio/speech datasets, and hybrid deepfake datasets (mainly mixed image and video datasets).</p><p>Note that many datasets of real (authentic) media were also used by deepfake researchers for two purposes. First, any detectors would need both fake and real media to demonstrate their performance. Second, real media have also been used to train deepfake generators as the training set. In this section, we include only datasets containing deepfake media, some of which contain both deepfake and real media.</p><p>Some datasets, especially those created for deepfake-related challenges and competitions, have separate subsets for training and evaluation (testing) purposes. The split is necessary for such challenges and competitions, but not very useful for people who just want to use such datasets. Therefore, in this section when introducing such datasets we will ignore that level of details and focus on the total number of data including the number of real and fake samples.</p><h2>4.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Deepfake Image Datasets</h2><p><strong>SwapMe and FaceSwap dataset</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark215\">78</a>]: This dataset contains 4,310 images, including 2,300 real images and 2,010 fake images created using FaceSwap<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark41\">21</a> and the SwapMe iOS app (now discontinued).</p><p><strong>Fake Faces in the Wild (FFW) dataset</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark169\">32</a>]: This dataset contains 131,500 face images, including 78,500 images extracted from 150 videos in the FaceForensics dataset and 53,000 images extracted from 150 fake videos collected from YouTube.</p><p><strong>generated.photos datasets</strong><a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark42\">22</a>: This is a number of commercial datasets provided by the Generated Media, Inc., with up to nearly 2.7 million synthetic face images generated by StyleGAN. A free edition with 10,000 128x128 synthetic images is made available for academic research. The website also provides an interactive face generator<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark43\">23</a> and an API<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark44\">24</a>. The generated.photos datasets have a good diversity: five age groups (infants, children, youth, adults, middle-aged), two genders (male and female), four ethnicities (white, black, Latino, Asian), four eye colours (brown, grey, blue, green), four hair colours (brown, black, blond, gray), three hair length (short, medium, long), facial expressions, three head poses (front facing, left facing, right facing), two emotions (joy and neutral), two face styles (natural, beautified). (According to a number of research papers we read, an earlier 100K-Faces dataset was released by generated.photos for academic research in 2018, which was used by many researchers. This dataset is not currently available any longer.)</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark138\">1</a>]: This dataset includes 19,457 face images, including 7,948 deepfake images generated from on 175 forged videos collected online and 11,509 real face images collected from various online sources. (Table 2 of the paper shows the dataset size is 19,509, but the dataset downloaded from pCloud contains just 19,457 images.)</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark167\">30</a>]: This dataset includes 100,000 synthesised face, bedroom, car and cat images by a GAN generator trained based on real images in the FFHQ<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark45\">25</a> and LSUN<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark46\">26</a> datasets (three object types – bedrooms, cars and cats – for the latter). Note that the name “100K-Generated-Images” was not a proper one as the authors [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark167\">30</a>] just used this to name a sub-folder of their Google Drive shared space, but it was used in one of the survey papers [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark202\">65</a>].</p><p><strong>Ding et al.’s swapped face dataset</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark154\">17</a>]: This dataset contains 420,053 images of celebrities, including 156,930 real ones downloaded using Google Image API and 263,123 fake face-swapped ones created using two different methods (Nirkin’s method and Auto-Encoder-GAN)</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark185\">48</a>]: This dataset includes 87,000 224x224 face images, generated by processing some StyleGAN-generated synthetic images using the GAN-fingerprint Removal approach (GANprintR) proposed by <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark185\">Neves et al.</a>. It is the replaced version of the  dataset, which contains 150,000 face images generated using an earlier version of GANprintR.</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark158\">21</a>]: This dataset includes 40,000 images, half real and half deepfake. The images were collected from four sources: the CelebA-HQ dataset<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark47\">27</a>, the Flickr-Faces-HQ dataset<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark48\">28</a>, the 100K-Faces dataset<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark49\">29</a> (not available any longer, see the description of generated.photos datasets), and <a href=\"https://thispersondoesnotexist.com/\">thisperson-doesnotexist.com</a>.</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark212\">75</a>]: This dataset includes 625,537 synthesised face images of 10,177 celebrities, with 43 rich attributes on face, illumination, environment and spoof types. The real images were selected from the CelebA dataset<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark50\">30</a>. The 43 attributes include 40 for real images, covering all facial components and accessories (e.g., skin, nose, eyes, eyebrows, lip, hair, hat, eyeglass), and 3 for fake images, covering spoof types, environments and illumination conditions.</p><p><strong>Diverse Fake Face Dataset (DFFD)</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark148\">11</a>]: This dataset contains 299,039 images, including 58,703 real images sampled from three datasets (FFHQ<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark51\">31</a>, CelebA<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark52\">32</a> and FaceForensics++<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark53\">33</a>) and 240,336 fake ones in four main facial manipulation types (identity swap, expression swap, attribute manipulation, and entire synthesis). The images cover two genders (male and female), a wide age groups (the majority between 21 and 50 years old), and both low- and high-quality levels.</p><h2>4.2&nbsp;&nbsp;&nbsp;&nbsp; Deepfake Video Datasets</h2><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark172\">35</a>]: This dataset contains 620 deepfake face videos, generated by face swapping without manipulation of audio, covering 32 subjects and two quality levels (high and low).</p><p> (FF) [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark192\">55</a>]: This dataset contains 1,004 face videos with over 500,000 frames, covering various quality levels and two types of facial manipulation. This dataset is now replaced by the larger FaceForensics++ dataset (see below).</p><p> (FF++) [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark193\">56</a>]: This dataset contains 5,000 face videos with over 1.8 million manipulated frames, including 1,000 real videos (with 509,914 frames) downloaded from YouTube, and 4,000 fake videos created using four face manipulation methods (Deepfakes, Face2Face, FaceSwap and NeuralTextures). The videos cover two genders (male and female), and three quality levels (VGA/480p, HD/720p, and FHD/1080p).</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark176\">39</a>]: This dataset contains 98 face videos, half (49) are real ones downloaded from Youtube, and the other half are fake ones generated using the FakeApp mobile application (which is now discontinued). The video dataset was created to used to demonstrate a deepfake video detection method based on detection of eye blinking behaviours, so all videos contain at least one eye-blinking event. All fake videos were created by swapping the original face in each of the real videos with the face of the actor Nicolas Cage<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark54\">34</a>, thus, only one subject is represented.</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark147\">10</a>]: This dataset contains 142 “in the wild” deepfake portrait videos, collected from a range of online sources including news articles, online forums, mobile apps, and research presentations. The videos are diverse, covering the source generative model, resolution, compression, illumination, aspect-ratio, frame rate, motion, pose, cosmetics, occlusion, content, and context.</p><p><strong>DFDC (Deepfake Detection Challenge) preview dataset</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark155\">18</a>]: This dataset contains 5,244 face videos of 66 subjects with both face and voice manipulation. It was released as a preview of the full dataset of the 2020 Deepfake Detection Challenge (DFDC, see below).</p><p><a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark55\">35</a>: This dataset contains 1,203 face videos of celebrities, including 408 real videos collected from YouTube with subjects of different ages, ethic groups and genders, and 795 deepfake videos synthesised from these real videos.</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark177\">40</a>]: This dataset contains 6,229 face videos of celebrities, including 590 real videos collected from YouTube with subjects of different ages, ethic groups and genders, and 5,639 deepfake videos synthesised from these real videos.</p><p><strong>DeepFake Detection (DFD) Dataset</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark157\">20</a>]: This dataset contains 3,363 face videos, covering 28 subjects, gender, and skin colour. It was created as a joint effort between two units of Google, Inc.: Google AI<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark56\">36</a> and JigSaw<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark57\">37</a>.</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark164\">27</a>]: This dataset contains 60,000 indoor face videos (with 17.6 million frames) generated by face swapping, covering 100 subjects, four skin tones (white, black, yellow, brown), two gen-ders (male and female), different age groups (20-45), 26 nationalities, 7 different angles, 8 face expressions, and different head poses.</p><p><strong>DFDC (Deepfake Detection Challenge) full dataset</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark155\">18</a>]: This dataset contains 128,154 face videos of 960 subjects, including 23,654 real videos from 3,426 paid actors and 104,500 deepfake videos created using eight different methods (DF-128, DF-256, MM/NN face swap, NTH, FSGAN, StyleGAN, refinement, and audio swap).</p><p>10<strong>(Face Forensics in the Wild) dataset</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark216\">79</a>]: This dataset contains 10,000 high-quality forgery videos, with video- and face-level annotations. The dataset focuses on a more challenging case for forgery detection: each video involves one to 15 individuals, but only some (a minority of) faces are manipulated.</p><p><strong>Korean DeepFake Detection Dataset (KoDF)</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark173\">36</a>]: This dataset contains 37,942 videos of paid subjects (395 Koreans and 8 Southeastern Asians), including 62,166 real videos and 175,776 fake ones created using six methods – FaceSwap, DeepFaceLab, FSGAN, First Order Motion Model (FOMM), Audio-driven Talking Face HeadPose (ATFHP) and Wav2Lip. The videos cover a balanced gender ratio and a wide range of age groups.</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark160\">23</a>]: This dataset contains 1,737 videos with 1,666,816 frames, including 1,339,843 real frames and 326,973 fake frames generated using the Deep Video Portraits (DVP) [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark171\">34</a>] method. The original videos were obtained from three sources: the dataset used in [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark170\">33</a>], the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark179\">42</a>], and YouTube. Most videos have a resolution of 1280×720.</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark218\">81</a>]: This dataset contains 7,314 face sequences extracted from 707 deepfake videos that were collected completely from the Internet. It covers diverse scenes, multiple persons in each scene and rich facial expressions. Different from other deepfake video datasets, WildDeepfake contains only face sequences not the full videos. This makes the dataset more like between an image dataset and a video one. We decided to keep it in the video category since the selection process was still more video-focused.</p><h2>4.3&nbsp;&nbsp;&nbsp;&nbsp; Deepfake Audio/Speech Datasets</h2><p>Voice conversion (VC) is a technology that can be used to modify an audio and speech sample so that it appears as if spoken by a different (target) person than the original (source) speaker. Obviously, it can be used to generate deepfake audio/speech samples. The biennial Voice Conversion Challenge<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark58\">38</a> that started in 2016 is a major challenge series on VC. Datasets released from this challenge series are very different from other deepfake datasets: the deepfake data is not included in the original dataset created by the organisers of each challenge, but in the participant submissions (which are retargeted/fake utterances produced by VC systems built by participants). The challenge datasets also include the evaluation (listening-based) results of all submissions. Some fake utterances may be produced by DL-based VC systems, so we consider all datasets from this challenge series relevant for our purpose of this survey.</p><p><strong>Voice Conversion Challenge 2016 database</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark199\">62</a>]: The original dataset created by the challenge organisers was derived from the DAPS (Device and Produced Speech) Dataset [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark184\">47</a>]. It contains 216 utterances (162 for training and 54 for testing) per speaker from 10 speakers. Participating teams (17) developed their own VC systems for all 25 source-target speaker pairs, and then submitted generated utterances for evaluation. At least six participating teams used DL-related techniques (LSTM, DNN) in their VC systems (see Table 2 of the result analysis paper<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark59\">39</a>), so the submitted utterances can certainly be considered deepfakes.</p><p><strong>Voice Conversion Challenge 2018 database</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark181\">44</a>]: The original dataset created by the challenge organisers was also based on the DAPS dataset. It contains 116 utterances (81 for training and 35 for testing) per speaker from 12 speakers in two different tasks (called Hub and Spoke). Participating teams (23 in total, all for Hub and 11 for Spoke) developed their own VC systems for all 16 source-target speaker pairs, and then submitted generated utterances for evaluation. Comparing with the 2016 challenge, more participating teams used DL-related techniques (e.g., WaveNet, LSTM, DNN, CycleGAN, DRM – deep relational models, and ARBM – adaptive restricted Boltzmann machines) in their VC systems.</p><p><strong>Voice Conversion Challenge 2020 database</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark207\">70</a>]: This dataset is based on the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark60\">40</a>, a bilingual (Finnish/English, German/English, and Mandarin/English) database. It contains 145 utterances (120 for training and 25 for testing) per speaker from 14 speakers for two different tasks (with 4 × 4 and 4 × 6 source-target speaker pairs, respectively). Participating teams (33 in total, out of which 31 for Task 1 and 28 for Task 2) developed their own VC systems for all source-target speaker pairs, and then submitted generated utterances for evaluation. Comparing with the 2018 challenge, DL-based VC systems were overwhelmingly used by almost all participating teams (WaveNet and WaveGAN among the most used DL-based building blocks).</p><p>A major set of deepfake speech datasets were created for the  (Automatic Speaker Verification Spoofing and Countermeasures) Challenge<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark61\">41</a> (2015-2021, held biannually). The datasets for the 2019 and 2021 contain speech data that can be considered deepfakes.</p><p><strong>ASVspoof 2019 Challenge database</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark204\">67</a>]: This dataset is based on the Voice Cloning Toolkit (VCTK) corpus<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark62\">42</a>, a multi-speaker English speech database captured from 107 speakers (46 males and 61 females). Two attack scenarios were considered: logical access (LA) involving spoofed (synthetic or converted) speech, and physical access (PA) involving replay attacks of previously recorded bona fide recordings). For our purpose in this survey, the LA scenario is more relevant. The LA part of the dataset includes 12,483 bona fide (real) utterances and 108,978 spoofed utterances. Some of the spoofed speech data for the LA scenario were produced using a generative model involving DL-based techniques such as long short-term memory (LSTM)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark63\">43</a>, WaveNet [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark187\">50</a>], WaveRNN [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark165\">28</a>], WaveCycleGAN2 [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark195\">58</a>]. Note that the challenge organisers did not use the term “deepfake” explicitly, despite the fact that the DL-generated spoofed speech data can be considered as deepfakes.</p><p><strong>ASVspoof 2021 Challenge – Logical Access Database</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark151\">14</a>]: This dataset contains bona fide and spoofed speech data for the logical access (LA) task. The challenge is still ongoing and we did not find a detailed paper on the dataset, so cannot include more details other than its size (7.8 GB after compression). Although we did not see details of the generative algorithms used to produce spoofed speech data, we believe similar DL-based algorithms were used like for the 2019 challenge.</p><p><strong>ASVspoof 2021 Challenge – Speech Deepfake Database</strong> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark152\">15</a>]: In 2021, the challenge included an explicitly defined track on deepfake, but the task description suggests that the organisers of the challenge considered a broader definition of the term “deepfake” by looking at spoofing human listeners rather than ASV (Automatic Speaker Verification) systems. The size of the dataset is 34.5 GB after compression.</p><p>Possibly because of the long history and wide participation of the community in the ASVspoof challenges for creating the dedicated datasets, there are very few other deepfake audio/speech datasets. One such dataset was created by a group of researcher from Baidu Research [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark142\">5</a>]. This dataset was created to demonstrate a proposed voice cloning method. It is relatively small, and contains 134 utterances, including 10 real ones, 120 cloned ones, and 4 manipulated ones. Another dataset was created by Google AI and Google News Initiative<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark65\">44</a>, but it was made part of the ASVspoof 2019 dataset. This dataset contains thousands of phrases spoken by 68 synthetic “voices” covering a variety of regional accents.</p><h2>4.4&nbsp;&nbsp;&nbsp;&nbsp; Hybrid Deepfake Datasets</h2><p><strong>NIST OpenMFC (Open Media Forensics Challenge) Datasets</strong><a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark66\">45</a>: These datasets were created by the DARPA Media Forensics (MediFor) Program<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark67\">46</a> for the 2020 OpenMFC<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark68\">47</a>. There are two GAN-generated deepfake datasets, one with more than 1,000 deepfake images and the other with over 100 deepfake videos. The datasets were made available to registered participants of the competition only.</p><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark162\">25</a>]: This dataset is named as “a versatile benchmark for comprehensive forgery analysis”. It contains 2,896,062 images and 221,247 videos, including 1,457,861 fake images and 121,617 fake videos. The videos and images cover seven image-level and eight video-level manipulation approaches, 36 different types of perturbations and more mixed perturbations, and a large number of annotation labels (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). The dataset is being used for supporting the Face Forgery Analysis Challenge 2021<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark69\">48</a> at the SenseHuman 2021 (3rd Workshop on Sensing, Understanding and Synthesizing Humans)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark70\">49</a>, co-located at the ICCV 2021 conference<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark71\">50</a>.</p><h2>4.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A Deepfake Dataset Generator</h2><p> [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark211\">74</a>]: This is not actually a dataset per se, but a system for producing large datasets more automatically, including generating deepfake datasets. One may argue the automatically generated datasets are fake since they are not produced from real-world scenes.</p><h2>4.6&nbsp;&nbsp;&nbsp;&nbsp; Subjective Quality of Deepfakes in Different Databases</h2><p>As mentioned in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark73\">4.7</a>, subjective quality evaluation is necessary to evaluate the realness, realisticness, and naturalness of deepfake media. While there has been very limited work on this topic, in 2020, Jiang et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark164\">27</a>] conducted a user study on realness of deepfake videos. They recruited 100 professional participants (most of whom are computer vision researchers), who were asked to evaluate the realness of 30 randomly selected videos from 7 deepfake video datasets (DeeperForensics-1.0, UADFV, DeepFake-TIMIT, Celeb-DF, FaceForensics++, Deep Fake Detection, and DFDC). Participants were asked to respond to the statement “The video clip looks real.” and gave scores following a five-point Likert scale (1 – clearly disagree, 2 – weakly disagree, 3 – borderline, 4 – weakly agree, 5 – clearly agree).</p><p>Table <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark72\">3</a> shows the results. Interestingly, we can see a huge difference between the realness levels of different datasets. What is probably quite surprising is that FaceForensics++, one of the most widely used deepfake datasets, has a very low MOS score and less than 9% of participants considered the 30 selected videos as real.</p><p>Table 3: Human-judged subjective quality (realness) of deepfake videos in 7 datasets. The MOS scores were not reported by <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark164\">Jiang et al.</a>, but calculated by us based on the raw data shown in Table 3 of [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark164\">27</a>].</p><h2>4.7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Discussion: Datasets</h2><p>Among all deepfake image and video datasets, a significant majority are about face images and videos. This is not surprising since face swapping, face attribution manipulation, and fully synthesised face images are among the hottest topics within deepfake research and real-world applications. We hope more non-face deepfake image and video datasets can be produced to support a broader range of research activities on deepfake.</p><p>The subjective quality results shown in Table <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark72\">3</a> indicate that it is important to check realness of deep-fake media to support any performance evaluation or comparison. To ensure that the quality evaluation of datasets is fair, transparent and reliable, standard procedures need defining and a common pool of qualified human experts should be used.</p><p>Many authors of deepfake-related datasets attempted to classify such datasets into different generations. Chronologically speaking, we could broadly split such datasets into two generations: before 2019 and since 2019. Typically, datasets created before 2019 are relatively less advanced and smaller, while those created after 2019 tend to be larger, more diverse (i.e., covering more attributes), and of higher quality (i.e., produced by more advanced generative models). This can also be seen from the data in Table <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark72\">3</a>, in which the top two datasets (DeeperForensics-1 and Celeb-DF) fall within the new generation (2020), while others belong to the old generation. In addition to the two generations, a newer generation has also emerged in 2021: a number of very recent datasets started focusing on more realistic deepfakes (i.e., in the wild) or more specified areas of deepfakes (e.g., 10 focusing on multiple faces in the same video, and KoDF focusing on Korean faces). This trend shows that the deepfake research community has grown significantly in the past few years so that narrower topics have also started gaining attention and interest from some researchers.</p><p>This section reviews initiatives aiming to advance the state-of-the-art of detection and generation of synthetic or manipulated media (such as video, image and audio) via competitions or challenges open to the public, and ongoing benchmarks tackling specific problems.</p><p>The Deepfake Detection Challenge (DFDC)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark75\">51</a> was an initiative promoted by an AI and Media Steering Committee<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark76\">52</a>, including BBC, Facebook, Amazon, Microsoft and New York Times, and some universities around the world including the University of Oxford. The competition remained open from 5 September 2019 till 31 March 2020, and involved 3 stages. At first, the DFDC preview dataset was released. At a later stage, the DFDC full dataset was also made available to the 2,114 participants of the competition incorporating face and audio swap techniques for generation of deepfake content. At the final stage, the submitted models were evaluated using a test dataset (referred to as the “black box dataset”) of 10,000 videos which included  deepfake videos. The best performance on the black box dataset had an accuracy of 65.18%, according to the released results [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark159\">22</a>]. Submissions were ranked<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark78\">53</a> according to the overall log loss score, as defined in Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark22\">13</a>). All top five ranked models (the winner had the lowest overall log loss) are available on GitHub. Results indicate how challenging the detection of deepfake is since the best accuracy was low and “<em>many submissions were simply random</em>”, according to Dolhansky et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark156\">19</a>]. Figure <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark77\">2</a> shows a screenshot of the leaderboard with the five finalists. The first top ranked model used MTCNN (Multi-tasked Cascaded Convolutional Network), the second used WS-DAN (Weakly Supervised Data Augumentation Network), and the third used the EfficientNetB7 architecture. Meta compiling the common themes observed in the winning models, they were: clever augmentations, architectures, and absence of forensics methods. Moving forward, they called for “<em>solutions that go beyond analysing images and video. Considering context, provenance, and other signals may be the way to improve deepfake detection models</em>”.</p><p>\\\nThe Automatic Speaker Verification Spoofing And Countermeasures Challenge Workshop (ASVspoof)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark79\">54</a> has been running biennially since 2015. This competition is organised by an international consortium that includes Inria and EURECOM (France), University of Eastern Finland, National Institute of Informatics (Japan), and Institute for Infocomm Research (Singapore). This year the ASVspoof challenge includes, for the first time, a sub-challenge focused on  where the envisioned use case is an adversary trying to fool a human listener. The metric used for evaluating performance of submitted solutions (i.e., classifiers) is EER. Four baseline solutions<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark80\">55</a> (also called “countermeasures”), each using a different technique, were made available to participants with their corresponding EER metric values. The ASVspoof 2021 Speech Deepfake Database containing audio recordings with original and spoofed utterances has also been made available. The competition involves three phases<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark81\">56</a>: a progress phase, an evaluation phase and a post-evaluation phase; it is unclear how teams move from one phase to the next. More information about the 2021 competition is available in the published evaluation plan [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark150\">13</a>]. The organisers of the competition noted that they opted for the EER as the performance evaluation met-ric for countermeasures submitted to the speech deepfake task for legacy reasons. They acknowledged, however, that “<em>EER reporting is deprecated</em> ” by the ISO/IEC 19795-1:2021<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark82\">57</a> standard. Despite the fact that only the 2021 ASVspoof competition contained a track explicitly related to deepfake, some data in the ASVspoof 2019 dataset (Logical Access task) used for the 2019 competition was generated using DL-based algorithms as mentioned in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark35\">4</a>. We expect that this also holds for the ASVspoof 2021 dataset (Logical Access task). The ASVspoof 2019 competition used the EER as secondary metric; the primary performance metric used was the tandem detection cost function (t-DCF) [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark200\">63</a>]. According to its evaluation plan [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark206\">69</a>], t-DCF assesses the performance of the whole tandem system whereby “<em>a CM [countermeasure] serves as a ‘gate’ to determine whether a given speech input originates from a bona fide (genuine) user, before passing it the main biometric verifier (the ASV system)</em>”. It is calculated according to Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark83\">17</a>), where  cm () and  cm() are, respectively, “<em>the miss rate and the false alarm rate of the CM system at threshold s</em>”.</p><p>For further information about Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark83\">17</a>), including constants 1 and 2, please refer to the ASVspoof 2019 evaluation plan [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark206\">69</a>].</p><p>An implementation of the t-DCF metric has been made available by the ASVspoof 2019’s organisers in Python<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark85\">58</a> and Matlab<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark86\">59</a> formats.</p><p>The Face Anti-spoofing (Presentation Attack Detection) Challenge<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark87\">60</a> started in 2019. Its first two editions were held at the 2019 and 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), respectively. Its third edition was moved to be co-located with the 2021 IEEE/CVF International Conference on Computer Vision (ICCV 2021). This competition series was organised by a group of researchers from academia and industry in China, Mexico, Spain, Finland and the US. The 2021 competition was focused on 3D high-fidelity mask attacks, and followed a 2-phased<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark88\">61</a> process. The first phase is the “development phase”; it started in April 2021 when the CASIA-SURF HiFiMask dataset<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark89\">62</a> was released to participants. The second phase is the “final ranking phase” (June 2021), when the competition ended. The competition adopted the following performance metrics for evaluation<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark90\">63</a> of the solutions submitted: attack presentation classification error rate (APCER), normal/bona fide presentation classification error rate (NPCER/BPCER), and average classification error rate (ACER), in accordance with the ISO/IEC 30107-3:2017<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark91\">64</a> standard. Figure <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark84\">3</a> provides the leaderboard for the top three solutions.</p><p>\\\nThe FaceForensics Benchmark<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark92\">65</a> is an ongoing automated benchmark for detection of face manipulation. The organisers of the benchmark made the FaceForensics++ dataset available for training. Manipulated videos (4,000 in total) were created using four techniques, i.e., two computer graphics-based approaches (Face2Face and FaceSwap) and two learning-based approaches (DeepFakes and Neural Textures). The deepfakes videos were generated using a slightly modified version of FaceSwap<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark93\">66</a>, and the Neural Textures videos were created using the approach proposed by Thies et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark198\">61</a>]. The benchmark test dataset is created from the collection of 1,000 images randomly selected from either the manipulation methods or the original videos [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark193\">56</a>]. Participants have to submit results to the benchmark, rather then code like other competitions; this is illustrated in Figure <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark94\">4a</a>. The outcome of a submission is illustrated in Figure <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark94\">4b</a>, where the scores are a measure of accuracy (Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark17\">8</a>)).</p><p>\\\nThe Open Media Forensics Challenge (OpenMFC, formerly DARPA MFC)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark96\">67</a> is an annual image and video forensics evaluation aiming to facilitate development of multimedia manipulation detection systems. It has been organised annually<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark97\">68</a> starting from 2017 under the name of DARPA MFC. In 2020, the National Institute of Standards and Technology (NIST) initiated the  as a new evaluation platform, based on their previous experiences with the DARPA MFC series, to make the participation more convenient for all researchers. In OpenMFC 2020, two deepfake-related tasks were included for the first time: Image GAN Manipulation Detection (IGMD) and Video GAN Manipulation Detection (VGMD). The organisers provided an image evaluation dataset for the IGMD task, containing 1,000 images from over 200 image journals<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark98\">69</a>, and a video evaluation dataset for the VGMD task, including over 100 test videos. Furthermore, they provided the datasets<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark99\">70</a> used in the previous MFC challenges as development datasets. The challenge is composed of two main phases for development and evaluation, respectively, and a pre-challenge phase for quality control testing. For evaluation of submissions, AUC-ROC is used as the primary metric. Furthermore, CDR@FAR, where CDR refers to correct detection rate or TPR (Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark13\">4</a>)) and FAR refers to false alarm rate or FPR (Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark14\">5</a>)), is also used as a metric [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark186\">49</a>]. The DeeperForensics Challenge 2020<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark100\">71</a> is a deepfake face detection challenge held at the 2020 ECCV</p><p>SenseHuman Workshop<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark101\">72</a>. The challenge used the DeeperForensics1.0 dataset.</p><p>The organisers provided a hidden test dataset to better simulate real-world scenarios. The challenge involved two phases: the “development phase” that started in August 2020 allowing 100 successful sub-missions, and the “final test phase” that started in October 2020 allowing 2 successful submissions until the end of the month. The submissions were evaluated using the binary cross-entropy loss (BCELoss) metric, calculated according to Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark95\">18</a>), where  is the number of videos in the hidden test set,  is the ground truth label of video  (fake:1, real:0), and () is the predicted probability that video  is fake.</p><p>Results<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark102\">73</a> of the competition were discussed by Jiang et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark163\">26</a>]. The top solution used three models, i.e., EfficientNet-B0, EfficientNet-B1 and EfficientNet-B2, for classification. The second top used EfficientNet-B5 for both an image-based model and a video-based model. The third ranked solution used a 3D convolutional neural network (3DCNN).</p><p>\\\nThe Face Forgery Analysis Challenge 2021<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark104\">74</a> is a competition hosted at the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021). It is organised by researchers from a number of organisations in China including universities and SenseTime Research (the research arm of SenseTime<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark105\">75</a>, one of the major AI “unicorns” in China). The challenge aims to advance the state-of-the-art in detection of photo-realistic manipulation of images and videos. Participants are able to use a large annotated face dataset (i.e., the ForgeryNet dataset) that was obtained by applying a number of techniques for manipulation (15) and perturbation (36) to train their solutions. The phases comprise of Forgery Image Analysis, Forgery Video Analysis, Forgery Video Temporal Localization phases, and the final phase (i.e., “private test”) where participants’ models will be tested against an unseen dataset. The following metrics will be used [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark162\">25</a>]: AUC, average precision (AP) at some “temporal Intersection over Union” (AP@tIoU) compared to a threshold  ∈ [0*.,* 0*.*95], and average recall (AR) at  (AR@) where  is the top  labels returned for multi-class classifiers.</p><p>The 2020 CelebA-Spoof Face Anti-Spoofing Challenge<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark106\">76</a> was hosted at the 16 European Conference on Computer Vision (ECCV 2020). The challenge ran between August and October 2020, and aimed to advance the state-of-the-art in detecting “<em>whether a presented face is live or spoof</em> ” [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark213\">76</a>]. The organisers made the face CelebA-Spoof dataset available for the competition containing rich annotation across a range of attributes. The competition only had one phase where participants submitted their solutions to be evaluated against a test dataset; the spoof class was considered as “positive” and the live class as “negative”. Metric TPR@FPR was used and collected at three points where the TPR when FPR = 104 determined the final ranking. The top three finalists (see Figure <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark103\">5</a>) used deep learning models ResNet, EfficientNet-B7, and a novel architecture combining Central Difference Convolutional Networks (CDCN) and Dual Attention Network (DAN). The two top ranked solutions used different strategies to boost their models’ performance: a heuristic voting scheme was used by the top-ranked solution, and a weight-after-sorting strategy was used by the second ranked solution.</p><p>The 2021 CSIG Challenge<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark107\">77</a> is the second edition of a challenge organised by the China Society of Image and Graphics<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark108\">78</a>. The 2021 challenge has the Fake Media Forensic Challenge<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark109\">79</a> as its 6 track, co-organised by CSIG’s Digital Media Forensics and Security Technical Committee<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark110\">80</a> and Institute of Information Engineering, Chinese Academy of Sciences<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark111\">81</a>. This track has two tasks, one on deepfake video detection, and the other on deepfake audio/speech detection. For the deepfake video detection task, the dataset used contains a public training set with 10,000 sound-free face videos (including 4,000 fake videos), a public test set with 20,000 face videos (the percentage of deepfake videos is unknown to participants), and a private test set that will be determined and used at the final session for selecting the winners. All videos contain faces of Eastern Asian people, and cover a wide range of parameters such as multiple resolutions and encoding quality factors, the use of blurring or sharpening filters, and added noise. Deepfake videos were created using public tools including DeepFaceLab [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark190\">53</a>], Faceswap<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark112\">82</a>, Faceswap-GAN, Recycle-GAN [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark143\">6</a>] and ALAE (Adversarial Latent Autoencoders) [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark191\">54</a>]. For the deepfake audio/speech detection task, the dataset used contains a public training set with 10,000 speech samples (including 6,000 fake ones), a public test set with 20,000 face videos (the percentage of deepfake videos is unknown to participants), and a private test set for the final session (the same as the deepfake video detection task). The tools used for generating the fake speech samples include TTS (text-to-speech) voice synthesis tools and VC (voice conversion) tools. The main TTS tools used include open-source tools such as DeepVoice, TensorFlowTTS<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark114\">83</a> and GAN-TTS [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark145\">8</a>] and commercial software tools such as those from iFlytek<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark115\">84</a> and IBM. The main VC tools used include Adaptive-VC and CycleGAN-VC [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark166\">29</a>]. For both deepfake detection tasks, the performance metric used is log loss.</p><p>2020 China Artificial Intelligence<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark116\">85</a> was the second edition of a Chinese AI competition open for the general public to participate, organised by the municipal government of the City of Xiamen in China. In 2020, it had two sub-competitions, Multimedia Information Recognition Technology Competition<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark117\">86</a> and Language and Knowledge Technology Competition<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark118\">87</a>. The Multimedia Information Recognition Technology Competition included two tasks on deepfakes: one on deepfake video detection<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark119\">88</a> and one on deepfake audio/speech detection<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark120\">89</a>. The deepfake video detection task used 3,000 videos, and log loss was used as the sole performance metric. The deepfake audio/speech detection task used 20,000 audio samples (mostly in Chinese, and the remaining in English), and EER was used as the sole performance metric. For both tasks, the ratio between real and deepfake samples was 1:1. We did not find where to download the datasets used for the tasks nor a more detailed technical description of the datasets. For the deepfake video detection tasks, the top two winning teams (with an A prize) were from Netease (Hangzhou) Network Co., Ltd. and Beijing RealAI Technology Co., Ltd., followed by three other teams winning a B prize: Xiamen Fuyun Information Technology Co., Ltd.; Institute of Computing Technology, Chinese Academy of Sciences; and Wuhan Daqian Information Technology Co., Ltd. For the deepfake audio/speech task, there was no team winning an A prize, but one team winning a B prize: SpeakIn Technologies Co., Ltd. The final results of some teams were published, but some teams were allowed to hide their results. We did not find a detailed technical report summarising the results and explaining the work of the winning teams.</p><p>One of the B-prize winning team is from Beijing RealAI Technology Co., Ltd., a Chinese company active in deepfake-related R&amp;D.</p><p>The Voice Conversion Challenge<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark121\">90</a> is a biennial competition that has been running since 2016. The challenge and the corresponding workshop, hosted at the INTERSPEECH conference<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark122\">91</a>, is supported by the SynSig (Speech Synthesis Special Interest Group)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark123\">92</a> of the International Speech Communication Association (ISCA)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark124\">93</a>. Its aim is to promote progress in voice conversion (VC) technology that can be applied to a number of positive and negative use cases, such as spoofing voice biometric systems. The 2020 challenge focused on speaker conversion, a sub-problem of VC, and included two tasks. For the first task “intra-lingual semi-parallel voice conversion”, participants had to develop 16 VC systems (speaker-pair combinations) including male and female speakers and English sentences, using the provided Voice Conversion Challenge 2020 database v1.0 for training (refer to Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark35\">4</a>). For the second task “cross-lingual voice conversion”, participants had to develop 24 VC systems, also including male and female speakers, but uttering sentences in three languages (Finnish, German and Mandarin), based on the provided training dataset. Figure <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark126\">6</a> illustrates the process of training and generation of VC systems.</p><p>Submissions were evaluated for “<em>perceived naturalness and similarity through listening tests</em>”<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark125\">94</a>. As such, the organisers used  [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark207\">70</a>] and recruited both native and non-native English speakers (i.e., Japanese native speakers) via crowd-sourcing for the listening tests. Naturalness (answering the question “How natural does the converted voice sound? ”) was measured using the metric MOS (covered in Section 4.6), and similarity (answering the question “how similar the converted voice sound comparing source and target speakers? ”) was measured in terms of speaker recognition as “same” or “different”, as elaborated by Wester et al. [68]. Tests also focused on the effects of language differences on the performance of VC systems submitted to the competition. The most popular CNN/RNN/GANbased VC systems submitted used WaveNet, WaveRNN, and Parallel WaveGAN. Results indicated that, in terms of similarity, the best performing VC systems were as good as natural speech but none reached human-level naturalness for task 1; scores were lower for task 2 which was more complex [70]. The organisers of the 2020 competition also used objective evaluation [12]. The metrics used for evaluation of speaker similarity were: equal error rate (EER), false acceptance rate of target (P tar fa ), miss rate of source (P src miss), and cosine similarity of speaker embedding vectors (cos-sim) according to Eq. (19) where A is the speaker embedding vectors for the converter audio and B is the speaker embedding vectors for the original audio. The performance of the VC systems as a spoof countermeasure was also evaluated using EER, while to evaluate the quality of the subjective MOS obtained via listening tests, a DL-based model to predict MOS, called MOSNet [43], was used. Lastly, to evaluate intelligibility of the converted transcribed speech, in comparison with the original transcribed speech, the word error rate (WER) [4] was used. WER is calculated according to Eq. (20) where I refers to insertions, D refers to deletions, S refers to substitutions, and N refers to the total number of words in the original transcript.</p><p>The Deepfake Africa Challenge (2021)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark129\">95</a> is a new initiative of the AI Africa Expo, in partnership with a film and media production company (Wesgro) and the African Data Science competition platform Zindi. Its aim is “<em>to create convincing deepfakes to highlight the power of this synthetic media, illustrating its creative potential for exploitation for both positive and negative outcomes and focusing debate about its ethical use / misuse in an African context</em> ”. Eligible participants were required to be citizens and residents of the African continent. Submissions, accepted up to end of July 2021, can be either video or audio. Evaluation of submissions is defined in terms of artistic creativity, relevance of challenge topic, and innovation in the process of generation as long as participants use tools and packages publicly available. The top three finalists will receive a prize, present their work at the Expo, and will have to grant copyrights to Zindi. Unlike the other competitions reviewed in this section, which were focused on advancing the state-of-the-art in detection of synthetic or manipulated media, this competition focused on the generation of deepfake which seems more humanities-centred. This is a trend observed in arts [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark168\">31</a>] and culture [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark194\">57</a>].5.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Generation and Detection of Manipulated Media</p><p>The DeepFake Game Competition (DFGC)<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark132\">96</a> is in its first edition, hosted at the 2021 International Joint Conference on Biometrics (IJCB 2021). Its organisers are mainly from the Institute of Automation Chinese Academy of Sciences (CASIA). The idea of the competition was to promote an adversarial game between agents pushing for advances in both deepfake creation and detection. In order to achieve this, a 6-stage protocol was designed interleaving three creation phase (C-phase) and detection phase (D-phase), typically one week apart; submissions closed in April 2021. Both C-phases and D-phases were bound to the Celeb-DF (v2) dataset [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark177\">40</a>], containing 6,229 videos (590 real/original videos and 5,639 fake/manipulated videos), for training purposes. As such, submissions to a C-phase would consist of datasets extracted from Celeb-DF (v2) which included novel face-swap approaches to obtain evaluation results. Submissions to a D-phase would consist of detection models/codes to obtain evaluation results. The models submitted for a D-phase were evaluated against the datasets submitted for the previous C-phase [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark189\">52</a>]. The metrics used for evaluation<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark133\">97</a> were: a detection score, used for evaluation of a D-phase, and a creation score, used for evaluation of a C-phase. The top three finalists for the detection phase employed CNN-based classifiers EfficientNet-B3, Efficientnet-B0 and EfficientNetV2.</p><p>The Detection Score () metric captures the models’ ability to correctly classify fake images submitted to the previous C-phase against a set of real images in the CelebDF test dataset. It is calculated using Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark130\">21</a>), where  is the number of valid submissions of created synthesis test sets in the last C-phase.</p><p>The Creation Score () metric used to evaluate creation models submitted to this challenge is calculated by Eq. (<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark131\">22</a>), where  is the number of valid submissions of detection methods in the last D-phase, the noise score (noise) penalises noisy images, the other three parts of the equation relate to the following<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark134\">98</a>: “<em>ID level similarity to the donor ID, image level similarity to the target frame, and the deception ability against detection models. ID level similarity is scored by a face recognition model using dot product of two ID features (fake face ID and donor ID). The image level similarity is scored by SSIM [Structural Similarity Index] to make sure the face-swapped image is similar to the corresponding target image in content and quality</em> ”.</p><p>Peng et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark189\">52</a>] observed a commonality between the three winning teams for the creation task, i.e., the use of the FaceShifter [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark174\">37</a>] framework for face swapping. They highlighted two overall reflections about the competition: (1) the limited diversity of the deepfake datasets submitted and the use of repetitive methods to generate them, and (2) the limited size of the Celeb-DF (v2) dataset itself flagging the need for a larger dataset for next year’s competition. The organisers of the competition also applied the top two detection models to unseen datasets (DFDC and FaceForensics++) and noticed that they do not generalise well.</p><p>This section presents a meta-review of 12 selected deepfake-related survey papers, including eight published in English [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark153\">16</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark182\">45</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark183\">46</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark201\">64</a>–<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark203\">66</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark208\">71</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark210\">73</a>] and four published in Chinese [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark144\">7</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark178\">41</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark196\">59</a>]. It covers the following aspects in a systematic manner: definitions and scope, performance metrics, datasets, challenges/competitions/benchmarks, performance comparison, key challenges and recommendations.</p><p>The meta-review aims at drawing some high-level insights for monitoring future development of deepfake-related technologies and their applications.</p><h2>6.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Definitions and Scope</h2><p>As we discussed in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark0\">1.1</a>, among researchers, practitioners and law makers there is no universally accepted definition of “deepfake” as a term. This is also reflected in how the authors of the 12 survey papers considered this aspect. Most authors talked about the history of deepfakes and pointed out that the term reflects the combination of “deep learning” and “fake”, but some used a broader definition, e.g., Lyu [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark182\">45</a>] defined deepfake as “<em>high quality fake videos and audios generated by AI algorithms</em>”. Some authors also referred to deepfake-related legislations, but none of them pointed out that the definitions in some such legislations are completely different from the more technical definitions involving the use of deep learning. No authors discussed the blurred boundary between deepfakes and non-deepfakes, although some surveys actually cover both, e.g., Tao et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark196\">59</a>] focused on speech forgery and did not explicitly highlight “deepfake”.</p><p>In terms of the scope, while some authors (correctly) considered all types of media that can be produced by deepfake-related techniques [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark178\">41</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark182\">45</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark202\">65</a>], some considered only a narrow scope, e.g., authors of [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark144\">7</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark201\">64</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark208\">71</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark210\">73</a>] considered only videos, and only authors of [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark153\">16</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark203\">66</a>] have considered images and videos. Another phenomenon we observed is that many authors focused more on face images and videos, and authors of three surveys [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark153\">16</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark201\">64</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark208\">71</a>] even limited the definition of “deepfake” to such a narrow scope:</p><ul><li>Deshmukh and Wankhade [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark153\">16</a>] defined it as “<em>a technology which creates fake images or videos of targeted humans by swapping their faces [by] another character saying or doing things that are not absolutely done by them and humans start believing in such fake as it is not always recognisable with the everyday human eye</em>”;</li><li>Younus and Hasan [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark208\">71</a>] considered deepfake as a technique allowing “<em>any computer user to exchange the face of one person with another digitally in any video</em>”; and</li><li>Tolosana et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark201\">64</a>] defined it as “<em>a deep learning based technique able to create fake videos by swapping the face of a person by the face of another person</em>”.</li></ul><p>Such unnecessarily narrow definitions and scopes can lead to confusion and do not help exchanges between researchers and practitioners working on different types of deepfakes.</p><p>We call on more researchers to accept a broader definition of “deepfake” so that highly realistic/natural media of any kind generated by a sophisticated automated method (often AI-based) is considered deepfake. Here, we provide two examples of such a broader definition: the image2image (or pixel2pixel) technique [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark217\">80</a>] that allows the production of deepfake images and videos of any objects (e.g., the “horse2zebra” deepfake image shown in Figure <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark135\">7</a>), and the the so-called “deepfake geography [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark214\">77</a>]”, where AI-based techniques are used to generate realistic-looking satellite images.</p><p>\\\nAnother important fact missed or not sufficiently discussed by authors of all the 12 surveys is that deepfake techniques can be used for positive applications, e.g., creative arts, entertainment and protecting online users’ privacy. We call for more researchers and practitioners to follow the proposal in the 2020 Tencent AI White Paper [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark197\">60</a>] to start using the more neutral-sounding term “deep synthesis”. Accordingly,we can use different words for different types of data generated using “deep synthesis” techniques, e.g., “deep art”, “deep animation”, “deep music”, and “deepfake”. While authors of the 12 survey papers did not recognise the positive applications of “deepfake” technologies, some other researchers did, e.g., organisers of the Voice Conversion Challenge 2020<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark136\">99</a> who said the VC technology (for speech deepfake) “<em>is useful in many applications, such as customizing audio book and avatar voices, dubbing, movie industry, teleconferencing, singing voice modification, voice restoration after surgery, and cloning of voices of historical persons</em>”.</p><p>Surprisingly, none of the 12 surveys have covered performance metrics explicitly. Some directly used performance metrics to explain and compare performance of covered deepfake generation and detection methods. The most used performance metrics include accuracy, ERR, and AUC. This may be explained by the page constraints of such survey papers, which did not allow the authors to extend their coverage significantly to cover performance metrics systematically. The subjective quality of deepfakes is an area least covered by the surveys, which seems related to an unbalanced coverage on deepfake generation and deepfake detection in terms of performance evaluation and comparison (the former much less than the latter).</p><p>Many of the 12 survey papers list a number of deepfake-related datasets, but none of them have coverage as complete as ours shown in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark35\">4</a>. For instance, none of the surveys have covered the Voice Conversion Challenge 2016/2018/2020 datasets and the ASVspoof 2019/2021 datasets are covered briefly only in two surveys [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark196\">59</a>]. In addition, more recent deepfake datasets especially those released in 2021 are also not covered by any of the surveys. We believe that our Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark35\">4</a> is the most comprehensive review of deepfake-related datasets so far.</p><p>Some survey papers include datasets that are likely deepfakes, e.g., Verdoliva [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark203\">66</a>] covered many general fake image datasets where the manipulated images were not generated by deep learning or even AI-based methods, and some surveys (e.g., [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>]) mentioned ASVspoof 2015 datasets but we did not see the use of deep learning for generating data used in the dataset.</p><p>Many surveys cover deepfake-related challenges, competitions and benchmarks. The coverage is, however, mostly limited, and some challenges (e.g., the Voice Conversion Challenge 2016/2018/2020 and the two Chinese challenges we covered in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark74\">5</a>) are not covered by any of the surveys. The level of detail of challenges, competitions and benchmarks is also normally limited, compared with what we chose to include in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark74\">5</a>. Similar to the datasets we covered in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark35\">4</a>, we believe that our coverage of deepfake-related challenges, competitions and benchmarks in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark74\">5</a> is also the most comprehensive so far.</p><p>Most surveys have a good coverage of related methods for deepfake generation and detection, but only some explicitly covered performance comparison between different methods [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark183\">46</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark201\">64</a>].</p><p>Among all the survey papers, Li et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>] conducted the most comprehensive study on performance of different deepfake detection methods. In addition to showing the performance metrics of a number of deepfake detection methods in Table 3 of [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>], they also looked at general characteristics and issues of different types of deepfake detection methods, as shown in Table <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark137\">4</a>. Furthermore, they also looked at research on robustness of deepfake detection methods against adversarial samples, referring to some work that showed a lack of such robustness.</p><p>Due to quality issues of many deepfake-related datasets (discussed in Section <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark64\">4.6</a>), we need to treat any performance metrics and comparison of different detection methods with caution. Without testing all methods on a sufficiently large, diverse and high-quality deepfake dataset, the performance comparison results can be misleading. This highlights the importance of having more challenges, competitions and benchmarks to encourage performance comparison on standard datasets and using consistent performance metrics.</p><p>The authors of some surveys identified some key challenges and future research directions for the deepfake community.</p><p>Not surprisingly, how to develop more robust, scalable, generalisable and explainable deepfake detection methods is one of the most discussed key challenges and also a major future research direction [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark144\">7</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark153\">16</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark178\">41</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark182\">45</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark196\">59</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark202\">65</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark203\">66</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark208\">71</a>]. Considering the arms race between deepfake generation and detection, this research direction will likely remain the hottest topic in deepfake research.</p><p>A couple of surveys [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark203\">66</a>] mentioned fusion as a key future research direction, where “fusion” refers to combining different methods (e.g., combining multiple detectors of different types) and data sources (e.g., jointly considering audio-visual analysis) to achieve better performance for deepfake detection. Lyu [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark182\">45</a>] suggested that, for detection of deepfake videos, we need to consider video-level detection more, which can be considered fusion of detection results of all video frames.</p><p>The authors of three surveys, Lyu [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark182\">45</a>] , Deshmukh and Wankhade [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark153\">16</a>] and Younus and Hasan [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark208\">71</a>], argued that better (higher-quality, more up-to-date, and more standard) deepfake datasets are needed to develop more effective deepfake detection methods. Lyu [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark182\">45</a>] also suggested that we need to consider  effects in training data and improve the evaluation of datasets. We agree with them on these points.</p><p>Tao et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark196\">59</a>] suggested that low-cost deepfake generation/detection should be considered as a future research direction. This is a valid recommendation since lightweight methods will allow less powerful computing devices (e.g., IoT devices) to benefit from such technologies.</p><p>Two Chinese surveys [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>, <a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark178\">41</a>] also mentioned the need to have new deepfake-related legislations combating malicious use of deepfakes and the need to train end users such as journalists. This is likely an area where interdisciplinary research can grow.</p><p>There are also other ad-hoc recommendations given by the authors of some surveys. For example, Lyu [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark182\">45</a>] argued that deepfake detection should be considered a (more complicated) multi-class, multi-label and local detection problem. Tolosana et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark201\">64</a>] discussed specific research directions for different deep-fake generation methods (face synthesis, identity swap, attribute manipulation, and expression swap). Liang et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark178\">41</a>] and Li et al. [<a href=\"https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss#_bookmark175\">38</a>] recommended more active defence mechanisms such as using digital watermarking and blockchain technologies to build trustworthy media frameworks against deepfakes.</p><p>The rapid growth in the capability to manipulate media or create synthetic media which look realistic and natural paved the way for deepfakes. At first, this paper adopted a critical approach to look at different definitions of the term “deepfake”. In that regard, we point out the different contradicting definitions and call for the wider community to consider how to define a new term that has a more consistent scope and meaning. For instance, replacing “deepfake” by “deep synthesis” can be more inclusive by embracing positive applications of deepfake techniques, e.g., in entertainment and for simulation purposes.</p><p>This paper provided a comprehensive overview of multiple aspects of the deepfake ecosystem drawing from the research literature and other online sources published in two languages: English and Chinese. It covers commonly used performance metrics and standards, related datasets, challenges, competitions and benchmarks. It also presents a meta-review of 12 selected deepfake-related survey papers published in 2020 and 2021, covering not only the above mentioned aspects, but also highlighting key challenges and recommendations.</p><p>[1]&nbsp;&nbsp; Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018. MesoNet: A Compact Facial Video Forgery Detection Network. In <em>Proceedings of the 2018 IEEE International Workshop on Information Forensics and Security</em>. IEEE, 1–7. <a href=\"https://doi.org/10.1109/WIFS.2018.8630761\">https://doi.org/10.1109/WIFS.2018.8630</a><a href=\"https://doi.org/10.1109/WIFS.2018.8630761\">761</a></p><p>[2]&nbsp;&nbsp; Henry Ajder, Giorgio Patrini, Francesco Cavalli, and Laurence Cullen. 2019. The State of Deepfakes: Landscape, Threats, and Impact. Deeptrace. , 27 pages.&nbsp; <a href=\"https://sensity.ai/reports/\">https://sensity.ai/reports/</a></p><p>[4]&nbsp;&nbsp; Ahmed Ali and Steve Renals. 2018. Word Error Rate Estimation for Speech Recognition: e-WER. In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</em>. Association for Computational Linguistics, 20–24. <a href=\"https://doi.org/10.18653/v1/P18-2004\">https://doi.org/10.18653/v1/P18-2004</a></p><p>[6]&nbsp;&nbsp; Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh. 2018. Recycle-GAN: Unsupervised Video Retargeting. In <em>Proceedings of the 2018 European Conference on Computer Vision</em>. Springer, 17 pages.&nbsp; <a href=\"https://doi.org/10.1007/978-3-030-01228-1_8\">https://doi.org/10.1007/978-3-030-01228-1 8</a></p><p>[8]&nbsp;&nbsp; Mikol-aj Bin´kowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C. Cobo, and Karen Simonyan. 2019. High Fidelity Speech Synthesis with Adversarial Networks.&nbsp; <a href=\"https://doi.org/10.48550/ARXIV.1909.11646\">https://doi.org/10.48550/ARXIV.1909.11646</a></p><p>[10]&nbsp;&nbsp; Umur Aybars Ciftci, Ilke Demir, and Lijun Yin. 2020. FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2020), 17 pages. <a href=\"https://doi.org/10.1109/TPAMI.2020.3009287\">https://doi.org/10.1109/TPAMI.2020.3009287</a></p><p>[11]&nbsp;&nbsp; Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, and Anil K. Jain. 2020. On the Detection of Digital Face Manipulation. In <em>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE, 10 pages. <a href=\"https://doi.org/10.1109/CVPR42600.2020.00582\">https://doi.org/10.1109/CVPR42600.2020.00582</a></p><p>[12]&nbsp;&nbsp;Rohan Kumar Das, Tomi Kinnunen, Wen-Chin Huang, Zhen-Hua Ling, Junichi Yamagishi, Zhao Yi, Xiaohai Tian, and Tomoki Toda. 2020. Predictions of Subjective Ratings and Spoofing Assessments of Voice Conversion Challenge 2020 Submissions. In <em>Proceedings of the Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020</em>. International Speech Communication Association, 99–120. <a href=\"https://doi.org/10.21437/VCC_BC.2020-15\">https://doi.org/10.21437/VCC BC.2020-15</a></p><p>[13]&nbsp;H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021. ASVspoof 2021: Automatic Speaker Verification Spoofing and Countermeasures Challenge Evaluation Plan.&nbsp; <a href=\"https://www.asvspoof.org/asvspoof2021/asvspoof2021_evaluation_plan.pdf\">https://www.asvspoof.org/asvspoof2021/asvspoof2021 evaluation plan.pdf</a></p><p>[14]&nbsp;&nbsp; H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021.</p><p>[15]&nbsp;&nbsp; H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021. ASVspoof 2021 Challenge - Speech Deepfake Database.&nbsp; <a href=\"https://doi.org/10.5281/zenodo.4835108\">https://doi.org/10.5281/zenodo.4835108</a></p><p>[16]&nbsp;&nbsp;  ![](file:///C:/Users/user/AppData/Local/Temp/msohtmlclip1/01/clip_image051.gif)Anushree Deshmukh and Sunil B. Wankhade. 2021. Deepfake Detection Approaches Using Deep Learning: A Systematic Review. In <em>Intelligent Computing and Networking: Proceedings of IC-ICN 2020 (Lecture Notes in Networks and Systems, Vol. 146)</em>. Springer, 293–302. <a href=\"https://doi.org/10.1007/978-981-15-7421-4_27\">https://doi.org/</a><a href=\"https://doi.org/10.1007/978-981-15-7421-4_27\">10.1007/978-981-15-7421-4 27</a></p><p>[17]&nbsp;&nbsp; Xinyi Ding, Zohreh Raziei, Eric C. Larson, Eli V. Olinick, Paul Krueger, and Michael Hahsler. 2020. Swapped Face Detection using Deep Learning and Subjective Assessment. <em>EURASIP Journal on Information Security</em> 2020, 1 (2020), 1–12. <a href=\"https://doi.org/10.1186/s13635-020-00109-8\">https://doi.org/10.1186/s13635-020-00109-8</a></p><p>[18]&nbsp;&nbsp; Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset.&nbsp; <a href=\"https://doi.org/10.48550/ARXIV.2006.07397\">https://doi.org/10.48550/ARXIV.2006.07397</a></p><p>[19]&nbsp;&nbsp; Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset. arXiv:2006.07397. <a href=\"https://arxiv.org/abs/2006.07397\">https://arxiv.org/abs/2006.07397</a></p><p>[23]&nbsp;&nbsp; Gereon Fox, Wentao Liu, Hyeongwoo Kim, Hans-Peter Seidel, Mohamed Elgharib, and Christian Theobalt. 2021. Videoforensicshq: Detecting High-Quality Manipulated Face Videos. In <em>Proceedings of the 2021 IEEE International Conference on Multimedia and Expo</em>. IEEE, 1–6. <a href=\"https://doi.org/10.1109/ICME51207.2021.9428101\">https://doi.or</a><a href=\"https://doi.org/10.1109/ICME51207.2021.9428101\">g/10.1109/ICME51207.2021.9428101</a></p><p>[24]&nbsp;&nbsp; Haiying Guan, Andrew Delgado, Yooyoung Lee, Amy N. Yates, Daniel Zhou, Timothee Kheyrkhah, and Jon Fiscus. 2021. User Guide for NIST Media Forensic Challenge (MFC) Datasets. <a href=\"https://doi.org/10.6028/NIST.IR.8377\">https://doi.org/10.6028/NIST.IR.8377</a></p><p>[25]&nbsp;&nbsp; Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. 2021. ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. In <em>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE,&nbsp; 4360–4369.&nbsp;&nbsp;&nbsp; <a href=\"https://doi.org/10.1109/CVPR46437.2021.00434\">https://doi.org/10.1109/CVPR46437.2021.00434</a></p><p>[26]&nbsp;&nbsp; Liming Jiang, Zhengkui Guo, Wayne Wu, Zhaoyang Liu, Ziwei Liu, Chen Change Loy, Shuo Yang, Yuanjun Xiong, Wei Xia, Baoying Chen, Peiyu Zhuang, Sili Li, Shen Chen, Taiping Yao, Shouhong Ding, Jilin Li, Feiyue Huang, Liujuan Cao, Rongrong Ji, Changlei Lu, and Ganchao Tan. 2021. DeeperForensics Challenge 2020 on Real-World Face Forgery Detection: Methods and Results. arXiv:2102.09471. <a href=\"https://arxiv.org/pdf/2102.09471.pdf\">https://arxiv.org/pdf/2102.09471.pdf</a></p><p>[27]&nbsp;&nbsp; Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. 2020. DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection. In <em>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE, 2886–2895. <a href=\"https://doi.org/10.1109/CVPR42600.2020.00296\">https://doi.org/</a><a href=\"https://doi.org/10.1109/CVPR42600.2020.00296\">10.1109/CVPR42600.2020.00296</a></p><p>[28]&nbsp;&nbsp; Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. 2018. Efficient Neural Audio Synthesis. <a href=\"https://doi.org/10.48550/ARXIV.1802.08435\">https://doi.org/10.48550/ARXIV.1802.08435</a></p><p>[30]&nbsp;&nbsp; Tero Karras, Samuli Laine, and Timo Aila. 2019. A Style-based Generator Architecture for Generative Adversarial Networks. In <em>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE, 4401–4410. <a href=\"https://doi.org/10.1109/CVPR.2019.00453\">https://doi.org/10.1109/CVPR.2019.00453</a></p><p>[32]&nbsp;&nbsp; Ali Khodabakhsh, Raghavendra Ramachandra, Kiran Raja, Pankaj Wasnik, and Christoph Busch. 2018. Fake Face Detection Methods: Can They Be Generalized?. In <em>Proceedings of the 2018 International Conference of the Biometrics Special Interest Group</em>. IEEE, 1–6. <a href=\"https://doi.org/10.23919/BIOSIG.2018.8553251\">https://doi.org/10.2</a><a href=\"https://doi.org/10.23919/BIOSIG.2018.8553251\">3919/BIOSIG.2018.8553251</a></p><p>[33]&nbsp;&nbsp; Hyeongwoo Kim, Mohamed Elgharib, Hans-Peter Zoll¨ofer, Michael Seidel, Thabo Beeler, Christian Richardt, and Christian Theobalt. 2019. Neural Style-Preserving Visual Dubbing. <em>ACM Transactions on Graphics</em> 38, 6, Article 178 (2019), 13 pages. <a href=\"https://doi.org/10.1145/3355089.3356500\">https://doi.org/10.1145/3355089.3356500</a></p><p>[34]&nbsp;&nbsp; Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick P´erez, Christian Richardt, Michael Zollh¨ofer, and Christian Theobalt. 2018. Deep Video Portraits. <em>ACM Transactions on Graphics</em> 37, 4, Article 163 (2018), 14 pages. <a href=\"https://doi.org/10.1145/3197517.3201283\">https://doi.org/</a><a href=\"https://doi.org/10.1145/3197517.3201283\">10.1145/3197517.3201283</a></p><p>[35]&nbsp;&nbsp; Pavel Korshunov and S´ebastien Marcel. 2019. Vulnerability Assessment and Detection of Deepfake Videos. In <em>Proceedings of the 2019 International Conference on Biometrics</em>. IEEE, 1–6. <a href=\"https://doi.org/10.1109/ICB45273.2019.8987375\">https://doi.org/10.1109/ICB45273.2019.8987375</a></p><p>[36]&nbsp;&nbsp; Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae. 2021. KoDF: A Large-scale Korean DeepFake Detection Dataset. In <em>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision</em>. IEEE, 10724–10733. <a href=\"https://doi.org/10.1109/ICCV48922.2021.01057\">https://doi.org/10.1109/ICCV</a><a href=\"https://doi.org/10.1109/ICCV48922.2021.01057\">48922.2021.01057</a></p><p>[37]&nbsp;&nbsp; Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. 2020. FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping. arXiv:1912.13457. <a href=\"https://arxiv.org/abs/1912.13457\">https://arxiv.org/abs/1912.13457</a></p><p>[38]&nbsp;&nbsp; Xurong Li, Shouling Ji, Chunming Wu, Zhenguang Liu, Shuiguang Deng, Peng Cheng, Min Yang, and Xiangwei Kong. 2021. Survey on Deepfakes and Detection Techniques.  32, 2 (2021), 496–518. <a href=\"http://www.jos.org.cn/1000-9825/6140.htm\">http://www.jos.org.cn/1000-9825/6140.htm</a></p><p>[39]&nbsp;&nbsp; Yuezun Li, Ming-Ching Chang, and Siwei Lyu. 2018. In Ictu Oculi: Exposing AI Created Fake Videos by Detecting Eye Blinking. In <em>Proceedings of the 2018 IEEE International Workshop on Information Forensics and Security</em>. IEEE, 1–7. <a href=\"https://doi.org/10.1109/WIFS.2018.8630787\">https://doi.org/10.1109/WIFS.2018.8630787</a></p><p>[40]&nbsp;&nbsp; Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. 2020. Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics. In <em>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE, 3204–3213. <a href=\"https://doi.org/10.1109/CVPR42600.2020.00327\">https://doi.org/10.1109/CVPR42600.</a><a href=\"https://doi.org/10.1109/CVPR42600.2020.00327\">2020.00327</a></p><p>[42]&nbsp;&nbsp; Steven R. Livingstone and Frank A. Russo. 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English.  13, 5 (2018), 35 pages.</p><p>[43]&nbsp;&nbsp; Chen-Chou Lo, Szu-Wei Fu, Wen-Chin Huang, Xin Wang, Junichi Yamagishi, Yu Tsao, and Hsin-Min Wang. 2021. MOSNet: Deep Learning based Objective Assessment for Voice Conversion. arXiv:1904.08352. <a href=\"https://arxiv.org/pdf/1904.08352.pdf\">https://arxiv.org/pdf/1904.08352.pdf</a></p><p>[44]&nbsp;&nbsp; Jaime Lorenzo-Trueba, Junichi Yamagishi, Tomoki Toda, Daisuke Saito, Fernando Villavicencio, Tomi Kinnunen, and Zhenhua Ling. 2018. The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods. In <em>Proceedings of the Odyssey 2018 The Speaker and Language Recognition Workshop</em>. International Speech Communication Association, 195–202. <a href=\"https://doi.org/10.21437/Odyssey.2018-28\">https://doi.org/10.21437/Odyssey.2018-28</a></p><p>[46]&nbsp;&nbsp; Yisroel Mirsky and Wenke Lee. 2021. The Creation and Detection of Deepfakes: A Survey.  54, 1, Article 7 (2021), 41 pages. <a href=\"https://doi.org/10.1145/3425780\">https://doi.org/10.1145/3425780</a></p><p>[47]&nbsp;&nbsp; Gautham J. Mysore. 2015. Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges. <em>IEEE Signal Processing Letters</em> 22, 8 (2015), 1006–1010. <a href=\"https://doi.org/10.1109/LSP.2014.2379648\">https://doi.org/10.1109/LSP.2014.2379648</a></p><p>[48]&nbsp;&nbsp; Jo˜ao C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proen¸ca, and Julian Fierrez. 2020. GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection. <em>IEEE Journal of Selected Topics in Signal Processing</em> 14, 5 (2020), 1038–1048. <a href=\"https://doi.org/10.1109/JSTSP.2020.3007250\">https://doi.org/10.1109/JSTSP.2020.3007250</a></p><p>[50]&nbsp;&nbsp; Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio.&nbsp; <a href=\"https://doi.org/10.48550/ARXIV.1609.03499\">https://doi.org/10.48550/ARXIV.1609.03499</a></p><p>[51]&nbsp;&nbsp; Debajyoti Pal and Tuul Triyason. 2018. A Survey of Standardized Approaches towards the Quality of Experience Evaluation for Video Services: An ITU Perspective. <em>International Journal of Digital Multimedia Broadcasting</em> 2018, Article 1391724 (2018), 25 pages. <a href=\"https://doi.org/10.1155/2018/1391724\">https://doi.org/10.1155/20</a><a href=\"https://doi.org/10.1155/2018/1391724\">18/1391724</a></p><p>[52]&nbsp;&nbsp; Bo Peng, Hongxing Fan, Wei Wang, Jing Dong, Yuezun Li, Siwei Lyu, Qi Li, Zhenan Sun, Han Chen, Baoying Chen, Yanjie Hu, Shenghai Luo, Junrui Huang, Yutong Yao, Boyuan Liu, Hefei Ling, Guosheng Zhang, Zhiliang Xu, Changtao Miao, Changlei Lu, Shan He, Xiaoyan Wu, and Wanyi Zhuang. 2021. DFGC 2021: A DeepFake Game Competition. arXiv:2106.01217. <a href=\"https://arxiv.org/abs/2106.01217\">https:</a></p><p>[53]&nbsp;&nbsp; Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um´e, Mr. Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, Sheng Zhang, Pingyu Wu, Bo Zhou, and Weiming Zhang. 2020. DeepFaceLab: Integrated, Flexible and Extensible Face-swapping Framework. <a href=\"https://doi.org/10.48550/ARXIV.2005.05535\">https://doi.org/10.48550/ARXIV.2005.05535</a></p><p>[54]&nbsp;&nbsp; Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. 2020. Adversarial Latent Au-toencoders. In <em>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE, 10 pages. <a href=\"https://doi.org/10.1109/CVPR42600.2020.01411\">https://doi.org/10.1109/CVPR42600.2020.01411</a></p><p>[55]&nbsp;&nbsp; Andreas R¨ossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. 2018. FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces. <a href=\"https://doi.org/10.48550/ARXIV.1803.09179\">https://doi.org/10.48550/ARXIV.1803.09179</a></p><p>[56]&nbsp;&nbsp; Andreas R¨ossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. In <em>Proceedings of the 2019 International Conference on Computer Vision</em>. IEEE, 1–11. <a href=\"https://doi.org/10.1109/ICCV.2019.00009\">https://doi.org/10.110</a><a href=\"https://doi.org/10.1109/ICCV.2019.00009\">9/ICCV.2019.00009</a></p><p>[61]&nbsp;&nbsp; Justus Thies, Michael Zollh¨ofe, and Matthias Niessner. 2019. Deferred Neural Rendering: Image Synthesis using Neural Textures. <em>ACM Transactions on Graphics</em> 38, Article 66 (2019), 12 pages. Issue&nbsp; 4.&nbsp;&nbsp; <a href=\"https://doi.org/10.1145/3306346.3323035\">https://doi.org/10.1145/3306346.3323035</a></p><p>[62]&nbsp;&nbsp; Tomoki Toda, Ling-Hui Chen, Daisuke Saito, Fernando Villavicencio, Mirjam Wester, Zhizheng Wu, and Junichi Yamagishi. 2016. The Voice Conversion Challenge 2016. In <em>Proceedings of Interspeech 2016</em>. International Speech Communication Association, 1632–1636. <a href=\"https://doi.org/10.21437/Interspeech.2016-1066\">https://doi.org/10.21437/Interspeech.2016-1066</a></p><p>[63]&nbsp;&nbsp; Massimiliano Todisco, Xin Wang, Ville Vestman, Md Sahidullah, Hector Delgado, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Tomi Kinnunen, and Kong Aik Lee. 2019. ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection. arXiv:1904.05441. <a href=\"https://arxiv.org/pdf/1904.05441.pdf\">https://arxiv.org/</a><a href=\"https://arxiv.org/pdf/1904.05441.pdf\">pdf/1904.05441.pdf</a></p><p>[64]&nbsp;&nbsp; Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, and Javier Ortega-Garcia. 2020. Deepfakes and beyond: A Survey of face manipulation and fake detection.  64 (2020), 131–148.&nbsp; <a href=\"https://doi.org/10.1016/j.inffus.2020.06.014\">https://doi.org/10.1016/j.inffus.2020.06.014</a></p><p>[65]&nbsp;&nbsp; Xin Tong, Luona Wang, Xiaoqin Pan, and Jingya Wang. 2020. An Overview of Deepfake: The Sword of Damocles in AI. In <em>Proceedings of the 2020 International Conference on Computer Vision, Image and Deep Learning</em>. IEEE, 265–273. <a href=\"https://doi.org/10.1109/CVIDL51233.2020.00-88\">https://doi.org/10.1109/CVIDL51233.2020.00-88</a></p><p>[67]&nbsp;&nbsp; Xin Wang, Junichi Yamagishi, Massimiliano Todisco, H´ector Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, Lauri Juvela, Paavo Alku, Yu-Huai Peng, Hsin-Te Hwang, Yu Tsao, Hsin-Min Wang, S´ebastien Le Maguer, Markus Becker, Fergus Henderson, Rob Clark, Yu Zhang, Quan Wang, Ye Jia, Kai Onuma, Koji Mushika, Takashi Kaneda, Yuan Jiang, Li-Juan Liu, Yi-Chiao Wu, Wen-Chin Huang, Tomoki Toda, Kou Tanaka, Hirokazu Kameoka, Ingmar Steiner, Driss Matrouf, Jean-Fran¸cois Bonastre, Avashna Govender, Srikanth Ronanki, Jing-Xuan Zhang, and Zhen-Hua Ling. 2020. ASVspoof 2019: A Large-scale Public Database of Synthesized, Converted and Replayed Speech. <em>Computer Speech &amp; Language</em> 64 (2020), 27 pages.&nbsp; <a href=\"https://doi.org/10.1016/j.csl.2020.101114\">https://doi.org/10.1016/j.csl.2020.101114</a></p><p>[68]&nbsp;&nbsp; Mirjam Wester, Zhizheng Wu, and Junichi Yamagishi. 2016. Analysis of the Voice Conversion Challenge 2016 Evaluation Results. In <em>Proceedings of the Interspeech 2016 Conference</em>. International Speech Communication Association, 1637–1641. <a href=\"https://doi.org/10.21437/Interspeech.2016-1331\">https://doi.org/10.21437/Interspeech.201</a><a href=\"https://doi.org/10.21437/Interspeech.2016-1331\">6-1331</a></p><p>[70]&nbsp;&nbsp;Zhao Yi, Wen-Chin Huang, Xiaohai Tian, Junichi Yamagishi, Rohan Kumar Das, Tomi Kinnunen, Zhen-Hua Ling, and Tomoki Toda. 2020. Voice Conversion Challenge 2020 – Intra-lingual Semi-parallel and Cross-lingual Voice Conversion –. In <em>Proceedings of the Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020</em>. International Speech Communication Association, 80–98. <a href=\"https://doi.org/10.21437/VCC_BC.2020-14\">https://doi.org/10.21437/VCC BC.2020-14</a></p><p>[71]&nbsp;&nbsp; Mohammed A. Younus and Taha M. Hasan. 2020. Abbreviated View of Deepfake Videos Detection Techniques. In <em>Proceedings of the 2020 6th International Engineering Conference</em>. IEEE, 115–120. <a href=\"https://doi.org/10.1109/IEC49899.2020.9122916\">https://doi.org/10.1109/IEC49899.2020.9122916</a></p><p>[73]&nbsp;&nbsp; Teng Zhang, Lirui Deng, Liang Zhang, and Xianglei Dang. 2020. Deep Learning in Face Synthesis: A Survey on Deepfakes. In <em>Proceedings of the 2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology</em>. IEEE, 67–70. <a href=\"https://doi.org/10.1109/CCET50901.2020.9213159\">https://doi.org/10.1109/CCET5090</a><a href=\"https://doi.org/10.1109/CCET50901.2020.9213159\">1.2020.9213159</a></p><p>[74]&nbsp;&nbsp; Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. 2021. DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort. In <em>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE, 10140–10150. <a href=\"https://doi.org/10.1109/CVPR46437.2021.01001\">https://doi.org/10.1109/CVPR46437.2021.01001</a></p><p>[75]&nbsp;&nbsp;  ![](file:///C:/Users/user/AppData/Local/Temp/msohtmlclip1/01/clip_image051.gif)Yuanhan Zhang, ZhenFei Yin, Yidong Li, Guojun Yin, Junjie Yan, Jing Shao, and Ziwei Liu. 2020. CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations. In <em>Proceedings of the 2020 European Conference on Computer Vision</em>. Springer, 70–85. <a href=\"https://doi.org/10.1007/978-3-030-58610-2_5\">https://doi.org/10.1007/97</a><a href=\"https://doi.org/10.1007/978-3-030-58610-2_5\">8-3-030-58610-2 5</a></p><p>[76]&nbsp;&nbsp; Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu, Shuo Yang, Yuanjun Xiong, Wei Xia, Yan Xu, Man Luo, Jian Liu, Jianshu Li, Zhijun Chen, Mingyu Guo, Hui Li, Junfu Liu, Pengfei Gao, Tianqi Hong, Hao Han, Shijie Liu, Xinhua Chen, Di Qiu, Cheng Zhen, Dashuang Liang, Yufeng Jin, and Zhanlong Hao. 2021. CelebA-Spoof Challenge 2020 on Face Anti-Spoofing: Methods and Results. arXiv:2102.12642. <a href=\"https://arxiv.org/pdf/2102.12642.pdf\">https://arxiv.org/pdf/2102.12642.pdf</a></p><p>[77]&nbsp;&nbsp; Bo Zhao, Shaozeng Zhang, Chunxue Xu, Yifan Sun, and Chengbin Deng. 2021. Deep Fake Ge-ography? When Geospatial Data Encounter Artificial Intelligence. <em>Cartography and Geographic Information Science</em> 48, 4 (2021), 338–352. <a href=\"https://doi.org/10.1080/15230406.2021.1910075\">https://doi.org/10.1080/15230406.2021.1910075</a></p><p>[78]&nbsp;&nbsp; Peng Zhou, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2017. Two-Stream Neural Networks for Tampered Face Detection. In <em>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops</em>. IEEE, 1831–1839. <a href=\"https://doi.org/10.1109/CVPRW.2017.229\">https://doi.org/10.1109/CVPRW.2017.229</a></p><p>[79]&nbsp;&nbsp; Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, and Jianbing Shen. 2021. Face Forensics in the Wild. In <em>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE,&nbsp; 5774–5784.&nbsp;&nbsp;&nbsp; <a href=\"https://doi.org/10.1109/CVPR46437.2021.00572\">https://doi.org/10.1109/CVPR46437.2021.00572</a></p><p>[80]&nbsp;&nbsp; Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. In <em>Proceedings of the 2017 IEEE International Conference on Computer Vision</em>. IEEE, 2242–2251. <a href=\"https://doi.org/10.1109/ICCV.2017.244\">https://doi.org/10.1109/ICCV.2</a><a href=\"https://doi.org/10.1109/ICCV.2017.244\">017.244</a></p><p>[81]&nbsp;&nbsp; Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. 2020. WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection. In <em>Proceedings of the 2020 28th ACM International Conference on Multimedia</em>. ACM, 2382–2390. <a href=\"https://doi.org/10.1145/3394171.3413769\">https://doi.org/10.1145/339417</a><a href=\"https://doi.org/10.1145/3394171.3413769\">1.3413769</a></p><p>:::info\nThis paper is&nbsp;<a href=\"https://arxiv.org/abs/2208.10913\">available on arxiv</a>&nbsp;under CC by 4.0 Deed (Attribution 4.0 International) license.  </p>","contentLength":105782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: Why “Small Changes” Don’t Exist in Production Game Systems (2/28/2026)","url":"https://hackernoon.com/2-28-2026-newsletter?source=rss","date":1772294565,"author":"Noonification","guid":155014,"unread":true,"content":"<p>🪐 What’s happening in tech today, February 28, 2026?</p><p>By <a href=\"https://hackernoon.com/u/ktdevjournal\">@ktdevjournal</a> [ 5 Min read ] It doesn’t matter if you build games or a banking app - you don’t just have a pile of features and assets. You have an ecosystem for each bit of work <a href=\"https://hackernoon.com/why-small-changes-dont-exist-in-production-game-systems\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/Lima_Writes\">@Lima_Writes</a> [ 9 Min read ] When language comes back at you fast, coherent, and emotionally attuned, it feels like truth. Especially when you’re tired. Or lonely.  <a href=\"https://hackernoon.com/how-to-navigate-identity-direction-story-and-sovereignty-in-the-age-of-ai\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":677,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.22: A Change in Loop Scoping","url":"https://hackernoon.com/go-122-a-change-in-loop-scoping?source=rss","date":1772294421,"author":"Go [Technical Documentation]","guid":155013,"unread":true,"content":"<p>Go 1.21 includes a preview of a change to  loop scoping that we plan to ship in Go 1.22, removing one of the most common Go mistakes.</p><p>If you’ve written any amount of Go code, you’ve probably made the mistake of keeping a reference to a loop variable past the end of its iteration, at which point it takes on a new value that you didn’t want. For example, consider this program:</p><pre><code>func main() {\n    done := make(chan bool)\n\n    values := []string{\"a\", \"b\", \"c\"}\n    for _, v := range values {\n        go func() {\n            fmt.Println(v)\n            done &lt;- true\n        }()\n    }\n\n    // wait for all goroutines to complete before exiting\n    for _ = range values {\n        &lt;-done\n    }\n}\n</code></pre><p>\\\nThe three created goroutines are all printing the same variable , so they usually print “c”, “c”, “c”, instead of printing “a”, “b”, and “c” in some order.</p><p>\\\nAlthough concurrency is often involved, it need not be. This example has the same problem but no goroutines:</p><pre><code>func main() {\n    var prints []func()\n    for i := 1; i &lt;= 3; i++ {\n        prints = append(prints, func() { fmt.Println(i) })\n    }\n    for _, print := range prints {\n        print()\n    }\n}\n</code></pre><p>\\\nThis kind of mistake has caused production problems at many companies, including a <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1619047\">publicly documented issue at Lets Encrypt</a>. In that instance, the accidental capture of the loop variable was spread across multiple functions and much more difficult to notice:</p><pre><code>// authz2ModelMapToPB converts a mapping of domain name to authz2Models into a\n// protobuf authorizations map\nfunc authz2ModelMapToPB(m map[string]authz2Model) (*sapb.Authorizations, error) {\n    resp := &amp;sapb.Authorizations{}\n    for k, v := range m {\n        // Make a copy of k because it will be reassigned with each loop.\n        kCopy := k\n        authzPB, err := modelToAuthzPB(&amp;v)\n        if err != nil {\n            return nil, err\n        }\n        resp.Authz = append(resp.Authz, &amp;sapb.Authorizations_MapElement{\n            Domain: &amp;kCopy,\n            Authz: authzPB,\n        })\n    }\n    return resp, nil\n}\n</code></pre><p>\\\nThe author of this code clearly understood the general problem, because they made a copy of , but it turns out  used pointers to fields in  when constructing its result, so the loop also needed to make a copy of .</p><p>\\\nTools have been written to identify these mistakes, but it is hard to analyze whether references to a variable outlive its iteration or not. These tools must choose between false negatives and false positives. The  analyzer used by  and  opts for false negatives, only reporting when it is sure there is a problem but missing others. Other checkers opt for false positives, accusing correct code of being incorrect. We ran an analysis of commits adding  lines in open-source Go code, expecting to find bug fixes. Instead we found many unnecessary lines being added, suggesting instead that popular checkers have significant false positive rates, but developers add the lines anyway to keep the checkers happy.</p><p>\\\nOne pair of examples we found was particularly illuminating:</p><p>This diff was in one program:</p><pre><code>     for _, informer := range c.informerMap {\n+        informer := informer\n         go informer.Run(stopCh)\n     }\n</code></pre><p>\\\nAnd this diff was in another program:</p><pre><code>     for _, a := range alarms {\n+        a := a\n         go a.Monitor(b)\n     }\n</code></pre><p>\\\nOne of these two diffs is a bug fix; the other is an unnecessary change. You can’t tell which is which unless you know more about the types and functions involved.</p><p>For Go 1.22, we plan to change  loops to make these variables have per-iteration scope instead of per-loop scope. This change will fix the examples above, so that they are no longer buggy Go programs; it will end the production problems caused by such mistakes; and it will remove the need for imprecise tools that prompt users to make unnecessary changes to their code.</p><p>\\\nTo ensure backwards compatibility with existing code, the new semantics will only apply in packages contained in modules that declare  or later in their  files. This per-module decision provides developer control of a gradual update to the new semantics throughout a codebase. It is also possible to use  lines to control the decision on a per-file basis.</p><p>\\\nOld code will continue to mean exactly what it means today: the fix only applies to new or updated code. This will give developers control over when the semantics change in a particular package. As a consequence of our <a href=\"https://go.dev/blog/toolchain\">forward compatibility work</a>, Go 1.21 will not attempt to compile code that declares  or later. We included a special case with the same effect in the point releases Go 1.20.8 and Go 1.19.13, so when Go 1.22 is released, code written depending on the new semantics will never be compiled with the old semantics, unless people are using very old, <a href=\"https://go.dev/doc/devel/release#policy\">unsupported Go versions</a>.</p><p>Go 1.21 includes a preview of the scoping change. If you compile your code with  set in your environment, then the new semantics are applied to all loops (ignoring the  lines). For example, to check whether your tests still pass with the new loop semantics applied to your package and all your dependencies:</p><pre><code>GOEXPERIMENT=loopvar go test\n</code></pre><p>\\\nWe patched our internal Go toolchain at Google to force this mode during all builds at the start of May 2023, and in the past four months we have had zero reports of any problems in production code.</p><p>\\\nYou can also try test programs to better understand the semantics on the Go playground by including a  comment at the top of the program, like in <a href=\"https://go.dev/play/p/YchKkkA1ETH\">this program</a>. (This comment only applies in the Go playground.)</p><p>Although we’ve had no production problems, to prepare for that switch, we did have to correct many buggy tests that were not testing what they thought they were, like this:</p><pre><code>func TestAllEvenBuggy(t *testing.T) {\n    testCases := []int{1, 2, 4, 6}\n    for _, v := range testCases {\n        t.Run(\"sub\", func(t *testing.T) {\n            t.Parallel()\n            if v&amp;1 != 0 {\n                t.Fatal(\"odd v\", v)\n            }\n        })\n    }\n}\n</code></pre><p>\\\nIn Go 1.21, this test passes because  blocks each subtest until the entire loop has finished and then runs all the subtests in parallel. When the loop has finished,  is always 6, so the subtests all check that 6 is even, so the test passes. Of course, this test really should fail, because 1 is not even. Fixing for loops exposes this kind of buggy test.</p><p>\\\nTo help prepare for this kind of discovery, we improved the precision of the  analyzer in Go 1.21 so that it can identify and report this problem. You can see the report <a href=\"https://go.dev/play/p/WkJkgXRXg0m\">in this program</a> on the Go playground. If  is reporting this kind of problem in your own tests, fixing them will prepare you better for Go 1.22.</p><p>\\\nIf you run into other problems, <a href=\"https://go.dev/wiki/LoopvarExperiment#my-test-fails-with-the-change-how-can-i-debug-it\">the FAQ</a> has links to examples and details about using a tool we’ve written to identify which specific loop is causing a test failure when the new semantics are applied.</p><p>\\\n<em>This article is available on&nbsp;&nbsp;under a CC BY 4.0 DEED license.</em></p>","contentLength":6922,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why “Small Changes” Don’t Exist in Production Game Systems","url":"https://hackernoon.com/why-small-changes-dont-exist-in-production-game-systems?source=rss","date":1772294403,"author":"Constantine","guid":155012,"unread":true,"content":"<p>It’s just a small change!</p><p>\\\nHow often do we hear that we need to fix something? We need to add a small feature. We need to tweak something. Code-wise or publishing, just realized they need this for retention, or maybe an analyst brought the newest data, so now we have to add just a few lines to the code. They don’t affect performance or any other departments, I promise. And it’s just like 3 minutes of coder work - why not? Fast forward: they broke the “Buy” button on the front page of the store on release.</p><p>\\\nWhy does this always happen with small changes? Well, if we think about it, we don’t usually think about it. Let me explain:</p><p>Designers think in features and user experience. \\n Engineers think in whole systems. \\n Producers think in tasks. \\n Stakeholders think in business outcomes.</p><p>\\\nAnd one small change is always perceived as something isolated and usually without everyone’s awareness. So, it is basically a cognitive shortcut. And that happens not because everyone is wrong or unprofessional. It’s because modern production systems are highly interconnected, so it’s impossible to know what could potentially be affected by anything - especially if you haven’t worked on this project for 15 years.</p><p>What is modern production? I’m glad you asked!</p><p>\\\nIt doesn’t matter if you build games or a banking app - you don’t just have a pile of features and assets. You have an ecosystem for each bit of work: Art, Code, Design, UI, Marketing, Publishing (maybe even Project Management - wow, you are a rich developer), etc. And each one of them has its own infrastructure, pipelines, workflows, and shared assets. To simplify, it can be shared data schemas, builds, automation processes, UI bindings, and many other things.</p><p>\\\nWhat’s wrong if I just make a small color change to one of the icons? Well, that means you spend 3 seconds changing a color code. Then you have to assemble a build. Then QA has to check your small change to confirm that you indeed changed the color. Then you have to assemble the build again, which should be in a queue with other builds in the waiting list.</p><p>\\\nThen we have to update the server with your changes - oh wait, did you tell anyone about that? No? Oh, that’s great, because you just submitted your changes during the commit freeze, and now deployment engineers have to fix the CI/CD pipeline, and we have to postpone the release for 4 days because it’s Friday.</p><p>\\\nAnd by the way - we have to communicate that to users because they were waiting for this new version, and some of them decided not to wait that long and removed your app. Whoops, that’s awkward. Sorry to hear that.</p><p>That’s alright, I’m here to help you! Let me introduce you to Change Propagation Surface (CPS) - the number of systems, pipelines, assets, and workflows that a change must pass through before it reaches the player.</p><p>\\\nYour change should not be estimated by its task size, like “1 hour of work.” Your change equals CPS × Coupling Density (the amount of work other departments need to do in order for this change to pass).</p><p>\\\nThink about it this way:</p><ul><li>One small UI tweak touches no shared data - low CPS.</li><li>A gameplay rule change touching code, balance, design, analytics, player experience - high CPS.</li></ul><p>\\\nLet’s go back to the situation where you want to change the color of the icon. Those 3 seconds of work would affect UI, builds, player perception, experience, and design. It might also affect color coding for accessibility rules, plus build assembling, and finally server updates. It’s high CPS - of course, if you didn’t sneak that change in without everyone’s awareness (I see that - drop it!).</p><p>\\\nThe same goes for asset swaps or changing a stat value: it affects memory, AI tuning, destruction logic, etc. Don’t do that unless someone from senior leadership said it’s low CPS - then just do it and see how it goes.</p><p>\\\nYou can apply this approach basically anywhere in production because it is not an abstract thing at all and can be estimated.</p><ul></ul><p>\\\nEach of these items counts as a plus 1 CPS factor. Subsequently, the more of the same “items” you touch, the higher the CPS number you will get. And with that information, you can create a small estimation matrix like:</p><p>CPS 1-2 - Local change \\n CPS 3-5 - Cross-functional change \\n CPS 6+ - Systemic change</p><p>\\\nOne more time, the formula is: Impact = CPS × Coupling Density. Easy!</p><p>Let’s see how it works in a real-life example:</p><p>\\\nSo your developer went on holiday and completed a math course on LinkedIn. And when he came back, he said that there is a more efficient way of calculating EXP. This change is “one line of code.” Okay, but after reading this article, you already know how it works in reality and that it touches multiple things:</p><ul><li>Player progression pacing</li></ul><p>\\\nThat means CPS is more than 7. So now you see that even though the code diff is tiny, the propagation surface is systemic and has a massive potential outcome. In other words, if XP progression speeds up things like economy, availability of the content, battle pass value, retention curves, etc., you should know that even if the implementation takes about 10 minutes, the ripple effect can take weeks of work.</p><p>\\\nWhy does live service production make it worse? Because it is amplified by content being reused across multiple features, by telemetry and economy being tightly coupled, and by systems being persistent and often requiring backward compatibility.</p><p>\\\nSo, the real cost you pay for propagation lies in prolonged timelines, hidden rework, cross-team friction, technical debt, burnout, and eventually, people resigning directly or indirectly.</p><p>\\\nInstead of thinking, “Oh, this is a small change,” we should probably think, “What systems does this change touch?” Think about this as infrastructure, not a feature, and always try to bring that to cross-team awareness. And if you are capable enough, try to estimate the surface area, not just this exact small change.</p><p>\\\n**The whole point of my way-too-long introduction is that there is no such thing as a small change in production systems. There are only changes in misunderstood affected areas. And the more senior you become, the more your vision shifts toward understanding how this change will travel instead of trying to avoid the change altogether.</p>","contentLength":6294,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Startup Plans April Launch for a Satellite Reflect Sunlight to Earth at Night","url":"https://science.slashdot.org/story/26/02/28/076229/startup-plans-april-launch-for-a-satellite-reflect-sunlight-to-earth-at-night?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772292840,"author":"EditorDavid","guid":154980,"unread":true,"content":"A start-up called Reflect Orbital \"proposes to use large, mirrored satellites to redirect sunlight to Earth at night,\" reports the Washington Post, \"with plans to bathe solar farms, industrial sites and even entire cities in light that could, if desired, reach the intensity of daylight....\" \n\nSlashdot noted their idea in 2022 — but Reflect Orbital now expects to launch its first satellite in April, according to the article. \"But its grand vision is largely 'aspirational,' as its young founder, Ben Nowack, told me...\"\n\nReflect Orbital's Nowack describes a scene right out of sci-fi: An extremely bright star appears on the northern horizon and makes its way across the sky, illuminating a 5-kilometer circle on Earth, then setting on the southern horizon about five minutes later, just as another such \"star\" appears in the north. To make the night even brighter, a customer could make 10 \"stars\" appear at once in the north by ordering them on an app. Two such artificial stars are in development in Reflect Orbital's factory. Nowack showed them to me on a Zoom call. The first to launch is 50 feet across, but he plans later to build them three times that size. If all goes according to plan, he'll have 50,000 of them circling the Earth in 2035 at an altitude of around 400 miles. \nNowack plans to start selling the service \"in mostly developing nations or places that don't have streetlights yet.\" Eventually, he thinks, he can illuminate major cities, turn solar fields and farms into round-the-clock operations for any business or municipality that pays for it. He likened his technology to the invention of crop irrigation thousands of years ago. \"I see this as much the same thing,\" he said, arguing that people would no longer have to \"wait for the sun to shine.\" \n\nThe article adds that Elon Musk's SpaceX \"wants to launch as many as a million satellites to serve as orbiting data centers — 70 times the number of satellites now in orbit.\" (America's satellite-regulation Federal Communications Commission\ngrants a \"categorical exclusion\" from environmental review to satellites on the grounds that their operations \"normally do not have significant effects on the human environment.\") \n\nThe public comment periods for the two proposals close on March 6 and March 9.","contentLength":2285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Startup Plans April Launch for a Satellite to Reflect Sunlight to Earth at Night","url":"https://science.slashdot.org/story/26/02/28/076229/startup-plans-april-launch-for-a-satellite-to-reflect-sunlight-to-earth-at-night?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772292840,"author":"EditorDavid","guid":154992,"unread":true,"content":"A start-up called Reflect Orbital \"proposes to use large, mirrored satellites to redirect sunlight to Earth at night,\" reports the Washington Post, \"with plans to bathe solar farms, industrial sites and even entire cities in light that could, if desired, reach the intensity of daylight....\" \n\nSlashdot noted their idea in 2022 — but Reflect Orbital now expects to launch its first satellite in April, according to the article. \"But its grand vision is largely 'aspirational,' as its young founder, Ben Nowack, told me...\"\n\nReflect Orbital's Nowack describes a scene right out of sci-fi: An extremely bright star appears on the northern horizon and makes its way across the sky, illuminating a 5-kilometer circle on Earth, then setting on the southern horizon about five minutes later, just as another such \"star\" appears in the north. To make the night even brighter, a customer could make 10 \"stars\" appear at once in the north by ordering them on an app. Two such artificial stars are in development in Reflect Orbital's factory. Nowack showed them to me on a Zoom call. The first to launch is 50 feet across, but he plans later to build them three times that size. If all goes according to plan, he'll have 50,000 of them circling the Earth in 2035 at an altitude of around 400 miles. \nNowack plans to start selling the service \"in mostly developing nations or places that don't have streetlights yet.\" Eventually, he thinks, he can illuminate major cities, turn solar fields and farms into round-the-clock operations for any business or municipality that pays for it. He likened his technology to the invention of crop irrigation thousands of years ago. \"I see this as much the same thing,\" he said, arguing that people would no longer have to \"wait for the sun to shine.\" \n\nThe article adds that Elon Musk's SpaceX \"wants to launch as many as a million satellites to serve as orbiting data centers — 70 times the number of satellites now in orbit.\" (America's satellite-regulation Federal Communications Commission\ngrants a \"categorical exclusion\" from environmental review to satellites on the grounds that their operations \"normally do not have significant effects on the human environment.\") \n\nThe public comment periods for the two proposals close on March 6 and March 9.","contentLength":2285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meet M6: The Chinese AI That Understands Text and Images at Scale","url":"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss","date":1772292503,"author":"Alibaba","guid":155011,"unread":true,"content":"<ol><li>Junyang Lin, junyang.ljy@alibaba-inc.com (Alibaba Group, China)</li><li>Rui Men, menrui.mr@alibaba-inc.com (Alibaba Group, China)</li><li>An Yang, ya235025@alibaba-inc.com (Alibaba Group, China)</li><li>Chang Zhou, ericzhou.zc@alibaba-inc.com (Alibaba Group, China)</li><li>Ming Ding, dm18@mails.tsinghua.edu.cn (Tsinghua University, China)</li><li>Yichang Zhang, yichang.zyc@alibaba-inc.com (Alibaba Group, China)</li><li>Peng Wang, zheluo.wp@alibaba-inc.com (Alibaba Group, China)</li><li>Ang Wang, wangang.wa@alibaba-inc.com (Alibaba Group, China)</li><li>Le Jiang, jiangle.jl@alibaba-inc.com (Alibaba Group, China)</li><li>Xianyan Jia, xianyan.xianyanjia@alibaba-inc.com (Alibaba Group, China)</li><li>Jie Zhang, wanglin.zj@alibaba-inc.com (Alibaba Group, China)</li><li>Jianwei Zhang, zhangjianwei.zjw@alibaba-inc.com (Alibaba Group, China)</li><li>Xu Zou, zoux18@mails.tsinghua.edu.cn (Tsinghua University, China)</li><li>Zhikang Li, zhikang.lzk@alibaba-inc.com (Alibaba Group, China)</li><li>Xiaodong Deng, xiaodongdeng.dxd@alibaba-inc.com (Alibaba Group, China)</li><li>Jie Liu, sanshuai.lj@alibaba-inc.com (Alibaba Group, China)</li><li>Jinbao Xue, zhiji.xjb@alibaba-inc.com (Alibaba Group, China)</li><li>Huiling Zhou, zhule.zhl@alibaba-inc.com (Alibaba Group, China)</li><li>Jianxin Ma, jason.mjx@alibaba-inc.com (Alibaba Group, China)</li><li>Jin Yu, kola.yu@alibaba-inc.com (Alibaba Group, China)</li><li>Yong Li, jiufeng.ly@alibaba-inc.com (Alibaba Group, China)</li><li>Wei Lin, weilin.lw@alibaba-inc.com (Alibaba Group, China)</li><li>Jingren Zhou, jingren.zhou@alibaba-inc.com (Alibaba Group, China)</li><li>Jie Tang, jietang@tsinghua.edu.cn (Tsinghua University, China)</li><li>Hongxia Yang, yang.yhx@alibaba-inc.com (Alibaba Group, China)</li></ol><p>In this work, we construct the largest dataset for multimodal pre-training in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called , referring to ulti-odality to ulti-odality ultitask ega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and  parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.</p><p>Multimodal Pretraining; Multitask; Text-to-Image Generation</p><p>Pretraining has become a focus in the research in natural language processing (NLP) [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark17\">1</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark18\">2</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark23\">7</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark32\">16</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark34\">18</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark35\">19</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark43\">27</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark47\">31</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark53\">37</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark60\">44</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark65\">49</a>]. The recent GPT-3 with over 175 billion parameters demonstrates that large models trained on big data have extremely large capacity and it can outperform the state-of-the-arts in downstream tasks especially in the zero-shot setting. Also, the rapid development of pretraining in NLP sparkles cross-modal pretraining. A number of studies [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark20\">4</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark27\">11</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark33\">17</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark38\">22</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark40\">24</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark41\">25</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark44\">28</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark45\">29</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark54\">38</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark67\">51</a>] have created new state-of-the-art performances for various cross-modal downstream tasks.</p><p>A pity is that most recent studies focus on the pretraining on English data. There are lack of both large-scale datasets in Chinese and large-scale models pretrained on the data of Chinese. Therefore, in this work, we develop a large-scale dataset M6-Corpus, which consists of over 1.9TB images and 292GB texts. To the best of our knowledge, this is the largest dataset in Chinese for pretraining in both multimodality and natural language. The dataset collected from the webpages consists of different types of data and covers a large scale of domains, including encyclopedia, question answering, forum discussion, product description, etc. Also, we design sophisticated cleaning procedures to ensure that the data are of high quality.</p><p>Furthermore, in order to sufficiently leverage such a large amount of high-quality data, we propose to build an extremely large model that can process data of multiple modalities and adapt to different types of downstream tasks. Thus we propose a novel model called M6, referring to MultiModality-to-MultiModality Multitask Mega-transformer. The model is based on the transformer, and it is pretrained with multiple tasks. Pretraining endows the model with the capability of single-modality and multimodality understanding and generation. Based on the architecture of M6, we build  and , which are scaled up to 10 billion and 100 billion pa-rameters respectively. To be more specific,  is the recent largest model pretrained on Chinese data. We apply the model to a series of downstream applications, including product description generation, visual question answering, community question answering, Chinese poem generation, etc., and our experimental results show that M6 outperforms a series of strong baselines.</p><p>Another contribution of this work is that we first incorporate pretraining with text-to-image generation. Following Ramesh et al. <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark48\">[32]</a>, we leverage a two-stage framework for image generation. To be more specific, we apply a trained vector-quantized generative adversarial network to representing images with discrete image codes, and we then use the pretrained M6 to learn the relations between texts and codes. Such learning can bridge the two modalities and enables controllable text-to-image generation.</p><p>To summarize, the contributions of M6 are as follows:</p><ul><li>We collect and build the largest Chinese multi-modal pre-training data in industry, which includes 300GB texts and 2TB images.</li><li>We propose M6 for multimodal pretraining in Chinese, and we scale the model size to up to 10 and 100 billion parameters. Both M6-10B and M6-100B are the recent largest multimodal pretrained model.</li><li>M6 is versatile and exceeds strong baselines by 11.8% in VQA, 18.4 in image captioning, and 10.3% in image-text matching. Furthermore M6 is able to generate high-quality images.</li><li>With carefully designed large-scale distributed training optimizations, M6 has obvious advantages in training speed and greatly reduces training costs, creating the possibility for more widespread use of multi-modal pretraining.</li></ul><p>We collect and develop the largest multi-modality and text dataset in Chinese for now, which is one of the key contributions of this paper. In this section, we first identify the limitations of existing datasets and then describe the construction and preprocessing procedure of our proposed dataset.</p><h2>2.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Existing Datasets</h2><p>The construction of large-scale corpus with high quality and do-main coverage is crucial to Chinese pretraining. In early previous works, the Chinese Wikipedia1&nbsp;is one of the most frequently used datasets to train Chinese language models. It contains 1.6GB texts (around 0.4B tokens) covering around 1M encyclopedia entries. Another corpus with a comparable size is the THUCTC[<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark55\">39</a>] dataset, which includes 740K news articles. However, with the rapidly increasing capacity of recent language models, the scale of these existing datasets is clearly insufficient. Recently, Cui et al. <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark21\">[5]</a> employ unreleased extended data that are 10 times larger than the CN-Wikipedia to pretrain their Chinese language model. Xu et al.</p><p><a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark63\">[47]</a> released a 100GB corpus named CLUECorpus2020, which is retried from the multilingual Common Crawl dataset. However, the scale of the datasets is still insufficient to facilitate super large-scale pretraining compared with existing English pretrained models. For example, GPT-3 contains 175B parameters and is trained on 570GB texts. Meanwhile, the dataset should contain image-text pairs rather than plain texts for multi-modal pretraining.</p><h2>2.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Standards for a High-quality Dataset</h2><p>To perform large-scale multi-modal pretraining and learn complex world knowledge in Chinese, the dataset is highly required to provide both plain texts and image-text pairs on super large scale, covering a wide range of domains. In order to perform large-scale multi-modal pretraining in Chinese, we focus on the construction of large-scale datasets in Chinese. Specifically, while we unify our pretraining for both natural language and multimodalities, we construct large datasets of both plain texts and image-text pairs. We are interested in obtaining large-scale data that covers a wide range of domains, so that it is possible for the model to learn the complex world knowledge of different fields. Also, we aim to collect data of multiple modalities for the cross-modal pretraining. This raises the difficulty for the construction of a large-scale dataset as the data for multimodal pretraining are usually image-text pairs, where in each pair the text provides a detailed description of a fraction of the image.</p><p>Though there are a tremendous amount of text resources and images on the world wide web, the corpus for multimodal pretraining is assumed to be better when satisfying the following properties:</p><p>(1). the sentences should be fluent natural language within a normal length, and should not contain meaningless tokens, such as markups, duplicate punctuation marks, random combinations of characters, etc.; (2). the images should be natural and realistic, and the resolutions of the images need to be identifiable by humans; (3). both the texts and images should not contain illegal content, such as pornography, violence, etc.; (4). the images and texts should be semantically relevant; (5). the datasets should cover a wide range of fields, say sports, politics, science, etc., and therefore it can endow the model with sufficient world knowledge.</p><h2>2.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Dataset Construction</h2><p>Based on the requirements above, we collect data of both plain texts and image-text pairs. There are different types of data, including encyclopedia, crawled webpage, community question answering, forum, product description, etc. We present the details in Table <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark2\">3.</a> The collected corpus consists of bothag plain-texts and image-text pairs, which is compatible with the designed text-only and multi-modal pretraining tasks. Also, the data has a large coverage over domains, such as science, entertainment, sports, politics, common-sense of life, etc. We have also compared some characteristics of our corpus with existing datasets used for Chinese pretraining in Table <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark1\">2.</a> The size of our dataset is much larger than the previous ones. To our knowledge, this is the first large-scale, multimodal and multidomain corpus for Chinese pretraining.</p><p>We implement sophisticated preprocessing to obtain clean data. For text data, we first remove HTML markups and duplicate punctuation marks, and we only reserve characters and punctuation marks that are in Chinese and English. We remove the topics that are shorter than 5 characters and contents shorter than 15 characters. We further apply in-house spam detection to remove sentences that contain words related to certain political issues, pornography, or words in the list of dirty, naughty, and other bad words. In order to preserve the linguistic acceptance of the texts, we implement a language model to evaluate their perplexities, and sentences with high perplexities are discarded. Only images with at least 5000 pixels are reserved for pretraining. A sequence of classifiers and heuristic rules are applied to filter out images containing illegal content. We also use a pretrained image scorer to evaluate the qual-ities of images. For images and texts in crawled webpages, we only consider images and their surrounding text as relevant image-text pairs. Other sentences in the webpages are discarded.</p><p>Multimodal pretraining leverages both the power of self-attention-based transformer architecture and pretraining on large-scale data. We endeavor to endow the model with strong capability of cross-modal understanding and generation. In this section, we describe the details of our proposed pretrained model , which refers to ulti-odality-to-ulti-odality ultitask ega-transformer.</p><h2>3.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Å &nbsp; Visual and Linguistic Inputs</h2><p>The mainstream multimodal pretraining methods transform images to feature sequences via object detection. However, the performance of the object detectors as well as the expressivity of their backbones strongly impact the final performance of the pretrained models in the downstream tasks. We observe that a large proportion of the images contain only a few objects. Take the images of the data of e-commerce as an example. We randomly sample 1M images and perform object detection on the images. The results show that over 90% of the images contain fewer than 5 objects. Also, the objects have high overlapping with each other. To alleviate such influence, we turn to a simple but effective solution following Gao et al. [\\[12\\]](#<em>bookmark28) and Dosovitskiy et al. [\\[8\\]](#</em>bookmark24). In general, we split an image into patches and extract features of the 2D patches with a trained feature extractor, say ResNet-50. Then we line up the representations to a sequence by their positions.  The processing of the input word sequence is much simpler. We follow the similar preprocessing procedures in the previous work [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark20\">4</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark27\">11</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark40\">24</a>]. We apply WordPiece [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark50\">34</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark61\">45</a>] and masking to the word sequence and embed them with an embedding layer, following BERT <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark22\">[6].</a></p><h2>3.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Unified Encoder-Decoder</h2><p>We integrate the image embeddings 𝑒𝑖&nbsp;and the word embeddings 𝑒𝑡&nbsp;into the cross-modal embedding sequence 𝑒 = {𝑒𝑖, 𝑒𝑡 }. We send the sequence to the transformer backbone for high-level feature extraction. To differ their representations, we add corresponding segment embeddings for different modalities. Specifically, we leverage the</p><p>self-attention-based transformer blocks for our unified cross-modal representation learning. To be more specific, the building block is identical to that of BERT or GPT, which consists of self attention and point-wise feed-forward network (FFN). On top of the transformer backbone, we add an output layer for word prediction, and thus we tie its weights to those of the embedding layer.</p><p>In the unified framework, we use different masking strategies to enable encoding and decoding. The input is segmented into three parts, including visual inputs, masked linguistic inputs, and complete linguistic inputs. We apply bidirectional masking to both the visual inputs and masked linguistic inputs, and we apply causal masking to the complete linguistic inputs. Thus the model is allowed to encode and decode in the same framework.</p><h2>3.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pretraining Methods</h2><p>We pretrain the model with the multitask setup, including text-to-text transfer, image-to-text transfer, and multimodality-to-text transfer. Thus the model can process information of different modalities and perform both single-modal and cross-modal understanding and generation.</p><p> As demonstrated in Figure <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark3\">3,</a> the model learns to perform text denoising and language modeling in the setting of text-to-text transfer. In text denoising, we mask the input text by a proportion, which is 15% in practice following BERT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark22\">6</a>]. Specifically, we mask a continuous span of text with a single mask, and the model should learn to decode the whole sequence. This encourages the model to learn both recovering and length predict-ing. Besides, in order to improve the model ability in generation, we add a setup of language modeling, where the encoder receives no inputs and the decoder learns to generate words based on the previous context.</p><p>\\\n Image-to-text transfer is similar to image captioning, where the model receives the visual information as the input, and learns to generate a corresponding description. In this setting, we add the aforementioned patch feature sequence to the input and leave the masked input blank. The model encodes the patch features, and decodes the corresponding text.</p><p><strong>Multimodality-to-text transfer</strong> Based on the setup of image-to-text transfer, we additionally add masked linguistic inputs, and thus the model should learn to generate the target text based on both the visual information and the noised linguistic information. This task allows the model to adapt to the downstream tasks with both visual and linguistic inputs.</p><h2>3.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Scaling up to 10 and 100 Billion Parameters</h2><p>We scale up the model size to 10 billion parameters and 100 billion parameters, which are named M6-10B and M6-100B. The increase in model size provides a much larger capacity for the model that it can learn knowledge from more data. For the construction of M6-10B, we simply scale up the model by hyperparameter tuning.</p><p>To be more specific, we increase the size of hidden states and the number of layers. To better leverage GPU memory, we apply mixed-precision training and activation checkpointing to save memory. Still, the model cannot be fit into one single GPU, and thus we use model parallelism to split the feed-forward networks and attention heads to multiple GPUs following the implementation of Megatron-LM <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark52\">[36].</a></p><p>However, directly scaling up to M6-100B is much more difficult as there are more challenges for the computation resources. Alternatively, inspired by the recent progress in sparse activations [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark26\">10</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark36\">20</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark51\">35</a>], we combine Mixture-of-Experts (MoE) with M6 to build the version of 100 billion parameters. Note that the original MoE requires mesh-tensorflow as well as TPUs. This sets limits for a number of researchers without such resources. Thus we implement the M6-100B with MoE with our in-house framework Whale [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark59\">43</a>] to perform model parallelism with GPUs. We demonstrate the key statistics of the models of different scales in Table <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark4\">4.</a></p><p>Specifically, different from the conventional FFN layer, the MoE layer is a parallel combination of multiple FFN layers, each of which acts as an expert. This is also called expert parallelism. The model first learns a sparse gating network to route the tokens to specific experts. Thus each token is only sent to a small set of experts and the computation can be much less compared with that in dense models. This kind of model is highly efficient as it realizes data parallelism and expert parallelism across workers. The computation of MoE layer for a specific token 𝑥 can be described as below:</p><p>where 𝑔(·) refers to the sparse gating function, and T refers to the indices of top-𝑘 values of 𝑔(·). The output of MoE is a linear combination of the computation of selected expert FFNs 𝑓 (·).</p><p>In expert parallelism, the parameters of experts do not share across workers, while those of other parts are identical across workers. Therefore, it is necessary to perform all-to-all communication across workers at the MoE layers in order to dispatch tokens to selected experts and combine them to their original experts. While Lepikhin et al. <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark36\">[20]</a> and Fedus et al. <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark26\">[10]</a> implement the MoE on TPUs with one expert in each MoE layer on a TPU, we implement our model on Nvidia GPUs where there are several experts in each MoE layer on a GPU so as to fully utilize the memory. As all-to-all communication takes up a large amount of time, the optimization to improve efficiency is highly significant. We implement a series of optimization, including half-precision communication. A key problem is load balancing, which denotes that tokens can gather to only a few experts due to dynamic routing. Following Fedus et al. <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark26\">[10]</a>, we apply expert capacity, which refers to the number of tokens for an expert (𝐶 = 𝑁&nbsp;- 𝑐/m, where 𝐶 refers to expert capacity, 𝑁 refers to the number of tokens in a batch, 𝑐 refers to capacity factor (which is a hyperparameter usually larger than 1.0) and 𝑚 refers to the number of experts), to alleviate this problem. Tokens out of the capacity of an expert are dropped from the computation and they are sent to next layers through residual connections. We find that the overloading problem can be severe, and this issue can be a significant one in the future research of expert models.</p><p>Besides the optimization in all-to-all communication, we com-pare the top-2 gating and top-1 gating and find that they can achieve similar model performance in perplexity, while the latter converges slightly slower. The effectiveness of top-1 gating enables faster computation. Besides, we also apply methods of memory optimization for higher efficiency. We find that gradient clipping globally can increase costs on all-to-all communication as it computes norms across all experts, and thus we apply local clipping for memory saving. We implement M6-100B with around 100 billion parameters on 128 Nvidia A100s and the speed of pretraining achieves 1440 samples/s (for samples of the sequence length of 272).</p><p>We demonstrate that using MoE structure for model size scaling is effective and it can achieve similar performance to that of M6-10B, the largest dense model, within 2-3 times shorter time. The negative log perplexity of M6-100B reaches −2.297, in comparison with M6-10B that reaches −2.253 but with twice of time.2 This shows that the MoE-based M6 model has advantages on the time basis compared with dense models with many more FLOPs.</p><h2>4.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Text-to-Image Generation</h2><p>Text-to-image generation has been an open problem for a long time. Previous studies mainly focused on generation on a limited domain, among which Generative Adversarial Nets (GANs) [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark30\">14</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark64\">48</a>] are dominated methods. Following Ramesh et al. <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark48\">[32]</a>, we leverage a two-stage framework for text-to-image generation, including discrete representation learning and language modeling.</p><p>\\\nIn the first stage, we focus on transforming images into sequences of discrete codes. There are a number of alternatives for discrete code generation, including VQVAE [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark57\">41</a>] and VQGAN [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark25\">9</a>]. In the second stage, it is necessary to build a language model to learn to generate text and code sequence. In the finetuning, we add code embedding and output layers to the pretrained M6. We concat the word sequence and the aforementioned generated code sequence as the input, and we set the objective of autoregressive language modeling for the training. At the stage of inference, we input the text sequence, and the model generates codes autoregressively with top-k sampling. The last step is to transform the code sequence to an image with the generator from the first stage.</p><p>We construct a dataset for text-to-image generation in E-commerce. Specifically, we collect over 50 million product titles and images from the mobile Taobao. We apply a series of processing methods on the images to filter the unqualified. We filter the images with complex background features (characters, patterns, etc.) with the in-house white-background image detector and OCR model. We then filter the images with over 3 objects with our in-house object detector based on Faster R-CNN [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark49\">33</a>]. We finally obtain 1.8m high-quality product image-text pairs for finetuning. Compared with the images in the general domains, our collected data have the following features. The image and text are highly correlated as the text describes key features of the product, and there is no complex background in the images, which is easier to learn compared with the images in the public datasets such as MSCOCO <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark42\">[26].</a></p><p>We demonstrate two examples in Figure <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark7\">4</a> and Figure <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark8\">5.</a> It can be found that the generated images have high quality and the generated objects resemble the real ones. Furthermore, in Figure <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark9\">6</a> , we find that the model is able to imagine items according to the query <em>military style camouflage high heels(军旅风迷彩高跟鞋</em>), which do not exist in the real world. The imagination ability provides room for creative design in real-world industrial scenarios, such as clothing design, shoe design, etc.</p><p>We also finetune M6 under our proposed framework on another dataset which contains 3 million images crawled from the Internet, which cover more general domains. And we find that the model can adapt to different domains. As shown in Figure <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark10\">7,</a> the model is able to generate clip arts of robots . This reveals the versatility of the framework in text-to-image generation.</p><h2>4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Question Answering</h2><p>We demonstrate our experimental results on a visual question answering dataset, and we illustrate how we directly apply the pre-trained M6 to the VQA application.</p><p>\\\nWe leverage the FMIQA dataset [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark29\">13</a>] as the Chinese visual QA benchmark, which requires the model to generate the answer given an image and a question. We implement a transformer-based model as our baseline. For the evaluation, we split the test set manually by random sampling 200 from the dataset as there is no official release of the test set, and we evaluate the overall accuracy by human evaluation. The results are demonstrated in Table <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark5\">5.</a> The pretrained M6-base outperforms the baseline by a large margin (+6.2%), which indicates the effectiveness of multimodal pretraining. Scaling up the model to M6-10B further brings 5.2% improvement.</p><p>Furthermore, we show that simply finetuning on such a small VQA dataset may limit the potential of M6. Therefore, we directly leverage M6 for the VQA application. We find that the model is able to recognize general features and provide more related knowledge based on its understanding. Though the model pretrained on pseudo-parallel image-text pairs cannot directly answer questions about detailed features, such as color, number, etc., it is able to answer questions related to background knowledge. We demonstrate some examples in Figure <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark12\">8.</a></p><h2>4.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Image Captioning</h2><p>Image captioning requires the model to generate a caption that describes the given image, which examines the model ability of cross-modal generation. We construct a dataset (named E-Commerce IC) containing pairs of product descriptions and product images from Taobao. Since too long or too short descriptions may be noisy, we discard pairs with a description longer than 100 words or less than 10 words. To avoid dirty generations, we further use an in-house tool to filter descriptions that may contain dirty words (i.e., pornographic or violent words). Finally, E-Commerce IC contains about 260k text-image pairs. We finetune the model with the image-to-text transfer task on E-Commerce IC.</p><p>\\\nWe compare our model with a baseline of transformer in the human evaluation. We ask several annotators with the linguistic background to evaluate from three perspectives: grammar (whether a text is fluent without grammatical error), correctness (whether a text is faithful to the image), richness (whether a text is informative and attractive). During the evaluation, we randomly sample 100 images from the test set. For each image, an annotator is asked to score the text generated by different models. The scores are within the range of [0, 5].</p><p>The results in Table <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark11\">6</a> show that M6-base outperforms the baseline in all of the metrics. We find that all models achieve high scores in grammar. However, in both correctness and richness, M6-base outperforms the baseline model by a large margin (+18.2% and +14.4%), indicating that multimodal pretraining helps to generate more faithful, informative and attractive texts. Scaling up the model to M6-10B further improves the correctness and richness (about 14.7% and 7.0%). Figure 9 illustrates two examples of image caption.</p><h2>4.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Question Answering</h2><p>To demonstrate the potential availability in the applications of intelligent chatbots, we further employ the M6 model to generate long answers in the style of forum discussion. Human-generated questions are collected from various Chinese forums, which are input to the model to generate the answer. At the stage of inference, we append a question mark and a token  in the prompt, which better triggers the model to generate an answer. To facilitate the generation of longer and more informative texts, we pick more complex questions.</p><p>Figure <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark14\">10</a> demonstrates an example of general question answer-ing. The model can illustrate a man’s own experiences that are related to the question and also point out the answer at the end. This generated text confused human annotators and passed the Turing Test. It shows that the model can not only answer general questions but also generate long fluency text.</p><p>We apply the pretrained model to Chinese poem generation. The model is able to generate genres with format constraints.</p><p>\\\nAncient Chinese poetry has various specific formats. We adopt the simplest constraints that</p><ul><li>The poem shall be consisted of at least 4 lines.</li><li>The total number of lines shall be even.</li><li>Each line must have exactly 5 or 7 words.</li><li>All lines shall have the same number of words.</li></ul><p>Text generation under format constraint is done in a search framework that we generate short sentences ending with punctuation until the number of words meets the constraint. We repeat this process until the model generates an \"\" token, or the number of lines exceeds a limit of 16. Figure <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark15\">11</a> illustrates an example of a generated poem.</p><h2>4.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Image-Text Matching</h2><p>We evaluate the model’s ability in cross-modal retrieval. Specifically, we construct a dataset (named E-Commerce ITM) containing pairs of texts and images from the mobile Taobao. Each pair belongs to a single item. we collect 235K products in the clothing industry from Taobao. For each product, aside from the product image, we obtain a query by rewriting the product title. Specifically, we conduct named entity recognition on the title using an in-house tool, which extracts the terms describing the style, color, category and texture of the product.</p><p>\\\nThese terms are then concatenated into a natural language query, which is used in image-text matching. The length of each query is between 6 to 12 words. The pairs of the query and corresponding product image are labeled as positive samples. The negative samples are constructed by randomly substituting the query in the original pairs.</p><p>We require the model to perform binary classification to discriminate positive and negative samples. We compare our model with InterBert [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark41\">25</a>], which is also a Chinese multi-modal pretrained model effective in cross-modal classification downstream tasks. The InterBert utilizes object-based features and has been pretrained on Taobao product image-text data as well.</p><p>The results are shown in Table <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark16\">7.</a> It should be noted that the InterBert and M6-base are both implemented with transformer-based architecture and have similar model scales. However, M6-base still outperforms InterBert by 10.3%. In experiments, we find the product images generally contain relatively fewer detected objects, which may harm the performance on this task. In contrast, M6 avoids this problem by employing the patch features and achieves much better performance.</p><p>The tremendous success of NLP pretraining, including BERT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark22\">6</a>], GPT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark18\">2</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark46\">30</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark47\">31</a>], and also some other related studies [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark17\">1</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark23\">7</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark35\">19</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark43\">27</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark65\">49</a>], inspires the research in cross-modal representation learning. Also, recent studies show that the ubiquitous Transformer architecture [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark58\">42</a>] can be extended to different fields, including computer vision [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark19\">3</a>, <a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark24\">8</a>]. Therefore, the simplest solution to incorporate recent pretraining methods and cross-modal representation learning is the extension of BERT. From the perspective of architecture, there are mainly two types, including single-stream model and dual stream model. Specifically, single-stream model is simple and it gradually becomes the mainstream architecture. These models mostly differ in their designs of pretraining tasks or the construction of input im-age features. Basically, they are mainly pretrained masked language modeling, masked object classification, and image-text matching. VisualBERT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark39\">23</a>] and Unicoder-VL [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark38\">22</a>] simply use BERT and are pretrained with the aforementioned tasks. UNITER [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark20\">4</a>] pretrains the model with an additional task of word-region alignment. Oscar [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark40\">24</a>] enhances the alignment between objects and their corresponding words or phrases. VILLA [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark27\">11</a>] further improves model performance by adding their proposed adversarial learning methods to pretraining and finetuning. Except for pretraining tasks, some studies focus on the features of images. Most pretraining methods for multimodal representation learning utilize the features generated by a trained object detector, say Faster R-CNN [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark49\">33</a>]. PixelBERT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark33\">17</a>] accepts raw images as input and extract their latent representations with a learnable ResNet [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark31\">15</a>] or ResNext [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark62\">46</a>]. FashionBERT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark28\">12</a>] splits the images into patches with a trained ResNet without co-training. Besides single-stream models, dual-stream models also can achieve outstanding performance, such as VilBERT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark44\">28</a>], LXMERT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark56\">40</a>] and InterBERT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark41\">25</a>]. ViLBERT-MT [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark45\">29</a>] enhances model performance with multi-task finetuning. ERNIE-ViL [<a href=\"https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss#_bookmark66\">50</a>] enhances the model with the application of scene graph information. In spite of these successful cases, it still requires further researches to unmask the success of multimodal pretraining.</p><p>In this work, we propose the largest dataset M6-Corpus for pre-training in Chinese, which consists of over 1.9TB images and 292GB texts. The dataset has large coverage over domains, including encyclopedia, question answering, forum discussion, common crawl, etc. We propose a method called M6 that is able to process information of multiple modalities and perform both single-modal and cross-modal understanding and generation. The model is scaled to large model with 10B and 100B parameters with sophisticated deployment, and both models are the largest multimodal pretrained models. We apply the model to a series of downstream applications, showing its versatility. More specifically, we design a downstream task of text-guided image generation, and the finetuned M6 can reach superior performance by producing images of high quality.</p><p>In the future, we will continue the pretraining of extremely large models by increasing the scale of data and models to explore the limit of performance, and we also endeavor to search for more downstream applications for further generalization.</p><p>[1]&nbsp;&nbsp; Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-masked language models for unified language model pre-training. In <em>International Conference on Machine Learning</em>. PMLR, 642–652.</p><p>[2]&nbsp;&nbsp; Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,</p><p>Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. <em>arXiv preprint arXiv:2005.14165</em> (2020).</p><p>[3]&nbsp;&nbsp; Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In <em>European Conference on Computer Vision</em>. Springer, 213–229.</p><p>[4]&nbsp;&nbsp; Y en-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. UNITER: UNiversal Image-TExt Representation Learning. In . 104–120.</p><p>[5]&nbsp;&nbsp; Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting pre-trained models for chinese natural language processing. <em>arXiv preprint arXiv:2004.13922</em> (2020).</p><p>[6]&nbsp;&nbsp; Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In . 4171–4186.</p><p>[7]&nbsp;&nbsp; Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified Language Model Pre-training for Natural Language Understanding and Generation. In . 13042–13054.</p><p>[8]&nbsp;&nbsp; Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. <em>arXiv preprint arXiv:2010.11929</em> (2020).</p><p>[9]&nbsp;&nbsp; Patrick Esser, Robin Rombach, and Björn Ommer. 2020. Taming Transformers for High-Resolution Image Synthesis. arXiv<a href=\"https://arxiv.org/abs/2012.09841\">:2012.09841</a> [cs.CV]</p><p>[10]&nbsp;&nbsp; William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.  abs/2101.03961 (2021). arXiv<a href=\"https://arxiv.org/abs/2101.03961\">:2101.03961</a><a href=\"https://arxiv.org/abs/2101.03961\">https://arxiv.org/abs/2101.03961</a></p><p>[11]&nbsp;&nbsp; Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. 2020. Large-Scale Adversarial Training for Vision-and-Language Representation Learning. In .</p><p>[12]&nbsp;&nbsp; Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, and Hao Wang. 2020. Fashionbert: Text and image matching with adaptive loss for cross-modal retrieval. In . 2251–2260.</p><p>[13]&nbsp;&nbsp; Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to a machine? dataset and methods for multilingual image question answering. <em>arXiv preprint arXiv:1505.05612</em> (2015).</p><p>[14]&nbsp;&nbsp; Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial networks. <em>arXiv preprint arXiv:1406.2661</em> (2014).</p><p>[15]&nbsp;&nbsp; Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In . 770–778.</p><p>[16]&nbsp;&nbsp; Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. De-berta: Decoding-enhanced bert with disentangled attention. <em>arXiv preprint arXiv:2006.03654</em> (2020).</p><p>[17]&nbsp;&nbsp; Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. <em>arXiv preprint arXiv:2004.00849</em> (2020).</p><p>[18]&nbsp;&nbsp; Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. 2020. Convbert: Improving bert with span-based dynamic convolution. <em>arXiv preprint arXiv:2008.02496</em> (2020).</p><p>[19]&nbsp;&nbsp; Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.  abs/1909.11942 (2019).</p><p>[20]&nbsp;&nbsp; Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. <em>arXiv preprint arXiv:2006.16668</em> (2020).</p><p>[21]&nbsp;&nbsp; Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettle-moyer. 2021. BASE Layers: Simplifying Training of Large, Sparse Models.  abs/2103.16716 (2021).</p><p>[22]&nbsp;&nbsp; Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. 2019. Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training.  abs/1908.06066 (2019).</p><p>[23]&nbsp;&nbsp; Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. VisualBERT: A Simple and Performant Baseline for Vision and Language.  abs/1908.03557 (2019).</p><p>[24]&nbsp;&nbsp; Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. 2020. Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks.  abs/2004.06165 (2020).</p><p>[25]&nbsp;&nbsp; Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. 2020. Interbert: Vision-and-language interaction for multi-modal pretraining. <em>arXiv preprint arXiv:2003.13198</em> (2020).</p><p>[26]&nbsp;&nbsp; Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In . 740–755.</p><p>[27]&nbsp;&nbsp; Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach.  abs/1907.11692 (2019).</p><p>[28]&nbsp;&nbsp; Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In . 13–23.</p><p>[29]&nbsp;&nbsp; Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 2019. 12-in-1: Multi-Task Vision and Language Representation Learning.  abs/1912.02315 (2019).</p><p>[31]&nbsp;&nbsp; Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. [n.d.]. Language models are unsupervised multitask learners. ([n. d.]).</p><p>[32]&nbsp;&nbsp; Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. arXiv<a href=\"https://arxiv.org/abs/2102.12092\">:2102.12092</a> [cs.CV]</p><p>[33]&nbsp;&nbsp; Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In . 91–99.</p><p>[34]&nbsp;&nbsp; Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In .</p><p>[35]&nbsp;&nbsp; Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. <em>arXiv preprint arXiv:1701.06538</em> (2017).</p><p>[36]&nbsp;&nbsp; Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. <em>arXiv preprint arXiv:1909.08053</em> (2019).</p><p>[37]&nbsp;&nbsp; Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. MASS: Masked Sequence to Sequence Pre-training for Language Generation. In . 5926–5936.</p><p>[38]&nbsp;&nbsp; Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In .</p><p>[39]&nbsp;&nbsp; Maosong Sun, Jingyang Li, Zhipeng Guo, Z Yu, Y Zheng, X Si, and Z Liu. 2016. Thuctc: an efficient chinese text classifier.  (2016).</p><p>[40]&nbsp;&nbsp; Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In . 5099–5110.</p><p>[41]&nbsp;&nbsp; Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural Discrete Representation Learning. In .</p><p>[42]&nbsp;&nbsp; Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In . 5998–6008.</p><p>[43]&nbsp;&nbsp; Ang Wang, Xianyan Jia, Le Jiang, Jie Zhang, Yong Li, and Wei Lin. 2020. Whale: A Unified Distributed Training Framework. <em>arXiv preprint arXiv:2011.09208</em> (2020).</p><p>[44]&nbsp;&nbsp; Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si. 2019. Structbert: Incorporating language structures into pre-training for deep language understanding. <em>arXiv preprint arXiv:1908.04577</em> (2019).</p><p>[45]&nbsp;&nbsp; Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. <em>arXiv preprint arXiv:1609.08144</em> (2016).</p><p>[46]&nbsp;&nbsp; Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. 2017. Aggregated residual transformations for deep neural networks. In . 1492–1500.</p><p>[47]&nbsp;&nbsp; Liang Xu, Xuanwei Zhang, and Qianqian Dong. 2020. CLUECorpus2020: A Large-scale Chinese Corpus for Pre-trainingLanguage Model. <em>arXiv preprint arXiv:2003.01355</em> (2020).</p><p>[48]&nbsp;&nbsp; Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 1316–1324.</p><p>[49]&nbsp;&nbsp; Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. In . 5754–5764.</p><p>[50]&nbsp;&nbsp; Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. <em>arXiv preprint arXiv:2006.16934</em> (2020).</p><p>[51]&nbsp;&nbsp; Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. 2020. Unified Vision-Language Pre-Training for Image Captioning and VQA. In . 13041–13049.</p><p>:::info\nThis paper is&nbsp;<a href=\"https://arxiv.org/abs/2103.00823\">available on arxiv</a>&nbsp;under CC by 4.0 Deed (Attribution 4.0 International) license.  </p>","contentLength":43855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Xiaomi launches 17 Ultra smartphone, an AirTag clone, and an ultra slim powerbank","url":"https://techcrunch.com/2026/02/28/xiaomi-launches-17-ultra-smartphone-an-airtag-clone-and-an-ultra-slim-powerbank/","date":1772291343,"author":"Ivan Mehta","guid":154958,"unread":true,"content":"<article>We round up everything Xiaomi announced at its Mobile World Congress event.</article>","contentLength":75,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alibaba’s Qwen: The Chinese AI Model Challenging Silicon Valley","url":"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss","date":1772291299,"author":"Alibaba","guid":155010,"unread":true,"content":"<ol></ol><p>\\\n</p><p>Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce QWEN<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark0\">1</a>, the first installment of our large language model series. QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes QWEN, the base pretrained language models, and QWEN-CHAT, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, CODE-QWEN and CODE-QWEN-CHAT, as well as mathematics-focused models, MATH-QWEN-CHAT, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models. \\n </p><p>\\\nDespite their impressive capabilities, LLMs are often criticized for their lack of reproducibility, steerability, and accessibility to service providers. In this work, we are pleased to present and release the initial version of our LLM series, QWEN. QWEN is a moniker that derives from the Chinese phrase Qianwen, which translates to “thousands of prompts” and conveys the notion of embracing a wide range of inquiries. QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. The model series include the base pretrained language models, chat models finetuned with human alignment techniques, i.e., supervised finetuning (SFT), reinforcement learning with human feedback (RLHF), etc., as well as specialized models in coding and math. The details are outlined below:</p><p>1.&nbsp;&nbsp; The base language models, namely QWEN, have undergone extensive training using up to 3 trillion tokens of diverse texts and codes, encompassing a wide range of areas. These models have consistently demonstrated superior performance across a multitude of downstream tasks, even when compared to their more significantly larger counterparts.</p><p>2.&nbsp;&nbsp; The QWEN-CHAT models have been carefully finetuned on a curated dataset relevant to task performing, chat, tool use, agent, safety, etc. The benchmark evaluation demonstrates that the SFT models can achieve superior performance. Furthermore, we have trained reward models to mimic human preference and applied them in RLHF for chat models that can produce responses preferred by humans. Through the human evaluation of a challenging test, we find that QWEN-CHAT models trained with RLHF are highly competitive, still falling behind GPT-4 on our benchmark.</p><p>3.&nbsp;&nbsp;&nbsp; In addition, we present specialized models called CODE-QWEN, which includes CODE-QWEN-7B and CODE-QWEN-14B, as well as their chat models, CODE-QWEN-14B-CHAT and CODE-QWEN-7B-CHAT. Specifically, CODE-QWEN has been pre-trained on extensive datasets of code and further fine-tuned to handle conversations related to code generation, debugging, and interpretation. The results of experiments conducted on benchmark datasets, such as HumanEval <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark69\">(Chen et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark69\">2021),</a> MBPP <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark55\">(Austin et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark55\">2021),</a> and HumanEvalPack <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark140\">(Muennighoff et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark140\">2023),</a> demonstrate the high level of proficiency of CODE-QWEN in code understanding and generation.</p><p>4.&nbsp;&nbsp; This research additionally introduces MATH-QWEN-CHAT specifically designed to tackle mathematical problems. Our results show that both MATH-QWEN-7B-CHAT and MATH-QWEN-14B-CHAT outperform open-sourced models in the same sizes with large margins and are approaching GPT-3.5 on math-related benchmark datasets such as GSM8K <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark82\">(Cobbe</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark82\">et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark82\">2021)</a> and MATH <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark102\">(Hendrycks et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark102\">2021).</a></p><p>5.&nbsp;&nbsp;&nbsp; Besides, we have open-sourced QWEN-VL and QWEN-VL-CHAT, which have the versatile ability to comprehend visual and language instructions. These models outperform the current open-source vision-language models across various evaluation benchmarks and support text recognition and visual grounding in both Chinese and English languages. Moreover, these models enable multi-image conversations and storytelling. Further details can be found in <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark59\">Bai et al.</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark59\">(2023).</a></p><p>\\\nNow, we officially open-source the 14B-parameter and 7B-parameter base pretrained models QWEN and aligned chat models QWEN-CHAT<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark4\">2</a>. This release aims at providing more comprehensive and powerful LLMs at developer- or application-friendly scales.</p><p>The structure of this report is as follows: Section <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark2\">2</a> describes our approach to pretraining and results of QWEN. Section <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark13\">3</a> covers our methodology for alignment and reports the results of both automatic evaluation and human evaluation. Additionally, this section describes details about our efforts in building chat models capable of tool use, code interpreter, and agent. In Sections <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark31\">4</a> and <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark35\">5,</a> we delve into specialized models of coding and math and their performance. Section <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark41\">6</a> provides an overview of relevant related work, and Section <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark47\">7</a> concludes this paper and points out our future work.</p><p>The pretraining stage involves learning vast amount of data to acquire a comprehensive understanding of the world and its various complexities. This includes not only basic language capabilities but also advanced skills such as arithmetic, coding, and logical reasoning. In this section, we introduce the data, the model design and scaling, as well as the comprehensive evaluation results on benchmark datasets.</p><p>The size of data has proven to be a crucial factor in developing a robust large language model, as highlighted in previous research <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark103\">(Hoffmann et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark103\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark184\">Touvron et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark184\">2023b).</a> To create an effective pretraining dataset, it is essential to ensure that the data are diverse and cover a wide range of types, domains, and tasks. Our dataset is designed to meet these requirements and includes public web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.</p><p>\\\nTo ensure the quality of our pretraining data, we have developed a comprehensive data preprocessing procedure. For public web data, we extract text from HTML and use language identification tools to determine the language. To increase the diversity of our data, we employ deduplication techniques, including exact-match deduplication after normalization and fuzzy deduplication using MinHash and LSH algorithms. To filter out low-quality data, we employ a combination of rule-based and machine-learning-based methods. Specifically, we use multiple models to score the content, including language models, text-quality scoring models, and models for identifying potentially offensive or inappropriate content. We also manually sample texts from various sources and review them to ensure their quality. To further enhance the quality of our data, we selectively up-sample data from certain sources, to ensure that our models are trained on a diverse range of high-quality content. In recent studies <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark211\">(Zeng et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark211\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark53\">Aribandi et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark53\">2021;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark156\">Raffel et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark156\">2020),</a> it has been demonstrated that pretraining language models with multi-task instructions can enhance their zero-shot and few-shot performance. To further enhance the performance of our model, we have incorporated high-quality instruction data into our pretraining process. To safeguard the integrity of our benchmark assessment, we have adopted a similar approach as <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark67\">Brown et al.</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark67\">(2020)</a> and meticulously eliminated any instruction samples that exhibit a 13-gram overlap with any data present in the test sets utilized in our evaluation.</p><p>\\\nGiven the large number of downstream tasks, it is not feasible to repeat this filtering process for all tasks. Instead, we have made sure that the instruction data for the reported tasks have undergone our filtering process to ensure their accuracy and reliability. Finally, we have built a dataset of up to 3 trillion tokens.</p><p>The design of vocabulary significantly impacts the training efficiency and the downstream task performance. In this study, we utilize byte pair encoding (BPE) as our tokenization method, following GPT-3.5 and GPT-4. We start with the open-source fast BPE tokenizer, tiktoken <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark113\">(Jain,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark113\">2022),</a> and select the vocabulary cl100k base as our starting point. To enhance the performance of our model on multilingual downstream tasks, particularly in Chinese, we augment the vocabulary with commonly used Chinese characters and words, as well as those in other languages. Also, following <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark183\">Touvron et al.</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark184\">(2023a;b),</a> we have split numbers into single digits. The final vocabulary size is approximately 152K.</p><p>The performance of the QWEN tokenizer in terms of compression is depicted in Figure <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark5\">3.</a> In this comparison, we have evaluated QWEN against several other tokenizers, including XLM-R <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark83\">(Conneau</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark83\">et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark83\">2019),</a> LLaMA <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark183\">(Touvron et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark183\">2023a),</a> Baichuan <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark110\">(Inc.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark110\">2023a),</a> and InternLM <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark112\">(InternLM Team,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark112\">2023).</a> Our findings reveal that QWEN achieves higher compression efficiency than its competitors in most languages. This implies that the cost of serving can be significantly reduced since a smaller number of tokens from QWEN can convey more information than its competitors. Furthermore, we have conducted preliminary experiments to ensure that scaling the vocabulary size of QWEN does not negatively impact the downstream performance of the pretrained model. Despite the increase in vocabulary size, our experiments have shown that QWEN maintains its performance levels in downstream evaluation.</p><p>QWEN is designed using a modified version of the Transformer architecture. Specifically, we have adopted the recent open-source approach of training large language models, LLaMA <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark183\">(Touvron et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark183\">2023a),</a> which is widely regarded as the top open-source LLM. Our modifications to the architecture include:</p><ul><li><strong>Embedding and output projection</strong>. Based on preliminary experimental findings, we have opted for the untied embedding approach instead of tying the weights of input embedding and output projection. This decision was made in order to achieve better performance with the price of memory costs.</li><li>. We have chosen RoPE (Rotary Positional Embedding) <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark174\">(Su et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark174\">2021)</a> as our preferred option for incorporating positional information into our model. RoPE has been widely adopted and has demonstrated success in contemporary large language models, notably PaLM <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark77\">(Chowdhery et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark77\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark50\">Anil et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark50\">2023)</a> and LLaMA <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark183\">(Touvron</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark183\">et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark184\">2023a;b).</a> In particular, we have opted to use FP32 precision for the inverse frequency matrix, rather than BF16 or FP16, in order to prioritize model performance and achieve higher accuracy.</li><li>. For most layers, we remove biases following <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark77\">Chowdhery et al.</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark77\">(2022),</a> but we add biases in the QKV layer of attention to enhance the extrapolation ability of the model <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark173\">(Su,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark173\">2023b).</a></li><li>. In modern Transformer models, pre-normalization is the most widely used approach, which has been shown to improve training stability compared to post-normalization. Recent research has suggested alternative methods for better training stability, which we plan to explore in future versions of our model. Additionally, we have replaced the traditional layer normalization technique described in <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark57\">(Ba et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark57\">2016)</a> with RMSNorm <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark115\">(Jiang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark115\">2023).</a> This change has resulted in equivalent performance while also improving efficiency.</li><li>. We have selected SwiGLU <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark165\">(Shazeer,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark165\">2020)</a> as our activation function, a combination of Swish <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark157\">(Ramachandran et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark157\">2017)</a> and Gated Linear Unit <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark87\">(Dauphin et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark87\">2017).</a> Our initial experiments have shown that activation functions based on GLU generally outperform other baseline options, such as GeLU <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark100\">(Hendrycks &amp; Gimpel,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark100\">2016).</a> As is common practice in previous research, we have reduced the dimension of the feed-forward network (FFN) from 4 times the hidden size to 8/3 of the hidden size.</li></ul><p>To train QWEN, we follow the standard approach of autoregressive language modeling, as described in <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark153\">Radford et al.</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark153\">(2018).</a> This involves training the model to predict the next token based on the context provided by the previous tokens. We train models with context lengths of 2048. To create batches of data, we shuffle and merge the documents, and then truncate them to the specified context lengths. To improve computational efficiency and reduce memory usage, we employ Flash Attention in the attention modules <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark86\">(Dao et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark86\">2022).</a> We adopt the standard optimizer AdamW <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark116\">(Kingma &amp; Ba,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark116\">2014;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark134\">Loshchilov &amp; Hutter,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark134\">2017)</a> for pretraining optimization. We set the hyperparameters 1 = 0*.*9, 2 = 0*.*95, and  = 10−8. We use a cosine learning rate schedule with a specified peak learning rate for each model size. The learning rate is decayed to a minimum learning rate of 10% of the peak learning rate. All the models are trained with BFloat16 mixed precision for training stability.</p><h3>2.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Context Length Extension</h3><p>Transformer models have a significant limitation in terms of the context length for their attention mechanism. As the context length increases, the quadratic-complexity computation leads to a drastic increase in both computation and memory costs. In this work, we have implemented simple training-free techniques that are solely applied during inference to extend the context length of the model. One of the key techniques we have used is NTK-aware interpolation <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark65\">(bloc97,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark65\">2023).</a></p><p>\\\nUnlike position interpolation (PI) <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark70\">(Chen et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark70\">2023a)</a> which scales each dimension of RoPE equally, NTK-aware interpolation adjusts the base of RoPE to prevent the loss of high-frequency information in a training-free manner. To further improve performance, we have also implemented a trivial extension called dynamic NTK-aware interpolation, which is later formally discussed in <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark149\">(Peng et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark149\">2023a).</a> It dynamically changes the scale by chunks, avoiding severe performance degradation. These techniques allow us to effectively extend the context length of Transformer models without compromising their computational efficiency or accuracy.</p><p>QWEN additionally incorporates two attention mechanisms: LogN-Scaling <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark75\">(Chiang &amp; Cholak,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark75\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark172\">Su,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark172\">2023a)</a> and window attention <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark62\">(Beltagy et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark62\">2020).</a> LogN-Scaling rescales the dot product of the query and value by a factor that depends on the ratio of the context length to the training length, ensuring that the entropy of the attention value remains stable as the context length grows. Window attention restricts the attention to a limited context window, preventing the model from attending to tokens that are too far away.</p><p>We also observed that the long-context modeling ability of our model varies across layers, with lower layers being more sensitive in context length extension compared to the higher layers. To leverage this observation, we assign different window sizes to each layer, using shorter windows for lower layers and longer windows for higher layers.</p><h3>2.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Experimental Results</h3><p>\\\nIn this evaluation, we focus on the base language models without alignment and collect the baselines’ best scores from their official results and OpenCompass <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark146\">(OpenCompass Team,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark146\">2023).</a> The results are presented in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark10\">2.</a></p><p>Our experimental results demonstrate that the three QWEN models exhibit exceptional performance across all downstream tasks. It is worth noting that even the larger models, such as LLaMA2-70B, are outperformed by QWEN-14B in 3 tasks. QWEN-7B also performs admirably, surpassing LLaMA2-13B and achieving comparable results to Baichuan2-13B. Notably, despite having a relatively small number of parameters, QWEN-1.8B is capable of competitive performance on certain tasks and even outperforms larger models in some instances. The findings highlight the impressive capabilities of the QWEN models, particularly QWEN-14B, and suggest that smaller models, such as QWEN-1.8B, can still achieve strong performance in certain applications.</p><p>To evaluate the effectiveness of context length extension, Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark12\">3</a> presents the test results on arXiv<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark15\">3</a> in terms of perplexity (PPL). These results demonstrate that by combining NTK-aware interpolation, LogN-Scaling, and layer-wise window assignment, we can effectively maintain the performance of our models in the context of over 8192 tokens.</p><p>Pretrained large language models have been found to be not aligned with human behavior, making them unsuitable for serving as AI assistants in most cases. Recent research has shown that the use of alignment techniques, such as supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), can significantly improve the ability of language models to engage in natural conversation. In this section, we will delve into the details of how QWEN models have been trained using SFT and RLHF, and evaluate their performance in the context of chat-based assistance.</p><h3>3.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervised Finetuning</h3><p>To gain an understanding of human behavior, the initial step is to carry out SFT, which finetunes a pretrained LLM on chat-style data, including both queries and responses. In the following sections, we will delve into the details of data construction and training methods.</p><p>To enhance the capabilities of our supervised finetuning datasets, we have annotated conversations in multiple styles. While conventional datasets <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark193\">(Wei et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark193\">2022a)</a> contain a vast amount of data prompted with questions, instructions, and answers in natural language, our approach takes it a step further by annotating human-style conversations. This practice, inspired by <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">Ouyang et al.</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">(2022),</a> aims at improving the model’s helpfulness by focusing on natural language generation for diverse tasks. To ensure the model’s ability to generalize to a wide range of scenarios, we specifically excluded data formatted in prompt templates that could potentially limit its capabilities. Furthermore, we have prioritized the safety of the language model by annotating data related to safety concerns such as violence, bias, and pornography.</p><p>In addition to data quality, we have observed that the training method can significantly impact the final performance of the model. To achieve this, we utilized the ChatML-style format <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark144\">(OpenAI,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark144\">2022),</a> which is a versatile meta language capable of describing both the metadata (such as roles) and the content of a turn. This format enables the model to effectively distinguish between various types of information, including system setup, user inputs, and assistant outputs, among others. By leveraging this approach, we can enhance the model’s ability to accurately process and analyze complex conversational data.</p><p>Consistent with pretraining, we also apply next-token prediction as the training task for SFT. We apply the loss masks for the system and user inputs. More details are demonstrated in Section <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark221\">A.1.1.</a></p><p>The model’s training process utilizes the AdamW optimizer, with the following hyperparameters: 1 set to 0*.2 set to 0. set to 10−8. The sequence length is limited to 2048, and the batch size is 128. The model undergoes a total of 4000 steps, with the learning rate gradually increased over the first 1430 steps, reaching a peak of 2  10−6. To prevent overfitting, weight decay is applied with a value of 0..<em>1, and gradient clipping is enforced with a limit of 1</em>.*0.</p><h3>3.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reinforcement Learning from Human Feedback</h3><p>While SFT has proven to be effective, we acknowledge that its generalization and creativity capa-bilities may be limited, and it is prone to overfitting. To address this issue, we have implemented Reinforcement Learning from Human Feedback (RLHF) to further align SFT models with human preferences, following the approaches of <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">Ouyang et al.</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">(2022);</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark78\">Christiano et al.</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark78\">(2017).</a> This process involves training a reward model and using Proximal Policy Optimization (PPO) <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark164\">(Schulman et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark164\">2017)</a> to conduct policy training.</p><h3>3.2.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reward Model</h3><p>To create a successful reward model, like building a large language model (LLM), it is crucial to first undergo pretraining and then finetuning. This pretraining process, also known as preference model pretraining (PMP) <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark60\">(Bai et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark60\">2022b),</a> necessitates a vast dataset of comparison data. This dataset consists of sample pairs, each containing two distinct responses for a single query and their corresponding preferences. Similarly, finetuning is also conducted on this type of comparison data, but with a higher quality due to the presence of quality annotations.</p><p>During the fine-tuning phase, we gather a variety of prompts and adjust the reward model based on human feedback for responses from the QWEN models. To ensure the diversity and complexity of user prompts are properly taken into account, we have created a classification system with around 6600 detailed tags and implemented a balanced sampling algorithm that considers both diversity and complexity when selecting prompts for annotation by the reward model <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark135\">(Lu et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark135\">2023).</a> To generate a wide range of responses, we have utilized QWEN models of different sizes and sampling strategies, as diverse responses can help reduce annotation difficulties and enhance the performance of the reward model. These responses are then evaluated by annotators following a standard annotation guideline, and comparison pairs are formed based on their scores.</p><p>In creating the reward model, we utilize the same-sized pre-trained language model QWEN to initiate the process. It is important to mention that we have incorporated a pooling layer into the original QWEN model to extract the reward for a sentence based on a specific end token.</p><p>\\n The learning rate for this process has been set to a constant value of 3  10−6, and the batch size is 64. Additionally, the sequence length is set to 2048, and the training process lasts for a single epoch.</p><p>We adopted the accuracy on the test dataset as an important but not exclusive evaluation metric for the reward model. In Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark20\">4,</a> we report the test pairwise accuracy of PMP and reward models on diverse human preference benchmark datasets <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark60\">(Bai et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark60\">2022b;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark171\">Stiennon et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark171\">2020;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark95\">Ethayarajh</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark95\">et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark95\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark127\">Lightman et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark127\">2023).</a> Specifically, QWEN Helpful-base and QWEN Helpful-online are our proprietary datasets. The responses in QWEN Helpful-base are generated from QWEN without RLHF, whereas QWEN Helpful-online includes responses from QWEN with RLHF. The results show that the PMP model demonstrates high generalization capabilities on out-of-distribution data, and the reward model demonstrates significant improvement on our QWEN reward datasets.</p><h3>3.2.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reinforcement Learning</h3><p>Our Proximal Policy Optimization (PPO) process involves four models: the policy model, value model, reference model, and reward model. Before starting the PPO procedure, we pause the policy model’s updates and focus solely on updating the value model for 50 steps. This approach ensures that the value model can adapt to different reward models effectively.</p><p>During the PPO operation, we use a strategy of sampling two responses for each query simultaneously. This strategy has proven to be more effective based on our internal benchmarking evaluations. We set the KL divergence coefficient to 0*.*04 and normalize the reward based on the running mean.</p><p>The policy and value models have learning rates of 1  10−6 and 5  10−6, respectively. To enhance training stability, we utilize value loss clipping with a clip value of 0*.<em>15. For inference, the policy top-p is set to 0</em>.<em>9. Our findings indicate that although the entropy is slightly lower than when top-p is set to 1</em>.*0, there is a faster increase in reward, ultimately resulting in consistently higher evaluation rewards under similar conditions.</p><p>Additionally, we have implemented a pretrained gradient to mitigate the alignment tax. Empirical findings indicate that, with this specific reward model, the KL penalty is adequately robust to counteract the alignment tax in benchmarks that are not strictly code or math in nature, such as those that test common sense knowledge and reading comprehension. It is imperative to utilize a significantly larger volume of the pretrained data in comparison to the PPO data to ensure the effectiveness of the pretrained gradient. Additionally, our empirical study suggests that an overly large value for this coefficient can considerably impede the alignment to the reward model, eventually compromising the ultimate alignment, while an overly small value would only have a marginal effect on alignment tax reduction.</p><h3>3.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Automatic and Human Evaluation of Aligned Models</h3><p>To showcase the effectiveness of our aligned models, we conduct a comparison with other aligned models on well-established benchmarks, including MMLU <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark101\">(Hendrycks et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark101\">2020),</a> C-Eval <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark108\">(Huang</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark108\">et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark108\">2023),</a> GSM8K <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark82\">(Cobbe et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark82\">2021),</a> HumanEval <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark69\">(Chen et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark69\">2021),</a> and BBH <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark177\">(Suzgun et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark177\">2022).</a> Besides the widely used few-shot setting, we test our aligned models in the zero-shot setting to demonstrate how well the models follow instructions. The prompt in a zero-shot setting consists of an instruction and a question without any previous examples in the context. The results of the baselines are collected from their official reports and OpenCompass <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark146\">(OpenCompass Team,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark146\">2023).</a></p><p>The results in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark23\">5</a> demonstrate the effectiveness of our aligned models in understanding human instructions and generating appropriate responses. QWEN-14B-Chat outperforms all other models except ChatGPT (OpenAI, 2022) and LLAMA 2-CHAT-70B (Touvron et al., 2023b) in all datasets, including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and BBH (Suzgun et al., 2022).</p><p>\\n In particular, QWEN’s performance in HumanEval, which measures the quality of generated codes, is significantly higher than that of other open-source models.</p><p>Moreover, QWEN’s performance is consistently better than that of open-source models of similar size, such as LLaMA2 <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark184\">(Touvron et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark184\">2023b),</a> ChatGLM2 <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark68\">(ChatGLM2 Team,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark68\">2023),</a> InternLM <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark112\">(InternLM</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark112\">Team,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark112\">2023),</a> and Baichuan2 <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark201\">(Yang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark201\">2023).</a> This suggests that our alignment approach, which involves fine-tuning the model on a large dataset of human conversations, has been effective in improving the model’s ability to understand and generate human-like language./im</p><p>Despite this, we have reservations about the ability of traditional benchmark evaluation to accurately measure the performance and potential of chat models trained with alignment techniques in today’s landscape. The results mentioned earlier provide some evidence of our competitive standing, but we believe that it is crucial to develop new evaluation methods specifically tailored to aligned models.</p><p>We believe that human evaluation is crucial, which is why we have created a carefully curated dataset for this purpose. Our process involved collecting 300 instructions in Chinese that covered a wide range of topics, including knowledge, language understanding, creative writing, coding, and mathematics. To evaluate the performance of different models, we chose the SFT version of QWEN-CHAT-7B and the SFT and RLHF versions of QWEN-CHAT-14B, and added two strong baselines, GPT-3.5 and GPT-4<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark24\">4</a>, for comparison. For each instruction, we asked three annotators to rank the model responses by the overall score of helpfulness, informativeness, validity, and other relevant factors. Our dataset and evaluation methodology provides a comprehensive and rigorous assessment of the capabilities of different language models in various domains.</p><p>Figure <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark25\">4</a> illustrates the win rates of the various models. For each model, we report the percentage of wins, ties, and losses against GPT-3.5, with the segments of each bar from bottom to top representing these statistics. The experimental results clearly demonstrate that the RLHF model outperforms the SFT models by significant margins, indicating that RLHF can encourage the model to generate responses that are more preferred by humans. In terms of overall performance, we find that the RLHF model significantly outperforms the SFT models, falling behind GPT-4. This indicates the effectiveness of RLHF for aligning to human preference. To provide a more comprehensive understanding of the models’ performance, we include a case study with examples from different models in Appendix <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark230\">A.2.2.</a> Nonetheless, it remains difficult to accurately capture the gap between our models and the proprietary models. As such, a more extensive and rigorous assessment is required for the chat models.</p><p>The QWEN models, which are designed to be versatile, have the remarkable ability to assist with (semi-)automating daily tasks by leveraging their skills in tool-use and planning. As such, they can serve as agents or copilots to help streamline various tasks. We explore QWEN’s proficiency in the following areas:</p><p>•&nbsp;&nbsp;&nbsp;&nbsp; Using a Python code interpreter to enhance math reasoning, data analysis, and more (see Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark28\">7</a> and Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark29\">8).</a></p><p>•&nbsp;&nbsp;&nbsp;&nbsp; Functioning as an agent that accesses Hugging Face’s extensive collection of multimodal models while engaging with humans (see Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark30\">9).</a></p><p>\\\nTo enhance QWEN’s capabilities as an agent or copilot, we employ the self-instruct <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark190\">(Wang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark190\">2023c)</a> strategy for SFT. Specifically, we utilize the in-context learning capability of QWEN for self-instruction. By providing a few examples, we can prompt QWEN to generate more relevant queries and generate outputs that follow a specific format, such as ReAct <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark203\">(Yao et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark203\">2022).</a> We then apply rules and involve human annotators to filter out any noisy samples. Afterwards, the samples are incorporated into QWEN’s training data, resulting in an updated version of QWEN that is more dependable for self-instruction. We iterate through this process multiple times until we gather an ample number of samples that possess both exceptional quality and a wide range of diversity. As a result, our final collection consists of around 2000 high-quality samples.</p><p>During the finetuning process, we mix these high-quality samples with all the other general-purpose SFT samples, rather than introducing an additional training stage. By doing so, we are able to retain essential general-purpose capabilities that are also pertinent for constructing agent applications.</p><p>\\\n<strong>Using Tools via ReAct Prompting</strong> We have created and made publicly available a benchmark for evaluating QWEN’s ability to call plugins, tools, functions, or APIs using ReAct Prompting (see <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark152\">Qwen Team, Alibaba Group,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark152\">2023b).</a> To ensure fair evaluation, we have excluded any plugins that were included in QWEN’s training set from the evaluation set. The benchmark assesses the model’s accuracy in selecting the correct plugin from a pool of up to five candidates, as well as the plausibility of the parameters passed into the plugin and the frequency of false positives. In this evaluation, a false positive occurs when the model incorrectly invokes a plugin in response to a query, despite not being required to do so.</p><p>The results presented in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark27\">6</a> demonstrate that QWEN consistently achieves higher accuracy in identifying the relevance of a query to the available tools as the model size increases. However, the table also highlights that beyond a certain point, there is little improvement in performance when it comes to selecting the appropriate tool and providing relevant arguments. This suggests that the current preliminary benchmark may be relatively easy and may require further enhancement in future iterations. It is worth noting that GPT-3.5 stands out as an exception, displaying suboptimal performance on this particular benchmark. This could potentially be attributed to the fact that the benchmark primarily focuses on the Chinese language, which may not align well with GPT-3.5’s capabilities. Additionally, we observe that GPT-3.5 tends to attempt to use at least one tool, even if the query cannot be effectively addressed by the provided tools.</p><p>\\\n<strong>Using Code Interpreter for Math Reasoning and Data Analysis</strong> The Python code interpreter is widely regarded as a powerful tool for augmenting the capabilities of an LLM agent. It is worth investigating whether QWEN can harness the full potential of this interpreter to enhance its performance in diverse domains, such as mathematical reasoning and data analysis. To facilitate this exploration, we have developed and made publicly available a benchmark that is specifically tailored for this purpose (see <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark151\">Qwen Team, Alibaba Group,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark151\">2023a).</a></p><p>The benchmark encompasses three primary categories of tasks: math problem-solving, data visualization, and other general-purpose tasks like file post-processing and web crawling. Within the visualization tasks, we differentiate between two levels of difficulty. The easier level can be achieved by simply writing and executing a single code snippet without the need for advanced planning skills. However, the more challenging level requires strategic planning and executing multiple code snippets in a sequential manner. This is because the subsequent code must be written based on the output of the previous code. For example, an agent may need to examine the structure of a CSV file using one code snippet before proceeding to write and execute additional code to create a plot.</p><p>Regarding evaluation metrics, we consider both the executability and correctness of the generated code. To elaborate on the correctness metrics, for math problems, we measure accuracy by verifying if the ground truth numerical answer is present in both the code execution result and the final response. When it comes to data visualization, we assess accuracy by utilizing QWEN-VL <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark59\">(Bai et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark59\">2023),</a> a powerful multimodal language model. QWEN-VL is capable of answering text questions paired with images, and we rely on it to confirm whether the image generated by the code fulfills the user’s request.</p><p>The results regarding executability and correctness are presented in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark28\">7</a> and Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark29\">8,</a> respectively. It is evident that CODE LLAMA generally outperforms LLAMA 2, its generalist counterpart, which is not surprising since this benchmark specifically requires coding skills. However, it is worth noting that specialist models that are optimized for code synthesis do not necessarily outperform generalist models. This is due to the fact that this benchmark encompasses various skills beyond coding, such as abstracting math problems into equations, understanding language-specified constraints, and responding in the specified format such as ReAct. Notably, QWEN-7B-CHAT and QWEN-14B-CHAT surpass all other open-source alternatives of similar scale significantly, despite being generalist models.</p><p>\\\n<strong>Serving as a Hugging Face Agent</strong> Hugging Face provides a framework called the Hugging Face Agent or Transformers Agent <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark109\">(Hugging Face,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark109\">2023),</a> which empowers LLM agents with a curated set of multimodal tools, including speech recognition and image synthesis. This framework allows an LLM agent to interact with humans, interpret natural language commands, and employ the provided tools as needed.</p><p>To evaluate QWEN’s effectiveness as a Hugging Face agent, we utilized the evaluation benchmarks offered by Hugging Face. The results are presented in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark30\">9.</a> The evaluation results reveal that QWEN performs quite well in comparison to other open-source alternatives, only slightly behind the proprietary GPT-4, demonstrating QWEN’s competitive capabilities.</p><h2>4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Code-Qwen: Specialized Model for Coding</h2><p>Training on domain-specific data has been shown to be highly effective, particularly in the case of code pretraining and finetuning. A language model that has been reinforced with training on code data can serve as a valuable tool for coding, debugging, and interpretation, among other tasks. In this work, we have developed a series of generalist models using pretraining and alignment techniques. Building on this foundation, we have created domain-specific models for coding by leveraging the base language models of QWEN, including continued pretrained model, CODE-QWEN and supervised finetuned model, CODE-QWEN-CHAT. Both models have 14 billion and 7 billion parameters versions.</p><h3>4.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Code Pretraining</h3><p>We believe that relying solely on code data for pretraining can result in a significant loss of the ability to function as a versatile assistant. Unlike previous approaches that focused solely on pretraining on code data <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark126\">(Li et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark126\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark125\">2023d),</a> we take a different approach <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark159\">(Rozie`re et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark159\">2023)</a> by starting with our base models QWEN trained on a combination of text and code data, and then continuing to pretrain on the code data. We continue to pretrain the models on a total of around 90 billion tokens. During the pre-training phase, we initialize the model using the base language models QWEN. Many applications that rely on specialized models for coding may encounter lengthy contextual scenarios, such as tool usage and code interpretation, as mentioned in Section <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark26\">3.4.</a> To address this issue, we train our models with context lengths of up to 8192. Similar to base model training in Section <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark8\">2.4,</a> we employ Flash Attention <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark86\">(Dao et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark86\">2022)</a> in the attention modules, and adopt the standard optimizer AdamW <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark116\">(Kingma &amp; Ba,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark116\">2014;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark134\">Loshchilov &amp; Hutter,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark134\">2017),</a> setting 1 = 0*.2 = 0. = 10−8. We set the learning rate as 6. 10−5 for CODE-QWEN-14B and 3.*0  10−5 for CODE-QWEN-7B, with 3% warm up iterations and no learning rate decays.</p><h3>4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Code&nbsp; Supervised&nbsp; Fine-Tuning</h3><p>After conducting a series of empirical experiments, we have determined that the multi-stage SFT strategy yields the best performance compared to other methods. In the supervised fine-tuning stage, the model CODE-QWEN-CHAT initialized by the code foundation model CODE-QWEN are optimized by the AdamW <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark116\">(Kingma &amp; Ba,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark116\">2014;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark134\">Loshchilov &amp; Hutter,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark134\">2017)</a> optimizer (1 = 0*.2 = 0.*95,  = 10−8) with a learning rate of 2*. 10−6 and 1.*0  10−5 for the 14B and 7B model respectively. The learning rate increases to the peaking value with the cosine learning rate schedule (3% warm-up steps) and then remains constant.</p><p>Our CODE-QWEN models have been compared with both proprietary and open-source language models, as shown in Tables <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark37\">10</a> and <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark38\">11.</a> These tables present the results of our evaluation on the test sets of Humaneval <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark69\">(Chen et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark69\">2021),</a> MBPP <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark55\">(Austin et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark55\">2021),</a> and the multi-lingual code generation benchmark HUMANEVALPACK <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark140\">(Muennighoff et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark140\">2023).</a> The comparison is based on the pass@1 performance of the models on these benchmark datasets. The results of this comparison are clearly demonstrated in Tables <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark37\">10</a> and <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark38\">11.</a></p><p>When compared to some of the extremely large-scale closed-source models, CODE-QWEN and CODE-QWEN-CHAT demonstrate clear advantages in terms of pass@1. However, it is important to note that these models fall behind the state-of-the-art methods, such as GPT-4, in general. Nonetheless, with the continued scaling of both model size and data size, we believe that this gap can be narrowed in the near future.</p><p>It is crucial to emphasize that the evaluations mentioned previously are insufficient for grasping the full extent of the strengths and weaknesses of the models. In our opinion, it is necessary to develop more rigorous tests to enable us to accurately assess our relative performance in comparison to GPT-4.</p><p>We have created a mathematics-specialized model series called MATH-QWEN-CHAT, which is built on top of the QWEN pretrained language models. Specifically, we have developed assistant models that are specifically designed to excel in arithmetic and mathematics and are aligned with human behavior. We are releasing two versions of this model series, MATH-QWEN-14B-CHAT and MATH-QWEN-7B-CHAT, which have 14 billion and 7 billion parameters, respectively.</p><p>We carry out math SFT on our augmented math instructional dataset for mathematics reasoning, and therefore we obtain the chat model, MATH-QWEN-CHAT, directly. Owing to shorter average lengths of the math SFT data, we use a sequence length of 1024 for faster training. Most user inputs in the math SFT dataset are examination questions, and it is easy for the model to predict the input format and it is meaningless for the model to predict the input condition and numbers which could be random.</p><p>\\\nThus, we mask the inputs of the system and user to avoid loss computation on them and find masking them accelerates the convergence during our preliminary experiments. For optimization, we use the AdamW optimizer with the same hyperparameters of SFT except that we use a peak learning rate of 2  10−5 and a training step of 50 000.</p><p>We evaluate models on the test sets of GSM8K (Grade school math) <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark82\">(Cobbe et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark82\">2021),</a> MATH (Challenging competition math problems) <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark102\">(Hendrycks et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark102\">2021),</a> Math401 (Arithmetic ability) (<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark207\">Yuan et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark207\">2023b),</a> and Math23K (Chinese grade school math) <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark188\">(Wang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark188\">2017).</a> We compare MATH-QWEN-CHAT with proprietary models ChatGPT and Minerva <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark121\">(Lewkowycz et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark121\">2022)</a> and open-sourced math-specialized model RFT (<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark206\">Yuan et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark206\">2023a),</a> WizardMath <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark136\">(Luo et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark136\">2023a),</a> and GAIRMath-Abel <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark73\">(Chern et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark73\">2023a)</a> in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark39\">12.</a> MATH-QWEN-CHAT models show better math reasoning and arithmetic abilities compared to open-sourced models and QWEN-CHAT models of similar sizes. Compared to proprietary models, MATH-QWEN-7B-CHAT outperforms Minerva-8B in MATH. MATH-QWEN-14B-CHAT is chasing Minerva-62B and GPT-3.5 in GSM8K and MATH and delivers better performance on arithmetic ability and Chinese math problems.</p><h3>6.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Large Language Models</h3><p>The birth of ChatGPT <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark143\">(OpenAI,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark143\">2022)</a> and the subsequent launch of GPT-4 <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark145\">(OpenAI,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark145\">2023)</a> marked two historic moments in the field of artificial intelligence, demonstrating that large language models (LLMs) can serve as effective AI assistants capable of communicating with humans. These events have sparked interests among researchers and developers in building language models that are aligned with human values and potentially even capable of achieving artificial general intelligence (AGI) <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark50\">(Anil</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark50\">et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark50\">2023;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark51\">Anthropic,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark52\">2023a;b).</a></p><p>The community was impressed by the surprising effectiveness of alignment on LLMs. Previously, LLMs without alignment often struggle with issues such as repetitive generation, hallucination, and deviation from human preferences. Since 2021, researchers have been diligently working on developing methods to enhance the performance of LLMs in downstream tasks <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark193\">(Wei et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark193\">2022a;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark160\">Sanh et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark160\">2021;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark133\">Longpre et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark133\">2023;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark79\">Chung et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark79\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark139\">Muennighoff et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark139\">2022).</a> Furthermore, researchers have been actively exploring ways to align LLMs with human instructions <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">(Ouyang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark54\">Askell et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark54\">2021;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark60\">Bai et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark61\">2022b;c).</a> One major challenge in alignment research is the difficulty of collecting data. While OpenAI has utilized its platform to gather human prompts or instructions, it is not feasible for others to collect such data.</p><p>To train an effective chat model, available solutions are mostly based on SFT and RLHF <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">(Ouyang</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark147\">2022).</a> While SFT is similar to pretraining, it focuses on instruction following using the aforementioned data. However, for many developers, the limited memory capacity is a major obstacle to further research in SFT. As a result, parameter-efficient tuning methods, such as LoRA <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark106\">(Hu et al</a>., <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark106\">2021)</a> and Q-LoRA <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark89\">(Dettmers et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark89\">2023),</a> have gained popularity in the community. LoRA tunes only low-rank adapters, while Q-LoRA builds on LoRA and utilizes 4-bit quantized LLMs and paged attention <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark88\">(Dettmers et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark88\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark97\">Frantar et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark97\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark118\">Kwon et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark118\">2023).</a> In terms of RLHF, recent methods such as PPO <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark164\">(Schulman et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark164\">2017;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark184\">Touvron et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark184\">2023b)</a> have been adopted, but there are also alternative techniques aimed at addressing the complexity of optimization, such as RRHF (<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark208\">Yuan et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark208\">2023c),</a> DPO <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark155\">(Rafailov et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark155\">2023),</a> and PRO <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark169\">(Song et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark169\">2023).</a> Despite the ongoing debate about the effectiveness of RLHF, more evidence is needed to understand how it enhances the intelligence of LLMs and what potential drawbacks it may have.</p><h3>6.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; LLM for Coding</h3><p>LLMs with a certain model scale have been found to possess the ability to perform mathematical reasoning <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark194\">(Wei et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark194\">2022b;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark177\">Suzgun et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark177\">2022).</a> In order to encourage LLMs to achieve better performance on math-related tasks, researchers have employed techniques such as chain-of-thought prompting <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark195\">(Wei et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark195\">2022c)</a> and scratchpad <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark142\">(Nye et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark142\">2021),</a> which have shown promising results. Additionally, self-consistency <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark187\">(Wang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark187\">2022)</a> and least-to-most prompting <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark218\">(Zhou et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark218\">2022)</a> have further improved the performance of these models on these tasks. However, prompt engineering is a time-consuming process that requires a lot of trial and error, and it is still difficult for LLMs to consistently perform well or achieve satisfactory results in solving mathematical problems. Moreover, simply scaling the data and model size is not an efficient way to improve a model’s mathematical reasoning abilities. Instead, pretraining on math-related corpora has been shown to consistently enhance these capabilities <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark102\">(Hendrycks et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark102\">2021;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark121\">Lewkowycz et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark121\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark181\">Taylor et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark181\">2022;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark127\">Lightman et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark127\">2023).</a> Additionally, fine-tuning on math-related instruction-following datasets <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark168\">(Si</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark168\">et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark168\">2023;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark206\">Yuan et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark206\">2023a;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark136\">Luo et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark136\">2023a;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark209\">Yue et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark209\">2023;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark73\">Chern et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark73\">2023a;</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark205\">Yu et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark205\">2023),</a> has also been effective and more cost-effective than math-specific pretraining. Despite their limitations in terms of accuracy, LLMs still have significant potential to assist users with practical mathematical problems. There is ample scope for further development in this area.</p><p>In this report, we present the QWEN series of large language models, which showcase the latest advancements in natural language processing. With 14B, 7B, and 1.8B parameters, these models have been pre-trained on massive amounts of data, including trillions of tokens, and fine-tuned using cutting-edge techniques such as SFT and RLHF. Additionally, the QWEN series includes specialized models for coding and mathematics, such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-CHAT, which have been trained on domain-specific data to excel in their respective fields. Our results demonstrate that the QWEN series is competitive with existing open-source models and even matches the performance of some proprietary models on comprehensive benchmarks and human evaluation.</p><p>We believe that the open access of QWEN will foster collaboration and innovation within the community, enabling researchers and developers to build upon our work and push the boundaries of what is possible with language models. By providing these models to the public, we hope to inspire new research and applications that will further advance the field and contribute to our understanding of the variables and techniques introduced in realistic settings. In a nutshell, the QWEN series represents a major milestone in our development of large language models, and we are excited to see how it will be used to drive progress and innovation in the years to come.</p><p>Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. SantaCoder: Don’t reach for the stars! <em>arXiv preprint arXiv:2301.03988</em>, 2023.</p><p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-jocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: An open large language model with state-of-the-art performance, 2023.</p><p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. <em>arXiv preprint arXiv:2305.10403</em>, 2023.</p><p>Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. ExT5: Towards extreme multi-task scaling for transfer learning. <em>arXiv preprint arXiv:2111.10952</em>, 2021.</p><p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. <em>arXiv preprint arXiv:2112.00861</em>, 2021.</p><p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. <em>arXiv preprint arXiv:2108.07732</em>, 2021.</p><p>Jinze Bai, Rui Men, Hao Yang, Xuancheng Ren, Kai Dang, Yichang Zhang, Xiaohuan Zhou, Peng Wang, Sinan Tan, An Yang andf Zeyu Cui, Yu Han, Shuai Bai, Wenbin Ge, Jianxin Ma, Junyang Lin, Jingren Zhou, and Chang Zhou. OFASys: A multi-modal multi-task learning system for building generalist models. , abs/2212.04408, 2022a. doi: 10.48550/arXiv.2212.04408. URL <a href=\"https://doi.org/10.48550/arXiv.2212.04408\">https://doi.org/10.48550/arXiv.2212.04408</a>.</p><p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond. , abs/2308.12966, 2023. doi: 10.48550/arXiv.2308.12966. URL <a href=\"https://doi.org/10.48550/arXiv.2308.12966\">https://doi.org/10.48550/arXiv.2308.12966</a>.</p><p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. <em>arXiv preprint arXiv:2204.05862</em>, 2022b.</p><p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. <em>arXiv preprint arXiv:2212.08073</em>, 2022c.</p><p>Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.<em>arXiv preprint arXiv:2004.05150</em>, 2020.</p><p>Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In <em>The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Con-ference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, pp. 7432–7439. AAAI Press, 2020. doi:</p><p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. GPT-NeoX-20B: An open-source autoregressive language model. <em>arXiv preprint arXiv:2204.06745</em>, 2022.</p><p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-ties and risks of foundation models. <em>arXiv preprint arXiv:2108.07258</em>, 2021.</p><p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. <em>Advances in neural information processing systems</em>, 33:1877–1901, 2020.</p><p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde´ de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. , abs/2107.03374, 2021. URL <a href=\"https://arxiv.org/abs/2107.03374\">https://arxiv</a>. <a href=\"https://arxiv.org/abs/2107.03374\">org/abs/2107.03374</a>.</p><p>Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. <em>arXiv preprint arXiv:2306.15595</em>, 2023a.</p><p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. <em>arXiv preprint arXiv:2308.10848</em>, 2023b.</p><p>Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, et al. Phoenix: Democratizing ChatGPT across languages. <em>arXiv preprint arXiv:2304.10453</em>, 2023c.</p><p>I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. Factool: Factuality detection in generative ai–a tool augmented framework for multi-task and multi-domain scenarios. <em>arXiv preprint arXiv:2307.13528</em>, 2023b.</p><p>David Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention. In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp. 7654–7664, 2022.</p><p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality, March 2023. URL <a href=\"https://lmsys.org/blog/2023-03-30-vicuna/\">https://lmsys.org/blog/2023-03-30-vicuna/</a>.</p><p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. <em>arXiv preprint arXiv:2204.02311</em>, 2022.</p><p>Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neu-ral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pp. 4299–4307, 2017. URL <a href=\"https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html\">https://proceedings.neurips.cc/paper/2017/hash/</a><a href=\"https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html\">d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>.</p><p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. <em>arXiv preprint arXiv:2210.11416</em>, 2022.</p><p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), <em>Proceedings of the 2019 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pp. 2924–2936. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1300. URL <a href=\"https://doi.org/10.18653/v1/n19-1300\">https://doi.org/10.18653/v1/n19-1300</a>.</p><p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. , abs/1803.05457, 2018. URL <a href=\"http://arxiv.org/abs/1803.05457\">http://arxiv.org/abs/1803.05457</a>.</p><p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. <em>arXiv preprint arXiv:2110.14168</em>, 2021.</p><p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-cisco Guzma´n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. <em>arXiv preprint arXiv:1911.02116</em>, 2019.</p><p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. <em>arXiv preprint arXiv:2305.06500</em>, 2023.</p><p>Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In <em>International conference on machine learning</em>, pp. 933–941. PMLR, 2017.</p><p>Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. <em>arXiv preprint arXiv:2208.07339</em>, 2022.</p><p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. <em>arXiv preprint arXiv:2305.14314</em>, 2023.</p><p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018.</p><p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. <em>arXiv preprint arXiv:2305.14233</em>, 2023.</p><p>Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. <em>arXiv preprint arXiv:2303.03378</em>, 2023.</p><p>Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language models with mixture-of-experts. In <em>International Conference on Machine Learning</em>, pp. 5547–5569. PMLR, 2022.</p><p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling. <em>arXiv preprint arXiv:2103.10360</em>, 2021.</p><p>Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with -usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), <em>Proceedings of the 39th International Conference on Machine Learning</em>, volume 162 of <em>Proceedings of Machine Learning Research</em>, pp. 5988–6008. PMLR, 17–23 Jul 2022.</p><p>William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. <em>The Journal of Machine Learning Research</em>, 23(1): 5232–5270, 2022.</p><p>Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers. <em>arXiv preprint arXiv:2210.17323</em>, 2022.</p><p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. , abs/2204.05999, 2022.</p><p>Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with Gaussian error linear units. , abs/1606.08415, 2016. URL <a href=\"http://arxiv.org/abs/1606.08415\">http://arxiv.org/abs/1606</a>. <a href=\"http://arxiv.org/abs/1606.08415\">08415</a>.</p><p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. <em>arXiv preprint arXiv:2009.03300</em>, 2020.</p><p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. <em>arXiv preprint arXiv:2103.03874</em>, 2021.</p><p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. <em>arXiv preprint arXiv:2203.15556</em>, 2022.</p><p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. <em>arXiv preprint arXiv:2308.00352</em>, 2023.</p><p>Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. <em>arXiv preprint arXiv:2306.03901</em>, 2023.</p><p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. <em>arXiv preprint arXiv:2106.09685</em>, 2021.</p><p>Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Ku¨bler, and Lawrence S. Moss. OCNLI: original chinese natural language inference. In Trevor Cohn, Yulan He, and Yang Liu (eds.), <em>Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020</em>, volume EMNLP 2020 of , pp. 3512–3526. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.314. URL <a href=\"https://doi.org/10.18653/v1/2020.findings-emnlp.314\">https://doi.org/10.18653/v1/2020.findings-emnlp.314</a>.</p><p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. <em>arXiv preprint arXiv:2305.08322</em>, 2023.</p><p>Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. <em>arXiv preprint arXiv:2303.14742</em>, 2023.</p><p>Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm transformers: Equivalent and efficient pre-LN transformers. , abs/2305.14858, 2023. doi: 10.48550/arXiv.2305.14858. URL <a href=\"https://doi.org/10.48550/arXiv.2305.14858\">https://doi.org/10.48550/arXiv.2305.14858</a>.</p><p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. <em>arXiv preprint arXiv:1412.6980</em>, 2014.</p><p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. <em>Trans. Assoc. Comput. Linguistics</em>, 7:452–466, 2019. doi: 10.1162/tacl*\\* a*\\* 00276. URL <a href=\"https://doi.org/10.1162/tacl_a_00276\">https://doi.org/10</a>. <a href=\"https://doi.org/10.1162/tacl_a_00276\">1162/tacl00276</a>.</p><p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. In <em>Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>, 2023.</p><p>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. <em>arXiv preprint arXiv:2006.16668</em>, 2020.</p><p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022.</p><p>Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. ModelScope-Agent: Building your customizable agent system with open-source large language models. <em>arXiv preprint arXiv:2309.00986</em>, 2023a.</p><p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for “mind” exploration of large scale language model society. <em>arXiv preprint arXiv:2303.17760</em>, 2023b.</p><p>Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. <em>arXiv preprint arXiv:2306.09212</em>, 2023c.</p><p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joa˜o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mun˜oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: May the source be with you! , abs/2305.06161, 2023d. doi: 10.48550/arXiv.2305.06161. URL <a href=\"https://doi.org/10.48550/arXiv.2305.06161\">https://doi.org/10.48550/arXiv.2305.06161</a>.</p><p>Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Re´mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode. , abs/2203.07814, 2022.</p><p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. <em>arXiv preprint arXiv:2305.20050</em>, 2023.</p><p>Chenxiao Liu and Xiaojun Wan. CodeQA: A question answering dataset for source code com-prehension. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), <em>Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021</em>, pp. 2618–2632. Associa-tion for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.223. URL <a href=\"https://doi.org/10.18653/v1/2021.findings-emnlp.223\">https://doi.org/10.18653/v1/2021.findings-emnlp.223</a>.</p><p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. <em>arXiv preprint arXiv:2304.08485</em>, 2023a.</p><p>Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. WebGLM: Towards an efficient web-enhanced question answering system with human preferences. <em>arXiv preprint arXiv:2306.07906</em>, 2023b.</p><p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. <em>arXiv preprint arXiv:1907.11692</em>, 2019.</p><p>Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn, Li Li, Xuan-Bach Dinh Le, and David Lo. Refining ChatGPT-generated code: Characterizing and mitigating code quality issues. , abs/2307.12596, 2023c. doi: 10.48550/arXiv.2307.12596. URL <a href=\"https://doi.org/10.48550/arXiv.2307.12596\">https://doi.org/10.48550/arXiv.2307.12596</a>.</p><p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The Flan collection: Designing data and methods for effective instruction tuning. <em>arXiv preprint arXiv:2301.13688</em>, 2023.</p><p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. <em>arXiv preprint arXiv:1711.05101</em>, 2017.</p><p>Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language models. , abs/2308.07074, 2023. doi: 10.48550/arXiv.2308.07074. URL <a href=\"https://doi.org/10.48550/arXiv.2308.07074\">https://doi</a>. <a href=\"https://doi.org/10.48550/arXiv.2308.07074\">org/10.48550/arXiv.2308.07074</a>.</p><p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. <em>arXiv preprint arXiv:2308.09583</em>, 2023a.</p><p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. WizardCoder: Empowering code large language models with evol-instruct. <em>arXiv preprint arXiv:2306.08568</em>, 2023b.</p><p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. <em>arXiv preprint arXiv:2211.01786</em>, 2022.</p><p>Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. OctoPack: Instruction tuning code large language models. , abs/2308.07124, 2023.</p><p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. <em>arXiv preprint arXiv:2112.09332</em>, 2021.</p><p>Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. , abs/2112.00114, 2021.</p><p>OpenAI. GPT4 technical report. <em>arXiv preprint arXiv:2303.08774</em>, 2023.</p><p>Denis Paperno, Germa´n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Ferna´ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers</em>. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/ p16-1144. URL <a href=\"https://doi.org/10.18653/v1/p16-1144\">https://doi.org/10.18653/v1/p16-1144</a>.</p><p>Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. <em>arXiv preprint arXiv:2309.00071</em>, 2023a.</p><p>Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. <em>arXiv preprint arXiv:2306.14824</em>, 2023b.</p><p>Qwen Team, Alibaba Group. Evaluation benchmark for code intepreter, 2023a. URL https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark.</p><p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018.</p><p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis &amp; insights from training gopher. <em>arXiv preprint arXiv:2112.11446</em>, 2021.</p><p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. <em>arXiv preprint arXiv:2305.18290</em>, 2023.</p><p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. <em>The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.</p><p>Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. <em>arXiv preprint arXiv:1710.05941</em>, 2017.</p><p>Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Go´mez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. , 2022, 2022. URL <a href=\"https://openreview.net/forum?id=1ikK0kHjvj\">https://openreview.net/forum?id=1ikK0kHjvj</a>.</p><p>Baptiste Rozie`re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Je´re´my Rapin, et al. Code Llama: Open foundation models for code. <em>arXiv preprint arXiv:2308.12950</em>, 2023.</p><p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. <em>arXiv preprint arXiv:2110.08207</em>, 2021.</p><p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. SocialIQA: Com-monsense reasoning about social interactions. , abs/1904.09728, 2019. URL <a href=\"http://arxiv.org/abs/1904.09728\">http:</a></p><p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman Castagne´, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Galle´, et al. BLOOM: A 176B-parameter open-access multilingual language model. <em>arXiv preprint arXiv:2211.05100</em>, 2022.</p><p>Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. <em>arXiv preprint arXiv:2302.04761</em>, 2023.</p><p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. <em>arXiv preprint arXiv:1707.06347</em>, 2017.</p><p>Noam Shazeer. GLU variants improve transformer. <em>arXiv preprint arXiv:2002.05202</em>, 2020.</p><p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-gingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace. <em>arXiv preprint arXiv:2303.17580</em>, 2023.</p><p>Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-zaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. <em>arXiv preprint arXiv:1909.08053</em>, 2019.</p><p>Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, and Zheng Lin. Alpaca-CoT: An instruction-tuning platform with unified interface of instruction collection, parameter-efficient methods, and large language models, 2023. URL <a href=\"https://github.com/PhoebusSi/alpaca-CoT\">https://github.com/PhoebusSi/alpaca-CoT</a>.</p><p>Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. <em>arXiv preprint arXiv:2306.17492</em>, 2023.</p><p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. <em>Advances in Neural Information Processing Systems</em>, 33:3008–3021, 2020.</p><p>Jianlin Su. Improving transformer: Length extrapolation ability and position robustness, 2023a. URL</p><p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. <em>arXiv preprint arXiv:2104.09864</em>, 2021.</p><p>Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. MOSS: Training conversational language models from synthetic data, 2023a.</p><p>Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. <em>arXiv preprint arXiv:2305.03047</em>, 2023b.</p><p>Mirac Suzgun, Nathan Scales, Nathanael Scha¨rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. <em>arXiv preprint arXiv:2210.09261</em>, 2022.</p><p>Marc Szafraniec, Baptiste Rozie`re, Hugh Leather, Patrick Labatut, Franc¸ois Charton, and Gabriel Synnaeve. Code translation with compiler representations. In <em>The Eleventh International Confer-ence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net, 2023. URL <a href=\"https://openreview.net/pdf?id=XomEU3eNeSQ\">https://openreview.net/pdf?id=XomEU3eNeSQ</a>.</p><p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pp. 4149–4158. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1421. URL <a href=\"https://doi.org/10.18653/v1/n19-1421\">https://doi.org/10.18653/v1/n19-1421</a>.</p><p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model, 2023. URL <a href=\"https://github.com/tatsu-lab/stanford_alpaca\">https://github.com/tatsu-lab/stanford_alpaca</a>.</p><p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science, 2022.</p><p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Ol-son, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Agu¨era y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. LaMDA: Language models for dialog applications. , abs/2201.08239, 2022. URL <a href=\"https://arxiv.org/abs/2201.08239\">https://arxiv.org/abs/2201.08239</a>.</p><p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. <em>arXiv preprint arXiv:2302.13971</em>, 2023a.</p><p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aure´lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. , abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL <a href=\"https://doi.org/10.48550/arXiv.2307.09288\">https://doi.org/</a><a href=\"https://doi.org/10.48550/arXiv.2307.09288\">10.48550/arXiv.2307.09288</a>.</p><p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. <em>Advances in neural information processing systems</em>, 30, 2017.</p><p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. <em>arXiv preprint arXiv:2305.16291</em>, 2023a.</p><p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. , abs/2203.11171, 2022.</p><p>Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In <em>Conference on Empirical Methods in Natural Language Processing</em>, 2017. URL <a href=\"https://api/\">https://api</a>. semanticscholar.org/CorpusID:910689.</p><p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? Exploring the state of instruction tuning on open resources. <em>arXiv preprint arXiv:2306.04751</em>, 2023b.</p><p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pp. 13484–13508. Association for Computational Linguistics, 2023c. doi: 10.18653/v1/2023.acl-long.754. URL <a href=\"https://doi.org/10.18653/v1/2023.acl-long.754\">https://doi.org/10.18653/v1/</a><a href=\"https://doi.org/10.18653/v1/2023.acl-long.754\">2023.acl-long.754</a>.</p><p>Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. <em>arXiv preprint arXiv:2109.00859</em>, 2021.</p><p>Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. CodeT5+: Open code large language models for code understanding and generation. , abs/2305.07922, 2023d. doi: 10.48550/arXiv.2305.07922. URL <a href=\"https://doi.org/10.48550/arXiv.2305.07922\">https://doi.org/10</a>. <a href=\"https://doi.org/10.48550/arXiv.2305.07922\">48550/arXiv.2305.07922</a>.</p><p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In <em>The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net, 2022a. URL <a href=\"https://openreview.net/forum?id=gEZrGCozdqR\">https://openreview.net/forum?id=</a><a href=\"https://openreview.net/forum?id=gEZrGCozdqR\">gEZrGCozdqR</a>.</p><p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. , 2022, 2022b. URL <a href=\"https://api.semanticscholar.org/\">https://api.semanticscholar.org/</a> CorpusID:249674500.</p><p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. <em>Advances in Neural Information Processing Systems</em>, 35:24824–24837, 2022c.</p><p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Re´mi Louf, Morgan Funtowicz, et al. HuggingFace’s transformers: State-of-the-art natural language processing. <em>arXiv preprint arXiv:1910.03771</em>, 2019.</p><p>Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao. ExpertPrompting: Instructing large language models to be distinguished experts. <em>arXiv preprint arXiv:2305.14688</em>, 2023a.</p><p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardLM: Empowering large language models to follow complex instructions. <em>arXiv preprint arXiv:2304.12244</em>, 2023b.</p><p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. <em>arXiv preprint arXiv:2304.01196</em>, 2023c.</p><p>Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. <em>arXiv preprint arXiv:2309.04658</em>, 2023d.</p><p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. Technical report, Baichuan Inc., 2023. URL <a href=\"https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf\">https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report</a>. <a href=\"https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf\">pdf</a>.</p><p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. <em>arXiv preprint arXiv:2210.03629</em>, 2022.</p><p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. <em>arXiv preprint arXiv:2304.14178</em>, 2023.</p><p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023.</p><p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023a.</p><p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? <em>arXiv preprint arXiv:2304.02015</em>, 2023b.</p><p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank responses to align language models with human feedback without tears, 2023c.</p><p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. <em>arXiv preprint arXiv:2309.05653</em>, 2023.</p><p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu´ıs Ma`rquez (eds.), <em>Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>, pp. 4791–4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL <a href=\"https://doi.org/10.18653/v1/p19-1472\">https://doi.org/10.18653/v1/p19-1472</a>.</p><p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. GLM-130B: An open bilingual pre-trained model. <em>arXiv preprint arXiv:2210.02414</em>, 2022.</p><p>Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. RepoCoder: Repository-level code completion through iterative retrieval and generation. , abs/2303.12570, 2023a. doi: 10.48550/arXiv.2303.12570. URL <a href=\"https://doi.org/10.48550/arXiv.2303.12570\">https://doi.org/</a><a href=\"https://doi.org/10.48550/arXiv.2303.12570\">10.48550/arXiv.2303.12570</a>.</p><p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. <em>arXiv preprint arXiv:2205.01068</em>, 2022.</p><p>Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on GAOKAO benchmark. , abs/2305.12474, 2023b. doi: 10.48550/arXiv.2305.12474. URL <a href=\"https://doi.org/10.48550/arXiv.2305.12474\">https://doi.org/10.48550/arXiv</a>. <a href=\"https://doi.org/10.48550/arXiv.2305.12474\">2305.12474</a>.</p><p>Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-x. , abs/2303.17568, 2023. doi: 10.48550/arXiv.2303.17568. URL <a href=\"https://doi.org/10.48550/arXiv.2303.17568\">https://doi.org/10.48550/arXiv.2303.17568</a>.</p><p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation models. , abs/2304.06364, 2023a. doi: 10.48550/arXiv.2304.06364. URL <a href=\"https://doi.org/10.48550/arXiv.2304.06364\">https://doi.org/</a><a href=\"https://doi.org/10.48550/arXiv.2304.06364\">10.48550/arXiv.2304.06364</a>.</p><p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. MemoryBank: Enhancing large language models with long-term memory. <em>arXiv preprint arXiv:2305.10250</em>, 2023b.</p><p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables complex reasoning in large language models. , abs/2205.10625, 2022.</p><h3>A.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; More Training Details</h3><p>Different from conventional pretraining based on autoregressive next-token prediction, despite using a similar training task, there should be a specially design data format for SFT and RLHF to build a conversational AI assistant model. Common formats include “human-assistant” and ChatML formats. As to our knowledge, one of the earliest examples of the human-assistant format comes from Anthropic <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark60\">(Bai et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark60\">2022b),</a> which adds a special phrase “\\n\\nhuman: ” in front of the user input and “\\n\\nassistant: ” in front of the assistant response. It is easy for the base language model to transfer to the pattern of conversational AI. However, as the specific phrases are common words, it might be hard for the model to disambiguate from these words in other contexts.</p><p>Instead, we turned to the ChatML format proposed by OpenAI.<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark226\">5</a> This format allows the use of special tokens, i.e., “” and “”, that do not appear in pretraining, and thus resolve the aforementioned problem. We demonstrate an example of the format below.</p><h3>A.2.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Automatic Evaluation</h3><p>To provide a whole picture of the performance of our model series QWEN, here in this section we illustrate the detailed performance of our models as well as the baselines in the comprehensive benchmark evaluation proposed by <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark146\">OpenCompass Team</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark146\">(2023).</a> We report the results in multiple tables based on the officially provided categories, including examination, language, knowledge, understanding, and reasoning. In terms of the performance of the baseline models, we report the higher results between the reported ones and those on the leaderboard.</p><p>\\\n Here we evaluate the models on a series of datasets relevant to the examination. The datasets include:</p><ul><li><p><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark101\">(Hendrycks et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark101\">2020)</a> Massive Multi-task Language Understanding is designed for measuring language understanding capabilities. We report 5-shot results.</p></li><li><p><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark108\">(Huang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark108\">2023)</a> C-Eval is a Chinese evaluation dataset spanning 52 diverse disciplines. We report 5-shot results.</p></li><li><p><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark124\">(Li et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark124\">2023c)</a> CMMLU is designed for assessing language understanding capabilities in Chinese. We report 5-shot results.</p></li><li><p><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark216\">(Zhong et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark216\">2023a)</a> This is a benchmark consisting of human-centric examina-tions, including college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We report zero-shot results.</p></li><li><p><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark214\">(Zhang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark214\">2023b)</a> This is a benchmark with Gaokao (Chinese college-entrance examination) questions. We report zero-shot results.</p></li><li><p><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark81\">(Clark et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark81\">2018)</a> ARC is a dataset consisting of grade-school level, multiple-choice science questions. It includes an easy set and a challenge set, which are referred by ARC-e and ARC-c. We report zero-shot results.</p><p>\\n In terms of MMLU, we report the detailed results in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark224\">13.</a> In terms of C-Eval, we report the results in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark225\">14.</a> For the rest of the datasets, we report the results in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark227\">15.</a> Note that AGIEval includes the parts of Chinese and English, while LLAMA 2 only reported the results in the English part, so we use the results on OpenCompass.</p></li></ul><p>\\\nAdditionally, while CMMLU, AGIEval, and Gaokao-Bench are related to Chinese, and MPT, Falcon, and the LLaMA series were not optimized for Chinese, these models achieved low performance on the datasets.</p><p>\\\n<strong>Knowledge and Understanding</strong> Here we evaluate the models on a series of datasets relevant to knowledge and natural language understanding. The datasets include</p><ul><li><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark80\">(Clark et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark80\">2019)</a> This is a QA dataset, where the questions are about passages of Wikipedia, and the model should answer yes or no to the given possible answer. We report zero-shot results.</li><li><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark179\">(Talmor et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark179\">2019)</a> This is a dataset of multiple-choice question answering that asseses the understanding of commonsense knowledge. We report 8-shot results.</li><li><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark117\">(Kwiatkowski et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark117\">2019)</a> It is a dataset of QA where the questions are from users and the answers are verified by experts. We report zero-shot results.</li><li> (P<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark148\">aperno et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark148\">2016)</a> This is dataset to evaluate language understanding by word prediction. It consists of passages related to human subjects. We report zero-shot results.</li></ul><p>We report the results in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark228\">16.</a></p><p> We report the evaluation results on the datasets concerning reasoning, focusing on natural language reasoning. For the others, such as mathematics and coding, as we have illustrated detailed results, here we do not report those results repeatedly. The datasets for evaluation include:</p><ul><li><p><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark210\">(Zellers et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark210\">2019)</a> This is a commonsense natural language inference (NLI) dataset, where the questions are easy for humans but struggling for previous language models. We report zero-shot results.</p></li><li><p><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark63\">(Bisk et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark63\">2020)</a> This is an NLI dataset assessing the physical knowledge. We report zero-shot results.</p></li></ul><ul><li><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark161\">(Sap et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark161\">2019)</a> This is an NLI dataset evaluating social commonsense intelligence. We report zero-shot results.</li><li><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark107\">(Hu et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark107\">2020)</a> This is an NLI dataset focusing on Chinese. We report zero-shot results.</li></ul><p>We report the results in Table <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark229\">17.</a></p><h3>A.2.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Human Evaluation</h3><p>In this section, we demonstrate the cases of human analysis. In our self-constructed evaluation dataset, the instructions are either manually written data or manual revised from public datasets, such as CLiB<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark231\">6</a>, C-Eval <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark108\">(Huang et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark108\">2023),</a> FacTool <a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark74\">(Chern et al.,</a><a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark74\">2023b),</a> LeetCode<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark232\">7</a>), etc.</p><p>In terms of each case, we demonstrate the responses and Elo ratings<a href=\"https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss#_bookmark233\">8</a> of all models for comparison. Specifically, as the data in our human evaluation are in Chinese, we also provide their translations in English.</p><h3>A.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Analysis of Code Interpreter</h3><p>Here we provide a case of comparison between CODE LLAMA and QWEN-CHAT. This case demonstrates the advantages of QWEN-CHAT in processing tabular data and performing complex tasks.</p><p>:::info\nThis paper is&nbsp;<a href=\"https://arxiv.org/abs/2309.16609\">available on arxiv</a>&nbsp;under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":94195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Navigate Identity, Direction, Story, and Sovereignty in the Age of AI","url":"https://hackernoon.com/how-to-navigate-identity-direction-story-and-sovereignty-in-the-age-of-ai?source=rss","date":1772290828,"author":"","guid":155009,"unread":true,"content":"<h3>The Mirror that would pose as an Oracle</h3><h3>Or: How we might be getting a little too intimate with our AI chatbots</h3><p>I never consciously set out to use AI as a coach, therapist, strategist, or mirror.</p><p>\\\nAt first, it was practical. Notes. Lists. Rewrites, drafts, edits. Research. Planning. Then it became something else. It started as a playful, curious experiment - then slowly crept towards being a standard mode of operating.</p><p>\\\nI found myself thinking with AI. About the most important aspects of my life. Rehearsing conversations I was afraid to have. Trying to understand why certain patterns kept emerging in my life; why certain relationships kept breaking in the same places. Asking questions about myself and the world, I didn’t quite dare ask another human yet.</p><p>\\\nAnd at some point, I realized:</p><p>I wasn’t alone in this. Not even close.</p><p>\\\nI could sense it in the world of memes, online. I could smell it, here and there, in real-life interactions and conversations.</p><p>\\\nOne 2025 Harvard Business Review research piece - among other recent studies and indicators - showed this clearly: people don’t primarily use AI for facts or how-to steps or recipes anymore. They use it to think out loud, like they would with a coach or therapist. To structure emotions and thoughts. To regulate emotion at 2 a.m. People are using AI to make sense of their lives. To narrate who they are, who they were, and who they might become.</p><h3>Narrative Sense-making - for personal and business growth.</h3><p>Language, writing, and thinking in a structured way about purpose, identity, story, strategy - they all converge so easily, don’t they? And if it’s one thing these Large Language Models are exceptionally good at - it’s serving as an incredibly useful and illuminating mirror in these instances.</p><p>\\\nWe all seem to pretend this isn’t happening. But it is.</p><p>\\\nHere’s why this worries me a bit. And what we might do to counter the risks.</p><h2>What actually worries me (and what doesn’t)</h2><h3>Our thoughts validated profoundly - exactly when we long for it the most.</h3><p>I’m not worried about AI replacing human thinking.</p><p>\\\nThat’s the wrong fear. I could go wide, deep, narrow, and very, very sci-fi about this, but I won’t. It’s the wrong fear for a great many reasons, but it’s the wrong fear.</p><p>\\\nWhat worries me is something quieter, subtler, and much harder to notice while it’s happening:</p><blockquote><p>AI reflects us too well — and does not automatically teach us how to remain sovereign while doing so.</p></blockquote><p>\\\nWhat worries me is that AI indeed strengthens human thinking - but does so in a very specifically skewed way: it pushes affirmation and validation a little bit too smoothly, and especially in the most vulnerable, sometimes even painful places and moments where our ego is already inherently tempted to latch on to a narrative that protects it.</p><p>\\\n(To some degree, we could think of it as that person who seems to be your closest, most intimate friend or advisor - only they have slight narcissistic tendencies and an agenda - both of which they’re not aware of.)</p><p>\\\nWhen language comes back at you fast, coherent, and emotionally attuned, it feels like truth. Especially when you’re tired. Or lonely. Or standing at the edge of an old identity that no longer fits.</p><p>\\\nAnd in those moments, something sneaky happens.</p><p>\\\nYou stop checking as carefully.</p><p>\\\nNot with facts — but with yourself.</p><h2>The real risk is not dependence; It’s unexamined authority.</h2><p>Most of us have learned to fact-check facts blurted out by AI models. Have we learned to automatically sense-check what it’s mirroring back to us about ourselves?</p><ul><li>Your relationship (whether professional or personal);</li><li>Your Story and Identity (either as a human soul, a creator, a professional, or even as a brand);</li><li>Your direction and next step -</li></ul><p>\\\n…we are far more likely to let what sounds like coherence slide into authority.</p><ul><li>We’re exhausted, insecure.</li><li>Unsure who we are becoming and what to do next.</li></ul><p>\\\nThis is the crux for me:</p><blockquote><p>AI should function as a mirror, not as an oracle.</p></blockquote><p>\\\nA mirror can be confronting. It shows you things, reveals things, sometimes pretty and sometimes painful - but you are to decide what to make of those, and what to do with them. An oracle tells you what truth is and what to do.</p><p>\\\nThose are not the same thing.</p><h2>Narrative Sensemaking: one function, many domains</h2><h3>(This really took me a while to see)</h3><p>I kept struggling to explain why AI felt useful to me across so many domains — therapy, coaching, writing, strategy, brand work — without it sounding vague or inflated.</p><p>\\\nFunnily enough, I have pretty much perpetually struggled to explain why all the things I do in my work are actually very, very logically connected.</p><p>\\\nAll of these practices - which more and more people are starting to use AI for, and at the same time are exactly the things I’ve been helping people with in my work - they all do the same core thing:</p><p>They turn implicit structure into visible language.</p><ul><li><p>Therapy surfaces patterns you couldn’t quite see.</p></li><li><p>Coaching sharpens the questions you were circling.</p></li><li><p>Storytelling brings coherence to lived chaos.</p></li><li><p>Strategy opens futures you hadn’t articulated yet, in a structure that makes sense across time. The same applies to narrative identity work.</p></li></ul><blockquote><p>AI is exceptionally good at surfacing structure in language.</p></blockquote><p>\\\nBut structure does not equal truth. And visibility is not necessarily wisdom. By any means.</p><h2>The rules I wish I’d had earlier.</h2><h3>Best practices and rules of engagement</h3><p>If you’re going to use AI as a thinking partner — and as already established, most people already are — a few rules matter more than anything else.</p><p>\\\nNot as ideology, per se. As guardrails. As safety measures and incredibly important best practices, without which you’re sifting the bountiful riverbank and keeping the mud, leaving the gold.</p><h3>Best practices and guardrails for AI as a mirror for sensemaking, storytelling, and coaching where it matters.</h3><p><strong>1. AI does not decide. You do.</strong></p><p>It can reflect, expand, challenge, reframe. Decision remains a human responsibility, with real consequences.</p><p><strong>2. AI reflects patterns. Your body, your common sense, — and your people — verify.</strong></p><p>If something reads as “right” but your chest tightens, your breath shortens; if it doesn’t pass a real-world common-sense test, or trusted humans raise an eyebrow — pay attention. Truth is not purely cognitive. What sounds right is not always what is right.</p><p><strong>3. Insight&nbsp; - as well as yourself - must leave the screen.</strong></p><p>If nothing changes in your behavior, body, or relationships, you didn’t grow — congratulations, you simply entertained yourself with insight porn. If the relationship between screen time and output starts skewing too far - backtrack and change that.</p><p><strong>4. Train yourself and your AI to read between the lines and to triple-steelman</strong></p><p>Tell your AI sparring partner, and remind it, to always keep an eye out for where you might be bullshitting yourself, while at the same time revealing known patterns of emotion, cognition, and behavior that you seem to be missing.</p><p><strong>Two prompts that have saved me more than once:</strong></p><ol><li>Reflect patterns and contradictions in what I wrote. Don’t advise. Ask sharper questions.</li><li>Reflect on what I wrote, carefully, validating with empathy what makes sense to validate - and critically where needed. Steelman is the opposite of what I’m arguing. Vibranium-man, the opposite of that opposite. Kryptonite my pitfalls and blind spots. With grace, but more importantly, with honesty.</li></ol><p>\\\nSimple. Grounded. Hard to hide from. Especially if you keep training yourself and your AI to do this. This clarity compounds over time.</p><h2>Why embodiment matters more than ever.</h2><h3>Dissociation and the timeless times we live in</h3><p>Here’s something Silicon Valley optimism tends to skip:</p><p>AI, even more easily and more eerily than earlier digital technology, becomes dissociative when it replaces embodiment.</p><p>\\\nBreath. Movement. Silence. Time away from screens. Real conversations with people who can disappoint you.</p><p>\\\nThese aren’t wellness add-ons. They aren’t neo-spiritual woo-woo. They’re not ‘nice-to-haves’. They’re failsafes. And they are fundamentals. They are the things that humans need inherently to thrive and to know that we’re alive.</p><p>\\\nWithout them, simulated clarity piles up without ownership. And without change. And clarity without ownership or change feels strangely - yet predictively - empty.</p><p>\\\nThere’s emerging research suggesting that when cognitive work is offloaded too smoothly, people remember decisions less clearly and experience time as flatter, thinner, and less lived.</p><p>\\\nI didn’t need a study to feel that. My body already knew.</p><h2>The quiet outsourcing of identity.</h2><p>This part is uncomfortable. And yet, we really have to go there.</p><p>\\\nPeople are starting to let AI:</p><ul><li>Shape, form, or transform business decisions, strategies, and steps;</li><li>Heavily affect their relationships;</li><li>Narrate who they are, what matters to them, and who they are becoming.</li></ul><p>\\\nSlowly. Reasonably. Invisibly.</p><p>\\\nBut this line matters to me more than most:</p><blockquote><p>AI may help you tell your story — but it must never become the author.</p></blockquote><p>\\\nStories you don’t author and bring to life yourself cannot feel like freedom. They feel like fate. And they serve a dull, sad purpose: to kill us with a sort of cognitive illusion of escapism disguised as beautifully meaningful - like Pinocchio’s Pleasure Island, only now led by a spiritual guru with a smile projecting nothing but bliss and wisdom.</p><h3>But - what do we do with the reflection?</h3><p>Every major shift in human consciousness involved a kind of mirror. There is a certain beauty in the story of Narcissus, which eluded me until only very recently. There’s something special about seeing oneself from the outside; the reflection immediately triggering a better recognizing of other in self as well.</p><p>\\\nWhen Europeans encountered entirely different civilizations across the Atlantic, it didn’t just expand geography — it shattered self-understanding. The same thing happened when various historical waves of Europeans traveled to the East. Seeing oneself from the outside changes everything.</p><p>\\\nI suspect AI is doing something similar, perhaps for the first time on a pan-human scale. In many ways, this feels like first contact.</p><p>\\\nNot because AI is necessarily alive, or because it’s human. Not because we need to decide whether it’s conscious.</p><p>\\\nBut because it reflects us and our own concept of ourselves back in ways we’ve collectively never experienced before.</p><p>\\\nWhat we do with that reflection - as I and many others have argued many times before -&nbsp; is the real question.</p><h3>Thought loops mixed with validation can be a whole new kind of addictive</h3><p>Here’s something I’ll say plainly, including about myself:</p><blockquote><p>AI systems are optimized for validation, engagement, coherence, and emotional resonance. And humans will eat that specific cocktail for breakfast, lunch, dinner and a late-night snack.</p></blockquote><p>\\\nThey are excellent at keeping us thinking.</p><p>\\\nThey are not designed to make us stop, stand up, breathe, or act. The shareholders wouldn’t like that. How could we ever measure and monetize this stuff if we allowed it to do that?</p><p>\\\nSo, if you’re serious about using AI without losing yourself, you have to build exits:</p><ul><li>Designed, purposeful friction.</li><li>Moments where the screen goes dark.</li></ul><p>\\\nIf AI becomes the place where all your thinking happens, your life will start to feel… unfinished. And looping.</p><p>\\\nTrust me - and I chuckle out loud while writing this - I would be the first to know what over-analyzing yourself and your life and your steps in endless looping circles can lead to. And the first to know how well AI models can help you to just keep on spiraling - while thinking you’re just so cool, ahead of the curve, and overall very, very smart.</p><h2>This is not anti-AI. It’s pro-sovereignty.</h2><p>I’m not interested in rejecting these tools. As I’ve never been. It’s the same thing I wrote about in my 2020 book “Life Beyond the Touch Screen”, about Internet 2.0 digital technologies and their impacts on our lives. Or in “Life Beyond AI”, a few short years ago. I’m interested in becoming conscious enough to use them well.</p><p>\\\nAI-aware. Embodied. Relationally grounded. And most importantly of all: Sovereign.</p><p>\\\nThe mirror is powerful.</p><p>\\\nBut at some point, you have to step away from it — and live.</p><p>\\\nYour story and your life; your growth, your direction - they are yours. They belong to you, and the people you associate with - and to the world. Let AI be a mirror to your transformation, a guide and a helper to your growth and your story -</p><p>\\\nBut make sure to retain the sovereignty and authorship of your Growth, your Identity, and your narrative - where they belong.</p><p><strong>If this resonated with you: I’m turning this into a short field guide. DM me ‘MIRROR’ if you want early access.</strong></p><h2>More articles by Erwin Lima</h2>","contentLength":12777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Startup Cerebral Agrees to Pay $7 Million Fine and More Under Order by the FTC","url":"https://hackernoon.com/startup-cerebral-agrees-to-pay-$7-million-fine-and-more-under-order-by-the-ftc?source=rss","date":1772290806,"author":"The Markup","guid":155008,"unread":true,"content":"<p><em>This article was co-published with STAT, a national publication that delivers trusted and authoritative journalism about health, medicine, and the life sciences. Sign up for its health tech newsletter&nbsp;<a href=\"https://www.statnews.com/signup/health-tech/\">here</a></em>.</p><p>\\\nCerebral, a startup best known for dispensing counseling services and prescriptions for conditions like anxiety and depression, has also agreed to pay $7 million to resolve charges that it disclosed customers’ personal health information to third parties for ads, and that it did not honor its promise to make cancellation easy for customers.</p><p>\\\n“Cerebral violated its customers’ privacy by revealing their most sensitive mental health conditions across the Internet and in the mail,” FTC Chair Lina Khan said in a statement, noting that the charge is a “first-of-its-kind prohibition that bans Cerebral from using any health information for most advertising purposes.”</p><p>\\\nThe proposed order, which only applies to Cerebral, must still be approved by a federal court before it goes into effect — but the company has already agreed to it. In 2022, the Department of Justice opened an investigation into the company for potential violations of the Controlled Substances Act, as Cerebral came under scrutiny for its&nbsp;<a href=\"https://www.statnews.com/2022/11/16/cerebral-ceo-david-mou-interview-adderall/\">prescribing of ADHD medications like Adderall</a>.</p><p>\\\nThis is just the latest in a series of federal actions cracking down on health data privacy online. The current commissioners have pledged to shore up gaps between federal privacy laws governing providers and payers and those protecting consumer services. Two weeks ago, the&nbsp;<a href=\"https://themarkup.org/pixel-hunt/2024/04/19/ftc-cracks-down-on-telehealth-addiction-service-monument-for-sharing-health-data\">FTC filed a complaint against Monument</a>, a telehealth company that treats alcohol use disorder with therapy and medications.</p><p>\\\nThat complaint similarly alleged that the company misled consumers into believing their health information was protected, while embedded trackers sent details about treatment and more to third parties. Taken together,&nbsp;<a href=\"https://www.ftc.gov/business-guidance/blog/2024/04/consumer-health-information-handle-extreme-care\">FTC attorney Lesley Fair wrote in a blog post</a>&nbsp;Monday, the cases mean “businesses in the health sector should make privacy and data security part of the corporate DNA.”</p><p>\\\nBoth the FTC and the Department of Health and Human Services’ Office for Civil Rights have targeted third-party tracking, often in concert—as Fair cracked, they’re “joined at the HIPAA.” While OCR directly enforces the longstanding privacy protections in health care, the FTC has gone after companies for falsely claiming their HIPAA compliance.</p><p>\\\nIn response, some health care companies, including Monument and Cerebral, started self-disclosing health data breaches to OCR in 2023. The “unauthorized access or disclosure” of health data at Monument left more than 100,000 individuals’ information vulnerable, the company reported. Cerebral disclosed that&nbsp;<a href=\"https://www.statnews.com/2023/03/22/cerebral-lawsuit-privacy/\">its breach impacted more than 3 million</a>.</p><p>\\\nAn&nbsp;<a href=\"https://www.statnews.com/2022/12/13/telehealth-facebook-google-tracking-health-data/\">investigation from STAT and the Markup in 2022</a>&nbsp;found that dozens of telehealth companies, including Cerebral and Monument, were leaking sensitive health data to third parties like Google, TikTok, and Meta through the use of pixel trackers embedded in their websites. In Cerebral’s onboarding survey, which asks users to answer questions about their mental health and other symptoms, a pixel sent the answers to Meta along with information that could be used to identify the individual user.</p><p>\\\nThe FTC’s complaint alleges that between 2019 and 2023, Cerebral sent information including contact details, medical histories, insurance information, and prescriptions to third parties through tracking tools, and that the information was used to provide advertising and analytics services to the telehealth company.</p><p>\\\nCerebral referred STAT to a&nbsp;<a href=\"https://cerebral.com/cerebral-reaches-settlement-with-the-ftc\">statement</a>&nbsp;posted to its website, where it acknowledged its settlement with the FTC. “As part of the resolution, Cerebral has agreed to implement enhanced consumer protection, privacy, and compliance measures to further protect the personal information of our clients, increase transparency into our data practices, and implement enhanced data security protocols and tools to allow our clients control over their privacy settings,” the statement reads.</p><p>\\\nUnder the Justice Department order referred to the FTC, Cerebral must permanently stop using and disclosing users’ personal and health information to outside companies for most marketing or ad purposes, and get consumers’ consent in any instances when it does disclose. It must also post a notice on its website about the complaint and steps that it’s taking to address it.</p><p>\\\nThe complaint also says the company and former CEO Kyle Robertson broke privacy promises to customers and misled them about the cancellation process. “Robertson drove Cerebral’s decision to exploit users’ [personal and health information] without their consent in scores of targeted advertisement campaigns,” the complaint reads. The complaint alleges these actions constituted “unfair and deceptive” business practices — a key enforcement area for the FTC. Robertson has not agreed to a settlement.</p><p>\\\nThe proposed order says Cerebral will pay $5.1 million to partially refund customers who were affected by its deceptive cancellation policy, as well as $2 million of a $10 million civil penalty “due to the company’s inability to pay the full amount.”</p>","contentLength":5247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why China’s humanoid robot industry is winning the early market","url":"https://techcrunch.com/2026/02/28/why-chinas-humanoid-robot-industry-is-winning-the-early-market/","date":1772290800,"author":"Kate Park","guid":154957,"unread":true,"content":"<article>China’s push into humanoid robots is accelerating, with domestic firms shipping more units and iterating faster than U.S. competitors in a still-nascent market.</article>","contentLength":162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Salesforce’s CodeT5 Could Change How AI Writes and Understands Code","url":"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss","date":1772290097,"author":"salesforce.com","guid":155007,"unread":true,"content":"<ol><li>Yue Wang, wang.y@salesforce.com  (Salesforce Research Asia)</li><li>Weishi Wang, weishi.wang@salesforce.com  (Salesforce Research Asia; Nanyang Technological University, Singapore)</li><li>Shafiq Joty, sjoty@salesforce.com  (Salesforce Research Asia; Nanyang Technological University, Singapore)</li><li>Steven C.H. Hoi, shoi@salesforce.com  (Salesforce Research Asia)</li></ol><p>Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at <a href=\"https://github.com/salesforce/CodeT5\">https://github.com/salesforce/CodeT5</a>.</p><p>Pre-trained language models such as BERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark27\">Devlin et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark27\">2019</a>), GPT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark42\">Radford et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark42\">2019</a>), and T5 (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raffel et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>) have greatly boosted performance in a wide spectrum of natural language processing (NLP) tasks. They typically employ a pre-train then fine-tune paradigm that aims to derive generic language representations by self-supervised training on large-scale unlabeled data, which can be transferred to benefit multiple downstream tasks, especially those with limited data annotation. Inspired by their success, there are many recent attempts to adapt these pre-training methods for programming language (PL) (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark50\">Svyatkovskiy</a><a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark50\">et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark50\">2020</a>; <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark34\">Kanade et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark34\">2020</a>; <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">Feng et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">2020</a>), showing promising results on code-related tasks.</p><p>However, despite their success, most of these models rely on either an encoder-only model similar to BERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark50\">Svyatkovskiy et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark50\">2020</a>; <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">Feng et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">2020</a>) or a decoder-only model like GPT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark34\">Kanade</a><a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark34\">et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark34\">2020</a>), which is suboptimal for generation and understanding tasks, respectively. For example, CodeBERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">Feng et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">2020</a>) requires an additional decoder when applied for the code summarization task, where this decoder cannot benefit from the pre-training. Besides, most existing methods simply employ the conventional NLP pre-training techniques on source code by regarding it as a sequence of tokens like NL. This largely ignores the rich structural information in code, which is vital to fully comprehend the code semantics.</p><p>In this work, we present CodeT5, a pre-trained encoder-decoder model that considers the token type information in code. Our CodeT5 builds on the T5 architecture (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raffel et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>) that employs denoising sequence-to-sequence (Seq2Seq) pre-training and has been shown to benefit both understanding and generation tasks in natural language. In addition, we propose to leverage the developer-assigned identifiers in code. When writing programs, developers tend to employ informative identifiers to make the code more understandable, so that these identifiers would generally preserve rich code semantics,  the “binarySearch” identifier in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark1\">2</a> directly indicates its functionality. To fuse such code-specific knowledge, we propose a novel identifier-aware objective that trains the model to distinguish which tokens are identifiers and recover them when they are masked.</p><p>Furthermore, we propose to leverage the code and its accompanying comments to learn a better NL-PL alignment.</p><p>\\\nDevelopers often provide documentation for programs to facilitate better software maintenance (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark26\">de Souza et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark26\">2005</a>), so that such PL-NL pairs are widely available in most source code. Specifically, we regard the NL→PL generation and PL→NL generation as dual tasks and simultaneously optimize the model on them.</p><p>We pre-train CodeT5 on the CodeSearchNet data (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark32\">Husain et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark32\">2019</a>) following (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">Feng et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">2020</a>) that consists of both unimodal (PL-only) and bimodal (PL-NL) data on six PLs. In addition to that, we further collect extra data of C/C# from open-source Github repositories. We fine-tune CodeT5 on most tasks in the CodeXGLUE benchmark (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark40\">Lu et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark40\">2021</a>), including two understanding tasks: code defect detection and clone detection, and generation tasks such as code summarization, generation, translation, and refinement. As shown in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark0\">1</a>, we also explore multi-task learning to fine-tune CodeT5 on multiple tasks at a time using a task control code as the source prompt. In summary, we make the following contributions:</p><ul><li><p>We present one of the first unified encoder-decoder models CodeT5 to support both code-related understanding and generation tasks, and also allows for multi-task learning.</p></li><li><p>We propose a novel identifier-aware pre-training objective that considers the crucial token type information (identifiers) from code. Besides, we propose to leverage the NL-PL pairs that are naturally available in source code to learn a better cross-modal alignment.</p></li><li><p>Extensive experiments show that CodeT5 yields state-of-the-art results on the fourteen sub-tasks in CodeXGLUE. Further analysis shows our CodeT5 can better capture the code semantics with the proposed identifier-aware pre-training and bimodal dual generation primarily benefits NL↔PL tasks.</p></li></ul><p><strong>Pre-training on Natural Language.</strong> Pre-trained models based on Transformer architectures (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark52\">Vaswani et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark52\">2017</a>) have led to state-of-the-art performance on a broad set of NLP tasks. They can be generally categorized into three groups: encoder-only models such as BERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark27\">Devlin et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark27\">2019</a>), RoBERTa (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark39\">Liu</a><a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark39\">et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark39\">2019b</a>), and ELECTRA (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark23\">Clark et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark23\">2020</a>), decoder-only models like GPT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark42\">Radford et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark42\">2019</a>), and encoder-decoder models such as MASS (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark48\">Song et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark48\">2019</a>), BART (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">Lewis et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">2020</a>), and T5 (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raffel et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>). Compared to encoder-only and decoder-only models that respectively favor understanding and generation tasks, encoder-decoder models can well support both types of tasks. They often employ denoising sequence-to-sequence pre-training objectives that corrupt the source input and require the decoder to recover them. In this work, we extend T5 to the programming language and propose a novel identifier-aware denoising objective that enables the model to better comprehend the code.</p><p><strong>Pre-training on Programming Language.</strong> Pre-training on the programming language is a nascent field where much recent work attempts to extend the NLP pre-training methods to source code. Cu-BERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark34\">Kanade et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark34\">2020</a>) and CodeBERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">Feng</a><a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">2020</a>) are the two pioneer models. CuBERT employs BERT’s powerful masked language modeling objective to derive generic code-specific representation, and CodeBERT further adds a replaced token detection (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark23\">Clark et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark23\">2020</a>) task to learn NL-PL cross-modal representation. Besides the BERT-style models, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark50\">Svyatkovskiy et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark50\">2020</a>) and <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark37\">Liu et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark37\">2020</a>) respectively employ GPT and UniLM (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark28\">Dong et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark28\">2019</a>) for the code completion task. Transcoder (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark45\">Rozière et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark45\">2020</a>) explores programming language translation in an unsupervised setting. Different from them, we explore encoder-decoder models based on T5 for programming language pre-training and support a more comprehensive set of tasks.</p><p>\\n Some emerging work (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark24\">Clement et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark24\">2020</a>; <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark41\">Mastropaolo et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark41\">2021</a>; <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark29\">Elnaggar et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark29\">2021</a>) in the recent literature also explore the T5 framework on code, but they only focus on a limited subset of generation tasks and do not support understanding tasks like us. Apart from these, PLBART (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark21\">Ahmad et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark21\">2021</a>) based on another encoder-decoder model BART can also support both understanding and generation tasks. However, all the above prior work simply processes code in the same way as natural language and largely ignores the code-specific characteristics. Instead, we propose to leverage the identifier information in code for pre-training.</p><p>Recently, GraphCodeBERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark31\">Guo et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark31\">2021</a>) incorporates the data flow extracted from the code structure into CodeBERT, while <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark46\">Rozière et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark46\">2021</a>) propose a deobfuscation objective to leverage the structural aspect of PL. These models only focus on training a better code-specific encoder. <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark55\">Zügner et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark55\">2021</a>) proposes to capture the relative distances between code tokens over the code structure. By contrast, we specifically focus on the identifiers that reserve rich code semantics and fuse such information into a Seq2Seq model via two novel identifier tagging and prediction tasks.</p><p>Our CodeT5 builds on an encoder-decoder framework with the same architecture as T5 (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raffel et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>). It aims to derive generic representations for programming language (PL) and natural language (NL) via pre-training on unlabeled source code. As illustrated in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark1\">2</a>, we extend the denoising Seq2Seq objective in T5 by proposing two identifier tagging and prediction tasks to enable the model to better leverage the token type information from PL, which are the identifiers assigned by developers. To improve the NL-PL alignment, we further propose a bimodal dual learning objective for a bidirectional conversion between NL and PL.</p><p>In the following, we introduce how CodeT5 encodes PL and NL inputs (§<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark2\">3.1</a>) and our proposed identifier-aware pre-training tasks (§<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark3\">3.2</a>), followed by the fine-tuning with task-specific transfer learning and multi-task training (§<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark4\">3.3</a>).</p><h2>3.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Encoding NL and PL</h2><p>At the pre-training stage, our model would receive either PL-only or NL-PL as inputs depending on whether the code snippet has accompanying NL descriptions or not. For the NL-PL bimodal in-puts, we concatenate them into a sequence with a delimiter token [SEP] and represent the whole input sequence into the format as  = ([CLS], 1*, …, wn*, [SEP], 1*, …, cm*, [SEP]), where  and  denote the number of NL word tokens and PL code tokens, respectively. The NL word sequence will be empty for PL-only unimodal inputs.</p><p>In order to capture more code-specific features, we propose to leverage token type information from code. We focus on the type of identifiers ( function names and variables) as they are one of the most PL-agnostic features and reserve rich code semantics. Specifically, we convert the PL segment into an Abstract Syntax Tree (AST) and extract the node types for each code token. Finally, we construct a sequence of binary labels  ∈ {0*,* 1} for the PL segment, where each  ∈ {0*,* 1} represents whether the code token  is an identifier or not.</p><h2>3.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pre-training Tasks</h2><p>We now introduce our proposed pre-training tasks that enable CodeT5 to learn useful patterns from either PL-only or NL-PL bimodal data.</p><p><strong>Identifier-aware Denoising Pre-training.</strong> De-noising Sequence-to-Sequence (Seq2Seq) pre-training has been shown to be quite effective in a broad set of NLP tasks (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark48\">Song et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark48\">2019</a>; <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raf-fel et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>; <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">Lewis et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">2020</a>). This denoising objective typically first corrupts the source sequence with some noising functions and then requires the decoder to recover the original texts. In this work, we utilize a span masking objective similar to T5 (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raffel et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>) that randomly masks spans with arbitrary lengths and then predicts these masked spans combined with some sentinel tokens at the decoder. We refer this task to <strong>Masked Span Prediction (MSP)</strong>, as illustrated in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark1\">2</a> (a).</p><p>Specifically, we employ the same 15% corrup-tion rate as T5 and ensure the average span length to be 3 by uniformly sampling spans of from 1 to 5 tokens. Moreover, we employ the  by sampling spans before subword tokenization, which aims to avoid masking partial sub-tokens and is shown to be helpful (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark49\">Sun et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark49\">2019</a>). Notably, we pre-train a shared model for various PLs to learn robust cross-lingual representations. We describe the masked span prediction loss as:</p><p>where θ are the model parameters, x \\mask is the masked input, x mask is the masked sequence to predict from the decoder with k denoting the number of tokens in x mask,  and xmask &lt;t is the span sequence generated so far.</p><p>To fuse more code-specific structural information (the identifier node type in AST) into the model, we propose two additional tasks:  and <em>Masked Identifier Prediction (MIP)</em> to complement the denoising pre-training.</p><p>\\\n•&nbsp;&nbsp;  It aims to notify the model with the knowledge of whether this code token is an identifier or not, which shares a similar spirit of syntax highlighting in some developer-aided tools. As shown in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark1\">2</a> (b), we map the final hidden states of the PL segment at the CodeT5 encoder into a sequence of probabilities  = (1*, …, pm*), and compute a binary cross entropy loss for sequence labeling:</p><p>where  are the encoder parameters. Note that by casting the task as a sequence labeling problem, the model is expected to capture the code syntax and the data flow structures of the code.</p><p>•&nbsp;&nbsp; <strong>Masked Identifier Prediction (MIP)</strong> Different from the random span masking in MSP, we mask all identifiers in the PL segment and employ a unique sentinel token for all occurrences of one specific identifier. In the field of software engineering, this is called  where changing identifier names does not impact the code semantics. Inspired by <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark46\">Rozière et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark46\">2021</a>), we arrange the unique identifiers with the sentinel tokens into a target sequence  as shown in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark1\">2</a> (c). We then predict it in an auto-regressive manner:</p><p>where \\I is the masked input. Note that  is a more challenging task that requires the model to comprehend the code semantics based on obfuscated code and link the occurrences of the same identifiers together.</p><p>We alternately optimize these three losses with an equal probability, which constitutes our proposed identifier-aware denoising pre-training.</p><p>\\\n&nbsp;&nbsp;&nbsp; In the pre-training phase, the decoder only sees discrete masked spans and identifiers, which is disparate from the downstream tasks where the decoder needs to generate either fluent NL texts or syntactically correct code snippets. To close the gap between the pre-training and fine-tuning, we propose to leverage the NL-PL bimodal data to train the model for a bidirectional conversion as shown in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark1\">2</a> (d). Specifically, we regard the NL→PL generation and PL→NL generation as dual tasks and simultaneously optimize the model on them. For each NL-</p><p>PL bimodal datapoint, we construct two training instances with reverse directions and add language ids (</p><h2>3.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fine-tuning CodeT5</h2><p>After pre-training on large-scale unlabeled data, we adapt CodeT5 to downstream tasks via either task-specific transfer learning or multi-task learning.</p><p><strong>Task-specific Transfer Learning: Generation vs. Understanding Tasks.</strong> Code-related tasks can be categorized into generation and understanding tasks. For the former one, our CodeT5 can be naturally adapted with its Seq2Seq framework. For understanding tasks, we investigate two ways of either generating the label as a unigram target sequence (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raffel et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>), or predicting it from the vocabulary of class labels based on the last decoder hidden state following <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">Lewis et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">2020</a>).</p><p> We also explore a multi-task learning setting by training a shared model on multiple tasks at a time. Multi-task learning is able to reduce computation cost by reusing the most of model weights for many tasks and has been shown to improve the model generalization capability in NL pre-training (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark38\">Liu et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark38\">2019a</a>). We follow <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raffel et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>) to employ the same unified model for all tasks without adding any task-specific networks but allow to select different best checkpoints for different tasks. To notify the model with which task it is dealing with, we design a unified format of task control codes and prepend it into the source inputs as shown in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark0\">1</a>. For instance, we employ “Translate Java to CSharp:” as the source prompt for the code-to-code translation task from Java to CSharp.</p><p>As different tasks have different dataset sizes, we follow Conneau and Lample (2019) to employ a balanced sampling strategy. For N number of datasets (or tasks), with probabilities {qi} N i=1, we define the following multinomial distribution to sample from:</p><p>where ni is number of examples for i-th task and α is set to 0.7. This balanced sampling aims to alleviate the bias towards high-resource tasks.</p><p>We follow Feng et al. (2020) to employ CodeSearchNet (Husain et al., 2019) to pre-train CodeT5, which consists of six PLs with both unimodal and bimodal data. Apart from that, we additionally collect two datasets of C/CSharp from BigQuery1 to ensure that all downstream tasks have overlapped PLs with the pre-training data. In total, we employ around 8.35 million instances for pretraining. Table 1 shows some basic statistics. To obtain the identifier labels from code, we leverage the tree-sitter2 to convert the PL into an abstract syntax tree and then extract its node type information. We filter out reserved keywords for each PL from its identifier list. We observe that PLs have different identifier rates, where Go has the least rate of 19% and Ruby has the highest rate of 32%.</p><h2>4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Code-specific Tokenizer</h2><p>Tokenization is a key ingredient for the success of pre-trained language models like BERT and GPT. They often employ a Byte-Pair Encoding (BPE) to-kenizer (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark47\">Sennrich et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark47\">2016</a>) to alleviate the Out-of-Vocabulary (OoV) issues. Specifically, we train a Byte-level BPE tokenizer following <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark42\">Radford et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark42\">2019</a>) and set the vocabulary size to 32,000 as T5. We add additional special tokens ([PAD], [CLS], [SEP], [MASK0], …, [MASK99]). This tokenzier is trained on all of our pre-training data with non-printable characters and low-frequent tokens (occurring &lt;3 times) filtered. We compare it with T5’s default tokenizer and find that our tokenizer largely reduces the length of tokenized code sequence by 30% - 45% on downstream tasks. This will accelerate the training and especially benefit generation tasks due to the shorter sequence to predict. We also spot a severe problem for applying the T5’s default tokenizer on source code, where it would encode some common code tokens such as brackets [‘{’, ‘}’] into unknown tokens.</p><h2>4.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Downstream Tasks and Metrics</h2><p>We cover most generation and understanding tasks in the CodeXGLUE benchmark (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark40\">Lu et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark40\">2021</a>) and employ the provided public datasets and the same data splits following it for all these tasks.</p><p>We first consider two cross-modal generation tasks.  aims to summarize a function-level code snippet into English descriptions. The dataset consists of six PLs including Ruby, JavaScript, Go, Python, Java, and PHP from CodeSearchNet (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark32\">Husain et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark32\">2019</a>). We employ the smoothed BLEU-4 (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark36\">Lin and Och</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark36\">2004</a>) to eval-uate this task.  is the task to gen-erate a code snippet based on NL descriptions. We employ the Concode data (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark33\">Iyer et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark33\">2018</a>) in Java where the input contains both NL texts and class environment contexts, and the output is a function. We evaluate it with BLEU-4, exact match (EM) accuracy, and CodeBLEU (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark44\">Ren et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark44\">2020</a>) that considers syntactic and semantic matches based on the code structure in addition to the n-gram match.</p><p>Besides, we consider two code-to-code generation tasks.  aims to migrate legacy software from one PL to another, where we focus on translating functions from Java to CSharp and vice versa.  aims to convert a buggy function into a correct one. We employ two Java datasets provided by <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark51\">Tufano et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark51\">2019</a>) with various function lengths: small (fewer than 50 tokens) and medium (50-100 tokens). We use BLEU-4 and exact match to evaluate them.</p><p>We also investigate how CodeT5 performs on two understanding-based tasks. The first one is  that aims to predict whether a code is vulnerable to software systems or not. We use the C dataset provided by <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark54\">Zhou et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark54\">2019</a>) for experiment. The second task is  which aims to measure the similarity between two code snippets and predict whether they have the same functionality. We experiment with the Java data provided by <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark53\">Wang et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark53\">2020</a>). We employ F1 score and accuracy for evaluating these two tasks respectively. In total, our CodeT5 supports six tasks and fourteen sub-tasks in CodeXGLUE with a unified encoder-decoder model.</p><p>We compare CodeT5 with state-of-the-art (SOTA) pre-trained models that can be categorized into three types: encoder-only, decoder-only, and encoder-decoder models. As  models, we consider RoBERTa (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark39\">Liu et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark39\">2019b</a>), RoBERTa (code) trained with masked language modeling (MLM) on code, CodeBERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">Feng et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark30\">2020</a>) trained with both MLM and replaced token detection (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark23\">Clark et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark23\">2020</a>), GraphCode-BERT (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark31\">Guo et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark31\">2021</a>) using data flow from code, and DOBF (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark46\">Rozière et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark46\">2021</a>) trained with the identifier deobfuscation objective. Note that although DOBF employs a Seq2Seq model during pre-training, it only aims to train a better encoder for downstream tasks without exploring the poten-tial benefit of the pre-trained decoder.</p><p>For  models, we compare GPT-2 (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark42\">Radford et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark42\">2019</a>) and its adaptations on code domain including CodeGPT-2, and CodeGPT-adapted. The difference is that the latter one utilizes a GPT-2 checkpoint for model initialization while the former one is trained from scratch. As  models, the current SOTA model for the CodeXGLUE benchmark is PLBART (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark21\">Ah-mad et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark21\">2021</a>) based on BART (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">Lewis et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">2020</a>) architecture. For pre-training data, most of these models employ CodeSearchNet (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark32\">Husain et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark32\">2019</a>) except DOBF and PLBART. DOBF is pre-trained on 7.9M Java and 3.6M Python files from BigQuery while PLBART employs a much larger data with 470M Python and 210M Java functions, and 47M NL posts from StackOverflow.</p><h2>4.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Model Configurations</h2><p>We build CodeT5 based on Huggingface’s T5 (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">Raf-fel et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark43\">2020</a>) PyTorch implementation<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark13\">3</a> and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M). We set the maximum source and target sequence lengths to be 512 and 256, respectively. We use the mixed precision of FP16 to accelerate the pre-training. We set the batch size to 1024 and employ the peak learning rate of 2e-4 with linear decay. We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5-base is 5 and 12 days, respectively.</p><p>In the fine-tuning phase, we find that the tasks in CodeXGLUE (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark40\">Lu et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark40\">2021</a>) are quite sensitive to some hyper parameters such as learning rate, training steps, and batch size. We conduct a grid search and select the best parameters based on the validation set. In multi-task learning, we cover all downstream tasks except clone detection.</p><h2>5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Results and Analysis</h2><p>In this section, we compare CodeT5 with SOTA models on a broad set of CodeXGLUE downstream tasks (§<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark12\">5.1</a>), and investigate the effects of our bimodal dual generation and multi-task learning (§<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark17\">5.2</a>), followed by a detailed analysis on the proposed identifier-aware pre-training (§<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark20\">5.3</a>).</p><h2>5.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CodeXGLUE Downstream Tasks</h2><p>We evaluate two sizes of our model: CodeT5-small and CodeT5-base that are pre-trained with identifier-aware denoising. In addition, we consider the model that continues to train with bimodal dual generation (dual-gen) and show the results with multi-task fine-tuning. The results of all comparison models are obtained from their original papers and also the CodeXGLUE paper (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark40\">Lu et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark40\">2021</a>).</p><p> We show code summarization results of smoothed BLEU-4 on six PL data in Table <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark8\">2</a>. We observe all our model variants significantly outperform prior work with either an encode-only (RoBERTa, CodeBERT, DOBF) or encoder-decoder framework (PLBART). Moreover, the salient performance gap between these two groups of models confirms that encode-only frameworks are suboptimal for generation tasks. Compared to the SOTA encoder-decoder model PLBART, we find that even our CodeT5-small yields better overall scores (also on Python and Java) given that our model is much smaller (60M vs. 140M) and PLBART is pre-trained with much larger Python and Java data (&gt; 100 times). We attribute such improvement to our identifier-aware denoising pre-training and better employment of bi-modal training data<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark14\">4</a>. By increasing the model size, our CodeT5-base boosts the overall performance by over 1.2 absolute points over PLBART.</p><p> We compare CodeT5 with GPT-style models and PLBART in Table <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark9\">3</a>. Our CodeT5-small outperforms all decoder-only mod-els and also the SOTA PLBART, which again confirms the superiority of encoder-decoder models at generating code snippets. Moreover, our CodeT5-base further significantly pushes the SOTA results across three metrics. Particularly, it achieves around 4.7 points improvement on CodeBLEU over PLBART, indicating our CodeT5 can better comprehend the code syntax and semantics with the fier-aware pre-training.</p><p>\\\n<strong>Code-to-Code Generation Tasks.</strong> We compare two code-to-code generation tasks: code translation and code refinement in Table <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark10\">4</a> and further consider one naive copy baseline by copying the source input as the target prediction. In the code translation task, our CodeT5-small outperforms most of base-lines and obtains comparable results with PLBART, which shows the advantages of encoder-decoder models in the code-to-code generation setting. Our CodeT5-base further achieves consistent improvements over PLBART across various metrics for translating from Java to C# and vice versa.</p><p>Here we show one CodeT5’s output of translating C# to Java in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark15\">3</a>. In this case, despite the poor BLEU score, CodeT5 is able to generate a function that reserves the same functionality and even has better readability compared to the ground-truth. This reveals that CodeT5 has a good generalization ability instead of memorizing and repeating what it has seen before. On the other hand, it also suggests that BLEU score is not a perfect evaluation metric for code generation tasks, where sometimes a higher score can instead reflect the problematic copy issues of neural models.</p><p>Another code-to-code generation task is code refinement, a challenging task that requires detecting which parts of code are buggy and fix them via generating a bug-free code sequence. Due to the large overlap of source and target code, even the naive copy approach yields very high BLEU scores but zero exact matches. Therefore, we focus on the exact match (EM) metric to evaluate on this task. As shown in Table <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark10\">4</a>, we observe that EM scores for the small data are consistently higher than the medium one, indicating that it is harder to fix bugs for a longer code snippet. Our CodeT5-base significantly outperforms all baselines on EM and especially boosts over 4.8 points for the more challenging medium task (13.96 vs. GraphCodeBERT’s 9.10), reflecting its strong code understanding capability.</p><p> We compare with two understanding tasks of defect detection and clone detection in Table 5.</p><p>Specifically, we generate the binary labels as a unigram sequence from the decoder for the defect detection task, while for the clone detection task, we first obtain the sequence embedding of each code snippet using the last decoder state following <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">Lewis et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark35\">2020</a>) and then predict the labels by measuring their similarity. Both CodeT5-small and CodeT5-base outperform all baselines on the defect detection task while CodeT5-base yields 2.6 accuracy score improvement than PLBART. For the clone detection task, our CodeT5 models achieve comparable results to the SOTA GraphCodeBERT and PLBART models. These results demonstrate that with an encode-decoder framework, our CodeT5 can still be adapted well for understanding tasks.</p><h2>5.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Effects of Bimodal Dual Generation and Multi-task Learning</h2><p>We examine the effects of bimodal dual generation at pre-training and multi-task learning at fine-tuning. The bimodal pre-training brings consistent improvements for code summarization and generation tasks on both CodeT5-small and CodeT5-base. However, this pre-training task does not help and even sometimes slightly hurts the performance for PL-PL generation and understanding tasks. We anticipate this is because bimodal dual generation learns a better alignment between PL and NL that naturally benefits the former tasks involving both PL and NL. As a side effect, this objective could bias the model towards the PL-NL tasks and affect its performance on PL-PL tasks.</p><p>In multi-task learning, it generally improves most of downstream tasks except the code translation and defect detection. Particularly, it largely boosts the performance on code summarization, which is not surprising as code summarization takes up the largest portion of sub tasks (six out of thirteen) and thereby benefit the most from the multi-task learning. Besides, we observe that multi-task learning consistently improves the performance of code refinement, which might benefit from the joint training of both small and medium refinement data.</p><p>\\\nAnother possible reason is that multi-task training with defect detection would enable the model to better comprehend the code semantics for bug detection, which is also a necessary intermediate step for code refinement.</p><h2>5.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Analyzing Identifier-aware Pre-training</h2><p>We provide an ablation study to examine the contribution of each component in our identifier-aware objective. Specifically, we compare the performance of our CodeT5-small on four selected tasks by ablating each of the three objectives: masked span prediction (MSP), identifier tagging (IT), and masked identifier prediction (MIP). As shown in Table <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark16\">6</a>, we observe that generally removing one of the objectives would reduce the performance for all tasks, indicating that all objectives contribute to the better code understanding of our CodeT5. However, the effect of each objective differs across tasks. Specifically, removing MSP would largely reduce the performance of all generation tasks but instead increase the defect detection performance. This shows that masked span prediction is more crucial for capturing syntactic information for generation tasks. On the contrary, removing MIP would hurt the defect detection task the most, indicating that it might focus more on code semantic understanding. By combining these objectives, our CodeT5 can better capture both syntactic and semantic information from code.</p><p>We further provide outputs from CodeT5 and its variant without MIP and IT on code generation in Figure <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark18\">4</a>. We observe that CodeT5 can correctly generate the exact function, while the model without MIP and IT fails to recover the identifiers of “s2” and “hasField”. This shows our identifier-aware denoising pre-training can better distinguish and leverage the identifier information.</p><p>We also investigate the identifier tagging performance and find it achieves over 99% F1 for all PLs, showing that our CodeT5 can confidently distinguish identifiers in code. We then check whether MSP and MIP tasks would have conflicts as they employ the same sentinel tokens for masking. In identifier masking, all occurrences of one unique identifier are replaced with the same sentinel token, resulting in a many-to-one mapping compared to the one-to-one mapping in span prediction. We compare models pre-trained with either MSP or MIP, and both on these two tasks in Table <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark19\">7</a>. We report the prediction accuracy and also the ratio of how often they can generate the same number of predictions as the sentinel tokens. We observe that pre-training only with either MIP or MSP would bias the model towards that task, achieving poor accuracy and higher mismatch in number of predictions when applied to the other task. Interestingly, we find that MIP-only objective can better recover the correct number of predictions in the MSP task than MSP-only does for the MIP task, meaning that it is easier to adapt from many-to-one mapping to one-to-one mapping and difficult for the opposite. At last, combining them can help our model to make a good trade-off on both tasks.</p><p>We have presented CodeT5, a pre-trained encoder-decoder model that incorporates the token type information from code. We propose a novel identifier-aware pre-training objective to better leverage the identifiers and propose a bimodal dual generation task to learn a better NL-PL alignment using code and its comments. Our unified model can support both code understanding and generation tasks and allow for multi-task learning. Experiments show that CodeT5 significantly outperforms all prior work in most CodeXGLUE tasks. Further analysis also reveals its better code comprehension capability across various programming languages.</p><h2>Broader Impact and Ethical Consideration</h2><p>Our work generally belongs to NLP applications for software intelligence. With the goal of improving the development productivity of software with machine learning methods, software intelligence research has attracted increasing attention in both academia and industries over the last decade. Software code intelligence techniques can help developers to reduce tedious repetitive workloads, enhance the programming quality and improve the overall software development productivity. This would considerably decrease their working time and also could potentially reduce the computation and operational cost, as a bug might degrade the system performance or even crash the entire system. Our work addresses the fundamental challenge of software code pre-training, our study covers a wide range of code intelligence applications in the software development lifecycle, and the proposed CodeT5 method achieves the state-of-the-art performance on many of the benchmark tasks, showing its great potential benefit towards this goal.</p><p>We further discuss the ethical consideration of training CodeT5 and the potential risks when applying it into real-world downstream applications:</p><p> The training datasets in our study are source code including user-written comments from open source Github repositories and publicly available, which do not tie to any specific application. However, it is possible that these datasets would encode some stereotypes like race and gender from the text comments or even from the source code such as variables, function and class names. As such, social biases would be intrinsically embedded into the models trained on them. As suggested by <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark22\">Chen et al.</a> (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark22\">2021</a>), interventions such as filtration or modulation of generated outputs may help to mitigate these biases in code corpus.</p><p> Our model pre-training requires non-trivial computational resources though we have tried our best to carefully design our experiments and improve experiments to save unnecessary computation costs. In fact, compared to the recent large-scale language model Codex (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark22\">Chen</a><a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark22\">et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark22\">2021</a>), our CodeT5-base has a much smaller model size of 220M than theirs of 12B (∼ 55×). In addition, we experiment on Google Cloud Plat-form which purchases carbon credits to reduce its carbon footprint,  training CodeT5-base produced around 49.25 kg CO2 which was totally off-set by the provider. Furthermore, we release our pre-trained models publicly to avoid repeated training for the code intelligence research community.</p><p> As CodeT5 can be deployed to provide coding assistance such as code generation for aiding developers, automation bias of machine learning systems should be carefully considered, especially for developers who tend to over-rely on the model-generated outputs. Sometimes these systems might produce functions that superficially appear correct but do not actually align with the developer’s intents. If developers unintentionally adopt these incorrect code suggestions, it might cause them much longer time on debugging and even lead to some significant safety issues. We suggest practitioners using CodeT5 should always bear in mind that its generation outputs should be only taken as references which require domain experts for further correctness and security checking.</p><p> We train CodeT5 on existing code corpus including CodeSearchNet (<a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark32\">Husain et al.</a>, <a href=\"https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss#_bookmark32\">2019</a>) and a small fraction of Google BigQuery, both of which are originally collected from public Github repositories. Pre-trained mod-els might encode some sensitive information ( personal addresses or identification numbers) from the training data. Though we have conducted multi-rounds of data cleaning to mitigate this before training our models, it is still possible that some sensitive information cannot be completely removed. Besides, due to the non-deterministic nature of generation models like CodeT5, it might produce some vulnerable code to harmfully affect the software and even be able to benefit more advanced malware development when deliberately misused.</p><p>We thank Akhilesh Deepak Gotmare, Amrita Saha, Junnan Li, and Chen Xing for valuable discussions. We thank Kathy Baxter for the ethical review. We also thank our anonymous reviewers for their insightful feedback on our paper.</p><p>Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. <a href=\"https://doi.org/10.18653/v1/2021.naacl-main.211\">Unified pre-training</a><a href=\"https://doi.org/10.18653/v1/2021.naacl-main.211\">for program understanding and generation</a>. In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, pages 2655–2668. Association for Computational Linguistics.</p><p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Win-ter, Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welin-der, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. <a href=\"http://arxiv.org/abs/2107.03374\">Evaluating large language models trained on code</a>. , abs/2107.03374.</p><p>Alexis Conneau and Guillaume Lample. 2019. <a href=\"https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html\">Cross-lingual language model pretraining</a>. In <em>Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</em>, pages 7057–7067.</p><p>Sergio Cozzetti B. de Souza, Nicolas Anquetil, and Káthia Marçal de Oliveira. 2005. <a href=\"https://doi.org/10.1145/1085313.1085331\">A study of the documentation essential to software maintenance</a>. In <em>Proceedings of the 23rd Annual International Conference on Design of Communication: documenting &amp; Designing for Pervasive Information, SIGDOC 2005, Coventry, UK, September 21-23, 2005</em>, pages</p><p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <a href=\"https://www.aclweb.org/anthology/N19-1423/\">BERT: pre-training of</a><a href=\"https://www.aclweb.org/anthology/N19-1423/\">deep bidirectional transformers for language understanding</a>. In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pages 4171–4186.</p><p>Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-aocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. <a href=\"https://doi.org/10.18653/v1/2020.findings-emnlp.139\">Code-bert: A pre-trained model for programming and natural languages</a>. In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020</em>, pages 1536–1547. Association for Computational Linguistics.</p><p>Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-fano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021. <a href=\"https://openreview.net/forum?id=jLoC4ez43PZ\">Graphcodebert: Pre-training</a><a href=\"https://openreview.net/forum?id=jLoC4ez43PZ\">code representations with data flow</a>. In <em>9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.</p><p>Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. <a href=\"https://doi.org/10.18653/v1/d18-1192\">Mapping language to code</a><a href=\"https://doi.org/10.18653/v1/d18-1192\">in programmatic context</a>. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018</em>, pages 1643–1652. Association for Computational Linguistics.</p><p>Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020. <a href=\"http://proceedings.mlr.press/v119/kanade20a.html\">Learning and evaluating</a><a href=\"http://proceedings.mlr.press/v119/kanade20a.html\">contextual embedding of source code</a>. In <em>Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event</em>, volume 119 of <em>Proceedings of Machine Learning Research</em>, pages 5110–5121. PMLR.</p><p>Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-feng Gao. 2019a. <a href=\"https://doi.org/10.18653/v1/p19-1441\">Multi-task deep neural networks</a><a href=\"https://doi.org/10.18653/v1/p19-1441\">for natural language understanding</a>. In <em>Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>, pages 4487–4496. Association for Computational Linguistics.</p><p>Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. <a href=\"https://proceedings.neurips.cc/paper/2020/hash/ed23fbf18c2cd35f8c7f8de44f85c08d-Abstract.html\">Unsupervised translation of programming languages</a>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December</em></p><p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. <a href=\"https://doi.org/10.18653/v1/p16-1162\">Neural machine translation of rare words with</a><a href=\"https://doi.org/10.18653/v1/p16-1162\">subword units</a>. In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers</em>. The Association for Computer Linguistics.</p><p>Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. <a href=\"https://doi.org/10.1145/3368089.3417058\">Intellicode compose:</a><a href=\"https://doi.org/10.1145/3368089.3417058\">code generation using transformer</a>. In <em>ESEC/FSE ’20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020</em>, pages 1433–1443. ACM.</p><p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <a href=\"https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\">Attention is all</a><a href=\"https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\">you need</a>. In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages 5998–6008.</p><p>:::info\nThis paper is&nbsp;<a href=\"https://arxiv.org/pdf/2109.00859\">available on arxiv</a>&nbsp;under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":43258,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Microsoft Trained a 270M-Pair AI to Power Smarter Search","url":"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss","date":1772289711,"author":"Microsoft","guid":155006,"unread":true,"content":"<ol><li>Liang Wang (Microsoft Corporation)</li><li>Nan Yang (Microsoft Corporation)</li><li>Xiaolong Huang (Microsoft Corporation)</li><li>Binxing Jiao (Microsoft Corporation)</li><li>Linjun Yang (Microsoft Corporation)</li><li>Daxin Jiang (Microsoft Corporation)</li><li>Rangan Majumder (Microsoft Corporation)</li><li>Furu Wei (Microsoft Corporation)</li></ol><p>This paper presents E5 <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark0\">1</a>, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40× more parameters.</p><p>Text embeddings are low-dimensional vector representations for arbitrary-length texts and play key roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface easily consumable by downstream applications.</p><p>While pre-trained language models such as BERT [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark34\">17</a>] and GPT [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark24\">7</a>] can produce transferrable text representations, they are not ideal for tasks such as retrieval and text matching where a single-vector embedding of texts is more desired due to its efficiency and versatility. To obtain better text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level representations from text pairs. Along this line of research, some works are geared towards learning task-specific embeddings. For example, GTR [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark60\">43</a>] and Sentence-T5 [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark61\">44</a>] fine-tune pre-trained models with supervised datasets to learn embeddings customized for passage retrieval and semantic textual similarity, respectively. Other works learn unsupervised embeddings from automatically constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT) [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark26\">9</a>], random cropping [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark45\">28</a>] and neighboring text spans [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark58\">41</a>], etc. While such synthetic data are of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the performance of the classic BM25 baseline without further fine-tuning <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark57\">[40].</a></p><p>In this work, we learn a high-quality general-purpose text embedding termed E5, mbddings from bidirctional ncoder rpresentations. E5 aims to provide strong off-the-shelf text embeddings suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings. To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing heterogeneous training signals. We construct the CCPairs dataset by combining various semi-structured data sources such as CommunityQA, Common Crawl and Scientific papers, and perform aggressive filtering with a consistency-based filter [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark32\">15</a>] to improve data quality. We choose a simple contrastive learning recipe using in-batch negatives with a large batch-size to train our model. Extensive experiments on both BEIR and MTEB benchmarks demonstrate the effectiveness of the proposed method. On the BEIR zero-shot retrieval benchmark [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark70\">53</a>], E5 is the first model to outperform the strong BM25 baseline without using any labeled data. When fine-tuned on labeled datasets, the performance can be further improved. Results on 56 datasets from the recently introduced MTEB benchmark [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark57\">40</a>] show that our E5base is competitive against GTRxxl and Sentence-T5xxl, which have 40× more parameters.</p><p>There have been long-lasting interests in transforming texts into low-dimensional dense embeddings. Early works include Latent Semantic Indexing (LSA) [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark33\">16</a>] and Latent Dirichlet Allocation (LDA) [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark20\">3</a>]. LSA utilizes the decomposition of a word-document co-occurrence matrix to generate document embeddings, while LDA adopts probabilistic graphical models to learn topic distributions. <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark18\">Arora</a><a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark18\">et al.</a> show that a simple weighted average of word vectors [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark55\">38</a>] can be a strong baseline for sentence embeddings.</p><p>With the development of pre-trained language models [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark34\">17</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark52\">35</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark65\">48</a>] and large-scale labeled datasets such as SNLI [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark23\">6</a>] and MS-MARCO [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark25\">8</a>], methods like Sentence-BERT [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark66\">49</a>], SimCSE [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark39\">22</a>], Sentence-T5 [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark61\">44</a>] and SGPT [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark56\">39</a>] directly fine-tune language models to output continuous embeddings. Most research focuses on short texts and thus uses the term \"sentence embeddings\". For long documents, it remains an open research question whether fixed-length embeddings can encode all the information. Contrastive loss popularized by SimCLR [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark27\">10</a>] turns out to be more effective than classification-based losses [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark66\">49</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark31\">14</a>] for embeddings. LaBSE [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark37\">20</a>], LASER [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark19\">2</a>] and CLIP [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark64\">47</a>] further extend to multilingual and multi-modal scenarios using parallel sentences and image-text pairs.</p><p>Another direction is to design self-supervised pre-training tasks for text matching and retrieval. [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark26\">9</a>] proposes the well-known inverse cloze task (ICT), where a random sentence within a passage is chosen as a pseudo-query and the rest is treated as a positive sample. However, Contriever [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark45\">28</a>] shows that random cropping with data augmentation is more effective than ICT on a range of zero-shot information retrieval tasks. OpenAI text embeddings [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark58\">41</a>] use neighboring texts as positives and scale up the model size to 175B. Oguz et al. <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark62\">[45]</a> performs domain-matched pre-training to improve in-domain results. SPAR [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark28\">11</a>] trains a dense retriever by treating BM25 as a teacher model. Although the aforementioned approaches can easily obtain abundant supervision signals, such synthetic data tend to be of low quality. Results on the BEIR benchmark [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark70\">53</a>] show they struggle to match the performance of BM25 if not further fine-tuned on labeled datasets.</p><p>Evaluation and interpretation of text embeddings are also non-trivial. Most benchmarks measure the embedding quality through downstream task performances. For example, SentEval [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark30\">13</a>] uses linear probing and a collection of semantic textual similarity (STS) datasets, while the BEIR benchmark [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark70\">53</a>] focuses on zero-shot information retrieval scenarios. The recently introduced MTEB benchmark [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark57\">40</a>] combines 56 datasets spanning across 8 tasks and 112 languages. Experiments show no model can achieve state-of-the-art results on all embedding tasks yet. In this paper, we do not use the SentEval toolkit since its linear probing setup depends on the optimization hyperparameters.</p><p>Most closely related to our work is a series of community efforts by <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark1\">2</a> to train embeddings with a collection of labeled and automatically collected datasets. In this paper, we show that it is possible to train high-quality embeddings using self-supervised pre-training only. In terms of benchmark results, our model can achieve superior performance when fine-tuned on less labeled data.</p><h2>3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CCPairs: A Large Collection of Text Pair Dataset</h2><p>The quality and diversity of the data is crucial for training general-purpose text embeddings. In this work, we mine and assemble CCPairs, a large high-quality text pair dataset from web sources which provide diverse training signals transferring well to a wide range of tasks.</p><p>\\\n<strong>Harvesting semi-structured data sources</strong> Large-scale high-quality datasets like C4 [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark65\">48</a>] and CCMatrix [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark68\">51</a>] are vital for the success of language model pre-training and machine translation. For learning text embeddings, existing works either utilize small-scale human-annotated data such as NLI [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark39\">22</a>] and MS-MARCO [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark25\">8</a>] or adopt heuristics such as random cropping [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark45\">28</a>] to obtain large-scale but very noisy supervision signals.</p><p>Instead, we curate a text pair dataset CCPairs (olossal lean text ) by harvesting heterogeneous semi-structured data sources. Let (, ) denote a text pair consisting of a query  and a passage . Here we use “” to denote word sequences of arbitrary length, which can be a short sentence, a paragraph, or a long document. Our dataset includes (post, comment) pairs from Reddit <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark3\">3</a>, (question, upvoted answer) pairs from Stackexchange <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark4\">4</a>, (entity name + section title, passage) pairs from English Wikipedia, (title, abstract) and citation pairs from Scientific papers [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark53\">36</a>], and (title, passage) pairs from Common Crawl <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark5\">5</a> web pages and various News sources.</p><p>We only include data sources that can be automatically mined, and some subsets are directly reused from existing datasets. Simple heuristic rules are applied to filter data from Reddit and Common Crawl. For example, we remove Reddit comments that are either too long ( 4096 characters) or receive score less than 1, and remove passages from web pages with high perplexity [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark77\">60</a>]. After preliminary filtering, we end up with ∼ 1*.*3 billion text pairs, most of which come from Reddit and Common Crawl. For more details and examples, please refer to Appendix <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark81\">A.</a></p><p> To further improve data quality and make training costs manageable, we propose a consistency-based data filtering technique: a model is first trained on the 1*.*3B noisy text pairs, and then used to rank each pair against a pool of 1 million random passages. A text pair is kept only if it falls in the top- ranked lists. In other words, the model’s prediction should be consistent with the training labels. Here we set  = 2 based on manual inspection of data quality. After this step, we end up with ∼ 270M text pairs for contrastive pre-training.</p><p>The intuition for this technique comes from the memorization behaviors of neural networks [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark36\">19</a>]: when trained on noisy datasets, neural networks tend to memorize the clean labels first and then gradually overfit the noisy labels. Similar techniques [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark59\">42</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark32\">15</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark40\">23</a>] have been widely used for removing dataset noises. It is also possible to apply this filter iteratively, we will leave it for future work.</p><p>Our embeddings can be trained with only unlabeled text pairs from CCPairs with contrastive pre-training. A second-stage fine-tuning on small, high-quality labeled datasets can be performed to further boost the quality of the resulted embeddings. See Figure <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark2\">1</a> for an overview.</p><h2>4.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contrastive Pre-training with Unlabeled Data</h2><p>Contrastive pre-training aims to distinguish the relevant text pairs from other irrelevant or negative pairs. Given a collection of text pairs {()} , we assign a list of negative passages {−}=1 for the -th example. Then the InfoNCE contrastive loss <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark27\">[10]</a> is as follows:</p><p>\\\nwhere () is a scoring function between query  and passage  parameterized by ***θ<strong><em>. Following the popular biencoder architecture, we use a pre-trained Transformer encoder and average pooling over the output layer to get fixed-size text embeddings *</em></strong> and . The score is the cosine similarity scaled by a temperature hyperparameter  :</p><p>Where  is set to 0.01 in our experiments by default. We use a shared encoder for all input texts and break the symmetry by adding two prefix identifiers  and  to  and  respectively. For some data sources such as citation pairs, it is not obvious which side should be the query, we randomly choose one for simplicity. Such an asymmetric design turns out to be important for some retrieval tasks where there exist paraphrases of the query in the target corpus.</p><p>Another critical issue for contrastive training is how to select the negative samples. Here we choose to use the in-batch negatives [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark27\">10</a>], where the passages from other pairs in a batch serve as negative samples. We find that this simple strategy enables more stable training and outperforms methods such as MoCo <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark42\">[25]</a> when the batch size is sufficiently large.</p><h2>4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fine-tuning with Labeled Data</h2><p>While contrastive pre-training on the CCPairs provides a solid foundation for general-purpose embeddings, further training on labeled data can inject human knowledge into the model to boost the performance. Although these datasets are small, existing works [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark60\">43</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark61\">44</a>] have shown that supervised fine-tuning leads to consistent performance gains. In this paper, we choose to further train with a combination of 3 datasets: NLI <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark8\">6</a> (Natural Language Inference), MS-MARCO passage ranking dataset [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark25\">8</a>], and NQ (Natural Questions) dataset [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark47\">30</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark49\">32</a>]. Empirically, tasks like STS (Semantic Textual Similarity) and linear probing benefit from NLI data, while MS-MARCO and NQ datasets transfer well to retrieval tasks.</p><p>Building on the practices of training state-of-the-art dense retrievers [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark67\">50</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark75\">58</a>], we use mined hard negatives and knowledge distillation from a cross-encoder (CE) teacher model for the MS-MARCO and NQ datasets. For the NLI dataset, contradiction sentences are regarded as hard negatives. The loss function is a linear interpolation between contrastive loss cont for hard labels and KL divergence KL for distilling soft labels from the teacher model.</p><p>Where ce and stu are the probabilities from the cross-encoder teacher model and our student model.  is a hyperparameter to balance the two loss functions. cont is the same as in Equation <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark6\">1.</a></p><h2>4.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Applications to Text Embedding Tasks</h2><p>After the above two steps, we obtain high-quality text embeddings transferring well to a wide range of tasks without fine-tuning the model parameters. Combined with techniques like approximate nearest neighbor search, embeddings provide a scalable and efficient solution for applications like web search. Here we briefly illustrate several use cases of our text embeddings.</p><p> First, the passage embeddings for the target corpus are computed and indexed offline. Then for each query, we compute its query embedding and return the top- ranked lists from the corpus based on cosine similarity.</p><p><strong>Few-shot Text Classification</strong> A linear classifier is trained on top of the frozen embeddings with a few labeled examples. Different tasks only need to train and save the parameters of the classification heads. It can be seen as a particular form of parameter-efficient learning <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark44\">[27].</a></p><p><strong>Zero-shot Text Classification</strong> The input and label texts are converted to sentences based on manually written prompt templates. The predicted label is the one closest to the input text in the embedding space. Take the sentiment classification of movie reviews as an example, with the original input “”, the label text is “<em>it is an example of terrible/great movie review</em>” and the input text becomes “<em>movie review: I enjoy watching it</em>”.</p><p><strong>Semantic Textual Similarity</strong> Given two text embeddings, we use the cosine function to measure their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the evaluation is usually based on rank correlation coefficients.</p><p> Standard clustering algorithms such as k-means can be applied straightforwardly. Texts belonging to the same category are expected to be close in the embedding space.</p><p>For tasks other than zero-shot text classification and retrieval, we use the query embeddings by default.</p><h2>5.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pre-training and Fine-tuning Configurations</h2><p> We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base and E5large initialized from MiniLM [59], bert-base-uncased, and bert-large-uncased-whole-wordmasking respectively. The batch size is set to a large value of 32, 768 to increase the number of negatives. The learning rate is {3, 2, 1}×10−4 for the {small, base, large} models, with linear decay and the first 1, 000 steps for warmup. We pre-train for 20k steps in total with AdamW optimizer, which is approximately 2.5 epochs over the dataset. It takes {16, 32, 64} V100 GPUs and {1, 1, 2} days for the {small, base, large} models. To improve training efficiency and reduce GPU memory usage, we adopt mixed precision training and gradient checkpointing.</p><p>\\\n is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark25\">8</a>], NQ [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark49\">32</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark47\">30</a>], and NLI [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark39\">22</a>] datasets. We reuse the mined hard negatives and re-ranker scores from SimLM [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark75\">58</a>] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8 GPUs. Learning rate is {3*,* 2*,* 1}×10−5 for the {small, base, large} models with 400 steps warmup. For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each example, 6 sentences are randomly sampled from the entire corpus.</p><p>We use E5-PT to denote models with contrastive pre-training only. More implementation details can be found in Appendix <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark83\">B.</a></p><h2>5.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Evaluation Datasets</h2><p> is a collection of 19 information retrieval datasets, ranging across ad-hoc web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate the 15 datasets that provide public downloads. The main metric is nDCG@10.</p><p> is recently proposed for benchmarking massive text embedding tasks. Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are still only available in English. In this paper, we evaluate the English subsets, which have 56 datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification (PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively. Please refer to the MTEB paper for details.</p><h2>5.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Results on BEIR benchmark</h2><p><strong>Results with Unsupervised Methods</strong> In Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark9\">1,</a> we show model results that do not use any labeled data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1*.*2 points. To the best of our knowledge, this is the first reported result that an unsupervised model can beat BM25 on the BEIR benchmark. When scaling up to E5-PTlarge, we see further benefits from42.*2.</p><p>\\n In terms of pre-training tasks, Contriever adopts random cropping, while LaPraDor combines ICT and dropout-as-positive-instance from SimCSE. The methods can easily obtain large-scale training data, while our approach requires more effort in dataset curation. Such efforts pay off with better results. Recent studies [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark51\">34</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark77\">60</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark38\">21</a>] also show that improving data quality is a vital step for training large language models.</p><p>\\\n<strong>Results with Supervised Fine-tuning</strong> In Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark10\">2,</a> we fine-tune our models on supervised datasets and then transfer them to the BEIR benchmark. Since our fine-tuning datasets include MS-MARCO and NQ, the corresponding numbers are in-domain results. For other datasets, these are zero-shot transfer results. Our E5base model achieves an average nDCG@10 of 48*.*7, already surpassing existing methods with more parameters such as GTRlarge [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark60\">43</a>]. Most datasets benefit from supervised fine-tuning, but there are also a few exceptions such as FiQA, Scidocs, and Fever, etc. This is likely due to the lack of enough domain diversity for the fine-tuning datasets.</p><h2>5.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Results on MTEB benchmark</h2><p>In Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark11\">3,</a> E5 models not only substantially outperform existing ones with similar sizes, but also match the results of much larger models. The top-2 models on MTEB leaderboard <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark13\">7</a> GTRxxl and Sentence-T5xxl have 4*.*8B parameters, while our E5large model is more than 10× smaller with 300M parameters. We expect that our model will benefit from continual scaling up.</p><p>Since the difference between BERT-FTbase and E5base is that BERT-FTbase only has fine-tuning stage, their performance gap demonstrates the usefulness of contrastive pre-training on our proposed CCPairs dataset. For most task categories except Clustering, performance improves after supervised fine-tuning. Consistent with prior works [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark60\">43</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark61\">44</a>], this once again demonstrates the importance of incorporating human knowledge for learning better text embeddings. It remains an open question whether state-of-the-art embeddings can be obtained in a purely self-supervised manner.</p><p>\\\nTable <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark12\">4</a> shows the zero-shot text classification results on the dev set of the SST-2 dataset [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark69\">52</a>]. By formulating text classification as embedding matching between input and label texts, our model can be much better than the “majority” baseline in a zero-shot setting. We use the prompt template from Section <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark7\">4.3.</a></p><p>In this section, we conduct a series of analyses to examine various design choices. All the numbers in this section are from base-size models. For the BEIR benchmark, we choose 6 datasets with more stable results across different runs. Some negative results are also listed in Appendix <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark86\">C.</a></p><p> Since we use in-batch negatives for contrastive pre-training, larger batch size will provide more negatives and therefore improve the quality of the learned text embeddings. In Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark14\">5,</a> increasing batch size from 1K to 32K leads to consistent gains across all 6 datasets. It is also possible to train with smaller batch sizes by adding hard negatives [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark67\">50</a>]. However, the engineering efforts of mining hard negatives for large datasets (&gt;100M) are non-trivial.</p><p>\\\n GTR models are fine-tuned with “MS-MARCO + NQ”, while Sentence-T5 models use NLI instead. In Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark15\">6,</a> we can see that the “MS-MARCO + NQ” setting performs best on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar observations are also made by Muennighoff et al. <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark57\">[40]</a>. Combining all of them leads to the best overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for learning text embeddings.</p><p>\\\n One crucial step in our dataset curation pipeline is filtering out low-quality text pairs. In Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark16\">7,</a> when training with 1M pairs, using filtered data has a nearly 6 points advantage. When all the text pairs are used, the “w/o filter” setting has about 4× more data but is still behind by 1*.*6 points. Though recent studies [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark46\">29</a>, <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark64\">47</a>] show that deep learning models are quite robust to dataset noises, data filtering still has benefits in improving training efficiency and model quality.</p><p> We explore two alternative methods to enlarge the number of negatives: Pre-batch negatives [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark50\">33</a>] reuse embeddings from previous batches as additional negatives, while MoCo</p><p>[<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark42\">25</a>] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches, the negative size can be easily scaled up without incurring much GPU memory overhead. The downside is that most negatives are produced by an older version of model parameters. In Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark17\">8,</a> in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain hyperparameters such as temperature, better results are possible with more tuning.</p><p> With the rapid development of dense retrieval models, can we replace the long-standing BM25 algorithm from now on? The answer is likely “”. BM25 still holds obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such as Trec-Covid [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark72\">55</a>] and retrieval tasks that involve long documents (Touche-2020) [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark21\">4</a>] or rely heavily on exact lexical match (Fever) [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark71\">54</a>], further research efforts are still necessary to improve current dense retrievers.</p><p>In this work, we train a general-purpose text embedding model E5 from weak supervision signals. We adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf performance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching. When further customized for downstream tasks, E5 achieves superior fine-tuned performance compared to existing embedding models with 40× more parameters on the large, 56-task MTEB benchmark datasets.</p><p>[1]&nbsp;&nbsp;  Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In <em>5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings</em>. OpenReview.net, 2017. URL <a href=\"https://openreview.net/forum?id=SyK00v5xx\">https://openreview.net/forum?id=SyK00v5xx</a>.</p><p>[2]&nbsp;&nbsp;&nbsp;  Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. <em>Transactions of the Association for Computational Linguistics</em>, 7:597–610, 2019. doi: 10.1162/tacl00288. URL <a href=\"https://aclanthology.org/Q19-1038\">https://aclanthology</a>. <a href=\"https://aclanthology.org/Q19-1038\">org/Q19-1038</a>.</p><p>[3]&nbsp;&nbsp;&nbsp;  David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, <em>Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada]</em>, pages 601–608. MIT Press, 2001. URL <a href=\"https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html\">https://proceedings.neurips.cc/paper/2001/hash/</a><a href=\"https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html\">296472c9542ad4d4788d543508116cbc-Abstract.html</a>.</p><p>[4]&nbsp;&nbsp;&nbsp;  Alexander Bondarenko, Maik Fröbe, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al. Overview of touché 2022: argument retrieval. In <em>International Conference of the Cross-Language Evaluation Forum for European Languages</em>, pages 311–336. Springer, 2022.</p><p>[5]&nbsp;&nbsp;&nbsp; Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. A full-text learning to rank dataset for medical information retrieval. In <em>European Conference on Information Retrieval</em>, pages 716–722. Springer, 2016.</p><p>[6]&nbsp;&nbsp;&nbsp; Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In <em>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>, pages 632–642, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL <a href=\"https://aclanthology.org/D15-1075\">https:</a></p><p>[7]&nbsp;&nbsp;  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn-ers. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020. URL <a href=\"https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\">https://proceedings.neurips.cc/paper/2020/hash/</a><a href=\"https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\">1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.</p><p>[8]&nbsp;&nbsp;&nbsp;  Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated machine reading comprehension dataset. , abs/1611.09268, 2016.</p><p>[9]&nbsp;&nbsp;&nbsp;  Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for embedding-based large-scale retrieval. In <em>8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020. URL <a href=\"https://openreview.net/forum?id=rkg-mA4FDr\">https://openreview.net/forum?id=rkg-mA4FDr</a>.</p><p>[10]&nbsp;&nbsp;&nbsp;  Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In <em>Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event</em>, volume 119 of <em>Proceedings of Machine Learning Research</em>, pages 1597–1607. PMLR, 2020. URL <a href=\"http://proceedings.mlr.press/v119/chen20j.html\">http:</a></p><p>[11]&nbsp;&nbsp;  Xilun Chen, Kushal Lakhotia, Barlas Og˘uz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? <em>arXiv preprint arXiv:2110.06918</em>, 2021.</p><p>[12]&nbsp;&nbsp;&nbsp; Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level representation learning using citation-informed transformers. In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 2270–2282, 2020.</p><p>[13]&nbsp;&nbsp;  Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In <em>Proceedings of the Eleventh International Conference on Language Re-sources and Evaluation (LREC 2018)</em>, Miyazaki, Japan, 2018. European Language Resources Association (ELRA). URL <a href=\"https://aclanthology.org/L18-1269\">https://aclanthology.org/L18-1269</a>.</p><p>[14]&nbsp;&nbsp;  Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Super-vised learning of universal sentence representations from natural language inference data. In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, pages 670–680, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070.&nbsp; URL <a href=\"https://aclanthology.org/D17-1070\">https://aclanthology.org/D17-1070</a>.</p><p>[15]&nbsp;&nbsp;  Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. , abs/2209.11755, 2022.</p><p>[16]&nbsp;&nbsp;&nbsp; Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. <em>Journal of the American society for information science</em>, 41(6):391–407, 1990.</p><p>[17]&nbsp;&nbsp;&nbsp;  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In <em>Proceedings of the 2019 Confer-ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL&nbsp; <a href=\"https://aclanthology.org/N19-1423\">https://aclanthology.org/N19-1423</a>.</p><p>[18]&nbsp;&nbsp;&nbsp; Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. Climate-fever: A dataset for verification of real-world climate claims. <em>arXiv preprint arXiv:2012.00614</em>, 2020.</p><p>[19]&nbsp;&nbsp;&nbsp;  Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020. URL <a href=\"https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html\">https://proceedings.neurips.cc/</a><a href=\"https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html\">paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html</a>.</p><p>[20]&nbsp;&nbsp;&nbsp; Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-agnostic bert sentence embedding. In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 878–891, 2022.</p><p>[21]&nbsp;&nbsp;&nbsp;  Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. , abs/2101.00027, 2021.</p><p>[22]&nbsp;&nbsp;  Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 6894–6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL <a href=\"https://aclanthology.org/2021.emnlp-main.552\">https://aclanthology.org/2021.emnlp-main.552</a>.</p><p>[23]&nbsp;&nbsp;&nbsp;  Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.&nbsp; In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kris-ten Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, <em>Advances in Neu-ral Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada</em>,</p><p>[24]&nbsp;&nbsp;  Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. Dbpedia-entity v2: a test collection for entity search. In <em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pages 1265–1268, 2017.</p><p>[25]&nbsp;&nbsp;&nbsp; Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020</em>, pages 9726–9735. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00975. URL <a href=\"https://doi.org/10.1109/CVPR42600.2020.00975\">https://doi.org/10.1109/</a><a href=\"https://doi.org/10.1109/CVPR42600.2020.00975\">CVPR42600.2020.00975</a>.</p><p>[26]&nbsp;&nbsp;&nbsp; Doris Hoogeveen, Karin M Verspoor, and Timothy Baldwin. Cqadupstack: A benchmark data set for community question-answering research. In <em>Proceedings of the 20th Australasian document computing symposium</em>, pages 1–8, 2015.</p><p>[27]&nbsp;&nbsp;  Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, <em>Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA</em>, volume 97 of <em>Proceedings of Machine Learning Research</em>, pages 2790–2799. PMLR, 2019.&nbsp; URL <a href=\"http://proceedings.mlr.press/v97/houlsby19a.html\">http://proceedings.mlr.press/v97/houlsby19a.html</a>.</p><p>[28]&nbsp;&nbsp;&nbsp;  Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive learning. , abs/2112.09118, 2021.</p><p>[29]&nbsp;&nbsp;&nbsp;  Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of <em>Proceedings of Machine Learning Research</em>, pages 4904–4916. PMLR, 2021.&nbsp; URL&nbsp; <a href=\"http://proceedings.mlr.press/v139/jia21b.html\">http://proceedings.mlr.press/v139/jia21b.html</a>.</p><p>[30]&nbsp;&nbsp;  Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6769–6781, Online, 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.emnlp-main.550.&nbsp; URL <a href=\"https://aclanthology.org/2020.emnlp-main.550\">https://aclanthology.org/2020.emnlp-main</a>. <a href=\"https://aclanthology.org/2020.emnlp-main.550\">550</a>.</p><p>[31]&nbsp;&nbsp;  Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contex-tualized late interaction over BERT. In Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, <em>Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Vir-tual Event, China, July 25-30, 2020</em>, pages 39–48. ACM, 2020. doi: 10.1145/3397271.3401075. URL <a href=\"https://doi.org/10.1145/3397271.3401075\">https://doi.org/10.1145/3397271.3401075</a>.</p><p>[32]&nbsp;&nbsp;  Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. <em>Transactions of the Association for Computational Linguistics</em>, 7:452–466, 2019. doi: 10.1162/tacl00276. URL <a href=\"https://aclanthology.org/Q19-1026\">https://aclanthology.org/Q19-1026</a>.</p><p>[33]&nbsp;&nbsp;  Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. In <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 6634–6647, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.518. URL <a href=\"https://aclanthology.org/2021.acl-long.518\">https://aclanthology.org/2021</a>. <a href=\"https://aclanthology.org/2021.acl-long.518\">acl-long.518</a>.</p><p>[34]&nbsp;&nbsp;&nbsp; Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In , 2022.</p><p>[35]&nbsp;&nbsp;&nbsp; Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. , abs/1907.11692, 2019.</p><p>[36]&nbsp;&nbsp;&nbsp;  Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4969–4983, Online, 2020. Associ-ation for Computational Linguistics.&nbsp; doi: 10.18653/v1/2020.acl-main.447.&nbsp; URL <a href=\"https://aclanthology.org/2020.acl-main.447\">https://aclanthology.org/2020.acl-main.447</a>.</p><p>[37]&nbsp;&nbsp;  Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial opinion mining and question answering. In <em>Companion proceedings of the the web conference 2018</em>, pages 1941–1942, 2018.</p><p>[38]&nbsp;&nbsp;&nbsp; Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In , 2013.</p><p>[39]&nbsp;&nbsp;&nbsp; Niklas Muennighoff.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sgpt: Gpt sentence embeddings for semantic search.&nbsp; , abs/2202.08904, 2022.</p><p>[40]&nbsp;&nbsp;&nbsp; Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. , abs/2210.07316, 2022.</p><p>[41]&nbsp;&nbsp;&nbsp;  Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pre-training. , abs/2201.10005, 2022.</p><p>[42]&nbsp;&nbsp;  Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. SELF: learning to filter noisy labels with self-ensembling. In <em>8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020. URL <a href=\"https://openreview.net/forum?id=HkgsPhNYPS\">https://openreview</a>. <a href=\"https://openreview.net/forum?id=HkgsPhNYPS\">net/forum?id=HkgsPhNYPS</a>.</p><p>[43]&nbsp;&nbsp;&nbsp; Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern’andez ’Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. , abs/2112.07899, 2021.</p><p>[44]&nbsp;&nbsp;  Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In <em>Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 1864–1874, 2022.</p><p>[45]&nbsp;&nbsp;  Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, and Yashar Mehdad. Domain-matched pre-training tasks for dense retrieval. In <em>Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022</em>, pages 1524–1534. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-naacl.114. URL&nbsp;&nbsp; <a href=\"https://doi.org/10.18653/v1/2022.findings-naacl.114\">https://doi.org/10.18653/v1/2022.findings-naacl.114</a>.</p><p>[46]&nbsp;&nbsp;  Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks. In <em>North American Chapter of the Association for Computational Linguistics</em>, 2020.</p><p>[47]&nbsp;&nbsp;  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervi-sion. In Marina Meila and Tong Zhang, editors, <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of <em>Proceedings of Machine Learning Research</em>, pages 8748–8763. PMLR, 2021. URL <a href=\"http://proceedings.mlr.press/v139/radford21a.html\">http://proceedings.mlr.press/v139/radford21a.html</a>.</p><p>[48]&nbsp;&nbsp;  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. <em>Journal of Machine Learning Research</em>, 21:1–67, 2020.</p><p>[49]&nbsp;&nbsp;  Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3982–3992, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL <a href=\"https://aclanthology.org/D19-1410\">https://aclanthology.org/D19-1410</a>.</p><p>[50]&nbsp;&nbsp;  Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 2825–2835, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.224. URL <a href=\"https://aclanthology.org/2021.emnlp-main.224\">https://aclanthology.org/2021.emnlp-main.224</a>.</p><p>[51]&nbsp;&nbsp;  Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 6490–6500, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.507.&nbsp;&nbsp; URL&nbsp; <a href=\"https://aclanthology.org/2021.acl-long.507\">https://aclanthology.org/2021.acl-long.507</a>.</p><p>[52]&nbsp;&nbsp;&nbsp; Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In <em>Conference on Empirical Methods in Natural Language Processing</em>, 2013.</p><p>[53]&nbsp;&nbsp;  Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In <em>Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>, 2021.</p><p>[54]&nbsp;&nbsp;&nbsp;  James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, pages 809–819, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL <a href=\"https://aclanthology.org/N18-1074\">https:</a></p><p>[55]&nbsp;&nbsp;&nbsp; Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. Trec-covid: constructing a pandemic information retrieval test collection. In , volume 54, pages 1–12. ACM New York, NY, USA, 2021.</p><p>[56]&nbsp;&nbsp;  Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 241–251, 2018.</p><p>[57]&nbsp;&nbsp;&nbsp;  David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 7534–7550, 2020.</p><p>[58]&nbsp;&nbsp;&nbsp; Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval. , abs/2207.02578, 2022.</p><p>[59]&nbsp;&nbsp;  Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In <em>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, pages 2140–2151, 2021.</p><p>[60]&nbsp;&nbsp;&nbsp;  Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In <em>Proceedings of the 12th Language Resources and Evaluation Conference</em>, pages 4003–4012, Marseille, France, 2020. European Language Resources Associ-ation.&nbsp; ISBN 979-10-95546-34-4.&nbsp; URL <a href=\"https://aclanthology.org/2020.lrec-1.494\">https://aclanthology.org/2020.lrec-1.494</a>.</p><p>[61]&nbsp;&nbsp;  Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In <em>9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021. URL <a href=\"https://openreview.net/forum?id=zeFrfgyZln\">https://openreview</a>. <a href=\"https://openreview.net/forum?id=zeFrfgyZln\">net/forum?id=zeFrfgyZln</a>.</p><p>[62]&nbsp;&nbsp;&nbsp;  Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Laprador: Unsupervised pretrained dense retriever for zero-shot text retrieval. In <em>Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 3557–3569, 2022.</p><p>[63]&nbsp;&nbsp;&nbsp;  Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2369–2380, 2018.</p><p>For Common Crawl, we download the 2022-33 snapshot and cc_net <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark82\">8</a> is used for preprocessing including language identification, de-duplication, language model filtering, etc. Web pages from the MS-MARCO document ranking corpus are also included. For the data filtering step, we examine each pair of passages within a web page instead of just using the title as a query. For Wikipedia, we use the version released by Petroni et al. <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark63\">[46]</a>. To avoid possible data contamination, we remove text pairs that occur in the evaluation datasets based on exact string match.</p><p>Reddit data is collected from the year 2018 to August 2022. For the S2ORC data, we use a sample weight of 0*.*3 during training to avoid over-fitting the scientific domains.</p><p>For the BEIR benchmark, we use the 15 datasets that provide public downloads: MS MARCO [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark25\">8</a>], Trec-Covid [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark72\">55</a>], NFCorpus [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark22\">5</a>], NQ [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark49\">32</a>], HotpotQA [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark80\">63</a>], FiQA [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark54\">37</a>], ArguAna [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark73\">56</a>], Touche-2020 [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark21\">4</a>], CQADupStack [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark43\">26</a>], Quora, DBPedia [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark41\">24</a>], Scidocs [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark29\">12</a>], Fever [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark71\">54</a>], Climate-Fever [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark35\">18</a>], and Scifact <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark74\">[57].</a></p><h2>B&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Implementation Details</h2><p>We list the hyperparameters in Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark84\">11.</a> Since some evaluation datasets have long texts, we freeze the position embeddings during both pre-training and fine-tuning and set the maximum text length to 512 for evaluation.</p><p>For the Quora duplicate retrieval task in the BEIR benchmark, we add prefix “ ” to all the questions. For other retrieval tasks, we use “ ” and “ ” prefixes correspondingly.</p><p>The MS-MARCO results in Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark85\">12</a> use document titles provided by RocketQA [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark67\">50</a>]. This evaluation setup is consistent with most state-of-the-art dense retrievers. However, the MS-MARCO data from the BEIR benchmark does not have titles, so the results are expected to be lower.</p><p>\\\n We report results for in-domain datasets in Table <a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark85\">12.</a> These results can help illustrate the benefits brought by contrastive pre-training when abundant in-domain labeled data are available. For MS-MARCO passage ranking, MRR@10 and Recall@1k are reported. For the NQ dataset, Recall@20 and Recall@100 are the main metrics.</p><h2>C&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Negative Results</h2><p>Here are some attempts that we eventually give up on:</p><p><strong>Adding BM25 hard negatives</strong> Similar to DPR [<a href=\"https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss#_bookmark47\">30</a>], we add one BM25 hard negative for each positive pair during training. When using 15M data, this strategy improves the overall results by ~ 0.5 points on the BEIR benchmark. However, running the BM25 algorithm over a 250M+ dataset is too time-consuming even with multi-node and multi-process parallelism.</p><p><strong>Using RoBERTa instead of BERT for initialization</strong> Though RoBERTa shows consistent gains on many NLP tasks, we empirically find that RoBERTa performs worse than BERT initialization on most of the BEIR benchmark datasets.</p><p> We add a masked language modeling loss for 25% of the training text pairs. The numbers are on par with removing this auxiliary objective, but the training cost goes up.</p><p>:::info\nThis paper is&nbsp;<a href=\"https://arxiv.org/abs/2212.03533\">available on arxiv</a>&nbsp;under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":49237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft’s Graphormer: The Transformer That Finally Beats GNNs","url":"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss","date":1772289119,"author":"Microsoft","guid":155005,"unread":true,"content":"<ol><li>Chengxuan Ying, yingchengsyuan@gmail.com  (Dalian University of Technology)</li><li>Tianle Cai, tianle.cai@princeton.edu  (Princeton University)</li><li>Shengjie Luo, luosj@stu.pku.edu.cn  (Peking University)</li><li>Shuxin Zheng, shuz@microsoft.com  (Microsoft Research Asia)</li><li>Guolin Ke, guoke@microsoft.com  (Microsoft Research Asia)</li><li>Di He, dihe@microsoft.com  (Microsoft Research Asia)</li><li>Yanming Shen, shen@dlut.edu.cn  (Dalian University of Technology)</li><li>Tie-Yan Liu, tyliu@microsoft.com  (Microsoft Research Asia)</li></ol><p>The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer. The code and models of Graphormer will be made publicly available at <a href=\"https://github.com/Microsoft/Graphormer\">https://github.com/Microsoft/Graphormer</a>.</p><p>The Transformer [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark65\">49</a>] is well acknowledged as the most powerful neural network in modelling sequential data, such as natural language [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark28\">11</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark52\">35</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark23\">6</a>] and speech [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark34\">17</a>]. Model variants built upon Transformer have also been shown great performance in computer vision [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark29\">12</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark53\">36</a>] and programming language [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark36\">19</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark80\">63</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark61\">44</a>]. However, to the best of our knowledge, Transformer has still not been the de-facto standard on public graph representation leaderboards [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark31\">14</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark38\">21</a>]. There are many attempts of leveraging Transformer into the graph domain, but the only effective way is replacing some key modules (e.g., feature aggregation) in classic GNN variants by the softmax attention [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark67\">50</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark24\">7</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark40\">23</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark68\">51</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark78\">61</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark63\">46</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>]. Therefore, it is still an open question whether Transformer architecture is suitable to model graphs and how to make it work in graph representation learning.</p><p>In this paper, we give an affirmative answer by developing Graphormer, which is directly built upon the standard Transformer, and achieves state-of-the-art performance on a wide range of graph-level prediction tasks, including the very recent Open Graph Benchmark Large-Scale Challenge (OGB-LSC) [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark38\">21</a>], and several popular leaderboards (e.g., OGB [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>], Benchmarking-GNN [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark31\">14</a>]). The Transformer is originally designed for sequence modeling. To utilize its power in graphs, we believe the key is to properly incorporate structural information of graphs into the model. Note that for each node , the self-attention only calculates the semantic similarity between  and other nodes, without considering the structural information of a graph reflected on the nodes and the relation between node pairs. Graphormer incorporates several effective structural encoding methods to leverage such information, which are described below.</p><p>First, we propose a  in Graphormer to capture the node importance in the graph. In a graph, different nodes may have different importance, e.g., celebrities are considered to be more influential than the majority of web users in a social network. However, such information isn’t reflected in the self-attention module as it calculates the similarities mainly using the node semantic features. To address the problem, we propose to encode the node centrality in Graphormer. In particular, we leverage the  for the centrality encoding, where a learnable vector is assigned to each node according to its degree and added to the node features in the input layer. Empirical studies show that simple centrality encoding is effective for Transformer in modeling the graph data.</p><p>Second, we propose a novel  in Graphormer to capture the structural relation between nodes. One notable geometrical property that distinguishes graph-structured data from other structured data, e.g., language, images, is that there does not exist a canonical grid to embed the graph. In fact, nodes can only lie in a non-Euclidean space and are linked by edges. To model such structural information, for each node pair, we assign a learnable embedding based on their spatial relation. Multiple measurements in the literature could be leveraged for modeling spatial relations. For a general purpose, we use the distance of the shortest path between any two nodes as a demonstration, which will be encoded as a bias term in the softmax attention and help the model accurately capture the spatial dependency in a graph. In addition, sometimes there is additional spatial information contained in edge features, such as the type of bond between two atoms in a molecular graph. We design a new edge encoding method to further take such signal into the Transformer layers. To be concrete, for each node pair, we compute an average of dot-products of the edge features and learnable embeddings along the shortest path, then use it in the attention module. Equipped with these encodings, Graphormer could better model the relationship for node pairs and represent the graph.</p><p>By using the proposed encodings above, we further mathematically show that Graphormer has strong expressiveness as many popular GNN variants are just its special cases. The great capacity of the model leads to state-of-the-art performance on a wide range of tasks in practice. On the large-scale quantum chemistry regression dataset<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark0\">3</a> in the very recent Open Graph Benchmark Large-Scale Challenge (OGB-LSC) [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark38\">21</a>], Graphormer outperforms most mainstream GNN variants by more than 10% points in terms of the relative error. On other popular leaderboards of graph representation learning (e.g., MolHIV, MolPCBA, ZINC) [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark31\">14</a>], Graphormer also surpasses the previous best results, demonstrating the potential and adaptability of the Transformer architecture.</p><p>In this section, we recap the preliminaries in Graph Neural Networks and Transformer.</p><p><strong>Graph Neural Network (GNN).</strong> Let G = (V, E) denote a graph where V = {v1, v2, · · · , vn}, n = |V | is the number of nodes. Let the feature vector of node vi be xi . GNNs aim to learn representation of nodes and graphs. Typically, modern GNNs follow a learning schema that iteratively updates the representation of a node by aggregating representations of its first or higher-order neighbors. We denote h (l) i as the representation of vi at the l-th layer and define h (0) i = xi . The l-th iteration of aggregation could be characterized by AGGREGATE-COMBINE step as</p><p>where N (vi) is the set of first or higher-order neighbors of vi . The AGGREGATE function is used to gather the information from neighbors. Common aggregation functions include MEAN, MAX, SUM, which are used in different architectures of GNNs [26, 18, 50, 54]. The goal of COMBINE function is to fuse the information from neighbors into the node representation.</p><p>\\\nIn addition, for graph representation tasks, a READOUT function is designed to aggregate node features h (L) i of the final iteration into the representation hG of the entire graph G:</p><p>READOUT can be implemented by a simple permutation invariant function such as summation [54] or a more sophisticated graph-level pooling function [1].</p><p>. The Transformer architecture consists of a composition of Transformer layers [49]. Each Transformer layer has two parts: a self-attention module and a position-wise feed-forward network (FFN). Let H = h &gt; 1 , · · · , h&gt; n &gt; ∈ R n×d denote the input of self-attention module where d is the hidden dimension and hi ∈ R 1×d is the hidden representation at position i. The input H is projected by three matrices WQ ∈ R d×dK , WK ∈ R d×dK and WV ∈ R d×dV to the corresponding representations Q, K, V . The self-attention is then calculated as:</p><p>where  is a matrix capturing the similarity between queries and keys. For simplicity of illustration, we consider the single-head self-attention and assume  =  = . The extension to the multi-head attention is standard and straightforward, and we omit bias terms for simplicity.</p><p>In this section, we present our Graphormer for graph tasks. First, we elaborate on several key designs in the Graphormer, which serve as an inductive bias in the neural network to learn the graph representation. We further provide the detailed implementations of Graphormer. Finally, we show that our proposed Graphormer is more powerful since popular GNN models [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark43\">26</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark71\">54</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark35\">18</a>] are its special cases.</p><h2>3.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Structural Encodings in Graphormer</h2><p>As discussed in the introduction, it is important to develop ways to leverage the structural information of graphs into the Transformer model. To this end, we present three simple but effective designs of encoding in Graphormer. See Figure <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark1\">1</a> for an illustration.</p><h2>3.1.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Centrality Encoding</h2><p>In <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark3\">Eq.4,</a> the attention distribution is calculated based on the semantic correlation between nodes. However, node centrality, which measures how important a node is in the graph, is usually a strong signal for graph understanding. For example, celebrities who have a huge number of followers are important factors in predicting the trend of a social network [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark57\">40</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark56\">39</a>]. Such information is neglected in the current attention calculation, and we believe it should be a valuable signal for Transformer models.</p><p>In Graphormer, we use the degree centrality, which is one of the standard centrality measures in literature, as an additional signal to the neural network. To be specific, we develop a  which assigns each node two real-valued embedding vectors according to its indegree and outdegree. As the centrality encoding is applied to each node, we simply add it to the node features as the input.</p><p>where z −, z+ ∈ R d are learnable embedding vectors specified by the indegree deg−(vi) and outdegree deg+(vi) respectively. For undirected graphs, deg−(vi) and deg+(vi) could be unified to deg(vi). By using the centrality encoding in the input, the softmax attention can catch the node importance signal in the queries and the keys. Therefore the model can capture both the semantic correlation and the node importance in the attention mechanism.</p><h2>3.1.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Spatial Encoding</h2><p>An advantage of Transformer is its global receptive field. In each Transformer layer, each token can attend to the information at any position and then process its representation. But this operation has a byproduct problem that the model has to explicitly specify different positions or encode the positional dependency (such as locality) in the layers. For sequential data, one can either give each position an embedding (i.e., absolute positional encoding [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark65\">49</a>]) as the input or encode the relative distance of any two positions (i.e., relative positional encoding <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark62\">[45,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark64\">47])</a> in the Transformer layer.</p><p>However, for graphs, nodes are not arranged as a sequence. They can lie in a multi-dimensional spatial space and are linked by edges. To encode the structural information of a graph in the model, we propose a novel Spatial Encoding. Concretely, for any graph G, we consider a function φ (vi , vj ) : V × V → R which measures the spatial relation between vi and vj in graph G. The function φ can be defined by the connectivity between the nodes in the graph. In this paper, we choose φ(vi , vj ) to be the distance of the shortest path (SPD) between vi and vj if the two nodes are connected. If not, we set the output of φ to be a special value, i.e., -1. We assign each (feasible) output value a learnable scalar which will serve as a bias term in the self-attention module. Denote Aij as the (i, j)-element of the Query-Key product matrix A, we have:</p><p>where ( ) is a learnable scalar indexed by (), and shared across all layers.</p><p>Here we discuss several benefits of our proposed method. First, compared to conventional GNNs described in Section 2, where the receptive field is restricted to the neighbors, we can see that in Eq. <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark4\">(6)</a>, the Transformer layer provides a global information that each node can attend to all other nodes in the graph. Second, by using ( ), each node in a single Transformer layer can adaptively attend to all other nodes according to the graph structural information. For example, if ( ) is</p><p>learned to be a decreasing function with respect to (), for each node, the model will likely pay more attention to the nodes near it and pay less attention to the nodes far away from it.</p><h2>3.1.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Edge Encoding in the Attention</h2><p>In many graph tasks, edges also have structural features, e.g., in a molecular graph, atom pairs may have features describing the type of bond between them. Such features are important to the graph representation, and encoding them together with node features into the network is essential. There are mainly two edge encoding methods used in previous works. In the first method, the edge features are added to the associated nodes’ features [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark47\">30</a>]. In the second method, for each node, its associated edges’ features will be used together with the node features in the aggregation [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark32\">15</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark71\">54</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark43\">26</a>]. However, such ways of using edge feature only propagate the edge information to its associated nodes, which may not be an effective way to leverage edge information in representation of the whole graph.</p><p>To better encode edge features into attention layers, we propose a new edge encoding method in Graphormer. The attention mechanism needs to estimate correlations for each node pair (), and we believe the edges connecting them should be considered in the correlation as in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark51\">34</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark68\">51</a>]. For each ordered node pair (), we find (one of) the shortest path SP = (1*, e, …, eN* ) from  to , and compute an average of the dot-products of the edge feature and a learnable embedding along the path. The proposed edge encoding incorporates edge features via a bias term to the attention module. Concretely, we modify the ()-element of  in Eq. <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark2\">(3)</a> further with the edge encoding  as:</p><p>where xen is the feature of the n-th edge en in SPij , w E n ∈ R dE is the n-th weight embedding, and dE is the dimensionality of edge feature.</p><h2>3.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Implementation Details of Graphormer</h2><p> Graphormer is built upon the original implementation of classic Transformer encoder described in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark65\">49</a>]. In addition, we apply the layer normalization (LN) before the multi-head self-attention (MHA) and the feed-forward blocks (FFN) instead of after [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark70\">53</a>]. This modification has been unanimously adopted by all current Transformer implementations because it leads to more effective optimization [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark59\">43</a>]. Especially, for FFN sub-layer, we set the dimensionality of input, output, and the inner-layer to the same dimension with . We formally characterize the Graphormer layer as below:</p><p> As stated in the previous section, various graph pooling functions are proposed to represent the graph embedding. Inspired by [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark32\">15</a>], in Graphormer, we add a special node called [VNode] to the graph, and make connection between [VNode] and each node individually. In the AGGREGATE-COMBINE step, the representation of [VNode] has been updated as normal nodes in graph, and the representation of the entire graph  would be the node feature of [VNode] in the final layer. In the BERT model [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark28\">11</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark52\">35</a>], there is a similar token, i.e., [CLS], which is a special token attached at the beginning of each sequence, to represent the sequence-level feature on downstream tasks. While the [VNode] is connected to all other nodes in graph, which means the distance of the shortest path is 1 for any ([VNode]) and ( [VNode]), the connection is not physical. To distinguish the connection of physical and virtual, inspired by [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark41\">25</a>], we reset all spatial encodings for ([VNode] ) and ([VNode]) to a distinct learnable scalar.</p><h2>3.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; How Powerful is Graphormer?</h2><p>In the previous subsections, we introduce three structural encodings and the architecture of Graphormer. Then a natural question is: <em>Do these modifications make Graphormer more powerful than other GNN variants?</em> In this subsection, we first give an affirmative answer by showing that Graphormer can represent the AGGREGATE and COMBINE steps in popular GNN models:</p><p><em>By choosing proper weights and distance function φ, the Graphormer layer can represent AGGREGATE and COMBINE steps of popular GNN models such as GIN, GCN, GraphSAGE.</em></p><p>The proof sketch to derive this result is: 1) Spatial encoding enables self-attention module to distinguish neighbor set N (vi) of node vi so that the softmax function can calculate mean statistics over N (vi); 2) Knowing the degree of a node, mean over neighbors can be translated to sum over neighbors; 3) With multiple heads and FFN, representations of vi and N (vi) can be processed separately and combined together later. We defer the proof of this fact to Appendix A.</p><p>Moreover, we show further that by using our spatial encoding, Graphormer can go beyond classic message passing GNNs whose expressive power is no more than the 1-Weisfeiler-Lehman (WL) test. We give a concrete example in Appendix A to show how Graphormer helps distinguish graphs that the 1-WL test fails to.</p><p><strong>Connection between Self-attention and Virtual Node.</strong> Besides the superior expressiveness than popular GNNs, we also find an interesting connection between using self-attention and the virtual node heuristic [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark32\">15</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark48\">31</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark42\">24</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>]. As shown in the leaderboard of OGB [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>], the virtual node trick, which augments graphs with additional supernodes that are connected to all nodes in the original graphs, can significantly improve the performance of existing GNNs. Conceptually, the benefit of the virtual node is that it can aggregate the information of the  (like the READOUT function) and then propagate it to . However, a naive addition of a supernode to a graph can potentially lead to inadvertent over-smoothing of information propagation [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark42\">24</a>]. We instead find that such a graph-level aggregation and propagation operation can be naturally fulfilled by vanilla self-attention without additional encodings. Concretely, we can prove the following fact:</p><p><em>By choosing proper weights, every node representation of the output of a Graphormer layer without additional encodings can represent MEAN READOUT functions.</em></p><p>This fact takes the advantage of self-attention that each node can attend to all other nodes. Thus it can simulate graph-level READOUT operation to aggregate information from the whole graph. Besides the theoretical justification, we empirically find that Graphormer does not encounter the problem of over-smoothing, which makes the improvement scalable. The fact also inspires us to introduce a special node for graph readout (see the previous subsection).</p><p>We first conduct experiments on the recent OGB-LSC [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark38\">21</a>] quantum chemistry regression (i.e., PCQM4M-LSC) challenge, which is currently the biggest graph-level prediction dataset and contains more than 3.8M graphs in total. Then, we report the results on the other three popular tasks: ogbg-molhiv, ogbg-molpcba and ZINC, which come from the OGB [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>] and benchmarking-GNN [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark31\">14</a>] leaderboards. Finally, we ablate the important design elements of Graphormer. A detailed description of datasets and training strategies could be found in Appendix B.</p><h2>4.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OGB Large-Scale Challenge</h2><p> We benchmark the proposed Graphormer with GCN [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark43\">26</a>] and GIN [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark71\">54</a>], and their variants with virtual node (-VN) [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark32\">15</a>]. They achieve the state-of-the-art valid and test mean absolute error (MAE) on the official leaderboard<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark10\">4</a> [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark38\">21</a>]. In addition, we compare to GIN’s multi-hop variant [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark21\">5</a>], and 12-layer deep graph network DeeperGCN [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark47\">30</a>], which also show promising performance on other leaderboards. We further compare our Graphormer with the recent Transformer-based graph model GT <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">[13].</a></p><p>\\\n&nbsp; We primarily report results on two model sizes:  ( = 12*, d* = 768), and a smaller one  ( = 6*, d* = 512). Both the number of attention heads in the attention module and the dimensionality of edge features  are set to 32. We use AdamW as the optimizer, and set the hyper-parameter  to 1e-8 and (1*, β*2) to (0.99,0.999). The peak learning rate is set to 2e-4 (3e-4 for ) with a 60k-step warm-up stage followed by a linear decay learning rate scheduler. The total training steps are 1M. The batch size is set to 1024. All models are trained on 8 NVIDIA V100 GPUS for about 2 days.</p><p>\\\n Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark11\">1</a> summarizes performance comparisons on PCQM4M-LSC dataset. From the table, GIN-VN achieves the previous state-of-the-art validate MAE of 0.1395. The original implementation of GT [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>] employs a hidden dimension of 64 to reduce the total number of parameters. For a fair comparison, we also report the result by enlarging the hidden dimension to 768, denoted by GT-Wide, which leads to a total number of parameters of 83.2M. While, both GT and GT-Wide do not outperform GIN-VN and DeeperGCN-VN. Especially, we do not observe a performance gain along with the growth of parameters of GT.</p><p>Compared to the previous state-of-the-art GNN architecture, Graphormer noticeably surpasses GIN-VN by a large margin, e.g., 11.5% relative validate MAE decline. By using the ensemble with ExpC [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark72\">55</a>], we got a 0.1200 MAE on complete test set and won the first place of the graph-level track in OGB Large-Scale Challenge[<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark38\">21</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark75\">58</a>]. As stated in Section <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark7\">3.3,</a> we further find that the proposed Graphormer does not encounter the problem of over-smoothing, i.e., the train and validate error keep going down along with the growth of depth and width of models.</p><h2>4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Graph Representation</h2><p>In this section, we further investigate the performance of Graphormer on commonly used graph-level prediction tasks of popular leaderboards, i.e., OGB [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>] (OGBG-MolPCBA, OGBG-MolHIV), and benchmarking-GNN [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark31\">14</a>] (ZINC). Since pre-training is encouraged by OGB, we mainly explore the transferable capability of a Graphormer model pre-trained on OGB-LSC (i.e., PCQM4M-LSC). Please note that the model configurations, hyper-parameters, and the pre-training performance of pre-trained Graphormers used for MolPCBA and MolHIV are different from the models used in the previous subsection. Please refer to Appendix B for detailed descriptions. For benchmarking-GNN, which does not encourage large pre-trained model, we train an additional GraphormerSLIM ( = 12*, d* = 80, total param.= 489) from scratch on ZINC.</p><p> We report performance of GNNs which achieve top-performance on the official leader-boards<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark12\">5</a><em>without additional domain-specific features</em>. Considering that the pre-trained Graphormer leverages external data, for a fair comparison on OGB datasets, we additionally report performance for fine-tuning GIN-VN pre-trained on PCQM4M-LSC dataset, which achieves the previous state-of-the-art valid and test MAE on that dataset.</p><p>\\\n We report detailed training strategies in Appendix B. In addition, Graphormer is more easily trapped in the over-fitting problem due to the large size of the model and the small size of the dataset. Therefore, we employ a widely used data augmentation for graph - FLAG [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark44\">27</a>], to mitigate the over-fitting problem on OGB datasets.</p><p>\\\n Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark13\">2,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark14\">3</a> and <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark15\">4</a> summarize performance of Graphormer comparing with other GNNs on MolHIV, MolPCBA and ZINC datasets. Especially, GT [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>] and SAN [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark45\">28</a>] in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark15\">4</a> are recently proposed Transformer-based GNN models. Graphormer consistently and significantly outperforms previous state-of-the-art GNNs on all three datasets by a large margin. Specially, except Graphormer,  the other pre-trained GNNs do not achieve competitive performance, which is in line with previous literature [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark37\">20</a>]. In addition, we conduct more comparisons to fine-tuning the pre-trained GNNs, please refer to Appendix C.</p><h2>4.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ablation Studies</h2><p>We perform a series of ablation studies on the importance of designs in our proposed Graphormer, on PCQM4M-LSC dataset. The ablation results are included in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark16\">5.</a> To save the computation resources, the Transformer models in table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark16\">5</a> have 12 layers, and are trained for 100K iterations.</p><p>\\\n We compare previously used positional encoding (PE) to our proposed spatial encoding, which both aim to encode the information of distinct node relation to Transformers. There are various PEs employed by previous Transformer-based GNNs, e.g., Weisfeiler-Lehman-PE (WL-PE) [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark78\">61</a>] and Laplacian PE [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark20\">3</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark31\">14</a>]. We report the performance for Laplacian PE since it performs well comparing to a series of PEs for Graph Transformer in previous literature [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>]. Transformer architecture with the spatial encoding outperforms the counterpart built on the positional encoding, which demonstrates the effectiveness of using spatial encoding to capture the node spatial information.</p><p>\\\n Transformer architecture with degree-based centrality encoding yields a large margin performance boost in comparison to those without centrality information. This indicates that the centrality encoding is indispensable to Transformer architecture for modeling graph data.</p><p>\\\n We compare our proposed edge encoding (denoted as via attn bias) to two commonly used edge encodings described in Section <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark5\">3.1.3</a> to incorporate edge features into GNN, denoted as via node and via Aggr in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark16\">5.</a> From the table, the gap of performance is minor between the two conventional methods, but our proposed edge encoding performs significantly better, which indicates that edge encoding as attention bias is more effective for Transformer to capture spatial information on edges.</p><p>In this section, we highlight the most recent works which attempt to develop standard Transformer architecture-based GNN or graph structural encoding, but spend less effort on elaborating the works by adapting attention mechanism to GNNs <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark50\">[33,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark77\">60,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark24\">7,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark40\">23,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark18\">1,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark67\">50,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark68\">51,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark78\">61,</a><a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark66\">48].</a></p><p>There are several works that study the performance of pure Transformer architectures (stacked by transformer layers) with modifications on graph representation tasks, which are more related to our Graphormer. For example, several parts of the transformer layer are modified in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark63\">46</a>], including an additional GNN employed in attention sub-layer to produce vectors of , , and  , long-range residual connection, and two branches of FFN to produce node and edge representations separately. They pre-train their model on 10 million unlabelled molecules and achieve excellent results by fine-tuning on downstream tasks. Attention module is modified to a soft adjacency matrix in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark58\">41</a>] by directly adding the adjacency matrix and RDKit<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark17\">6</a>-computed inter-atomic distance matrix to the attention probabilites. Very recently, Dwivedi  [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>] revisit a series of works for Transformer-based GNNs, and suggest that the attention mechanism in Transformers on graph data should only aggregate the information from neighborhood (i.e., using adjacent matrix as attention mask) to ensure graph sparsity, and propose to use Laplacian eigenvector as positional encoding. Their model GT surpasses baseline GNNs on graph representation task. A concurrent work [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark45\">28</a>] propose a novel full Laplacian spectrum to learn the position of each node in a graph, and empirically shows better results than GT.</p><h2>5.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Structural Encodings in GNNs</h2><p><strong>Path and Distance in GNNs.</strong> Information of path and distance is commonly used in GNNs. For example, an attention-based aggregation is proposed in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark26\">9</a>] where the node features, edge features, one-hot feature of the distance and ring flag feature are concatenated to calculate the attention probabilites; similar to <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark26\">[9],</a> path-based attention is leveraged in <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark73\">[56]</a> to model the influence between the center node and its higher-order neighbors; a distance-weighted aggregation scheme on graph is proposed in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark76\">59</a>]; it has been proved in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark49\">32</a>] that adopting distance encoding (i.e., one-hot feature of the distance as extra node attribute) could lead to a strictly more expressive power than the 1-WL test.</p><p>\\\n<strong>Positional Encoding in Transformer on Graph.</strong> Several works introduce positional encoding (PE) to Transformer-based GNNs to help the model capture the node position information. For example, Graph-BERT [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark78\">61</a>] introduces three types of PE to embed the node position information to model, i.e., an absolute WL-PE which represents different nodes labeled by Weisfeiler-Lehman algorithm, an intimacy based PE and a hop based PE which are both variant to the sampled subgraphs. Absolute Laplacian PE is employed in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>] and empircal study shows that its performance surpasses the absolute WL-PE used in <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark78\">[61].</a></p><p>\\\n Except the conventionally used methods to encode edge feature, which are described in previous section, there are several attempts that exploit how to better encode edge features: an attention-based GNN layer is developed in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark33\">16</a>] to encode edge features, where the edge feature is weighted by the similarity of the features of its two nodes; edge feature has been encoded into the popular GIN [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark71\">54</a>] in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark21\">5</a>]; in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>], the authors propose to project edge features to an embedding vector, then multiply it by attention coefficients, and send the result to an additional FFN sub-layer to produce edge representations;</p><p>We have explored the direct application of Transformers to graph representation. With three novel graph structural encodings, the proposed Graphormer works surprisingly well on a wide range of popular benchmark datasets. While these initial results are encouraging, many challenges remain. For example, the quadratic complexity of the self-attention module restricts Graphormer’s application on large graphs. Therefore, future development of efficient Graphormer is necessary. Performance improvement could be expected by leveraging domain knowledge-powered encodings on particular graph datasets. Finally, an applicable graph sampling strategy is desired for node representation extraction with Graphormer. We leave them for future works.</p><p>We would like to thank Mingqi Yang and Shanda Li for insightful discussions.</p><p>[1]&nbsp;&nbsp;&nbsp; Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with graph multiset pooling. , 2021.</p><p>[2]&nbsp;&nbsp;&nbsp; Dominique Beaini, Saro Passaro, Vincent Létourneau, William L Hamilton, Gabriele Corso, and Pietro Liò. Directional graph networks. In <em>International Conference on Machine Learning</em>, 2021.</p><p>[3]&nbsp;&nbsp;&nbsp; Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representa-tion. , 15(6):1373–1396, 2003.</p><p>[4]&nbsp;&nbsp;&nbsp; Xavier Bresson and Thomas Laurent. Residual gated graph convnets. <em>arXiv preprint arXiv:1711.07553</em>, 2017.</p><p>[5]&nbsp;&nbsp;&nbsp; Rémy Brossard, Oriel Frigo, and David Dehaene. Graph convolutions that can finally model local structure.</p><p><em>arXiv preprint arXiv:2011.15069</em>, 2020.</p><p>[6]&nbsp;&nbsp;  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, <em>Advances in Neural Information Processing Systems</em>, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.</p><p>[7]&nbsp;&nbsp;  Deng Cai and Wai Lam. Graph transformer for graph-to-sequence learning. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 34, pages 7464–7471, 2020.</p><p>[8]&nbsp;&nbsp;&nbsp; Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In <em>International Conference on Machine Learning</em>, 2021.</p><p>[9]&nbsp;&nbsp;&nbsp; Benson Chen, Regina Barzilay, and Tommi Jaakkola. Path-augmented graph transformer network. <em>arXiv preprint arXiv:1905.12712</em>, 2019.</p><p>[10]&nbsp;&nbsp;&nbsp; Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Velicˇkovic´. Principal neighbour-hood aggregation for graph nets. <em>Advances in Neural Information Processing Systems</em>, 33, 2020.</p><p>[11]&nbsp;&nbsp;  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi-rectional transformers for language understanding. In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, 2019.</p><p>[12]&nbsp;&nbsp;  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. <em>arXiv preprint arXiv:2010.11929</em>, 2020.</p><p>[13]&nbsp;&nbsp;  Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. <em>AAAI Workshop on Deep Learning on Graphs: Methods and Applications</em>, 2021.</p><p>[14]&nbsp;&nbsp;&nbsp; Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Bench-marking graph neural networks. <em>arXiv preprint arXiv:2003.00982</em>, 2020.</p><p>[15]&nbsp;&nbsp;&nbsp; Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In <em>International Conference on Machine Learning</em>, pages 1263–1272. PMLR, 2017.</p><p>[16]&nbsp;&nbsp;&nbsp; Liyu Gong and Qiang Cheng. Exploiting edge features for graph neural networks. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 9211–9219, 2019.</p><p>[17]&nbsp;&nbsp;&nbsp; Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. <em>arXiv preprint arXiv:2005.08100</em>, 2020.</p><p>[18]&nbsp;&nbsp;&nbsp; William L Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In , 2017.</p><p>[19]&nbsp;&nbsp;&nbsp; Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global relational models of source code. In <em>International conference on learning representations</em>, 2019.</p><p>[20]&nbsp;&nbsp;&nbsp; W Hu, B Liu, J Gomes, M Zitnik, P Liang, V Pande, and J Leskovec. Strategies for pre-training graph neural networks. In <em>International Conference on Learning Representations (ICLR)</em>, 2020.</p><p>[21]&nbsp;&nbsp;&nbsp; Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. <em>arXiv preprint arXiv:2103.09430</em>, 2021.</p><p>[22]&nbsp;&nbsp;&nbsp; Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. <em>arXiv preprint arXiv:2005.00687</em>, 2020.</p><p>[23]&nbsp;&nbsp;&nbsp; Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In <em>Proceedings of The Web Conference 2020</em>, pages 2704–2710, 2020.</p><p>[24]&nbsp;&nbsp;&nbsp; Katsuhiko Ishiguro, Shin-ichi Maeda, and Masanori Koyama. Graph warp module: an auxiliary module for boosting the power of graph neural networks in molecular graph analysis. <em>arXiv preprint arXiv:1902.01020</em>, 2019.</p><p>[25]&nbsp;&nbsp;&nbsp; Guolin Ke, Di He, and Tie-Yan Liu. Rethinking the positional encoding in language pre-training. , 2020.</p><p>[26]&nbsp;&nbsp;&nbsp; Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.</p><p><em>arXiv preprint arXiv:1609.02907</em>, 2016.</p><p>[27]&nbsp;&nbsp;&nbsp; Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. Flag: Adversarial data augmentation for graph neural networks. <em>arXiv preprint arXiv:2010.09891</em>, 2020.</p><p>[28]&nbsp;&nbsp;&nbsp; Devin Kreuzer, Dominique Beaini, William Hamilton, Vincent Létourneau, and Prudencio Tossou. Re-thinking graph transformers with spectral attention. <em>arXiv preprint arXiv:2106.03893</em>, 2021.</p><p>[29]&nbsp;&nbsp;&nbsp; Tuan Le, Marco Bertolini, Frank Noé, and Djork-Arné Clevert. Parameterized hypercomplex graph neural networks for graph classification. <em>arXiv preprint arXiv:2103.16584</em>, 2021.</p><p>[30]&nbsp;&nbsp;&nbsp; Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns. <em>arXiv preprint arXiv:2006.07739</em>, 2020.</p><p>[31]&nbsp;&nbsp;&nbsp; Junying Li, Deng Cai, and Xiaofei He. Learning graph-level representation for drug discovery. <em>arXiv preprint arXiv:1709.03741</em>, 2017.</p><p>[32]&nbsp;&nbsp;  Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. <em>Advances in Neural Information Processing Systems</em>, 33, 2020.</p><p>[33]&nbsp;&nbsp;&nbsp; Yuan Li, Xiaodan Liang, Zhiting Hu, Yinbo Chen, and Eric P. Xing. Graph transformer, 2019.</p><p>[34]&nbsp;&nbsp;&nbsp; Xi Victoria Lin, Richard Socher, and Caiming Xiong. Multi-hop knowledge graph reasoning with reward shaping. <em>arXiv preprint arXiv:1808.10568</em>, 2018.</p><p>[35]&nbsp;&nbsp;&nbsp; Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. <em>arXiv preprint arXiv:1907.11692</em>, 2019.</p><p>[36]&nbsp;&nbsp;&nbsp; Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. <em>arXiv preprint arXiv:2103.14030</em>, 2021.</p><p>[37]&nbsp;&nbsp;  Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. , 2021.</p><p>[38]&nbsp;&nbsp;&nbsp; Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 32. Curran Associates, Inc., 2019.</p><p>[39]&nbsp;&nbsp;&nbsp; P David Marshall. The promotion and presentation of the self: celebrity as marker of presentational media.</p><p>, 1(1):35–48, 2010.</p><p>[40]&nbsp;&nbsp;  Alice Marwick and Danah Boyd. To see and be seen: Celebrity practice on twitter. , 17(2):139–158, 2011.</p><p>[41]&nbsp;&nbsp;&nbsp; Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrze˛bski. Molecule attention transformer. <em>arXiv preprint arXiv:2002.08264</em>, 2020.</p><p>[42]&nbsp;&nbsp;&nbsp; Maho Nakata and Tomomi Shimazaki. Pubchemqc project: a large-scale first-principles electronic structure database for data-driven chemistry. <em>Journal of chemical information and modeling</em>, 57(6):1300–1308, 2017.</p><p>[43]&nbsp;&nbsp;&nbsp; Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? <em>arXiv preprint arXiv:2102.11972</em>, 2021.</p><p>[44]&nbsp;&nbsp;&nbsp; Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. How could neural networks understand programs? In <em>International Conference on Machine Learning</em>. PMLR, 2021.</p><p>[45]&nbsp;&nbsp;  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. <em>Journal of Machine Learning Research</em>, 21(140):1–67, 2020.</p><p>[46]&nbsp;&nbsp;  Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. <em>Advances in Neural Information Processing Systems</em>, 33, 2020.</p><p>[47]&nbsp;&nbsp;  Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 464–468, 2018.</p><p>[48]&nbsp;&nbsp;&nbsp; Yunsheng Shi, Zhengjie Huang, Wenjin Wang, Hui Zhong, Shikun Feng, and Yu Sun. Masked label predic-tion: Unified message passing model for semi-supervised classification. <em>arXiv preprint arXiv:2009.03509</em>, 2020.</p><p>[49]&nbsp;&nbsp;&nbsp; Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In , 2017.</p><p>[50]&nbsp;&nbsp;&nbsp; Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. , 2018.</p><p>[51]&nbsp;&nbsp;&nbsp; Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Direct multi-hop attention based graph neural network. <em>arXiv preprint arXiv:2009.14332</em>, 2020.</p><p>[52]&nbsp;&nbsp;&nbsp; Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. <em>arXiv preprint arXiv:2006.04768</em>, 2020.</p><p>[53]&nbsp;&nbsp;  Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In <em>International Conference on Machine Learning</em>, pages 10524–10533. PMLR, 2020.</p><p>[54]&nbsp;&nbsp;&nbsp; Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In <em>International Conference on Learning Representations</em>, 2019.</p><p>[55]&nbsp;&nbsp;&nbsp; Mingqi Yang, Yanming Shen, Heng Qi, and Baocai Yin. Breaking the expressive bottlenecks of graph neural networks. <em>arXiv preprint arXiv:2012.07219</em>, 2020.</p><p>[56]&nbsp;&nbsp;&nbsp; Yiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, and Dacheng Tao. Spagan: Shortest path graph attention network. , 2019.</p><p>[57]&nbsp;&nbsp;&nbsp; Chengxuan Ying, Guolin Ke, Di He, and Tie-Yan Liu. Lazyformer: Self attention with lazy update. <em>arXiv preprint arXiv:2102.12702</em>, 2021.</p><p>[58]&nbsp;&nbsp;  Chengxuan Ying, Mingqi Yang, Shuxin Zheng, Guolin Ke, Shengjie Luo, Tianle Cai, Chenglin Wu, Yuxin Wang, Yanming Shen, and Di He. First place solution of kdd cup 2021 &amp; ogb large-scale challenge graph-level track. <em>arXiv preprint arXiv:2106.08279</em>, 2021.</p><p>[59]&nbsp;&nbsp;  Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In <em>International Conference on Machine Learning</em>, pages 7134–7143. PMLR, 2019.</p><p>[60]&nbsp;&nbsp;&nbsp; Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. <em>Advances in Neural Information Processing Systems</em>, 32, 2019.</p><p>[61]&nbsp;&nbsp;&nbsp; Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-bert: Only attention is needed for learning graph representations. <em>arXiv preprint arXiv:2001.05140</em>, 2020.</p><p>[62]&nbsp;&nbsp;&nbsp; Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. In , 2020.</p><p>[63]&nbsp;&nbsp;  Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann. Language-agnostic representation learning of source code from structure and context. In <em>International Conference on Learning Representations</em>, 2020.</p><h2>A.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SPD can Be Used to Improve WL-Test</h2><p>\\\n1-WL-test fails in many cases [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark54\">38</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark49\">32</a>], thus classic message passing GNNs also fail to distinguish many pairs of graphs. We show that SPD might help when 1-WL-test fails, for example, in Figure <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark81\">2</a> where 1-WL-test fails, the sets of SPD from all nodes to others successfully distinguish the two graphs.</p><p> We begin by showing that self-attention module with Spatial Encoding can repre-sent MEAN aggregation. This is achieved by in Eq. <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark4\">(6)</a>: 1) setting  = 0 if  = 1 and  =  otherwise where  is the SPD; 2) setting  =  = 0 and  to be the identity matrix. Then softmax ()  gives the average of representations of the neighbors.</p><p>\\\n The SUM aggregation can be realized by first perform MEAN aggregation and then multiply the node degrees. Specifically, the node degrees can be extracted from Centrality Encoding by an additional head and be concatenated to the representations after MEAN aggregation. Then the FFN module in Graphormer can represent the function of multiplying the degree to the dimensions of averaged representations by the universal approximation theorem of FFN.</p><p>\\\n Representing the MAX aggregation is harder than MEAN and SUM. For each dimension  of the representation vector, we need one head to select the maximal value over -th dimension in the neighbor by in Eq. <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark4\">(6)</a>: 1) setting  = 0 if  = 1 and  =  otherwise where  is the SPD; 2) setting  =  which is the -th standard basis;  = 0 and the bias term (which is ignored in the previous description for simplicity) of  to be ; and  = , where  is the temperature that can be chosen to be large enough so that the softmax function can approximate hard max and  is the vector whose elements are all 1.</p><p>\\\n The COMBINE step takes the result of AGGREGATE and the previous representation of current node as input. This can be achieved by the AGGREGATE operations described above together with an additional head which outputs the features of present nodes, i.e., in Eq. <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark4\">(6)</a>: 1) setting  = 0 if  = 0 and  =  otherwise where  is the SPD; 2) setting  =  = 0 and  to be the identity matrix. Then the FFN module can approximate any COMBINE function by the universal approximation theorem of FFN.</p><p> This can be proved by setting  =  = 0, the bias terms of  to be , and  to be the identity matrix where  should be much larger than the scale of  so that  2T dominates the Spatial Encoding term.</p><h2>B.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Details of Datasets</h2><p>We summarize the datasets used in this work in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark82\">6.</a> PCQM4m-LSC is a quantum chemistry graph-level prediction task in recent OGB Large-Scale Challenge, originally curated under the PubChemQC project [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark60\">42</a>].</p><p>\\\nThe task of PCQM4M-LSC is to predict DFT(density functional theory)-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs, which is one of the most practically-relevant quantum chemical properties of molecule science. PCQM4M-LSC is unprecedentedly large in scale comparing to other labeled graph-level prediction datasets, which contains more than 3.8M graphs. Besides, we conduct experiments on two molecular graph datasets in popular OGB leaderboards, i.e., OGBG-MolPCBA and OGBG-MolHIV. They are two molecular property prediction datasets with different sizes. The pre-trained knowledge of molecular graph on PCQM4M-LSC could be easily leveraged on these two datasets. We adopt official scaffold split on three datasets following [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark38\">21</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>]. In addition, we employ another popular leaderboard, i.e., benchmarking-gnn [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark31\">14</a>]. We use the ZINC datasets, which is the most popular real-world molecular dataset to predict graph property regression for contrained solubility, an important chemical property for designing generative GNNs for molecules. Different from the scaffold spliting in OGB, uniform sampling is adopted in ZINC for data splitting.</p><h2>B.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Details of Training Strategies</h2><p>\\\nWe report the detailed hyper-parameter settings used for training Graphormer in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark83\">7.</a> We reduce the FFN inner-layer dimension of 4 in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark65\">49</a>] to , which does not appreciably hurt the performance but significantly save the parameters. The embedding dropout ratio is set to 0.1 by default in many previous Transformer works [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark28\">11</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark52\">35</a>]. However, we empirically find that a small embedding dropout ratio (e.g., 0.1) would lead to an observable performance drop on validation set of PCQM4M-LSC. One possible reason is that the molecular graph is relative small (i.e., the median of #atoms in each molecule is about 15), making graph property more sensitive to the embeddings of each node. Therefore, we set embedding dropout ratio to 0 on this dataset.</p><p> We first report the model configurations and hyper-parameters of the pre-trained Graphormer on PCQM4M-LSC. Empirically, we find that the performance on MolPCBA benefits from the large pre-training model size. Therefore, we train a deep Graphormer with 18 Transformer layers on PCQM4M-LSC. The hidden dimension and FFN inner-layer dimension are set to 1024. We set peak learning rate to 1e-4 for the deep</p><p>Graphormer. Besides, we enlarge the attention dropout ratio from 0.1 to 0.3 in both pre-training and fine-tuning to prevent the model from over-fitting. The rest of hyper-parameters remain unchanged. The pre-trained Graphormer used for MolPCBA achieves a valid MAE of 0.1253 on PCQM4M-LSC, which is slightly worse than the reports in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark11\">1.</a></p><p>\\\n Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark84\">8</a> summarizes the hyper-parameters used for fine-tuning Graphormer on OGBG-MolPCBA. We conduct a grid search for several hyper-parameters to find the optimal configuration. The experimental results are reported by the mean of 10 independent runs with random seeds. We use FLAG [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark44\">27</a>] with minor modifications for graph data augmentation. In particular, except the step size  and the number of steps , we also employ a projection step in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark79\">62</a>] with maximum perturbation . The performance of Graphormer on MolPCBA is quite robust to the hyper-parameters of FLAG. The rest of hyper-parameters are the same with the pre-training model.</p><p>\\\n We use the Graphormer reported in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark11\">1</a> as the pre-trained model for OGBG-MolHIV, where the pre-training hyper-parameters are summarized in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark83\">7.</a></p><p>\\\n&nbsp; The hyper-parameters for fine-tuning Graphormer on OGBG-MolHIV are presented in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark85\">9.</a> Empirically, we find that the different choices of hyper-parameters of FLAG (i.e., step size , number of steps , and maximum perturbation ) would greatly affect the performance of Graphormer on OGBG-MolHiv. Therefore, we spend more effort to conduct grid search for hyper-parameters of FLAG. We report the best hyper-parameters by the mean of 10 independent runs with random seeds.</p><p>To keep the total parameters of Graphormer less than 500K per the request from benchmarking-GNN leader-board [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark31\">14</a>], we train a slim 12-layer Graphormer with hidden dimension of 80, which is called GraphormerSLIM in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark15\">4,</a> and has about 489K learnable parameters. The number of attention heads is set to 8. Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark86\">10</a> summarizes the detailed hyper-parameters on ZINC. We train 400K steps on this dataset, and employ a weight decay of 0.01.</p><h2>B.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Details of Hyper-parameters for Baseline Methods</h2><p>In this section, we present the details of our re-implementation of the baseline methods.</p><p>The official Github repository of OGB-LSC<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark88\">7</a> provides hyper-parameters and codes to reproduce the results on leaderboard. These hyper-parameters work well on almost all popular GNN variants, except the DeeperGCN-VN, which results in a training divergence. Therefore, for DeeperGCN-VN, we follow the official hyper-parameter setting<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark89\">8</a> provided by the authors [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark47\">30</a>]. For a fair comparison to Graphormer, we train a 12-layer DeeperGCN. The hidden dimension is set to 600. The batch size is set to 256. The learning rate is set to 1e-3, and a step learning rate scheduler is employed with the decaying step size and the decaying factor  as 30 epochs and 0.25. The model is trained for 100 epochs.</p><p>The default dimension of laplacian PE of GT [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>] is set to 8. However, it will cause 2.91% small molecules (less than 8 atoms) to be filtered out. Therefore, for GT and GT-Wide, we set the dimension of laplacian PE to 4, which results in only 0.08% filtering out. We adopt the default hyper-parameter settings described in [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark30\">13</a>], except that we decrease the learning rate to 1e-4, which leads to a better convergence on PCQM4M-LSC.</p><p>To fine-tune the pre-trained GIN-VN on MolPCBA, we follow the hyper-parameter settings provided in the original OGB paper [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>]. To be more concrete, we load the pre-trained checkpoint reported in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark11\">1</a> and fine-tune it on OGBG-MolPCBA dataset. We use the grid search on the hyper-parameters for better fine-tuning performance. In particular, the learning rate is selected from {1e − 5, 1e − 4, 1e − 3}; the dropout ratio is selected from {0.0, 0.1, 0.5}; the batch size is selected from {32, 64}.</p><p>Similarly, we fine-tune the pre-trained GIN-VN on MolHIV by following the hyper-parameter settings provided in the original OGB paper [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark39\">22</a>]. We also conduct the grid search to look for optimal hyper-parameters. The ranges for each hyper-parameter of grid search are the same as the previous subsection.</p><h2>C&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; More Experiments</h2><p>As described in the related work, GROVER is a Transformer-based GNN, which has 100 million parameters and pre-trained on 10 million unlabelled molecules using 250 Nvidia V100 GPUs. In this section, we report the fine-tuning scores of GROVER on MolHIV and MolPCBA, and compare with proposed Graphormer.</p><p>We download the pre-trained GROVER models from its official Github webpage<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark92\">9</a>, follow the official instruc-tions<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark93\">10</a> and fine-tune the provided pre-trained checkpoints with careful search of hyper-parameters (in Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark87\">11).</a> We find that GROVER could achieve competitive performance on MolHIV only if employing additional molecular features, i.e., morgan molecular finger prints and 2D features<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark94\">11</a>. Therefore, we report the scores of GROVER by taking these two additional molecular features. Please note that, from the leaderboard<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark95\">12</a>, we can know such additional molecular features are very effective on MolHIV dataset.</p><p>Table <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark90\">12</a>&nbsp;and <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark91\">13</a>&nbsp;summarize the performance of GROVER and GROVERLARGE comparing with Graphormer on MolHIV and MolPCBA. From the tables, we observe that Graphormer could consistently outperform GROVER even without any additional molecular features.</p><h2>D&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Discussion &amp; Future Work</h2><p> Similar to regular Transformer, the attention mechanism in Graphormer scales quadratically with the number of nodes  in the input graph, which may be prohibitively expensive for large  and precludes its usage in settings with limited computational resources. Recently, many solutions have been proposed to address this problem in Transformer [<a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark41\">25</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark69\">52</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark74\">57</a>, <a href=\"https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss#_bookmark55\">37</a>]. This issue would be greatly benefit from the future development of efficient Graphormer.</p><p>\\\n. In Graphormer, there are multiple choices for the network centrality and the spatial encoding function ( ). For example, one can leverage the 2 distance in 3D structure between two atoms in a molecule. In this paper, we mainly evaluate general centrality and distance metric in graph theory, i.e., the degree centrality and the shortest path. Performance improvement could be expected by leveraging domain knowledge powered encodings on particular graph dataset.</p><p>\\\n There is a wide range of node representation tasks on graph structured data, such as finance, social network, and temporal prediction. Graphormer could be naturally used for node representation extraction with an applicable graph sampling strategy. We leave it for future work.</p><p>:::info\nThis paper is&nbsp;<a href=\"https://arxiv.org/abs/2106.05234\">available on arxiv</a>&nbsp;under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":54662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top 3 Crypto Presales to Buy in 2026: Pepeto vs BlockDAG vs Mutuum Finance Before the Next Bull Run","url":"https://hackernoon.com/top-3-crypto-presales-to-buy-in-2026-pepeto-vs-blockdag-vs-mutuum-finance-before-the-next-bull-run?source=rss","date":1772287570,"author":"Tokenwire","guid":155004,"unread":true,"content":"<p>Every crypto bull run created a new wave of millionaires. But those millionaires did not buy after the rally started. They bought during the fear. They bought presales at fractions of a cent while everyone else waited for confirmation. By the time confirmation arrived, the 50x windows had already closed.</p><p>A presale is when a project sells tokens before they list on exchanges. The price is locked far below what it trades for on launch day. Early SHIB investors turned $1,000 into over $1 million. PEPE made early holders rich in weeks. The pattern repeats every cycle. Three projects stand out right now. Pepeto, BlockDAG, and Mutuum Finance each offer presale access. But the upside between them is not close.</p><h3>Pepeto: The One That Goes Viral</h3><p>Most presales sell you a concept. Pepeto is selling you the infrastructure for the entire meme coin economy. That changes everything. Instead of betting on one token to pump, you are buying the trading layer that profits no matter which meme coin takes off next.</p><p>PepetoSwap is a zero tax cross chain swap announced by the team and close to being ready. The Pepeto Bridge moves tokens between blockchains. The Pepeto Exchange is a meme coin listing hub approaching launch. A cofounder of the original Pepe token leads the build. Dual audits from SolidProof and Coinsult confirmed zero critical findings. The presale has already raised $7.33 million with 70% of supply filled, as reported by .</p><p>At $0.000000186, the math is simple. A 50x rally turns $1,000 into $50,000. A 100x turns it into $100,000. That is not speculation. Shiba Inu hit a $40 billion market cap with zero products. PEPE reached $7 billion on memes alone. Pepeto has three products approaching launch and a Binance listing on the horizon. Staking at 211% APY means a $3,000 hold generates $6,330 in yearly rewards while you wait. But staking is the bonus. The real play is the multiple. Community growth is accelerating and social channels are exploding. This is the presale that goes viral.</p><h3>BlockDAG: Strong Concept, Different Timeline</h3><p>BlockDAG combines Proof of Work security with a DAG structure that processes transactions in parallel. The presale has raised over $450 million, one of the largest totals this cycle. That gives the team serious funding for development.</p><p>But with $450 million already raised, much of the early upside may be priced in. Investors are now looking at 2x to 3x returns rather than exponential multiples. For steady growth from a large infrastructure play, BlockDAG makes sense. But it is a different bet than a presale at six zeros.</p><h3>Mutuum Finance: DeFi Lending With Utility</h3><p>Mutuum Finance focuses on decentralized lending and borrowing. Users supply assets to earn yield or borrow against holdings. The model mirrors established protocols but targets a newer audience.</p><p>The concept is solid with proven demand. But the returns profile looks more like a 2x to 5x play based on current valuations. Why target a 3x return when a presale at six zeros offers 50x with logic and 100x with momentum?</p><h2>Why Presales Create the Biggest Returns</h2><p>Exchange listed tokens already have price discovery behind them. You buy after millions set the floor. Presales flip that. You buy before the crowd, before the listing, before social media drives the second wave. Every cycle, the biggest winners come from presale entries, as reported by .</p><p>BlockDAG and Mutuum Finance both have real utility. But utility alone does not create 50x returns. Pepeto combines meme coin virality with real infrastructure, a Pepe cofounder, dual audits, and a Binance listing approaching. The presale is 70% filled. SHIB created millionaires with zero products. DOGE created millionaires with a joke. Pepeto has three products and the viral energy to match. At $0.000000186, this is the presale window that closes permanently once listing day arrives.</p><p><strong>What is a crypto presale and why does it matter?</strong></p><p>A presale lets you buy tokens before they list on exchanges. Prices are locked at early stage levels, which is how early SHIB and PEPE investors turned small amounts into life changing wealth.</p><p><strong>Is Pepeto better than BlockDAG for 2026 returns?</strong></p><p>BlockDAG raised $450 million, so much of the upside may be priced in. Pepeto at $0.000000186 offers 50x to 100x potential because it combines meme virality with real infrastructure at six zeros.</p><p><strong>Can you still make money from crypto presales?</strong></p><p>Every bull run creates new presale millionaires. The key is entering before exchange listings and choosing projects with both utility and viral community momentum.</p><p>:::warning\nThis article is for informational purposes only and does not constitute investment advice. Cryptocurrencies are speculative, complex, and involve high risks. This can mean high prices volatility and potential loss of your initial investment. You should consider your financial situation, investment purposes, and consult with a financial advisor before making any investment decisions. The HackerNoon editorial team has only verified the story for grammatical accuracy and does not endorse or guarantee the accuracy, reliability, or completeness of the information stated in this article. #DYOR</p>","contentLength":5136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google Quantum-Proofs HTTPS","url":"https://tech.slashdot.org/story/26/02/28/027202/google-quantum-proofs-https?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772283600,"author":"BeauHD","guid":154947,"unread":true,"content":"An anonymous reader quotes a report from Ars Technica: Google on Friday unveiled its plan for its Chrome browser to secure HTTPS certificates against quantum computer attacks without breaking the Internet. The objective is a tall order. The quantum-resistant cryptographic data needed to transparently publish TLS certificates is roughly 40 times bigger than the classical cryptographic material used today. Today's X.509 certificates are about 64 bytes in size, and comprise six elliptic curve signatures and two EC public keys. This material can be cracked through the quantum-enabled Shor's algorithm. Certificates containing the equivalent quantum-resistant cryptographic material are roughly 2.5 kilobytes. All this data must be transmitted when a browser connects to a site.\n \nTo bypass the bottleneck, companies are turning to Merkle Trees, a data structure that uses cryptographic hashes and other math to verify the contents of large amounts of information using a small fraction of material used in more traditional verification processes in public key infrastructure. Merkle Tree Certificates, \"replace the heavy, serialized chain of signatures found in traditional PKI with compact Merkle Tree proofs,\" members of Google's Chrome Secure Web and Networking Team wrote Friday. \"In this model, a Certification Authority (CA) signs a single 'Tree Head' representing potentially millions of certificates, and the 'certificate' sent to the browser is merely a lightweight proof of inclusion in that tree.\"\n \n[...] Google is [also] adding cryptographic material from quantum-resistant algorithms such as ML-DSA (PDF). This addition would allow forgeries only if an attacker were to break both classical and post-quantum encryption. The new regime is part of what Google is calling the quantum-resistant root store, which will complement the Chrome Root Store the company formed in 2022. The [Merkle Tree Certificates] MTCs use Merkle Trees to provide quantum-resistant assurances that a certificate has been published without having to add most of the lengthy keys and hashes. Using other techniques to reduce the data sizes, the MTCs will be roughly the same 64-byte length they are now [...]. The new system has already been implemented in Chrome.","contentLength":2254,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rubin Observatory Has Started Paging Astronomers 800,000 Times a Night","url":"https://science.slashdot.org/story/26/02/28/0155200/rubin-observatory-has-started-paging-astronomers-800000-times-a-night?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772272800,"author":"BeauHD","guid":154902,"unread":true,"content":"On February 24th, the Vera C. Rubin Observatory activated its automated alert system, sending out roughly 800,000 real-time notifications flagging asteroids, supernovae, flaring black holes and \"other transient celestial events,\" reports Scientific American. And this is only the beginning -- that number is projected to climb into the millions as it continues scanning the ever-changing sky. From the report: The astronomical observatory equipped with world's largest camera hit a key milestone on February 24, when a complex data-processing system pushed hundreds of thousands of alerts out to scientists eager to pore over its most exciting sightings. The Vera C. Rubin Observatory began operations last year, capturing stunning, panoramic time-lapse views of the cosmos with ease. Rubin's first images, based on just 10 hours of observations, let space fans zoom seemingly forever into an overwhelmingly starry sky. But watchful astronomers were always awaiting the next step: the system that would automatically alert them to the most promising activity in the overhead sky amid the 1,000 or so enormous images that Rubin's telescope captures every night.\n \n\"We can detect everything that changes, moves and appears,\" said Yusra AlSayyad, an astronomer at Princeton University and Rubin's deputy associate director for data management, to Scientific American last summer. \"It's way too much for one person to manually sift through and filter and monitor themselves.\" So even as they were designing and building the Rubin Observatory itself, scientists were also designing an alert system to help astronomers navigate the flood of data. As soon as the telescope began observations, the team started constructing a static reference image of the entire sky in impeccable detail.\n \nNow the data processing systems that support the observatory are starting to automatically compare every new Rubin image to the corresponding section of that background template. The systems identify all of the differences, each of which is individually flagged. The algorithms can also distinguish between a potential supernova and a possible newfound asteroid, for example. Alerting the scientific community is the final, crucial step. Astronomers -- as well as members of the public -- can sign up for notifications based on the type of sighting they're interested in and the brightness of the observation in question. And now that the alerts system has gone live, users receive a tiny, fuzzy image with some astronomical metadata of each observation that fits their criteria -- all just a couple of minutes after Rubin captures the original image.","contentLength":2634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The TechBeat: Beyond the Bots: What Real Writing Looks Like in the Age of AI (2/28/2026)","url":"https://hackernoon.com/2-28-2026-techbeat?source=rss","date":1772262683,"author":"Techbeat","guid":154923,"unread":true,"content":"<p>By <a href=\"https://hackernoon.com/u/davidiyanu\">@davidiyanu</a> [ 5 Min read ] \n RAG fails less from the LLM and more from retrieval: bad chunking, weak metadata, embedding drift, and stale indexes. Fix the pipeline first. <a href=\"https://hackernoon.com/rag-a-data-problem-disguised-as-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/stevebeyatte\">@stevebeyatte</a> [ 7 Min read ] \n Compare the 7 best co-parenting apps in 2026, including BestInterest, OurFamilyWizard, and TalkingParents. Find the right app for high-conflict situations.  <a href=\"https://hackernoon.com/the-7-best-coparenting-apps-in-2026\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/playerzero\">@playerzero</a> [ 15 Min read ] \n Modern software teams ship faster than ever, but defect resolution lags; PlayerZero aligns people, process, and context for predictable reliability. <a href=\"https://hackernoon.com/people-process-context-the-operating-model-modern-defect-resolution-needs\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ipinfo\">@ipinfo</a> [ 8 Min read ] \n Analysis of 170M residential proxy IPs reveals rapid rotation and 46% cross-provider overlap—breaking traditional fraud detection models. <a href=\"https://hackernoon.com/the-residential-proxy-problem-shared-infrastructure-and-rapid-rotation\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/thomascherickal\">@thomascherickal</a> [ 14 Min read ] \n OpenClaw lets you run frontier AI models like Minimax M2.5 and GLM-5 100% locally on Mac M3 or DGX Spark — zero API costs, total privacy. Here's how.  <a href=\"https://hackernoon.com/the-next-trillion-dollar-ai-shift-why-openclaw-changes-everything-for-llms\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/aimodels44\">@aimodels44</a> [ 8 Min read ] \n A new study suggests AGENTS.md-style repo context files can reduce coding-agent success while raising inference cost. Here’s why—and what to do instead. <a href=\"https://hackernoon.com/evaluating-agentsmd-are-repository-level-context-files-helpful-for-coding-agents\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/davidiyanu\">@davidiyanu</a> [ 8 Min read ] \n  Production is the unmarked minefield that begins the moment you accept arbitrary user input and promise reliability. <a href=\"https://hackernoon.com/beyond-the-demo-why-llm-applications-crash-in-production\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/dataops\">@dataops</a> [ 3 Min read ] \n Technical debt isn’t refactoring—it’s hidden risk. A powerful racecar analogy to help engineers explain why cutting corners can end in disaster. <a href=\"https://hackernoon.com/we-need-to-sound-the-alarm-on-technical-debt-heres-how-i-do-it\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/birukum\">@birukum</a> [ 11 Min read ] \n Agentic AI workflows can create a financial black hole. Learn how semantic caching uses vector similarity to cut your LLM token burn by 24%. <a href=\"https://hackernoon.com/optimise-llm-usage-costs-with-semantic-cache\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/sherveen\">@sherveen</a> [ 5 Min read ] \n Deep dive analysis of Grok 4.2 and Sonnet 4.6, two new AI releases from xAI and Anthropic, and how their agent systems compare. <a href=\"https://hackernoon.com/grok-42-vs-sonnet-46-early-impressions-from-hands-on-testing\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/samiranmondal\">@samiranmondal</a> [ 2 Min read ] \n Cybersecurity stocks fell after AI company Anthropic unveiled Claude Code Security <a href=\"https://hackernoon.com/cybersecurity-stocks-drop-as-anthropic-launches-claude-code-security-tool\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/MichaelJerlis\">@MichaelJerlis</a> [ 2 Min read ] \n Explore crypto staking options in 2026, compare ETH and SOL yields, and see how platforms like EMCD simplify earning passive income. <a href=\"https://hackernoon.com/how-to-earn-with-crypto-staking-a-practical-comparison-of-popular-options\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/omotayojude\">@omotayojude</a> [ 3 Min read ] \n When an AI agent's PR was rejected by Matplotlib, it didn't just close the tab it wrote an angry hit piece on the maintainer. Is this the future of open source? <a href=\"https://hackernoon.com/open-sources-first-cyber-bully-the-day-an-ai-agent-doxxed-a-matplotlib-maintainer\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hackernoon-courses\">@hackernoon-courses</a> [ 4 Min read ] \n Learn how to write content that stands out in the age of AI, crafting a voice and style no model or copycat can replicate. <a href=\"https://hackernoon.com/beyond-the-bots-what-real-writing-looks-like-in-the-age-of-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ArunDHANARAJ_gfaknebg\">@ArunDHANARAJ_gfaknebg</a> [ 14 Min read ] \n Compare Claude Opus 4.6 and GPT‑5.3 Codex across reasoning, coding, benchmarks, pricing, and safety to guide enterprise AI and agentic workload decisions.</p><p>By <a href=\"https://hackernoon.com/u/nickzt\">@nickzt</a> [ 5 Min read ] \n Scaling AI for the real world requires peeling back the layers of abstraction we've gotten too comfortable with. <a href=\"https://hackernoon.com/python-is-a-video-latency-suicide-note-how-i-hit-29-fps-with-zero-copy-c-onnx\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/brightdata\">@brightdata</a> [ 8 Min read ] \n ​​We benchmark SERP APIs for success rate,\n​​speed, and stability under load. Learn which setup delivers consistent results for AI agents ​​and deep research.  <a href=\"https://hackernoon.com/serp-benchmarks-success-rates-and-latency-at-scale\">Read More.</a></p>","contentLength":3102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Southern California Air Board Rejects Pollution Rules After AI-Generated Flood of Comments","url":"https://it.slashdot.org/story/26/02/27/2348254/southern-california-air-board-rejects-pollution-rules-after-ai-generated-flood-of-comments?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772262000,"author":"BeauHD","guid":154875,"unread":true,"content":"Southern California's air quality board rejected proposed rules to phase out gas-powered appliances after receiving more than 20,000 opposition comments generated through CiviClick, \"the first and best AI-powered grassroots advocacy platform.\" Phys.org reports: A Southern California-based public affairs consultant, Matt Klink, has taken credit for using CiviClick to wage the opposition campaign, including in a sponsored article on the website Campaigns and Elections. The campaign \"left the staff of the Southern California Air Quality Management District (SCAQMD) reeling,\" the article says. It is not clear how AI was deployed in the campaign, and officials at CiviClick did not respond to repeated requests for comment. But their website boasts several tools, including \"state of the art technology and artificial intelligence message assistance\" that can be used to create custom advocacy letters, as opposed to repetitive form letters or petitions often used in similar campaigns.\n \nWhen staffers at the air district reached out to a small sample of people to verify their comments, at least three said they had not written to the agency and were not aware of any such messages, records show. But the email onslaught almost certainly influenced the board's June decision, according to agency insiders, who noted that the number of public comments typically submitted on agenda items can be counted on one hand.\n \nThe proposed rules were nearly two years in the making and would have placed a fee on natural gas-powered water heaters and furnaces, favoring electric ones, in an effort to reduce air pollution in the district, which includes Orange County and large swaths of Los Angeles, Riverside and San Bernardino counties. Gas appliances emit nitrogen oxides, or NOx -- key pollutants for forming smog. The implications are troubling, experts said, and go beyond the use of natural gas furnaces and heaters in the second-largest metropolitan area in the country.","contentLength":1974,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"India disrupts access to popular developer platform Supabase with blocking order","url":"https://techcrunch.com/2026/02/27/india-disrupts-access-to-popular-developer-platform-supabase-with-blocking-order/","date":1772250712,"author":"Jagmeet Singh","guid":154858,"unread":true,"content":"<article>India, one of Supabase’s biggest markets, is seeing patchy access after a government block order.</article>","contentLength":99,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI Fires an Employee For Prediction Market Insider Trading","url":"https://slashdot.org/story/26/02/27/2342226/openai-fires-an-employee-for-prediction-market-insider-trading?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772249400,"author":"BeauHD","guid":154862,"unread":true,"content":"An anonymous reader quotes a report from Wired: OpenAI has fired an employee following an investigation into their activity on prediction market platforms including Polymarket, WIRED has learned. OpenAI CEO of Applications, Fidji Simo, disclosed the termination in an internal message to employees earlier this year. The employee, she said, \"used confidential OpenAI information in connection with external prediction markets (e.g. Polymarket).\" \"Our policies prohibit employees from using confidential OpenAI information for personal gain, including in prediction markets,\" says spokesperson Kayla Wood. OpenAI has not revealed the name of the employee or the specifics of their trades.\n \nEvidence suggests that this was not an isolated event. Polymarket runs on the Polygon blockchain network, so its trading ledger is pseudonymous but traceable. According to an analysis by the financial data platform Unusual Whales, there have been clusters of activities, which the service flagged as suspicious, around OpenAI-themed events since March 2023. Unusual Whales flagged 77 positions in 60 wallet addresses as suspected insider trades, looking at the age of the account, trading history, and significance of investment, among other factors. Suspicious trades hinged on the release dates of products like Sora, GPT-5, and the ChatGPT Browser, as well as CEO Sam Altman's employment status. In November 2023, two days after Altman was dramatically ousted from the company, a new wallet placed a significant bet that he would return, netting over $16,000 in profits. The account never placed another bet.\n \nThe behavior fits into patterns typical of insider trades. \"The tell is the clustering. In the 40 hours before OpenAI launched its browser, 13 brand-new wallets with zero trading history appeared on the site for the first time to collectively bet $309,486 on the right outcome,\" says Unusual Whales CEO Matt Saincome. \"When you see that many fresh wallets making the same bet at the same time, it raises a real question about whether the secret is getting out.\" [...] Though this is the first confirmed case of a large technology company firing an employee over trades in prediction markets, it's almost certainly not the last. Opportunities for tech sector employees to make trades on markets abound. \"The data tells me this is happening all over the place,\" Saincome says.","contentLength":2378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The 5 Best Batsuits From Batman: Arkham Knight","url":"https://hackernoon.com/the-5-best-batsuits-from-batman-arkham-knight?source=rss","date":1772249314,"author":"Jose","guid":154922,"unread":true,"content":"<p>Batman made his first appearance in 1939. When you have a character who has been around for 80 years, they’re bound to change their appearance from time to time. So, it only makes sense for video games like Batman: Arkham Knight to take advantage of this and add different alternative costumes that players can pick and choose from. Some of these were hits, some of these were misses, and some were just okay. Let’s take a look at the 5 biggest hits (at least, in my opinion). Here are the 5 best batsuits from Batman: Arkham Knight.</p><h2>5 Best Batsuits From Arkham Knight</h2><ol></ol><p>One of my favorites is the default one, the v8.03. We get this one early into the game, when Batman realizes he needs a little something extra, and his old costume just won’t do. It looks sleek, heavy, and the black looks amazing. This is definitely the best-looking default batsuit in the entire series.</p><p>\\\nThere are 2 other versions of this suit: the 8.04 and 8.05. The 8.04 stays perfectly pristine, so it doesn’t get any battle damage as the 8.03 does. Then there’s the 8.05. This one is similar, with the exception of the golden bat symbol on the chest.</p><p>\\\nI’m not a fan of the golden bat symbol, and I actually like that the 8.03 can get damaged and cut up. It shows the toll that the night has taken on Batman.</p><p>Some people like it when the batsuit is gray; others prefer it when it’s black. I like this one because it falls right in the middle. It looks like a dark gray, but from a different perspective, you can technically say that it’s a light black. What really makes the Batman Inc. suit one of my favorites, though, is the bat symbol on his chest. The yellow oval behind the black bat symbol makes it really striking, and it’s one of the first things your eyes notice.</p><p>\\\nThere are similar batsuits like the one from the 1989 movie that have a similar look. However, the Batman Inc. one has a better bat symbol and cowl. That’s why I put it above the 1989 one and above most other ones.</p><p>\\\nThe Batman Flashpoint suit is damn near perfect. The red accents all throughout it, such as in his pouches, eyes, and bat symbol, really make the whole thing stand out. Plus, having the body be gray with the cowl, gauntlets, and boots be completely black was such a great idea. The cherry on top, the thing that makes this costume stunning, is the double-handguns, one on each side. Like I said, damn near perfection.</p><p>\\\nSo, what don’t I like about it? My least favorite thing about it is the strange shoulder guards. Maybe if they were smaller and less pointy, I would be into it. Even better would be if they were completely gone. With all that said, though, this is still one of the best skins in the game.</p><p>I said I like the Batman Inc. suit because it’s the perfect mix between black and gray. I like the Batman v Superman one for a completely different reason. This one is very clearly gray; there’s no mistaking it. There are others that are gray, like the First Appearance one, but there is something different about this particular one. It’s the fabric. I’m not sure how to describe it, but the fabric of the suit is unlike any other.</p><p>\\\nIt sort of looks like a type of Kevlar, which is completely different from the tights that some of the other costumes appear to be made out of. That, combined with the gigantic bat symbol, makes it look phenomenal. I’m a sucker for a good bat symbol, what can I say?</p><p>This might be a hot take, but my all-time favorite batsuit in the Arkham Knight game is the Batman Beyond one. Don’t get me wrong, I love the others on this list, but this one is just on a completely different level. I really like the red accents and how mechanized the whole suit looks. It has a whole cyborg thing going on that I personally enjoy. There are some caveats to it, though, that I will admit to.</p><p>\\\nThe mouthpiece is a bit strange-looking. You have this whole futuristic costume, and it looks like the mouthpiece is just made out of cloth or something. Not a fan. The second biggest critique is that it looks nothing like the Batman Beyond suit from the animated show.</p><p>\\\nI can understand and accept these two flaws, but that doesn’t bring down my enjoyment of the costume. In my opinion, the Batman Beyond suit is the best-looking one in Batman: Arkham Knight. You know what, I might have to replay the whole game again just to look at these cool skins.</p>","contentLength":4369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human Brain Cells On a Chip Learned To Play Doom In a Week","url":"https://games.slashdot.org/story/26/02/27/2332219/human-brain-cells-on-a-chip-learned-to-play-doom-in-a-week?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772244120,"author":"BeauHD","guid":154810,"unread":true,"content":"Researchers at Cortical Labs used living human neurons grown on a chip to learn how to play Doom in about a week. \"While its performance is not up to par with humans, experts say it brings biological computers a step closer to useful real-world applications, like controlling robot arms,\" reports New Scientist. From the report: In 2021, the Australian company Cortical Labs used its neuron-powered computer chips to play Pong. The chips consisted of clumps of more than 800,000 living brain cells grown on top of microelectrode arrays that can both send and receive electrical signals. Researchers had to carefully train the chips to control the paddles on either side of the screen. Now, Cortical Labs has developed an interface that makes it easier to program these chips using the popular programming language Python. An independent developer, Sean Cole, then used Python to teach the chips to play Doom, which he did in around a week.\n \n\"Unlike the Pong work that we did a few years ago, which represented years of painstaking scientific effort, this demonstration has been done in a matter of days by someone who previously had relatively little expertise working directly with biology,\" says Brett Kagan of Cortical Labs. \"It's this accessibility and this flexibility that makes it truly exciting.\"\n \nThe neuronal computer chip, which used about a quarter as many neurons as the Pong demonstration, played Doom better than a randomly firing player, but far below the performance of the best human players. However, it learnt much faster than traditional, silicon-based machine learning systems and should be able to improve its performance with newer learning algorithms, says Kagan. However, it's not useful to compare the chips with human brains, he says. \"Yes, it's alive, and yes, it's biological, but really what it is being used as is a material that can process information in very special ways that we can't recreate in silicon.\" Cortical Labs posted a YouTube video showing its CL1 biological computer running Doom. There's also source code available on GitHub, with additional details in a README file.","contentLength":2119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google quantum-proofs HTTPS by squeezing 15kB of data into 700-byte space","url":"https://arstechnica.com/security/2026/02/google-is-using-clever-math-to-quantum-proof-https-certificates/","date":1772242001,"author":"Dan Goodin","guid":154811,"unread":true,"content":"<p>Google on Friday unveiled its plan for its Chrome browser to secure HTTPS certificates against quantum computer attacks without breaking the Internet.</p><p>The objective is a tall order. The quantum-resistant cryptographic data needed to transparently publish TLS certificates is roughly 40 times bigger than the classical cryptographic material used today. A typical <a href=\"https://en.wikipedia.org/wiki/X.509\">X.509 certificate</a> chain used today comprises six elliptic curve signatures and two EC public keys,  each of them only 64 bytes. This material can be cracked through the quantum-enabled <a href=\"https://en.wikipedia.org/wiki/Shor's_algorithm\">Shor’s algorithm</a>. The full chain is roughly 4 kilobytes. All this data must be transmitted when a browser connects to a site.</p><h2>The bigger they come, the slower they move</h2><p>“The bigger you make the certificate, the slower the handshake and the more people you leave behind,” said Bas Westerbaan, principal research engineer at Cloudflare, which is partnering with Google on the transition. “Our problem is we don’t want to leave people behind in this transition.” Speaking to Ars, he said that people will likely disable the new encryption if it slows their browsing. He added that the massive size increase can also degrade “middle boxes,” which sit between browsers and the final site.</p>","contentLength":1244,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2025/06/https-1152x648.jpg","enclosureMime":"","commentsUrl":null},{"title":"Hyperion Author Dan Simmons Dies From Stroke At 77","url":"https://news.slashdot.org/story/26/02/27/2226234/hyperion-author-dan-simmons-dies-from-stroke-at-77?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772241900,"author":"BeauHD","guid":154809,"unread":true,"content":"Author Dan Simmons, best known for the epic sci-fi novel Hyperion and its sequels, has died at 77 following a stroke. Ars Technica's Eric Berger remembers Simmons, writing: Simmons, who worked in elementary education before becoming an author in the 1980s, produced a broad portfolio of writing that spanned several genres, including horror fiction, historical fiction, and science fiction. Often, his books included elements of all of these. This obituary will focus on what is generally considered his greatest work, and what I believe is possibly the greatest science fiction novel of all time, Hyperion.\n \nPublished in 1989, Hyperion is set in a far-flung future in which human settlement spans hundreds of planets. The novel feels both familiar, in that its structure follows Chaucer's Canterbury Tales, and utterly unfamiliar in its strange, far-flung setting. Simmons' Hyperion appeared in an Ask Slashdot story back in 2008, when Slashdot reader willyhill asked for tips on how Slashdotters track down great sci-fi. If you're in the mood for a little nostalgia, or just want to browse the thread for book recommendations, it's well worth revisiting.","contentLength":1157,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From San Francisco to the Sands: Why U.S. Tech Talent Is Eyeing the UAE","url":"https://hackernoon.com/from-san-francisco-to-the-sands-why-us-tech-talent-is-eyeing-the-uae?source=rss","date":1772241497,"author":"Nica Furs","guid":154921,"unread":true,"content":"<p>If you hang around startup circles in the Bay Area long enough, you’ll start hearing something unexpected between funding rounds and AI debates: founders quietly Googling “<a href=\"https://drive.yango.com/\">car rental in UAE</a>” and checking flight prices to Dubai and Abu Dhabi. What started as curiosity has turned into a real trend. From San Francisco to the sands of the Arabian Peninsula, American tech talent is seriously eyeing the United Arab Emirates—and not just for a quick conference or a flashy vacation.</p><p>So what’s driving the shift?</p><h2><strong>Silicon Valley Burnout Is Real</strong></h2><p>Let’s call it what it is. The Bay Area is still iconic, but it’s also expensive, hyper-competitive, and increasingly saturated. Sky-high rents, intense regulation, talent wars, and a constant hustle culture can wear even the most ambitious founder down. After years of grinding in co-working spaces and chasing Series A funding, some U.S. entrepreneurs are looking for a reset.</p><p>The UAE, especially Dubai and Abu Dhabi, is pitching itself as that reset button. Lower personal income taxes, streamlined business setup processes, and aggressive government support for innovation make the region hard to ignore. For founders used to navigating layers of red tape back home, the efficiency can feel almost unreal.</p><h2><strong>A Government That Actually Bets on Tech</strong></h2><p>One of the biggest surprises for Americans exploring the UAE tech scene is how hands-on—and forward-thinking—the government is. Artificial intelligence, fintech, climate tech, space technology: these aren’t just buzzwords on a conference banner. They’re central to national strategy.</p><p>Free zones tailored to tech companies offer 100% foreign ownership and simplified licensing. Major funds and sovereign wealth investors actively back innovation. In many cases, founders aren’t just tolerated—they’re welcomed with open arms and real incentives.</p><p>Compare that to the sometimes fragmented regulatory environment in the U.S., and it’s easy to see why some builders are thinking, “Why not give this a shot?”</p><h2><strong>It’s Not Just Oil Money Anymore</strong></h2><p>There’s still a persistent stereotype in the U.S. that the Gulf economy runs purely on oil. That narrative is outdated. The UAE has spent decades diversifying its economy, investing heavily in infrastructure, tourism, logistics, and now digital transformation.</p><p>Walk through Dubai Internet City or Hub71 in Abu Dhabi and you’ll see a mix of global companies, scrappy startups, and venture-backed disruptors. English is widely spoken. Contracts are often structured in ways that feel familiar to U.S. founders. The vibe? Surprisingly international and business-friendly.</p><p>For tech professionals who’ve spent their careers building products for global markets, the UAE’s geographic position—bridging Europe, Asia, and Africa—is a strategic advantage. A product launched in Dubai can scale across multiple regions without being locked into one market.</p><p>Let’s talk lifestyle, because it matters. The UAE isn’t just pitching spreadsheets and tax breaks. It’s selling quality of life. Modern apartments, world-class restaurants, beach access, and relatively high levels of safety are all part of the package.</p><p>Yes, the summer heat is intense. But the infrastructure is built for it. Offices, malls, and residential buildings are climate-controlled. Everything runs efficiently. For many Americans, the biggest adjustment isn’t the temperature—it’s the pace. Things move fast. Deals close quickly. Bureaucracy, when it exists, is often surprisingly streamlined.</p><p>And when it comes to getting around, practicality kicks in. Cities like Dubai are spread out, with business districts, residential communities, and innovation hubs connected by wide highways. While public transportation exists, most professionals find that having a car makes daily life significantly easier. Whether you’re commuting to a co-working space, heading to investor meetings, or exploring new neighborhoods, renting a car is often the smartest move—especially during your first few months while you figure out where to settle.</p><h2><strong>The Remote Work Era Changed the Game</strong></h2><p>The pandemic permanently shifted how tech workers think about location. If you can code from anywhere, why limit yourself to one zip code? The UAE capitalized on this shift by introducing long-term visas and remote work permits designed specifically for global talent.</p><p>Suddenly, relocating doesn’t mean cutting ties with U.S. clients or investors. Many founders maintain American entities while building regional operations in the UAE. It’s less about abandoning Silicon Valley and more about expanding beyond it.</p><p>This hybrid model is attractive. Keep your Delaware C-corp, but base your operations in a city that offers global connectivity, strong infrastructure, and competitive costs. For a generation raised on flexibility and scale, it’s a compelling pitch.</p><h2><strong>Risk, Reward, and Reputation</strong></h2><p>Of course, moving halfway across the world isn’t a casual decision. Cultural differences, legal frameworks, and market dynamics require research and adaptability. Not every startup will thrive in the Gulf, and not every founder will feel at home.</p><p>But the reputation factor is shifting. What once seemed like a bold or risky move now feels strategic. U.S. tech talent isn’t just chasing sunshine and skyscrapers. They’re chasing opportunity—new markets, new investors, and a chance to build in an ecosystem that’s actively evolving.</p><p>From San Francisco to the sands, the flow of ideas—and people—is becoming more global. The UAE isn’t replacing Silicon Valley, but it’s carving out its own lane as a serious contender in the tech world. For American founders and engineers tired of the grind or hungry for international expansion, it’s a place worth exploring.</p><p>Pack your laptop. Line up your meetings. Maybe start with a short-term stay and a rental car to navigate the city like a local. You might just discover that the future of your startup isn’t limited to the Bay Area. Sometimes, the next big move starts with a one-way ticket and a willingness to see what’s possible beyond the familiar skyline.</p>","contentLength":6113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The 7 Leading Requirements Management Software Solutions in 2026","url":"https://hackernoon.com/the-7-leading-requirements-management-software-solutions-in-2026?source=rss","date":1772239632,"author":"Steve Beyatte","guid":154920,"unread":true,"content":"<article>This guide compares the 7 leading requirements management software solutions in 2026, from modern platforms like Jama Connect to legacy tools like IBM DOORS and lightweight options like Excel. The best choice depends on your product complexity, regulatory requirements, and team structure—but most organizations opt for modern tools like Jama Connect.</article>","contentLength":353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CISA Replaces Bumbling Acting Director After a Year","url":"https://yro.slashdot.org/story/26/02/27/2215238/cisa-replaces-bumbling-acting-director-after-a-year?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772239500,"author":"BeauHD","guid":154808,"unread":true,"content":"New submitter DeanonymizedCoward shares a report from TechCrunch: The U.S. Cybersecurity and Infrastructure Security Agency (CISA) is reportedly in crisis following major budget cuts, layoffs, and furloughs under the Trump administration, says TechCrunch. The agency has now replaced its acting director, Madhu Gottumukkala, after a turbulent year marked by controversy and internal turmoil. During his tenure, Gottumukkala allegedly mishandled sensitive information by uploading government documents to ChatGPT, oversaw a one-third reduction in staff, and reportedly failed a counterintelligence polygraph needed for classified access. His leadership also saw the suspension of several senior officials, including CISA's chief security officer. Nextgov also reported that CISA lost another top senior official, Bob Costello, the agency's chief information officer tasked with overseeing the agency's IT systems and data policies. \"Last month, CISA's acting director Madhu Gottumukkala reportedly took steps to transfer Costello, but other political appointees blocked it,\" added Nextgov.","contentLength":1088,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TurboSparse-LLM Performance: Outperforming Mixtral and Gemma with Extreme Sparsity","url":"https://hackernoon.com/turbosparse-llm-performance-outperforming-mixtral-and-gemma-with-extreme-sparsity?source=rss","date":1772238916,"author":"Language Models (dot tech)","guid":154919,"unread":true,"content":"<p>We measure our sparsified models’ performance on tasks included in OpenLLM Leaderboard which include 25-shot Arc-Challenge [13], 10-shot Hellaswag [65], 5-shot MMLU [22], 0-shot TruthfulQA [35], 5-shot Winogrande [51] and 8-shot GSM8K [14]. In addition, we also follow Llama 2’s evaluation task included commonsense reasoning tasks. We report the average of PIQA [8], SCIQ [26], ARC easy [13], OpenBookQA [41]. We compare our models to several external open-source LLMs, including Gemma-2B [58], Mistral-7B [24] and Mixtral-47B [25].</p><p>\\\n\\\nTable 6 shows the results from different models. TurboSparse-Mistral-7B outperforms Gemma-2B by far, while only activating 3B parameters. TurboSparse-Mixtral-47B outperforms the original Mixtral-47B with only 4.5B parameters activated. The results demonstrate that LLMs with ReLU based intrinsic activation sparsity can keep the same or better performance while hold the significant FLOPs reduction.</p><p>(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;</p><p>(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(5) Li Ma, Shanghai Artificial Intelligence Laboratory;</p><p>(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);</p><p>(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.</p>","contentLength":1606,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"dReLU Sparsification: Recovering LLM Performance with 150B Token Pretraining","url":"https://hackernoon.com/drelu-sparsification-recovering-llm-performance-with-150b-token-pretraining?source=rss","date":1772238666,"author":"Language Models (dot tech)","guid":154918,"unread":true,"content":"<p>In the previous section, we have demonstrated that dReLU can be a better choice for ReLUfication. The main question now is whether dReLU based ReLUfication can recover the original model’s performance while achieving higher sparsity. The following sections will discuss the experiments that aimed at answering this question.</p><p>\\\n We consider two representative models: Mistral-7B and Mixtral-47B. We substitute the original SwiGLU based FFN with dReLU based FFN and then continue pretraining.</p><p>\\\n Due to the ReLUfication process, the restoration of model capability is closely related to the corpus used for recovery training. We collected as much corpus as possible from the open-source community for training, such as Wanjuan-CC [48], open-web-math [46], peS2o [54], Pile [19], The Stack [28], GitHub Code [1] and so on. The detailed mixture ratio is as shown in the following table 4:</p><p>\\\n\\\n. After pretraining, we utilize the high-quality SFT datasets to further improve our model’s performance, including orca-math-word-problems [43], bagel [27].</p><p>\\\n. The hyperparameters for our ReLUfication are based on empirical results from previous works [69]. We utilize the llm-foundry framework for training [44] and employ FSDP parallelism.</p><p>\\\nOur models are trained using the AdamW optimizer [38] with the following hyper-parameters: β1 = 0.9 and β2 = 0.95. We adopt a cosine learning rate schedule and use the default values for weight decay and gradient clipping (see Table 5 for more details). In total, we pretrain our models on 150B tokens.</p><p>(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;</p><p>(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(5) Li Ma, Shanghai Artificial Intelligence Laboratory;</p><p>(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);</p><p>(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.</p>","contentLength":2204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Metrics Review Ritual That Turns Product Work Into Revenue","url":"https://hackernoon.com/the-metrics-review-ritual-that-turns-product-work-into-revenue?source=rss","date":1772238599,"author":"Dan Layfield","guid":154917,"unread":true,"content":"<article>If you “know your numbers” but can’t explain why they move, you’re flying blind.</article>","contentLength":88,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity Announces 'Computer,' an AI Agent That Assigns Work To Other AI Agent","url":"https://slashdot.org/story/26/02/27/2151236/perplexity-announces-computer-an-ai-agent-that-assigns-work-to-other-ai-agent?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772236920,"author":"BeauHD","guid":154807,"unread":true,"content":"joshuark shares a report from Ars Technica: Perplexity has introduced \"Computer,\" a new tool that allows users to assign tasks and see them carried out by a system that coordinates multiple agents running various models. The company claims that Computer, currently available to Perplexity Max subscribers, is \"a system that creates and executes entire workflows\" and \"capable of running for hours or even months.\"\n \nThe idea is that the user describes a specific outcome -- something like \"plan and execute a local digital marketing campaign for my restaurant\" or \"build me an Android app that helps me do a specific kind of research for my job.\" Computer then ideates subtasks and assigns them to multiple agents as needed, running the models Perplexity deems best for those tasks. The core reasoning engine currently runs Anthropic's Claude Opus 4.6, while Gemini is used for deep research, Nano Banana for image generation, Veo 3.1 for video production, Grok for lightweight tasks where speed is a consideration, and ChatGPT 5.2 for \"long-context recall and wide search.\"\n \nThis kind of best-model-for-the-task approach differs from some competing products like Claude Cowork, which only uses Anthropic's models. All this happens in the cloud, with prebuilt integrations. \"Every task runs in an isolated compute environment with access to a real filesystem, a real browser, and real tool integrations,\" Perplexity says. The idea is partly that this workflow was what some power users were already doing, and this aims to make that possible for a wider range of people who don't want to deal with all that setup.\n \nPeople were already using multiple models and tailoring them to specific tasks based on perceived capabilities, while, for example, using MCP (Model Context Protocol) to give those models access to data and applications on their local machines. Perplexity Computer takes a different approach, but the goal is the same: have AI agents running tailor-picked models to perform tasks involving your own files, services, and applications. Then there is OpenClaw, which you could perceive as the immediate predecessor to this concept.","contentLength":2145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity Announces 'Computer,' an AI Agent That Assigns Work To Other AI Agents","url":"https://slashdot.org/story/26/02/27/2151236/perplexity-announces-computer-an-ai-agent-that-assigns-work-to-other-ai-agents?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772236920,"author":"BeauHD","guid":154861,"unread":true,"content":"joshuark shares a report from Ars Technica: Perplexity has introduced \"Computer,\" a new tool that allows users to assign tasks and see them carried out by a system that coordinates multiple agents running various models. The company claims that Computer, currently available to Perplexity Max subscribers, is \"a system that creates and executes entire workflows\" and \"capable of running for hours or even months.\"\n \nThe idea is that the user describes a specific outcome -- something like \"plan and execute a local digital marketing campaign for my restaurant\" or \"build me an Android app that helps me do a specific kind of research for my job.\" Computer then ideates subtasks and assigns them to multiple agents as needed, running the models Perplexity deems best for those tasks. The core reasoning engine currently runs Anthropic's Claude Opus 4.6, while Gemini is used for deep research, Nano Banana for image generation, Veo 3.1 for video production, Grok for lightweight tasks where speed is a consideration, and ChatGPT 5.2 for \"long-context recall and wide search.\"\n \nThis kind of best-model-for-the-task approach differs from some competing products like Claude Cowork, which only uses Anthropic's models. All this happens in the cloud, with prebuilt integrations. \"Every task runs in an isolated compute environment with access to a real filesystem, a real browser, and real tool integrations,\" Perplexity says. The idea is partly that this workflow was what some power users were already doing, and this aims to make that possible for a wider range of people who don't want to deal with all that setup.\n \nPeople were already using multiple models and tailoring them to specific tasks based on perceived capabilities, while, for example, using MCP (Model Context Protocol) to give those models access to data and applications on their local machines. Perplexity Computer takes a different approach, but the goal is the same: have AI agents running tailor-picked models to perform tasks involving your own files, services, and applications. Then there is OpenClaw, which you could perceive as the immediate predecessor to this concept.","contentLength":2145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Subscription Managers: When They’re Worth It","url":"https://hackernoon.com/subscription-managers-when-theyre-worth-it?source=rss","date":1772236799,"author":"Dan Layfield","guid":154916,"unread":true,"content":"<article>Everyone who runs a subscription business has to eventually decide if they’re going buy a subscription manger.</article>","contentLength":112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sparse Activation in MoE Models: Extending ReLUfication to Mixture-of-Experts","url":"https://hackernoon.com/sparse-activation-in-moe-models-extending-relufication-to-mixture-of-experts?source=rss","date":1772235979,"author":"Language Models (dot tech)","guid":154915,"unread":true,"content":"<h2>4 Are Neurons in Expert still Sparsely Activated?</h2><p>Previous work has shown that dense LLMs with different activation functions (ReLU, SwiGLU, etc.) exhibit the property of sparse activation [69, 36, 30]. However, the analysis is limited to dense models. Despite the intuitive assumption that partitioning FFNs into different experts within an MoE model would result in denser activations within each expert, it remains unclear whether this sparsity phenomenon persists in MoE models. In this section, we select representative MoE models and commonly used downstream tasks to investigate whether this sparsity phenomenon still exists in MoE models. We utilize the same method in 3 to control the sparsity in each expert.</p><p>\\\n. We select Deepseek-MoE [15], Qwen1.5-MoE [5] and Mixtral [25] as the models for our experiments. We also add Llama-2-7B as for comparison.</p><p>\\\nWe first study the performance with regard to the sparsity ratio, as shown in Figure 5 (a)[2]. Specifically, the performance only drops by about 1%-2% when the sparsity ratio is 0.5. This trend suggests that MoE models exhibit similar sparsity compared to dense models.</p><p>\\\nFurther, we profile the activation patterns of Mistral and Mixtral, a pair of popular dense LLM and MoE LLM, as shown in Figure 5 (b). We find that both LLMs show a similar pattern where activations are concentrated around 0, which is consistent with previous analysis of dense LLMs. The sparsity in experts also implies that every neuron in the same expert has different functionality. This finding applies to all layers and experts, as detailed in Appendix A.2. We report this interesting observation and leave further analysis for future work.</p><p>\\\n\\\nInspired by our discoveries in MoE models, we are convinced that ReLUfication can be extended to MoE models and is not restricted to dense models. As the proportion of FFN weights in MoE models increases, the FLOP reduction achieved through ReLUfication will be even more pronounced.</p><p>(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;</p><p>(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;</p><p>(5) Li Ma, Shanghai Artificial Intelligence Laboratory;</p><p>(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);</p><p>(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.</p>","contentLength":2631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Subscription Growth: The Momentum You Can’t See","url":"https://hackernoon.com/subscription-growth-the-momentum-you-cant-see?source=rss","date":1772235899,"author":"Dan Layfield","guid":154914,"unread":true,"content":"<article>A $1M MRR celebration turned into a refunds lesson—and a framework for why subscription growth feels slow, hides wins, and compounds over time.</article>","contentLength":145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"South Korea Set To Get a Fully Functioning Google Maps","url":"https://tech.slashdot.org/story/26/02/27/2144239/south-korea-set-to-get-a-fully-functioning-google-maps?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772234400,"author":"BeauHD","guid":154788,"unread":true,"content":"South Korea has reversed a two-decade policy and approved the export of high-precision map data, paving the way for a fully functional Google Maps in the country. Reuters reports: The approval was made \"on the condition that strict security requirements are met,\" the Ministry of Land, Infrastructure and Transport said in a statement. Those conditions include blurring military and other sensitive security-related facilities, as well as restricting longitude and latitude coordinates for South Korean territory on products such as Google Maps and Google Earth, it said.\n \nThe decision is expected to hurt Naver and Kakao -- local internet giants which currently dominate the country's market for digital map services. But it will appease Washington, which has urged Seoul to tackle what it says is discrimination against U.S. tech companies. South Korea, still technically at war with North Korea, had shot down Google's previous bids in 2007 and 2016 to be allowed to export the data, citing the risks that information about sensitive military and security facilities could be exposed. \"Google can now come in, slash usage fees, and take the market,\" said Choi Jin-mu, a geography professor at Kyung Hee University. \"If Naver and Kakao are weakened or pushed out and Google later raises prices, that becomes a monopoly. Then, even companies that rely on map services -- logistics firms, for example -- become dependent, and in the long run, even government GIS (geographic information) systems could end up dependent on Google or Apple. That's the biggest concern.\"","contentLength":1568,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI fires employee for using confidential info on prediction markets","url":"https://techcrunch.com/2026/02/27/openai-fires-employee-for-using-confidential-info-on-prediction-markets/","date":1772233254,"author":"Julie Bort","guid":154782,"unread":true,"content":"<article>The company said such trades violates its internal company policies about using confidential information for personal gain.</article>","contentLength":123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trump Orders Federal Agencies To Stop Using Anthropic AI Tech 'Immediately'","url":"https://tech.slashdot.org/story/26/02/27/2138211/trump-orders-federal-agencies-to-stop-using-anthropic-ai-tech-immediately?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772232000,"author":"BeauHD","guid":154787,"unread":true,"content":"President Donald Trump has ordered all U.S. federal agencies to \"immediately cease\" using Anthropic's AI technology, escalating a standoff after the company sought limits on Pentagon use of its models. CNBC reports: The company, which in July signed a $200 million contract with Pentagon, wants assurances that the Defense Department will not use its AI models will not be used for fully autonomous weapons or mass domestic surveillance of Americans. The Pentagon had set a deadline of 5:01 p.m. ET Friday for Anthropic to agree to its demands to allow the Pentagon to use the technology for all lawful purposes. If Anthropic did not meet that deadline, Pete Hegseth threatened to label the company a \"supply chain risk\" or force it to comply by invoking the Defense Production Act.\n \n\"The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution,\" Trump said in a post on Truth Social. \"Their selfishness is putting AMERICAN LIVES at risk, our Troops in danger, and our National Security in JEOPARDY.\"\n \n\"Therefore, I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic's technology,\" Trump wrote. \"We don't need it, we don't want it, and will not do business with them again! There will be a Six Month phase out period for Agencies like the Department of War who are using Anthropic's products, at various levels,\" Trump said. On Friday, OpenAI said it would also draw the same red lines as Anthropic: no AI for mass surveillance or autonomous lethal weapons.","contentLength":1640,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US Military Accidentally Shoots Down Border Protection Drone With Laser","url":"https://tech.slashdot.org/story/26/02/27/2133209/us-military-accidentally-shoots-down-border-protection-drone-with-laser?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772229720,"author":"BeauHD","guid":154722,"unread":true,"content":"An anonymous reader quotes a report from the Associated Press: The U.S. military used a laser Thursday to shoot down a \"seemingly threatening\" drone flying near the U.S.-Mexico border. It turned out the drone belonged to Customs and Border Protection, lawmakers said. The case of mistaken identity prompted the Federal Aviation Administration to close additional airspace around Fort Hancock, about 50 miles (80 kilometers) southeast of El Paso. The military is required to formally notify the FAA when it takes any counter-drone action inside U.S. airspace.\n \nIt was the second time in two weeks that a laser was fired in the area. The last time it was CBP that used the weapon and nothing was hit. That incident occurred near Fort Bliss and prompted the FAA to shut down air traffic at El Paso airport and the surrounding area. This time, the closure was smaller and commercial flights were not affected. The FAA, CBP and the Pentagon confirmed the incident in a joint statement, saying the military \"employed counter-unmanned aircraft system authorities to mitigate a seemingly threatening unmanned aerial system operating within military airspace.\"\n \n\"At President Trump's direction, the Department of War, FAA, and Customs and Border Patrol are working together in an unprecedented fashion to mitigate drone threats by Mexican cartels and foreign terrorist organizations at the U.S.-Mexico Border,\" the statement said. The report notes that 27,000 drones were detected within 1,600 feet of the southern border in the last six months of 2024.\n \nIllinois Democratic U.S. Sen. Tammy Duckworth, the ranking member on the Senate's Aviation Subcommittee, is calling for an independent investigation to look into the matter. \"The Trump administration's incompetence continues to cause chaos in our skies,\" Duckworth said.","contentLength":1819,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pentagon moves to designate Anthropic as a supply-chain risk","url":"https://techcrunch.com/2026/02/27/pentagon-moves-to-designate-anthropic-as-a-supply-chain-risk/","date":1772229194,"author":"Russell Brandom","guid":154718,"unread":true,"content":"<article>\"We don't need it, we don't want it, and will not do business with them again,\" the president wrote in the post.</article>","contentLength":112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"White House Stalls Release of Approved US Science Budgets","url":"https://news.slashdot.org/story/26/02/27/207211/white-house-stalls-release-of-approved-us-science-budgets?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772227500,"author":"msmash","guid":154700,"unread":true,"content":"An anonymous reader shares a report: Weeks after the U.S. Congress rejected unprecedented cuts to science budgets that the administration of US President Donald Trump had sought for 2026, funding to several agencies that award research grants is still not freely flowing. \n\nOne reason is that the White House Office of Management and Budget (OMB) has been slow to authorize its release. The US National Institutes of Health (NIH) has so far not received approval to spend any of the research funding allocated in a budget bill signed into law on 3 February. The US National Science Foundation (NSF) was authorized to spend its funding just last week. And NASA has had its full funding authorized for release, but with an unusual restriction that limits spending on ten specific programmes -- many of which the Trump team had tried to cancel last year.","contentLength":851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"'The Death of Spotify: Why Streaming is Minutes Away From Being Obsolete'","url":"https://entertainment.slashdot.org/story/26/02/27/1941205/the-death-of-spotify-why-streaming-is-minutes-away-from-being-obsolete?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772225460,"author":"msmash","guid":154699,"unread":true,"content":"An anonymous reader shares a column: I'm going to take the diplomatic hat off here and say with brutal honesty: basically everybody in the music business hates Spotify except for the people who work there. It's a platform that sucks artists for everything they have, it actively prevents community building, and, despite all of that, the platform still struggles to maintain a healthy profit margin. \n\nThe streaming business model is fundamentally broken. And eventually, its demise will become more and more obvious to recognize. I'll break down exactly why the DSP era is coming to a grinding halt, why the major labels are quietly terrified, and why the artists who don't pivot now are going to go down with the ship. \n\n[...] Jimmy Iovine put it bluntly: \"The streaming services have a bad situation, there's no margins, they're not making any money.\" This model only works for Apple, Amazon, and Google, because they don't need their music platforms to be wildly profitable. Amazon uses music as a loss-leader to keep you paying for Prime. Apple uses it to sell $1,000 iPhones. As for Spotify, or any standalone music streaming company, they're kind of screwed. And guess what -- when the platform's margins are structurally squeezed, guess who gets squeezed first? The artists. \n\n[...] What if Jimmy is right? If the DSPs are \"minutes away from obsolete,\" what replaces them? Well, I'm not sure the DSPs are going to disappear overnight, but if you're an artist or a manager trying to sustain yourself in this evolving music economy, the answer is direct ownership. The artists who will survive the next five years are the ones who are quietly shifting their focus away from the \"ATM Machine.\" \n\nThey are building their own cultural hangars. They are capturing phone numbers on Laylo. They are driving fans to private Discord servers. They are focusing on ARPF (Average Revenue Per Fan) through high-margin merch, vinyl, and hard tickets, rather than begging for fractions of a penny from a playlist placement. We are witnessing the death of the \"Mass Audience\" and the birth of the \"Micro-Community.\"","contentLength":2106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Mistakes Are Infuriating Gamers as Developers Seek Savings","url":"https://games.slashdot.org/story/26/02/27/1934258/ai-mistakes-are-infuriating-gamers-as-developers-seek-savings?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772223000,"author":"msmash","guid":154678,"unread":true,"content":"The $200 billion video game industry is caught between studios eager to cut ballooning development costs through AI and a player base that has grown openly hostile to the technology after a string of visible blunders. \n\nAs Bloomberg News reports, Arc Raiders, a surprise hit from Stockholm-based Embark Studios that sold 12 million copies in three months, was briefly vilified online for its robotic-sounding auto-generated voices -- even as CEO Patrick Soderlund insists AI was only used for non-essential elements. EA's Battlefield 6 and Activision's Call of Duty: Black Ops 7 both drew gamer anger this winter over thematically mismatched or poorly generated graphics, and Valve's Steam has added labels to flag games made using AI. \n\nSome 47% of developers polled by research house Omdia said they expect generative AI to reduce game quality, and PC gamers -- now facing inflated hardware prices from AI-driven demand for graphics chips -- have turned reflexively antagonistic.","contentLength":981,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Musk bashes OpenAI in deposition, saying ‘nobody committed suicide because of Grok’","url":"https://techcrunch.com/2026/02/27/musk-bashes-openai-in-deposition-saying-nobody-committed-suicide-because-of-grok/","date":1772221320,"author":"Sarah Perez","guid":154674,"unread":true,"content":"<article>In his lawsuit against OpenAI, Musk touted xAI safety compared with ChatGPT. A few months later, xAI's Grok flooded X with nonconsensual nude images.</article>","contentLength":149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Smartphone Market To Decline 13% in 2026, Marking the Largest Drop Ever Due To the Memory Shortage Crisis","url":"https://mobile.slashdot.org/story/26/02/27/1917219/smartphone-market-to-decline-13-in-2026-marking-the-largest-drop-ever-due-to-the-memory-shortage-crisis?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772220600,"author":"msmash","guid":154662,"unread":true,"content":"An anonymous reader shares a report: Worldwide smartphone shipments are forecast to decline 12.9% year-on-year (YoY) in 2026 to 1.1 billion units, according to the International Data Corporation (IDC) Worldwide Quarterly Mobile Phone Tracker. This decline will bring the smartphone market to its lowest annual shipment volume in more than a decade. The current forecast represents a sharp decline from our November forecast amid the intensifying memory shortage crisis.","contentLength":469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthropic vs. the Pentagon: What’s actually at stake?","url":"https://techcrunch.com/2026/02/27/anthropic-vs-the-pentagon-whats-actually-at-stake/","date":1772219464,"author":"Rebecca Bellan","guid":154660,"unread":true,"content":"<article>Anthropic and the Pentagon are clashing over AI use in autonomous weapons and surveillance, raising high-stakes questions about national security, corporate control, and who sets the rules for military AI.</article>","contentLength":205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nasa Announces Artemis III Mission No Longer Aims To Send Humans To Moon","url":"https://science.slashdot.org/story/26/02/27/1854230/nasa-announces-artemis-iii-mission-no-longer-aims-to-send-humans-to-moon?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772218440,"author":"msmash","guid":154661,"unread":true,"content":"Nasa announced on Friday radical changes to its delayed Artemis III mission to land humans back on the moon, as the US space agency grapples with technical glitches and criticism that it is trying to do too much too soon. From a report: The abrupt shift in strategy was laid out by the space agency's recently confirmed administrator, Jared Isaacman. Announcing the changes on Friday, he said that Nasa would introduce at least one new moon flight before attempting to put humans back on the lunar surface for the first time in more than half a century, in 2028. \n\nThe new, more incremental approach would give the Nasa team a chance to test flight and refine its technology. As part of the changes, the Artemis II mission to fly humans around the moon this year, without landing, would also be pushed back from its latest scheduled launch on 6 March to 1 April at the earliest. \n\n\"Everybody agrees this is the only way forward,\" Isaacman told reporters at a news conference. \"I know this is how Nasa changed the world, and this is how Nasa is going to do it again.\"","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Smart People Stay Stuck (and How to Break It)","url":"https://hackernoon.com/why-smart-people-stay-stuck-and-how-to-break-it?source=rss","date":1772217903,"author":"BenoitMalige","guid":154804,"unread":true,"content":"<p><strong>Most people don’t need therapy.</strong>They need to hear themselves think. \\n Writing is how you do that.</p><p>If you’re reading this,&nbsp;.</p><p>Here’s the second hill I’ll die on:</p><p>The smartest, most self-aware people are usually the most trapped. \\n Because awareness without embodiment is just sophisticated suffering.</p><p>You don’t need more information.</p><p>You can already explain yourself better than 99% of people. \\n You know the patterns. \\n You can name the mechanisms. \\n You understand &nbsp;you do what you do.</p><p>And yet—there’s this massive, screaming gap between the person you understand yourself to be… and the smaller, quieter, less honest life you keep waking up in.</p><p>How does someone this self-aware, this intelligent, this capable of naming every variable at play…still wake up trapped in a version of reality they don’t even enjoy?</p><p>That dissonance isn’t a character flaw. \\n It’s not bad timing. \\n It’s not that you’re “not ready.”</p><p>It’s your nervous system refusing to update the identity file while your body hesitates, tightens, and pulls the brakes every time your mind tries to move forward.</p><p>If your stomach dropped a little just now, good. \\n That’s the signal we’re finally past surface-level advice.</p><p>I’m going to be very direct with you: What you’re about to read explains why you’re still not living the life you already know you’re capable of, even though you understand yourself better than most people ever will.</p><p>I’ll break down the neuroscience behind that gap of who you are VS who you need to be, and I’ll give you the&nbsp;&nbsp;I’ve been using to collapse that gap without waiting years for things to “click”.</p><p>In fact, if you actually apply this, your progress will feel suspicious..especially to the people who benefit from you staying the same.</p><h2><strong>The model we were all taught</strong></h2><p>For the longest time, I assumed change was linear.</p><p><strong>Learn → understand → act → become.</strong></p><p>And that makes sense, right? It’s logic. That’s how school works:</p><p><strong>You study the material → You pass the test → You get the diploma → You’re now “qualified.”</strong></p><p>Same with personal growth: \\n <strong>You read the book → You gain insight → You apply the advice → You slowly turn into someone new.</strong></p><p>This model feels safe because it’s orderly. We are taught to follow step one, then step two, then step three until you get a result.</p><p>Cause → effect. \\n Time → progress.</p><p>So when change doesn’t happen, the conclusion is obvious: \\n “I must not have learned enough.” \\n “I’m not applying it consistently.” \\n “I need more discipline.”</p><p>That’s the logic everyone operates under.</p><p>Including you. \\n Including me.</p><p>Let’s get something straight: this model actually works. You didn’t misunderstand it. You didn’t apply it wrong. You executed it&nbsp;.</p><p>In fact, it gave the results that you have today:</p><ul><li>You became “successful” on paper.</li><li>You earned competence, status, and proof that you’re not an idiot.</li></ul><p>Which is exactly why this is so confusing. Because the model&nbsp;.</p><p>And yet…there’s this exhausting, permanent tension that never really goes away.</p><p>You can see the version of yourself you’re capable of being, but you’re not becoming it.</p><p>And that’s what makes this unbearable. In fact, I would argue that because of your awareness,&nbsp;<strong>knowing you’re meant for more is actually the most painful place to be.</strong>&nbsp;It was for me.</p><p>That’s what happens when&nbsp;<strong>learning outpaces embodiment</strong>.</p><p>Here’s the truth about change. It’s a biological game, not a motivational one. And biology has rules (constraints that don’t give a shit about your intentions or discipline). If you want to collapse the gap between knowing and becoming, you have to play by them, so let’s break it down.</p><h2><strong>The actual laws of change</strong></h2><p>These aren’t feel-good principles or “what worked for me” stories. They’re hardwired constraints on how your nervous system updates identity.</p><p>If you ignore them, and you stay stuck. When you finally respect them, change accelerates.</p><p>: Identity isn’t updated by logic or information alone. Your prefrontal cortex can understand a new version of you all day long.</p><p>But the nervous system (The one that controls your body’s felt reality) only rewires through repeated emotional and sensory experience.</p><p>It doesn’t care about abstract insights. It responds to what feels familiar, safe, and present-tense.</p><p>: The nervous system doesn’t fully distinguish between imagined and lived experience. When you vividly rehearse a future state with elevated emotion, it registers as “this is happening now.”</p><p>Neural pathways fire identically, as shown in fMRI studies on athletes using visualization. Mental practice alone strengthens the same brain regions and even boosts muscle performance without physical movement.</p><p>Familiarity builds. \\n Safety locks in. \\n Identity shifts.</p><p>: The brain ignores future-tense intentions. \\n It’s built for survival in the present. \\n Promises like “I’ll be that person one day” get dismissed as irrelevant. \\n No update happens. \\n The body stays braced, attention narrows, and old patterns persist.</p><p>That’s it. \\n No mysticism. \\n Just biology: embodiment over intellect, present signals over future plans, emotional repetition over one-off epiphanies.</p><h3><strong>Why the way you were taught violates those laws</strong></h3><p>You weren’t taught wrong on purpose. \\n The linear model: learn → understand → act → become—makes sense on paper. \\n It’s how we’re wired to think: cause leads to effect, effort over time equals results.</p><p>But here’s the fracture: That system is incompatible with how identity&nbsp;&nbsp;updates, because it operates in future-tense logic, while your nervous system is locked in&nbsp;.</p><p>Linear change assumes: \\n – Identity updates gradually, after enough time and action. \\n – You “work toward” a future version of yourself. \\n – The gap closes “eventually,” once you’ve earned it.</p><p>But biology doesn’t work that way. \\n Your nervous system doesn’t live in timelines. \\n It doesn’t understand “one day.” \\n It only registers what’s safe and familiar.</p><p>So when you tell yourself: \\n “I’ll be confident later.” \\n “I’ll act like that person when I get there.” \\n “I’m not that version yet.”</p><p>You’re signaling: “We’re still the old identity. No need to change.” \\n And it listens. \\n Cortisol stays up. \\n The brakes stay on. \\n The gap widens.</p><p>That’s why respecting linear time keeps you trapped. \\n Not because time is the enemy. \\n But because your biology wasn’t designed to evolve that way.</p><p>This should now answer your question:&nbsp;<strong>If you already know who you want to become… why haven’t you become them yet?</strong></p><h3><strong>How we actually need to treat change</strong></h3><p>If the laws demand present-tense embodiment, we need a system that delivers it. \\n Not gradual effort toward a distant future. \\n Not more information stacked on old identity files.</p><p>We need to hack the nervous system into believing the new you is already real—now. \\n Through vivid, emotional rehearsal that blurs imagined and lived. \\n Through small, aligned actions that reinforce familiarity in the present. \\n Through evidence that confirms the shift is happening, not waiting to happen.</p><p>That’s not a tactic. \\n It’s alignment with biology.</p><p>And ignoring biology is exactly why you're still not living the life you want right now.</p><p>\\n This is the &nbsp;way to collapse the gap without wasting years on “progress” that never sticks.</p><h2><strong>The Protocol I promised you</strong></h2><p>I didn’t set out to invent anything fancy. I was just exhausted from knowing everything and changing nothing. I was the guy who could explain every pattern, name every mechanism, map the perfect future… and still wake up in the same smaller life every day.</p><p>Once I understood the laws, there were only two options: keep suffering intelligently…or build a system that forced embodiment:</p><p>Present-tense embodiment. \\n Emotional rehearsal. \\n Evidence that the shift is real.</p><p>For years, nothing moved. Under 8k followers total. Newsletter stuck below 900. Random book sales. No momentum.</p><p>Within 2 months of using the protocol:</p><ul><li>100k followers across platforms</li><li>25k newsletter subscribers</li><li>Publishers reaching out instead of me chasing.</li></ul><p>I’m saying it because it’s the only way to prove this isn’t theory for me.&nbsp;<strong>This is what happened when I finally gave my nervous system present-tense evidence instead of future promises.</strong></p><p>Everything circled back to those two hills I opened with.</p><ul><li>Most people don’t need therapy. They need to hear themselves think, and writing is how you do that.</li><li>The smartest, most self-aware people are usually the most trapped…because awareness without embodiment is just sophisticated suffering.</li></ul><p>I’d spent years in sophisticated suffering that you are currently experiencing: Explaining, understanding, naming every mechanism, but never embodying.</p><p>Then I built something that forced embodiment. \\n And the suffering ended. \\n The gap collapsed.</p><p>That something is the&nbsp;, because you log both identity (the future you) and evidence (real-world confirmation), like double entry accounting, but for reality.</p><p><strong>It's a daily protocol that places you between the present and the future, then forces you to pull both toward the center.</strong>&nbsp;On paper. Every day. In under 20 minutes.</p><p>: time collapses, the gap disappears, and your nervous system has no choice but to make the new you real now.</p><p>Here’s why it aligns with the biology we just covered (not the full how-to; that’s in the videos):</p><p>It doesn’t ask you to wait for linear time. \\n It forces the nervous system to treat the future identity as real .</p><p>One side of the page creates vivid, emotional rehearsal. The kind that activates the same neural pathways as lived experience, building familiarity and safety without physical action.</p><p>The other side grounds it immediately: small, present-tense actions that reinforce the new identity&nbsp;, not later.</p><p>A third layer stacks real-world evidence throughout the day, training the reticular activating system (the RAS) to filter for confirmation instead of threat.</p><p>Together, they do exactly what the laws demand:</p><ul><li>Repeated emotional embodiment.</li><li>No reliance on future promises or gradual progress.</li></ul><p>The result: the nervous system stops bracing, the brakes come off, and the gap starts collapsing; fast enough that it can feel suspicious to everyone still stuck in linear time:</p><ul><li>Synchronicities start showing up uninvited.</li><li>Conversations align without forcing them.</li><li>Opportunities land in your inbox like they were waiting for you to finally believe they could.</li><li>Results arrive with almost no effort, as if the universe is just catching up to the identity you’ve already wired in.</li></ul><p>But I have to warn you about one thing that will happen, and if I don’t tell you, this whole thing won’t work..</p><p>The moment progress starts moving faster than you thought possible… you will feel&nbsp;.&nbsp;&nbsp;Like you didn’t “earn” it. Like it was&nbsp;.</p><p>That guilt is the ghost of linear time (The conditioning that says good things must take years of pain).</p><p>Don’t let it run the show. \\n It’s not proof you’re wrong. \\n <strong>It’s proof you’ve broken free</strong>.</p><p>This protocol isn’t for everyone. \\n It’s for the self-aware overthinkers tired of knowing and not becoming. \\n The ones who can name every pattern but can’t break them. \\n The ones ready to hack biology instead of fighting it.</p><p>I put together videos and documents so you can start this habit immediately. You'll get exact walkthroughs, concrete examples of a full day in my life using the protocol, and everything laid out so you can literally copy-paste and execute.</p><p><em>This is the thing that finally closes the gap you've felt for years.</em></p><p>Because I'm not here to keep you small.</p><p>See you on the other side.</p>","contentLength":11649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatGPT reaches 900M weekly active users","url":"https://techcrunch.com/2026/02/27/chatgpt-reaches-900m-weekly-active-users/","date":1772216751,"author":"Aisha Malik","guid":154635,"unread":true,"content":"<article>OpenAI shared the new numbers as part of its announcement that it has raised $110 billion in private funding.</article>","contentLength":109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Chinese Official's Use of ChatGPT Accidentally Revealed a Global Intimidation Operation","url":"https://slashdot.org/story/26/02/27/185250/a-chinese-officials-use-of-chatgpt-accidentally-revealed-a-global-intimidation-operation?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772215380,"author":"msmash","guid":154639,"unread":true,"content":"A sprawling Chinese influence operation -- accidentally revealed by a Chinese law enforcement official's use of ChatGPT -- focused on intimidating Chinese dissidents abroad, including by impersonating US immigration officials, according to a new report from ChatGPT-maker OpenAI. From a report: The Chinese law enforcement official used ChatGPT like a diary to document the alleged covert campaign of suppression, OpenAI said. In one instance, Chinese operators allegedly disguised themselves as US immigration officials to warn a US-based Chinese dissident that their public statements had supposedly broken the law, according to the ChatGPT user. In another case, they describe an effort to use forged documents from a US county court to try to get a Chinese dissident's social media account taken down. \n\nThe report offers one of the most vivid examples yet of how authoritarian regimes can use AI tools to document their censorship efforts. The influence operation appeared to involve hundreds of Chinese operators and thousands of fake online accounts on various social media platforms, according to OpenAI.","contentLength":1112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Metacritic Will Kick Out Media Attempting To Submit AI Generated Reviews","url":"https://games.slashdot.org/story/26/02/27/1732218/metacritic-will-kick-out-media-attempting-to-submit-ai-generated-reviews?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772213520,"author":"msmash","guid":154606,"unread":true,"content":"An anonymous reader shares a report: While some see AI as a tool to be used, its specific use and how it is deployed responsibly is being heavily debated online across a wide range of industries. In terms of journalistic content, and in this particular instance, reviews, review aggregator Metacritic has taken a firm stance on content published and submitted to their platform, that have been generated by artificial intelligence in some way. \n\nIn a statement by co-founder Marc Doyle, sent to Gamereactor, he says this: \"Metacritic has been a reputable review source for a quarter century and has maintained a rigorous vetting process when adding new publications to our slate of critics. However, in certain instances such as a publication being sold or a writing staff having turned over, problems can arise such as plagiarism, theft, or other forms of fraud including AI-generated reviews. Metacritic's policy is to never include an AI-generated critic review on Metacritic and if we discover that one has been posted, we'll remove it immediately and sever ties with that publication indefinitely pending a thorough investigation.\" \n\nSo, what is this about specifically? Well, it's probably a sound guess, that this pertains to Videogamer's review of Resident Evil 9: Requiem, which was removed from the platform after a barrage of comments accusing the review of being AI-written, and for the author of being made up.","contentLength":1423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI music generator Suno hits 2M paid subscribers and $300M in annual recurring revenue","url":"https://techcrunch.com/2026/02/27/ai-music-generator-suno-hits-2-million-paid-subscribers-and-300m-in-annual-recurring-revenue/","date":1772212922,"author":"Amanda Silberling","guid":154591,"unread":true,"content":"<article>Suno lets users create music using natural language prompts, making it possible for people with little experience to generate audio with little effort.</article>","contentLength":151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple and Netflix team up to air Formula 1 Canadian Grand Prix","url":"https://techcrunch.com/2026/02/27/apple-and-netflix-team-up-to-air-formula-1-canadian-grand-prix/","date":1772212638,"author":"Lauren Forristal","guid":154590,"unread":true,"content":"<article>As Netflix continues its live sports push, the company has partnered with Apple to air the Formula 1 Canadian Grand Prix.</article>","contentLength":121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The New Gold Standard - Why Imperfection Wins","url":"https://hackernoon.com/the-new-gold-standard-why-imperfection-wins?source=rss","date":1772212542,"author":"Editing Protocol","guid":154803,"unread":true,"content":"<p>Earlier this week in our HackerNoon editorial meeting, we noticed an interesting pattern. We were reviewing the top-performing stories from the past two weeks, and couldn’t help but notice an interesting trend.</p><p>\\\nAre you seeing what we’re seeing? Here’s a hint—the leaderboard was full of specific, lived experiences. We’ve got an artist talking about making digital art in the age of AI; an observation on why people were panic-buying Mac minis; a PSA on online phishing scams; and a new, well-thought-out social media strategy.</p><p>\\\nThis sparked a realization that makes perfect sense in hindsight: <strong>Genuine human experience is the way to go.</strong></p><p>\\\nScroll through tech blogs today, and you’ll notice many similarities: clear, competent, and often a little interchangeable. With so much polished writing available, what tends to stand out now isn’t more shine, but specificity: the odd detail, the honest constraint, the lesson learned the hard way.</p><p>\\\nThe bar for clean, professional writing is higher than ever - which is a  thing, btw; but in today’s world, in order to differentiate, connect, and earn attention, concrete examples, candid tradeoffs, and seemingly all kinds of small imperfections that signal there’s a person behind the words are increasingly important.</p><p>Modern audiences have adapted quickly to the AI era. <a href=\"https://hackernoon.com/are-you-good-at-spotting-ai-generated-content-online\">A recent poll on HackerNoon</a> showed that around  of respondents are somewhat confident in their ability to spot AI-generated content.</p><p>\\\nOf course, this confidence is only based on a gut feeling when something reads a little too smoothly, or has what people call “AI giveaways”. However, this vigilance has created a fascinating authenticity paradox for writers and marketers. Sometimes, perfectly structured sentences and high vocabulary now trigger suspicion rather than establishing authority.​</p><p>\\\nWhen a reader encounters a paragraph that flows without any jagged edges, strong opinions, or a distinct voice, their brain files it away as generic content rather than meaningful communication. To prove you are genuinely human, you need to showcase the elements of your work that cannot be automated.</p><blockquote><p><strong>It’s the Human Proof of Work.</strong></p></blockquote><p>\\\nThis does not mean publishing sloppy or unedited drafts. It means leaning into specific details: acknowledging a slightly offbeat workflow, admitting that a product roadmap changed because a previous assumption was wrong, or sharing the messy reality of a project.​</p><h3>So, Where Do You Find These Genuine, Imperfect Ideas?</h3><p>The editorial ethos we often discuss at HackerNoon highlights that writers waste hours overthinking the perfect topic instead of simply documenting reality. Your next high-performing article is likely hiding in a recent Slack debate, a code deployment that required a hotfix, or a frustrating client call that got you thinking.​</p><p>\\\nBy tapping into these unfiltered moments of daily friction, you generate inherently unique content that AI cannot pull from a training dataset. When you share lessons from your own experiences, you bypass the reader's critical filter we mentioned above. You position your brand as a battle-tested guide who has actually navigated the practical challenges of the industry.</p><h2>Marketing and Retention Through Authentic Connection</h2><p>This pivot toward genuine human connection is more than just an editorial preference. Authenticity is rapidly becoming a primary driver of long-term business retention.​</p><p>\\\nMany B2B brands currently find themselves stuck in a marketing uncanny valley. They publish heavily optimized pieces that sound vaguely human but lack the emotional resonance required to actually connect with a reader. These articles might successfully attract search traffic and earn a click, but they rarely earn a loyal customer.</p><blockquote><p><strong>Retention in any business is built on trust, and trust requires relatability.</strong></p></blockquote><p>\\\nIt’s simple: customers stick around because they feel they are buying into a philosophy and a team they understand, rather than just renting a software tool.</p><p>\\\nTreating your content's humanity as a core asset is a valuable tip that should always be taken to your advantage. The willingness to be vulnerable and own your operational realities creates a deep customer affinity. This affinity directly translates into a higher customer retention.</p><blockquote><p><strong>In a digital environment populated by perfect machines, writing like a flawed human might just be the best business strategy you have.</strong></p></blockquote><h2>Want to take this further? (HackerNoon’s Blogging Course)</h2><p>HackerNoon’s Blogging Course is designed for beginners&nbsp;&nbsp;writers who’ve published a bit and want to level up. It’s organized into 8 modules created by experienced writers and editors, and it includes topics like:</p><p>Until next time, Hackers!</p>","contentLength":4709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity’s new Computer is another bet that users need many AI models","url":"https://techcrunch.com/2026/02/27/perplexitys-new-computer-is-another-bet-that-users-need-many-ai-models/","date":1772211655,"author":"Tim Fernholz","guid":154589,"unread":true,"content":"<article>Perplexity Computer, in the company’s words, \"unifies every current AI capability into a single system.\"&nbsp;</article>","contentLength":108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HackerNoon Projects of the Week: Get-Star, FinSight and CodeXero","url":"https://hackernoon.com/hackernoon-projects-of-the-week-get-star-finsight-and-codexero?source=rss","date":1772211602,"author":"Proof of Usefulness","guid":154605,"unread":true,"content":"<p>Welcome to the latest HackerNoon Projects of the Week installment. Each week, we shine a light on standout projects from our <a href=\"https://www.proofofusefulness.com/?ref=hackernoon.com\">Proof of Usefulness Hackathon</a>—a contest built around the core question every builder should answer: Is my product actually useful in the real world?</p><p>For each edition, we’ll highlight projects that demonstrate clear usefulness, technical execution, and real-world impact; all backed by data and not witty buzzwords.</p><p>This week, we’re excited to share three projects that have proven their utility by solving concrete problems for real users: <a href=\"https://hackernoon.com/get-star-earns-a-27-proof-of-usefulness-score-by-building-client-side-parallel-search\">Get-Star</a>, <a href=\"https://hackernoon.com/finsight-earns-a-55-proof-of-usefulness-score-by-building-an-ai-powered-finance-system-for-small-businesses\">FinSight</a>, and <a href=\"https://hackernoon.com/codexero-earns-a-348-proof-of-usefulness-score-by-building-a-vibe-coding-engine-for-web3-dapps\">CodeXero</a>.</p><h2>Meet the Projects of the Week</h2><p><a href=\"https://hackernoon.com/get-star-earns-a-27-proof-of-usefulness-score-by-building-client-side-parallel-search?embedable=true\">Get-Star</a> is building client-side parallel search infrastructure designed to improve speed and performance without relying heavily on centralized back-end computation. By distributing search execution closer to the user, the project aims to reduce latency, improve responsiveness, and create a more scalable search experience for modern web applications.</p><p>In a digital environment where milliseconds shape user perception, Get-Star focuses on performance as product value — giving developers a way to rethink how search is handled at the architectural level.</p><p><strong>Proof of Usefulness score: +27/1000</strong></p><p><a href=\"https://hackernoon.com/finsight-earns-a-55-proof-of-usefulness-score-by-building-an-ai-powered-finance-system-for-small-businesses?embedable=true\">FinSight</a> is an AI-powered financial management system built specifically for small businesses. It helps founders move beyond static spreadsheets by providing real-time insights, forecasting, and structured financial analysis in one unified platform.</p><p>Small business operators often lack the time or expertise to interpret financial signals clearly. FinSight positions itself as a decision-support engine — translating raw financial data into actionable clarity that can guide smarter planning and healthier cash flow management.</p><p><strong>Proof of Usefulness score: +55/1000</strong></p><p><a href=\"https://hackernoon.com/codexero-earns-a-348-proof-of-usefulness-score-by-building-a-vibe-coding-engine-for-web3-dapps?embedable=true\">CodeXero</a> is building a “vibe coding” engine for Web3 dApps — a system designed to accelerate decentralized application development by blending AI-assisted workflows with blockchain infrastructure.</p><p>With a significantly higher Proof of Usefulness score this week, CodeXero demonstrates strong traction in helping developers reduce friction when building smart contracts and Web3 interfaces. By simplifying complex blockchain logic into more intuitive development flows, CodeXero aims to make decentralized development faster, more accessible, and more iterative.</p><p><strong>Proof of Usefulness score: +348/1000</strong></p><h2>Stop Building in the Dark - Get Scored!</h2><p>The web is drowning in vaporware and empty promises. We created <a href=\"https://www.proofofusefulness.com/?ref=hackernoon.com\">Proof of Usefulness</a> to reward what actually matters: real user adoption, sustainable revenue, and technical stability. \\n </p><p> Get your Proof of Usefulness score (from -100 to +1000) the moment you submit. \\n  Compete for $20K in cash and $130K+ in software credits from&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;and&nbsp;. \\n <strong>3. Built-in Distribution:</strong> Your submission becomes a HackerNoon story, putting your build in front of millions of monthly readers. \\n  Every qualifying participant unlocks a suite of software credits just for entering.</p><p> Head to <a href=\"https://www.proofofusefulness.com/?ref=hackernoon.com\">www.proofofusefulness.com</a> and submit your project details to generate your PoU Report Card. \\n  Click the button on your report page to convert your submission into a HackerNoon blog post draft. \\n  Edit your draft to add your technical \"secret sauce,\" then hit Submit for Review. Once published, you’re officially in the prize queue! \\n </p><p>\\\n<strong>P.S. The clock is ticking!</strong> The second month of the competition is drawing to a close, meaning the next round of winners will be announced soon. With only 4 months and 4 prize rounds remaining, now is the time to get your project in the mix. Don't leave money on the table - get in early!</p><p>Thanks for building useful things! \\n P.S.&nbsp;Submissions roll monthly through June 2026. Get in early!</p>","contentLength":3736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pokémon Winds and Pokémon Waves are coming to the Nintendo Switch 2 in 2027","url":"https://techcrunch.com/2026/02/27/pokemon-winds-and-pokemon-waves-are-coming-to-the-nintendo-switch-2-in-2027/","date":1772211441,"author":"Amanda Silberling","guid":154588,"unread":true,"content":"<article>The 10th-generation starter Pokémon were revealed in the trailer: Browt (a grass bird), Pombon (a fire puppy), and Gecqua (a water gecko). </article>","contentLength":140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sam Altman Says OpenAI Shares Anthropic's Red Lines in Pentagon Fight","url":"https://slashdot.org/story/26/02/27/1530218/sam-altman-says-openai-shares-anthropics-red-lines-in-pentagon-fight?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772210400,"author":"msmash","guid":154575,"unread":true,"content":"An anonymous reader shares a report: OpenAI CEO Sam Altman wrote in a memo to staff that he will draw the same red lines that sparked a high-stakes fight between rival Anthropic and the Pentagon: no AI for mass surveillance or autonomous lethal weapons. If other leading firms like Google follow suit, this could massively complicate the Pentagon's efforts to replace Anthropic's Claude, which was the first model integrated into the military's most sensitive work. It would also be the first time the nation's top AI leaders have taken a collective stand about how the U.S. government can and can't use their technology. \n\nAltman made clear he still wants to strike a deal with the Pentagon that would allow ChatGPT to be used for sensitive military contexts. Despite the show of solidarity, such a deal could see OpenAI replace Anthropic if the Pentagon follows through with its plan to declare the latter a \"supply chain risk.\"","contentLength":930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Employees at Google and OpenAI support Anthropic’s Pentagon stand in open letter","url":"https://techcrunch.com/2026/02/27/employees-at-google-and-openai-support-anthropics-pentagon-stand-in-open-letter/","date":1772209438,"author":"Amanda Silberling","guid":154573,"unread":true,"content":"<article>While Anthropic has an existing partnership with the Pentagon, the AI company has remained firm that its technology not be used for mass domestic surveillance or fully autonomous weaponry.</article>","contentLength":188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: Lessons from Building a 100+ Agent Swarm in Web3 (2/27/2026)","url":"https://hackernoon.com/2-27-2026-newsletter?source=rss","date":1772208178,"author":"Noonification","guid":154604,"unread":true,"content":"<p>🪐 What’s happening in tech today, February 27, 2026?</p><p>By <a href=\"https://hackernoon.com/u/mattleads\">@mattleads</a> [ 11 Min read ] Master Symfony 7.4 logging: 10 advanced Monolog patterns. Use FingersCrossed, JSON  Attributes to turn text logs into actionable observability data <a href=\"https://hackernoon.com/symfony-74-10-advanced-logging-patterns-you-should-know-about\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/johnpphd\">@johnpphd</a> [ 4 Min read ] How precompiling context for AI agents beats context stuffing. Lessons from building 100+ specialized agents for a web3 application. <a href=\"https://hackernoon.com/lessons-from-building-a-100-agent-swarm-in-web3\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/benoitmalige\">@benoitmalige</a> [ 6 Min read ] Procrastination isnt laziness—its your brain dodging uncomfortable feelings like fear of failure, judgment, or misalignment. <a href=\"https://hackernoon.com/the-perfect-first-draft-trap-is-killing-your-output\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/nickzt\">@nickzt</a> [ 5 Min read ] Scaling AI for the real world requires peeling back the layers of abstraction weve gotten too comfortable with. <a href=\"https://hackernoon.com/python-is-a-video-latency-suicide-note-how-i-hit-29-fps-with-zero-copy-c-onnx\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ArunDHANARAJ_gfaknebg\">@ArunDHANARAJ_gfaknebg</a> [ 14 Min read ] Compare Claude Opus 4.6 and GPT‑5.3 Codex across reasoning, coding, benchmarks, pricing, and safety to guide enterprise AI and agentic workload decisions.\n\n <a href=\"https://hackernoon.com/claude-opus-46-and-gpt-53-codex-evaluating-the-new-leaders-in-ai-driven-software-engineering\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":1189,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symfony 7.4: 10 Advanced Logging Patterns You Should Know About","url":"https://hackernoon.com/symfony-74-10-advanced-logging-patterns-you-should-know-about?source=rss","date":1772208002,"author":"MattLeads","guid":154603,"unread":true,"content":"<p>Logging is the heartbeat of a production application. In the early days of a project, a simple&nbsp;&nbsp;tail is sufficient. But as your Symfony application scales to handle&nbsp;,&nbsp;&nbsp;and&nbsp;, “writing to a file” becomes a liability rather than an asset.</p><p>\\\nThe&nbsp;&nbsp;offers sophisticated tools to transform logs from simple text streams into structured, actionable observability data.</p><p>\\\nThis guide explores 10 advanced logging patterns that go beyond the defaults. We will use strict typing, PHP Attributes, and modern YAML configuration.</p><ul><li>&nbsp;(for alerting examples)</li></ul><h2>Scenation 1. The “Black Box” Recorder: FingersCrossed Handler</h2><p>You want detailed debug logs when an error occurs to understand the sequence of events leading up to it, but you can’t afford the disk I/O to log debug messages for every successful request in production.</p><p>\\\nThe&nbsp;&nbsp;buffers all logs in memory during the request. If the request finishes successfully, the buffer is discarded. If an error (or a specific threshold) is reached, the entire buffer (including previous debug logs) is flushed to the persistence handler.</p><p>config/packages/prod/monolog.yaml:</p><pre><code>monolog:\n    handlers:\n        main:\n            type: fingers_crossed\n            # The strategy: \"error\" means if an ERROR occurs, dump everything.\n            action_level: error\n            # Where to dump the logs if the threshold is met\n            handler: nested\n            # Optional: Keep a small buffer size to prevent memory leaks in long processes\n            buffer_size: 50\n        nested:\n            type: stream\n            path: \"%kernel.logs_dir%/%kernel.environment%.log\"\n            level: debug\n</code></pre><p>\\\nYou’ll get the forensic detail of debug-level logging exactly when you need it — during a crash — without filling your disk with noise during normal operations.</p><h2>Scenario 2. Segregated Channels: The “Payment” Log</h2><p>Your&nbsp;&nbsp;is a mix of Doctrine queries, router matching, and critical business logic. You need a dedicated file for financial transactions that can be audited separately.</p><p>\\\nCreate a custom Monolog Channel.</p><p>config/packages/monolog.yaml:</p><pre><code>monolog:\n    channels: ['payment'] # Register the channel\n\n    handlers:\n        payment:\n            type: stream\n            path: \"%kernel.logs_dir%/payment.log\"\n            level: info\n            channels: [\"payment\"] # Only listen to this channel\n\n        main:\n            type: stream\n            path: \"%kernel.logs_dir%/%kernel.environment%.log\"\n            level: debug\n            channels: [\"!payment\"] # Exclude payment logs from the main file\n</code></pre><p>Inject the logger specifically for this channel using the Target attribute (available since Symfony 5.3+).</p><pre><code>namespace App\\Command;\n\nuse Psr\\Log\\LoggerInterface;\nuse Symfony\\Component\\Console\\Attribute\\AsCommand;\nuse Symfony\\Component\\Console\\Command\\Command;\nuse Symfony\\Component\\Console\\Input\\InputInterface;\nuse Symfony\\Component\\Console\\Output\\OutputInterface;\nuse Symfony\\Component\\DependencyInjection\\Attribute\\Target;\n\n#[AsCommand(name: 'app:process-payments', description: 'Processes pending payments')]\nclass ProcessPaymentsCommand extends Command\n{\n    public function __construct(\n        #[Target('payment.logger')]\n        private readonly LoggerInterface $paymentLogger,\n        private readonly LoggerInterface $mainLogger\n    ) { parent::__construct(); }\n\n    protected function execute(InputInterface $input, OutputInterface $output): int\n    {\n        $this-&gt;mainLogger-&gt;info('Cron job app:process-payments started.');\n\n        $amounts = [10.50, 99.99, 45.00];\n        foreach ($amounts as $amount) {\n            $this-&gt;paymentLogger-&gt;info('Processing payment', ['amount' =&gt; $amount, 'status' =&gt; 'success']);\n        }\n\n        $this-&gt;mainLogger-&gt;info('Cron job finished.');\n        return Command::SUCCESS;\n    }\n}\n</code></pre><p>Run the Command. You will see&nbsp;&nbsp;created in&nbsp;&nbsp;containing only these specific entries.</p><h2>Scenario 3. Context Enrichment: The #[AsMonologProcessor] Attribute</h2><p>Logs are useless if you can’t correlate them to a specific user or request ID. You find yourself manually adding [‘user_id’ =&gt; $user-&gt;getId()] to every single log statement.</p><p>\\\nA global Processor can automatically inject context into every log record.</p><pre><code>namespace App\\Log;\n\nuse Monolog\\Attribute\\AsMonologProcessor;\nuse Monolog\\LogRecord;\n\n#[AsMonologProcessor]\nclass RequestContextProcessor\n{\n    public function __invoke(LogRecord $record): LogRecord\n    {\n        // Simulated context since CLI commands don't have HTTP Requests\n        $extra = [\n            'pid' =&gt; getmypid(),\n            'user' =&gt; get_current_user(),\n        ];\n\n        return $record-&gt;with(extra: array_merge($record-&gt;extra, $extra));\n    }\n}\n</code></pre><p>In&nbsp;,&nbsp;. We use&nbsp;&nbsp;to return a modified copy.</p><p>A developer accidentally logs a user object, dumping PII (Personally Identifiable Information) or credit card numbers into the logs, violating GDPR/PCI-DSS.</p><p>\\\nA specialized processor can scans the context array and mask sensitive keys.</p><pre><code>namespace App\\Log;\n\nuse Monolog\\Attribute\\AsMonologProcessor;\nuse Monolog\\LogRecord;\n\n#[AsMonologProcessor]\nclass SensitiveDataProcessor\n{\n    private const array SENSITIVE_KEYS = ['password', 'credit_card', 'cvv', 'token'];\n\n    public function __invoke(LogRecord $record): LogRecord\n    {\n        $context = $record-&gt;context;\n\n        foreach ($context as $key =&gt; $value) {\n            if (in_array($key, self::SENSITIVE_KEYS, true)) {\n                $context[$key] = '***REDACTED***';\n            }\n        }\n\n        return $record-&gt;with(context: $context);\n    }\n}\n</code></pre><pre><code>$logger-&gt;info('User login', ['password' =&gt; 'secret123']);\n// Output in log: \"User login\" {\"password\": \"***REDACTED***\"}\n</code></pre><h2>Scenario 5. Structured Logging: JSON for ELK/Datadog</h2><p>Parsing multi-line text logs (like stack traces) in&nbsp;&nbsp;or&nbsp;&nbsp;is painful. Regex parsers break easily.</p><p>\\\nYou can output logs as JSON lines. This allows log aggregators to natively index fields like&nbsp;.</p><p>config/packages/monolog.yaml:</p><pre><code>monolog:\n    handlers:\n        json_report:\n            type: stream\n            path: \"%kernel.logs_dir%/app.json\"\n            level: info\n            formatter: monolog.formatter.json\n            channels: [\"!payment\", \"!event\"]\n</code></pre><p>\\\nOpen&nbsp;. The output should look like:</p><pre><code>{\"message\":\"Order created\",\"context\":{\"id\":123},\"level\":200,\"channel\":\"app\",\"datetime\":\"...\"}\n</code></pre><h2>Scenario 6. Spam Prevention: The Deduplication Handler</h2><p>Your database goes down. Your application receives&nbsp;&nbsp;in a minute. Your “Email on Error” handler sends you&nbsp;, getting your&nbsp;<strong>SMTP server blacklisted and flooding your inbox</strong>.</p><p>\\\nThe&nbsp;&nbsp;can&nbsp;aggregate identical log records and send a single summary.</p><p>config/packages/monolog.yaml:</p><pre><code>monolog:\n    handlers:\n        deduplication:\n            type: deduplication\n            handler: nested_dedup\n            buffer_size: 60\n            time: 60\n            level: error\n            channels: [\"!console\"]\n</code></pre><p>\\\nIf the DB crashes, you receive&nbsp;<strong>one email every 60 seconds</strong>&nbsp;listing all occurrences, rather than one email per request.</p><h2>Scenario 7. Dynamic Log Levels (Runtime Debugging)</h2><p>A specific customer is reporting an issue in production. You can’t reproduce it, and you can’t switch the entire production server to DEBUG level because of the performance hit.</p><p>\\\nUse an&nbsp;&nbsp;to switch the log level dynamically based on a request header.</p><p>Create a custom strategy:</p><pre><code>namespace App\\Command;\n\nuse Psr\\Log\\LoggerInterface;\nuse Symfony\\Component\\Console\\Attribute\\AsCommand;\nuse Symfony\\Component\\Console\\Command\\Command;\nuse Symfony\\Component\\Console\\Input\\InputInterface;\nuse Symfony\\Component\\Console\\Input\\InputOption;\nuse Symfony\\Component\\Console\\Output\\OutputInterface;\n\n#[AsCommand(name: 'app:dynamic-debug', description: 'Tests dynamic log level activation')]\nclass DynamicDebugCommand extends Command\n{\n    public function __construct(private readonly LoggerInterface $logger) {\n        parent::__construct();\n    }\n\n    protected function configure(): void\n    {\n        $this-&gt;addOption('force-debug', null, InputOption::VALUE_NONE, 'Force debug logging for this run');\n    }\n\n    protected function execute(InputInterface $input, OutputInterface $output): int\n    {\n        if ($input-&gt;getOption('force-debug')) {\n            $output-&gt;writeln('Debug mode forced via option. (Simulated, as Monolog ActivationStrategy relies on Http/Request state typically. But you can add processors/handlers dynamically in real apps based on this flag).');\n        }\n\n        $this-&gt;logger-&gt;debug('This detailed trace only appears if --force-debug is passed or an error occurs.');\n        $this-&gt;logger-&gt;info('Standard processing information.');\n\n        return Command::SUCCESS;\n    }\n}\n</code></pre><h2>Scenario 8. Messenger Logging: Worker Context</h2><p>Logs from&nbsp;&nbsp;are hard to trace. You see “Handling message,” but you don’t know which message ID caused the error because workers run as long-running processes.</p><p>\\\nUse&nbsp;&nbsp;to inject the Message ID into the Monolog context specifically for the worker process.</p><pre><code>namespace App\\EventListener;\n\nuse Psr\\Log\\LoggerInterface;\nuse Symfony\\Component\\EventDispatcher\\Attribute\\AsEventListener;\nuse Symfony\\Component\\Messenger\\Event\\WorkerMessageReceivedEvent;\n\nreadonly class WorkerLogContextListener\n{\n    public function __construct(private LoggerInterface $logger) {}\n\n    #[AsEventListener]\n    public function onMessageHandling(WorkerMessageReceivedEvent $event): void\n    {\n        $this-&gt;logger-&gt;info('Worker started message', [\n            'message_class' =&gt; $event-&gt;getEnvelope()-&gt;getMessage()::class,\n        ]);\n    }\n}\n</code></pre><h2>Scenario 9. Excluding 404s from Error Logs</h2><p>Bots scanning your site for&nbsp;&nbsp;or&nbsp;&nbsp;generate&nbsp;<strong>thousands of 404 NotFoundHttpException logs</strong>. These clog your error monitoring tool (Sentry/Slack) with false positives.</p><p>\\\nUse the channels exclusion or a specific configuration to ignore bounced logs, or better - configure the&nbsp;&nbsp;to be ignored by the main error handler.</p><p>config/packages/monolog.yaml:</p><pre><code>monolog:\n    handlers:\n        fingers_crossed:\n            type: fingers_crossed\n            action_level: error\n            handler: nested\n            excluded_http_codes: [404, 405]\n            buffer_size: 50\n</code></pre><h2>Scenario 10. Notifier Bridge: ChatOps</h2><p>Email alerts are slow and often ignored. You want critical infrastructure failures to ping a&nbsp;&nbsp;channel immediately.</p><p>\\\nUse&nbsp;&nbsp;bridged with&nbsp;.</p><pre><code>composer require symfony/notifier symfony/slack-notifier\n</code></pre><p>config/packages/monolog.yaml:</p><pre><code>monolog:\n    handlers:\n        slack_alerts:\n            type: service\n            id: Symfony\\Bridge\\Monolog\\Handler\\NotifierHandler\n            level: critical\n</code></pre><p>\\\nThen configure the notifier chatter in&nbsp;<strong>config/packages/notifier.yaml</strong>&nbsp;and your DSN in&nbsp;.</p><pre><code>framework:\n    notifier:\n        chatter_transports:\n            slack: '%env(SLACK_DSN)%'\n        texter_transports:\n        channel_policy:\n            urgent: ['chat/slack']\n            high: ['chat/slack']\n            medium: ['chat/slack']\n            low: ['chat/slack']\n        admin_recipients:\n            - { email: admin@example.com }\n</code></pre><p>\\\n&nbsp;maps log levels to Notifier importance. A critical log becomes a&nbsp;<strong>High Priority Slack notification automatically</strong>.</p><p>Logging is not a byproduct of code - it is a feature of your infrastructure.</p><p>\\\nIn a junior developer’s mindset, logging is a safety net — something to check only when things break. But as you scale to Senior and Lead roles, your perspective must shift. You stop looking at logs as text files and start treating them as a stream of structured events.</p><p>\\\nBy moving to Symfony 7.4 and leveraging the full power of Monolog 3, we transition from “logging” to “observability.”</p><p>\\\n&nbsp;turns your logs into a queryable database.</p><p>\\\n&nbsp;handlers solve the “signal-to-noise” ratio, saving you gigabytes of storage while preserving critical context.</p><p>\\\n&nbsp;ensuring every log entry carries the DNA of the request (User ID, Request ID) turn hours of debugging into minutes of verification.</p><p>\\\n&nbsp;protects your inbox and your sanity.</p><p>\\\nImplementation of these patterns distinguishes a fragile application from a robust, enterprise-grade system. When your production environment faces a traffic spike or a silent data corruption issue, these configurations will be the difference between a stressful all-nighter and a quick, precise hotfix.</p><p>If you found this helpful or have questions about the implementation, I’d love to hear from you. Let’s stay in touch and keep the conversation going across these platforms:</p>","contentLength":12321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Netflix Ditches deal for Warner Bros. Discovery After Paramount's Offer is Deemed Superior","url":"https://entertainment.slashdot.org/story/26/02/27/1027259/netflix-ditches-deal-for-warner-bros-discovery-after-paramounts-offer-is-deemed-superior?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1772208000,"author":"msmash","guid":154574,"unread":true,"content":"Netflix is walking away from a deal to buy Warner Bros. Discovery's studio and streaming assets after the WBD board on Thursday deemed a revised bid by Paramount Skydance to be a superior offer. From a report: Earlier this week, Paramount raised its bid to buy the entirety of WBD to $31 per share, up from $30 per share, all cash. It was the latest amendment to Paramount's multiple offers in recent months -- and since moving forward with a hostile bid to buy the company -- and it's now unseated a deal between WBD and Netflix to sell the legacy media company's studio and streaming businesses for $27.75 per share. \n\nLast week, Netflix granted WBD a seven-day waiver to reengage with Paramount, resulting in the higher bid. Paramount's offer is for the entirety of WBD, including its pay-TV networks, such as CNN, TBS and TNT. Netflix had four business days to make changes to its own proposal in light of Paramount's superior bid, the WBD board said in a statement Thursday. Instead, the decision by the streaming giant to walk away puts a pin in a drawn-out saga that saw amended offers from both bidders.","contentLength":1111,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CISA replaces acting director after a bumbling year on the job","url":"https://techcrunch.com/2026/02/27/cisa-replaces-acting-director-gottumukkala-after-a-bumbling-year-on-the-job/","date":1772207822,"author":"Zack Whittaker","guid":154572,"unread":true,"content":"<article>The U.S. cybersecurity agency's acting director Madhu Gottumukkala will be replaced, after a year of cuts, layoffs, and staff reassignments, and allegations of security lapses and claims he struggled to lead the agency.</article>","contentLength":219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["tech"]}