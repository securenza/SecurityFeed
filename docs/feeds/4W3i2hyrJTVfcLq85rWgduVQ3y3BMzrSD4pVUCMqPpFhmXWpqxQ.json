{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":16,"items":[{"title":"LinkedIn is the worst social media I've ever seen","url":"https://news.ycombinator.com/item?id=43052409","date":1739563136,"author":"bitreducer","guid":219,"unread":true,"content":"Besides being full of AI-generated, useless content, the platform is also riddled with dark patterns.<p>I'm a newcomer, I’d never used LinkedIn before until I joined three months ago. Since then, I've been banned twice and shadow-banned several times.</p><p>1) Right after I created my account, I got shadow-banned. My friends couldn’t find me, and my profile wasn’t accessible via direct link. This issue wasn’t resolved until I contacted support; they confirmed that restrictions had been placed on my account and then removed them.</p><p>2) I shouldn’t even have to mention this, but as a professional in my field, after that incident I always ended up on the very last pages of search results for keywords related to my field. People without those keywords in their resumes, or whose work isn’t at all related to the field, kept being ranked ahead of me. I can’t say I was deliberately pessimized, but that’s exactly how it appears.</p><p>3) Some time later, I decided to get Premium and paid with my own card (issued in another country, with the same name as on my profile and my passport). That’s when I received the first restriction: I was locked out of my account and couldn’t sign in until I contacted support. They made me take a photo of myself and of my passport, and after that they lifted the restrictions and apologized.</p><p>4) Everything seemed to be going along normally, yet I remained stuck on the last pages of search, being outranked by completely irrelevant profiles, until I decided to write a post about my own article on Medium. Immediately after publishing the post with a link to Medium, I received a second restriction. I was locked out of my account, couldn’t access my messages or interview invitations, and even missed a call because of this. They once again required me to submit a photo of myself along with a copy of my passport. Although they eventually restored my account, this time the process took 4–5 days, which caused me to miss an interview.</p><p>5) Now my profile is accessible via direct link, but I’ve disappeared from search. No one can find me, and hardly anyone visits my page. My post was hidden while support was \"sorting out\" my account, and I lost all the potential post views it would have gained because it wasn't featured in the recommendations.</p><p>All of this happened within the first three months of using LinkedIn. I’ve never seen a more appalling social media, one so full of dark patterns and outright abuse towards its users, forcing them through humiliating identity verification processes and hiding them from search.</p><p>Needless to say, I have never violated any of the platform’s rules. I don’t spam, I don’t bother people, and I don’t advertise anything. Meanwhile, my friend, whose account is over five years old—can do whatever he wants. He uses VPNs, changes his profile location several times a week, and switches his VPN location from Dubai to Europe multiple times a day without ever facing any restrictions.</p><p>And yes, I completely forgot to add: when your account gets restricted, it doesn’t matter whether you’re a premium user or not — the treatment is equally poor and the response is equally slow.</p><p>I’ve never seen a worse social media, and I’d be thrilled if a worthy competitor to LinkedIn ever emerged, I’d be one of the first to join.</p><p>BONUS: This content was originally posted on reddit, quickly became popular in the LinkedIn subreddit and was then deleted by moderator who works for M$ (which owns LinkedIn). Frankly, it feels like the whole LinkedIn subreddit is highly censored by him.</p>","contentLength":3575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43052409"},{"title":"Delphi Is 30","url":"https://blog.marcocantu.com/blog/2025-february-delphi-is-30.html","date":1739558515,"author":"andsoitis","guid":218,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43051598"},{"title":"A study on how turtles navigate using the Earth’s magnetic field","url":"https://www.unc.edu/posts/2025/02/12/dancing-turtles-unlock-scientific-discovery/","date":1739557799,"author":"gmays","guid":217,"unread":true,"content":"<p>When captive loggerhead sea turtles anticipate food, they do a little dance. The turtles raise their heads out of water, open their mouths, flap their flippers and spin in circles.</p><p>Not only is this “turtle dancing behavior” cute, it was also the key to unlocking a significant scientific discovery by researchers at UNC-Chapel Hill.</p><p>The prestigious international science journal Nature published the study Feb. 12. Kayla Goforth, a recent doctoral graduate in the College of Arts and Sciences’ biology department, is the lead author of “Learned magnetic map cues and two mechanisms of magnetoreception in turtles.”</p><p>Early on in her graduate studies at Carolina, Goforth was fascinated by sea turtles returning to the same feeding sites again and again throughout their lifetimes — despite traveling up to 10,000 miles across the planet. She hypothesized that these turtles used Earth’s magnetic field to memorize specific geographic areas that they associated with food, and she devised an experiment in the biology department’s <a href=\"https://lohmannlab.web.unc.edu/\">Lohmann Lab</a> — run by husband-and-wife duo Kenneth and Catherine Lohmann.</p><p>“Kayla began to wonder if we could get the turtles to associate the magnetic signature of a geographic area with food — and therefore act out this turtle dance behavior,” said Kenneth Lohmann, the Charles P. Postelle Jr. Distinguished Professor of Biology. “She really took the lead in this. I wasn’t at all sure in the beginning whether it would work, but we were happy to have her try, and it turned out remarkably well.”</p><div><img fetchpriority=\"high\" decoding=\"async\" aria-describedby=\"caption-attachment-252848\" src=\"https://www.unc.edu/wp-content/uploads/2025/02/Untitled-design9.gif\" alt=\"Short, soundless video of a small sea turtle flapping around in water.\" width=\"800\" height=\"533\"><p>The team of Carolina researchers analyzed how sea turtles use Earth’s magnetic field to guide their long-distance migrations across the ocean, and how the turtles sense magnetic fields.</p></div><p>Goforth and the rest of the all-Carolina team conditioned loggerhead turtles to magnetic fields replicating those that exist in various oceanic locations, repeatedly feeding the turtles in some magnetic fields while not feeding them in others. When later exposed to the fields in which they were fed, the turtles exhibited “turtle dancing behavior,” indicating that they associated that specific magnetic signature with food.</p><p>“That’s how we figured out what to measure, just by watching them do this as we were feeding them in the lab,” said Goforth, who is now conducting postdoctoral research at Texas A&amp;M University. “When they get really excited, they stick their heads out; they slap their flippers wildly; they spin in place sometimes.”</p><p>The assay suggests that loggerhead sea turtles can learn the magnetic signatures of geographical areas — much like dropping a pin in a GPS app.</p><div><img decoding=\"async\" aria-describedby=\"caption-attachment-252840\" src=\"https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-225x300.jpeg\" alt=\"Aerial image of many sea turtles on a beach.\" width=\"225\" height=\"300\" srcset=\"https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-225x300.jpeg 225w, https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-768x1024.jpeg 768w, https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-1152x1536.jpeg 1152w, https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-1536x2048.jpeg 1536w, https://www.unc.edu/wp-content/uploads/2025/02/HatchlingLoggerheadMultitudes-scaled.jpeg 1920w\" sizes=\"(max-width: 225px) 100vw, 225px\"><p>The groundbreaking study was published in the prestigious international science journal Nature. (Ken Lohmann/biology department)</p></div><p>When turtles use magnetic signatures, they are using their magnetic map sense, but they also have a magnetic compass sense that enables them to move north, south or some other direction. The research further showed that the map and compass senses detect magnetic fields in different ways.</p><p>Working in collaboration with the College’s physics and astronomy department, the team investigated the effects of radiofrequency oscillating magnetic fields on the turtles’ magnetic senses. What the team found surprised them. The oscillating magnetic fields had no impact on the turtles’ magnetic map ability but did disrupt the turtles’ ability to use the magnetic compass and orient themselves.</p><p>“It suggested there are two different mechanisms for the magnetic map and compass,” Goforth said. “And they might have evolved separately.”</p><p>Exactly how turtles and other migratory species use these senses is a question that has baffled scientists for more than 50 years, and it’s one that Goforth continues to research at Texas A&amp;M, now studying monarch butterflies instead of sea turtles.</p><p>“The big question is always the mechanism,” Goforth said. “What’s the mechanism and what is the sensor? We know that for the visual sense, you have eyes; for the sense of smell, you have a nose; and for hearing, you have ears. But no receptor like that has been identified for the magnetic sense, and the mechanism remains unknown.”</p><div><img decoding=\"async\" aria-describedby=\"caption-attachment-252838\" src=\"https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-1024x576.jpg\" alt=\"Hand holding a tiny sea turtle.\" width=\"1024\" height=\"576\" srcset=\"https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-1024x576.jpg 1024w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-300x169.jpg 300w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-768x432.jpg 768w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-1536x864.jpg 1536w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-1200x675.jpg 1200w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-800x450.jpg 800w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-600x338.jpg 600w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-550x310.jpg 550w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-450x253.jpg 450w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-350x196.jpg 350w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-250x141.jpg 250w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal-200x112.jpg 200w, https://www.unc.edu/wp-content/uploads/2025/02/TurtleEmbedFinal.jpg 1600w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"><p>(Ken Lohmann/biology department)</p></div>","contentLength":4230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43051465"},{"title":"Detecting AI agent use and abuse","url":"https://stytch.com/blog/detecting-ai-agent-use-abuse/","date":1739549910,"author":"mattmarcus","guid":216,"unread":true,"content":"<p>AI agents aren’t just indexing your content anymore. With tools like OpenAI’s Operator, Anthropic’s Computer Use API, and BrowserBase’s Open Operator, these agents can navigate the web, mimic real users, and even take actions at scale. The challenge? Knowing whether they’re enhancing your user experience—or opening the door to abuse.</p><p>In some scenarios, apps might encourage agent use if it improves usability and adoption, but in other cases, it could present unacceptable risks for application developers or be used as a method for malicious attacks (e.g. credential stuffing or fake account creation).</p><p>In either scenario, observability is paramount. Applications need to know what traffic is on their site (is this a human? A bot? A good bot or a bad one?) in order to make intelligent decisions about how to shape traffic and enforce desired usage patterns. AI agents add an additional wrinkle as users are already sharing their credentials with tools like Operator, meaning even a well-intentioned agent creates potential risk for these applications and their users.</p><p>The key question is: <strong>Can you detect AI agent traffic on your application today?</strong></p><p>We tested multiple AI agent toolkits across high-traffic consumer sites, and the results were clear—legacy detection techniques (CAPTCHAs, IP blocking, user-agent filtering) are largely ineffective. Here’s what we found.</p><h2>The New Reality of Bot Traffic:What AI Agent Traffic Looks Like</h2><p>Traditionally, bot detection relied on CAPTCHAs, IP blocking, and user-agent filtering. But modern AI agents are engineered to look like actual users:</p><ul><li> They use genuine IP addresses, user agents, and even simulate mouse movements.</li><li><strong>Headless browsing at human speeds:</strong> Their interactions mimic natural browsing behavior, evading rate-limit triggers.</li><li><strong>Datacenter origins—but not always:</strong> While some (like OpenAI’s Operator) come from known Azure datacenters, others (like Anthropic’s API) can run locally, borrowing your machine’s properties.</li></ul><p>On the surface level, AI agent traffic can look quite similar to regular human user traffic. Here are examples of browser &amp; network properties across different AI agents:</p><table><thead><tr><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody><tr><td><div variant=\"listItem\"><p>Chrome on Linux (standard fingerprint)</p></div></td><td><div variant=\"listItem\"><p>Firefox on Ubuntu (stable, unless run locally)</p></div></td><td><div variant=\"listItem\"><p>Chrome on Mac (with stealth features)</p></div></td></tr><tr><td><div variant=\"listItem\"><p>Known Azure Datacenter IP</p></div></td><td><div variant=\"listItem\"><p>Varies—depends on local vs. cloud deployment</p></div></td></tr><tr></tr><tr><td><div variant=\"listItem\"><p>San Francisco, California</p></div></td></tr><tr></tr><tr></tr></tbody></table><p>Both OpenAI’s Operator and BrowserBase’s Open Operator spin up <strong>remote datacenter or container‑hosted</strong> Chromium instances, rather than installing any software on your local machine. Because these solutions originate from a cloud‑hosted environment (with IP addresses and other signatures tied to the provider’s infrastructure), they can be  via methods like ASN lookups if the provider’s IP ranges are well known.</p><p>By contrast, Anthropic’s “Computer Use” functionality is available only through its API as of now, and you can choose how to run that API (e.g., directly on your own machine vs. inside a VM or container). If you run it locally, it inherits your local system’s properties like IP and ASN; if you host it in the cloud, it uses whichever environment that provider offers. Each approach has its own implications for fingerprinting and detection.</p><p>Currently, Anthropic provides two options for using their agent:</p><ol><li>Direct use of your computer environment (not recommended due to security concerns around unrestricted access).</li><li><a rel=\"noreferrer noopener\" href=\"https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo\">Setting up a virtual machine or container</a> with minimal privileges to run the API. You can either run this locally or deploy to a cloud provider using Docker, in which case it won’t inherit the user’s network properties.</li></ol><p>Whereas stricter sites like Reddit block OpenAI and BrowserBase outright, Anthropic’s approach allows it to successfully bypass even these strict sites when run locally:</p><p>And with tools like BrowserBase, there are also now open-sourced options for building browsing AI agents that allow potential attackers to increase the stealth of their headless browsing setup. This means we should expect continued, quick iteration on some agent use cases that will make them even more difficult to detect. As an example, BrowserBase offers premium plans with more advanced stealth to bypass CAPTCHA and other detection techniques:</p><p>Today, most sites are allowing agent traffic to navigate freely – whether these sites are detecting the abnormalities of this traffic (and choosing not to take action) is hard to say. For some like Reddit &amp; Youtube, their block on OpenAI Operator indicates they want to lock down agentic traffic. When these AI Agents bypass their restrictions (as detailed below), it’s a good signal those tools are actually flying below their radar today versus being officially sanctioned.</p><p>For others that allow this traffic, they may actually encourage it for user convenience or potentially have perverse incentives in some cases to allow a certain amount of bot traffic (Ticketmaster is a potential example of this given they can benefit from some level of bot activity increasing ticket purchase rates). Many more sites, however, simply don’t have the traffic intelligence to detect these AI agents, which explains why they can operate freely.</p><p>In testing the three primary AI Agent browsing toolkits on a set of high traffic consumer sites, we found that Youtube and Reddit were the only ones that consistently blocked this traffic:</p><table><thead><tr><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody></tbody></table><p>Few websites are blocking popular AI Agents today – indicating either a lack of detection, ambivalence towards enforcement, or both. Reddit &amp; Youtube are a couple of key outliers that block this traffic when they can discern it.</p><p>Still, the differences are interesting. It’s worth understanding why Anthropic or BrowserBase are sometimes able to bypass sites that otherwise are blocking this traffic. In BrowserBase’s case, they generate a slightly different user-agent each session, which sometimes aligns with the underlying chromium runtime but sometimes attempts to be deceptive by claiming to be a different version, which leads to more inconsistency in whether companies detect it correctly.</p><p>Companies like LinkedIn and X are particularly sensitive to scraping on their sites, so the lack of enforcement is surprising. This suggests they’re not currently able to confidently discern this is programmatic traffic.</p><h2>How to Tell an Agent from a Human</h2><p>The obvious question here is: <strong>shouldn't this be easy to detect by using just an IP or user agent?</strong></p><p>And the answer is yes and no. For OpenAI Operator, it’s pretty straightforward to detect given its stability &amp; origin point. For Anthropic and BrowserBase (and the new agents entering the market), it’s a bit more complicated.</p><table><thead><tr><th><h4>Anthropic Computer Use API</h4></th><th><h4>BrowserBase Open Operator</h4></th></tr></thead><tbody><tr></tr><tr><td><div variant=\"listItem\"><p>Depends on how you run it:</p><ul><li>Unlike Operator, you can run it locally directly on your machine or (recommended) on a dockerized container. In this case, it inherits your own device’s IP &amp; characteristics</li><li>In production, you’d likely host it in AWS/Azure/etc and traffic would come from known data centers</li></ul></div></td></tr><tr></tr><tr></tr><tr><td><div variant=\"listItem\"><p>Offers options for automatically solving</p></div></td></tr></tbody></table><p>Some key elements worth knowing:</p><ul><li>OpenAI &amp; BrowserBase both operate on top of chromium – being able to discern chromium browsers vs. true chrome is key for detecting the latter since BrowserBase modifies things like IP, user agent, etc for increased stealth.</li><li>OpenAI originates out of a known Azure datacenter while BrowserBase originates from a known AWS datacenter.</li><li>OpenAI hands off non-invisible CAPTCHAs to the end user to complete, while BrowserBase offers options for automatically solving.</li></ul><p>Ideally, you want to detect this traffic beyond more coarse identifiers like known datacenter IPs so that your detection is fully resistant to adjustments agentic traffic will make in the future. And core to that is having high confidence in whether a browser interacting with your site is a real browser or a headless instrumentation.</p><p>This is where we’ve personally found Machine Learning to be incredibly useful by building a browser lie detector. We look at a very wide range of browser signals &amp; we have downloaded the entire historical archives of every browser version that has been released that we can find and trained our model on what authentic browsers look like vs. anomalous or deceitful browsers. </p><p>To provide this visibility, we’ve automated additions of new legitimate browser signatures upon new releases, we fingerprint it, run it past the model to establish an authentic set of signals for the current browser version, allowing us to detect anyone trying to emulate that version inorganically. With that fingerprinted dataset, machine learning can be used to detect anomalies with high accuracy. It’s this signal collection that has allowed us to see that OpenAI &amp; BrowserBase are full chromium builds, while Anthropic’s Firefox browser fails to emulate certain characteristics that would be present on a real user’s Firefox instance.</p><h2><strong>Charting Your Path Forward</strong></h2><p>Whether you choose to block, restrict, or harness AI agent traffic, effective observability is non-negotiable. Here’s our current playbook:</p><ul><li> Start by detecting and monitoring AI agent fingerprints. Learn from their behavior before enforcing hard limits.</li><li><strong>Embrace legitimate use cases:</strong> Some users may leverage AI agents to streamline workflows (e.g., automating reporting on your dashboard). Recognize these opportunities while safeguarding against abuse.</li><li> As AI agents evolve, so must your detection strategies. Invest in ML-based solutions that adapt alongside new agent behaviors.</li></ul><p>AI agents are rewriting the rules of web interaction. They offer exciting UX advancements—but also present new security challenges. The race is on: evolve your detection or risk being outpaced by bad actors and unintended misuse.</p><div><div><div><div><div><h3>Try Device Fingerprinting</h3><p>99.99% bot detection, intelligent rate limiting, and reverse engineering protection.</p></div><a href=\"https://stytch.com/docs/fraud\">Read the docs</a></div></div></div></div>","contentLength":9979,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43049959"},{"title":"The History of S.u.S.E","url":"https://www.abortretry.fail/p/the-history-of-suse","date":1739541229,"author":"ibobev","guid":215,"unread":true,"content":"<p>In 1992, four college students, Roland Dyroff, Thomas Fehr, Hubert Mantel, and Burchard Steinbild in Fürth, Germany were dreaming of independence. They wanted to chart their own paths, make money for themselves, and have fun doing so. They’d had significant exposure to UNIX at university, and there was a new, free, and open source kernel called Linux from Helsinki. At the same time, there was Peter MacDonald’s Linux distribution, the Softlanding System (SLS), and Patrick Volkerding was publishing fixes for SLS and making them freely available via FTP. Somewhat problematically, most UNIX systems and most Linux systems were exclusively English language oriented. These four students were ready to do something for themselves and they began the process of localizing SLS into German.</p><p>On the 2nd of September in 1992, the quartet founded Gesellschaft für Software und Systementwicklung mbH (or S.u.S.E.). In English that would be Software and System Development, but the contraction S.u.S.E. also bore more than a passing similarity to the name of Konrad Zuse who’d built the world’s first programmable and Turing-complete computer (depending upon how one defines this) the Z3 in May of 1941.</p><p><a href=\"https://www.abortretry.fail/p/the-history-of-slackware\" rel=\"\">Slackware Linux</a></p><p>In these halcyon days, the young company had no real hierarchy, no regular meetings, no reports, and no bureaucracy. The company operated in a sort of democratic fashion. Being so small, everyone knew everything that was occurring at any given time. If a major decision needed to be made, they’d simply take an informal vote. If they had unanimity, they’d make the change and this meant that the company could change direction in a single night which was when they normally got the most work done.</p><p>In 1994, S.u.S.E released a fully localized version of Slackware Linux 2.0 with some patches and software incorporated from other Linux distributions, S.u.S.E. Linux 1.0. This release was quickly followed by 1.1.27 in July. This was a Linux system with the XFree86 2.1.1, TCP/IP, NFS, kernel 1.0.9, GNU CC 2.5.8 and 2.4.5, and documentation on disc. The system requirements were a 386 with at least 4MB of RAM, but if one intended to use X11 the RAM requirement was a minimum of 8MB. The system supported ISA, EISA, VLB, and PCI, but it did not support MCA.</p><p>Most references of version 4.2 state that the distribution was only available in German, but this is incorrect. The initial boot configuration allows a choice of either English or German.</p><p>After the installation environment is loaded, when init handed off control to a tty, the user was greeted with a logon screen, again in both languages.</p><p>From there, YaST would take the user through the installation in a well design curses environment.</p><p><a href=\"https://www.tindie.com/products/weird/isa-8-bit-ethernet-controller/\" rel=\"\">8bit RTL8019</a></p><p>At this point, the system was ready to load Linux.</p><p>After creating a Linux disk partition, the user is required to reboot (though nothing informs you of this). At that point, the partition changes are written to disk. In my case, this meant that I would need to boot from a floppy. There is an option to create a Linux boot floppy, but I chose to use a Win98 boot floppy instead. This essentially runs through the LOADLIN portion for a second time, after which I restarted the setup, loaded the config for all packages, and proceeded with the installation.</p><p>Here, selecting the main menu will allow the installer to progress to finishing the installation.</p><p>After login, there’s a brief delay and then we go through the first run setup procedures.</p><p>That took an incredibly long time on my modest 1990s system.</p><p>S.u.S.E. Linux version 5 was released in July of 1997, and that was the version to begin the transition of the distribution to RPMs. Slackware’s TGZs were still supported. That release used kernel 2.0.30, Bash 2.0, Qt 1.2, XFree86 3.3 and FVWM 4.0.45. October of that year saw the release of version 5.1 with the most notable changes being the support of FTP package updates and KDE. This version was five CD-ROMs. Four of these were for the installation, and one was for the LiveCD environment. The final version 5 release was on the 10th of September in 1998, version 5.3 with kernel 2.0.35, KDE 1.0, and Netscape Communicator version 4.05.</p><p>So, here in the installer, we can see a clear evolution from the prior release. Everything is a bit more refined, the installation was far more straight forward, and we can easily see that what has been reported as a complete rebase on Jurix, was truly more of a merger between the Slackware-rooted origin, and the RPM-based Jurix with its automatic build system. We can also see that S.u.S.E. was now evolving into something unique, something gestalt. That said, my installation on a K6-III crashed repeatedly.</p><p>S.u.S.E. opened a branch office in Oakland in 1997 as well as more offices across Europe. Not long after these expansions came new partnerships with IBM, SAP, Compaq, and Oracle. For IBM and Compaq, this partnership included S.u.S.E. offering 24-hour support services. The same year, SuSE began offering servers pre-installed with S.u.S.E. Linux. For 1998, the company moved their corporate office from Fürth to Nürnberg. In 1999, the company opened a branch office in the UK.</p><p>The company released version 6.0 on the 21st of December in 1998, and with this release, the company rebranded as SuSE Linux. Version 6 shipped with kernel 2.0.36, and it saw point release up to 6.4 which released on the 9th of March in 2000. Version 6.4 included YaST2 which was graphical.</p><p>On the 27th of September in 2000, SuSE Linux version 7 was released. SuSE Linux Professional 7.0 consisted of six CD-ROMs and one DVD for those who had a DVD-ROM drive, two floppies for those who couldn’t boot from optical media, a quick install guide, a handbook, a configuration guide, and an applications guide. By this point, like many other commercial Linux distributions, SuSE included StarOffice, Netscape, Acrobat, GIMP, Sane, KDE, and other familiar applications. It had a graphical YaST2 setup tool, ReiserFS and ext2 filesystems, RPM package management, and retailed for $69.99. SuSE Linux Personal lacked many of the proprietary applications and shipped on three CDs. This version saw released for x86, Alpha, PPC, and SPARC. The last of the version 7 systems was 7.3 released on the 23rd of October in 2001.</p><p><a href=\"https://www.abortretry.fail/p/the-itanic-saga\" rel=\"\">Itanium</a></p><p>By November of 2000, something around 66% of SuSE’s revenues were from Linux CD-ROM sales at retail, online, over the phone. Roughly 24% was from services, and the remaining 8% or so was from hardware sales. The company had around forty developers working on open source software, and this didn’t include people who handled the compiling and packaging into RPMs (release engineers), or those who handled support services. Beyond this, the company contributed both money and talent to projects like ReiserFS, KDE, and the Linux kernel.</p><p><a href=\"https://www.abortretry.fail/p/the-history-of-red-hat\" rel=\"\">Red Hat</a><a href=\"https://www.abortretry.fail/p/the-history-of-caldera-part-1\" rel=\"\">Caldera</a><a href=\"https://www.abortretry.fail/p/the-history-of-linux-mandrake\" rel=\"\">Mandrake</a></p><p>SuSE Linux 8.0 was released on the 22nd of April in 2002 with Linux kernel 2.4.18, KDE3, KOffice, and several other enhancements. The installer was more automated, had better hardware detection, and YaST2 could detect Windows partitions. Upon the detection of a Windows partition, the installer would suggest changes to hard disk allocation for the installation of SuSE. YaST2 was also much better at configuring the X server with SaX2 (SuSE advanced X configuration). On the media side of things, SuSE 8 included applications for audio mixing, editing, and CD writing. These audio applications were aided by the inclusion of ALSA 0.9 (Advanced Linux Sound Architecture) which allowed for the playback of more than one audio stream at a time. Security enhancements also came to SuSE with SuSE Personal Firewall 2, GNU Privacy Guard, OpenSSH, PAM, and IPsec. Version 8 concluded with the release of 8.2 on the 7th of April in 2003.</p><p>SuSE Linux 9.0 was released on the 15th of October in 2003 with Linux kernel 2.4.21, GNOME 2.2.3, and KDE 3.1.4. This version brought two major changes. First, SuSE Linux 9 was the first distribution to support AMD64. SuSE had reached out to AMD early in the Hammer architecture’s development, and they built an emulator from AMD’s specifications. The company then proceeded to port the entire distribution AMD64 with AutoBuild. When the first hardware arrived at their offices, the system worked flawlessly. The second big addition was support for NTFS (non-root) filesystems.</p><p><a href=\"https://www.abortretry.fail/p/the-history-of-novell\" rel=\"\">Novell</a></p><p>The first release after acquisition was SUSE Linux 9.1 on the 23rd of April in 2004. This version shipped with kernel 2.6.4, GNOME 2.4, and KDE 3.2.1. SUSE 9.1 brought in some visual changes between the personal and professional editions as well. The personal editions maintained the green theming while the professional editions had blue theming. This was also evident in the default background images.</p><p>For the first time, with SUSE 9.1, people could download the distribution without any monetary cost. A single CD release was available without support of any kind. For the most part, this was just SUSE Personal without some extra packages.</p><p>SUSE Linux Enterprise Server 9 was released in August of 2004 and Novell Linux Desktop 9 (built on much the same package base) was released on the 8th of November in 2004. This release saw official support from IBM, HP, Sun, Dell, SGI, Lenovo, and Fujitsu. For the first time, GNOME became the default, but KDE remained present. Naturally, Ximian’s Evolution became Novell Evolution. Given events at Sun, StarOffice was replaced by OpenOffice. With more of the Ximian acquisition in evidence, the availability of software updates were made apparent to the user with a Red Carpet icon.</p><p>While SUSE had been an enterprise focused distribution for quite a while, on the 4th of August in 2005, Novell announced that the creation of the openSUSE Project. This would allow for a more open development process, and what had previously been the personal edition became a freely available product without professional support. SUSE Linux 10 was released on the 6th of October in 2005 and it was the first SUSE release to follow this model. SUSE Linux 10 shipped with kernel 2.6.13, GNOME 2.12, and KDE 3.4.2.</p><p>SUSE Linux 10.1 was released on the 11th of May in 2006 with Xgl, AppArmor, Xen, the ext3 filesystem, Firefox 2, and NetworkManager. To my knowledge, this was the first major inclusion of Xen in any Linux system. Both enterprise server and desktop versions of 10 were released in July of 2006, and the Novell Linux Desktop was dropped in favor of SUSE Linux Enterprise Deskstop. Version bumps and the name change to openSUSE arrived with 10.2 on the 7th of Decemer in 2006. The final version 10 release was made on the 4th of October in 2007 with 10.3 featuring kernel 2.6.22, GNOME 2.12, KDE 3.5.1, and XFCE 4.4.1. This version also included a preview release of KDE4 Beta 2. On the enterprise side, updates were shipped in Service Packs with SP4, the final version 10 service pack, having shipped on the 15th of April in 2011.</p><p>In November of 2006, Microsoft and Novell signed an intellectual property agreement that involved immunity over patent litigation and freed SUSE Linux users over any possibility of lawsuits from Microsoft. This followed claims from Microsoft that Linux systems were in violation of 235 patents without any disclosure over what those violations were. While some in the community viewed this negatively and were quite vocal about it, this did gain some piece of mind for SUSE to include things like Exchange support, MS Office file format support, and interoperability between the two companies’ directory implementations. This would prove to have lasting effects on both the wider Linux ecosystem and on Microsoft. In the following years, improvements in MS Office support would proliferate, Active Directory support would be common, and Microsoft’s Azure would have Linux support. Microsoft even made SQL Server, VS Code, and Powershell available on Linux.</p><p>On the 22nd of November in 2010, Novell and Attachmate announced that Novell would be acquired by Attachmate. Jeff Hawn, chairman and CEO of Attachmate, commented:</p><blockquote><p><em>The openSUSE project is an important part of the SUSE business. As noted in the agreement announced today, Attachmate plans to operate SUSE as a stand-alone business unit after the transaction closes. If this transaction closes, then after closing, Attachmate Corporation anticipates no change to the relationship between the SUSE business and the openSUSE project as a result of this transaction.</em></p></blockquote><p>On the 30th of April in 2011, Attachmate completed their acquisition of Novell. On Monday, the 2nd of May, eight hundred employees were laid off. </p><p>openSUSE 12.1 came into the world on the 16th of November in 2011 with kernel 3.1.0, GNOME 3.2, and KDE 4.7. This release also included Snapper for Btrfs snapshot management, and systemd for the init system. System V init was still available to users should they want it. Version 12.2 hit on the 5th of September in 2012 with KDE Plasma 4.8, GNOME 3.4, XFCE 4.10, Firefox 14, Plymouth, GRUB 2, and kernel 3.4.6. Version 12.3, the final for the 12 series, released on the 13th of March in 2013. This version gave version bumps to most software, did away with LiveCDs in favor of USB images, and introduced a “rescue” image with XFCE. </p><p>openSUSE version 13.1 was released on the 19th of November in 2013 with kernel 3.11.6, GNOME 3.10, KDE Plasma 4.11.2, Firefox 25, LibreOffice 4.1, experimental Wayland support, and YaST rewritten in Ruby. SUSE Linux Enterprise 12 was built on openSUSE 13, and it shipped on the 28th of October in 2014. openSUSE 13.2 followed on the 4th of November in 2014 with version bumps across the board. That same month, the openSUSE project introduced openSUSE Tumbleweed. This is a rolling release distribution with periodic snapshots made into installable images for i686, AMD64, ARM 64, PPC64, IBM zSystems, KVM, Xen, HyperV, and public and private cloud. Live system images are available with GNOME, KDE, XFCE, and rescue.</p><p>On the 20th of November in 2014, Micro Focus completed its acquisition of Attachmate. Under Micro Focus, SUSE was operated independently from the rest of the organization with its own products. Nils Brauckmann became CEO of SUSE and gained a board seat at Micro Focus.</p><p>With rolling releases now available, the traditional openSUSE system became openSUSE Leap. The first release of Leap hearkened back to the distribution’s history with version 42.1 becoming available on the 4th of November in 2015. This system was based off of enterprise 12 SP1. With another reference to history, Leap brought about a return to KDE as the default desktop with KDE Plasma 5.8. The 42 series saw two updated releases with 42.2 on the 16th of November in 2016, and 42.3 on the 26th of July in 2017. These later releases were based off of SP2 and SP3 respectively.</p><p>On the 25th of May in 2018, SUSE released Leap 15 with SUSE Linux Enterprise following on the 25th of June. These were built on the same codebase with GNOME 3.26, LibreOffice 6, GCC 7, Wayland as the display server, and kernel 4.12. Updates of this release are the current offering as of this writing.</p><p>Micro Focus sold SUSE to EQT AB on the 2nd of July in 2018 for $2.535 billion. Melissa Di Donato became CEO on the 22nd of July in 2019. SUSE finally went public on the Frankfurt stock exchange on the 19th of May in 2021. The company was valued at about $6.1 billion on the first day of trading. Di Donato resigned on the 21st of March in 2023 with Dirk-Peter van Leeuwen being appointed as CEO. On the 17th of August in 2023, EQT delisted SUSE. On the 13th of November in 2023, SUSE merged with Marcel New Lux IV S.A. and the combined entity became SUSE S.A.</p><p>I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment.</p>","contentLength":15900,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43048261"},{"title":"Show HN: Transform your codebase into a single Markdown doc for feeding into AI","url":"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html","date":1739539403,"author":"tesserato","guid":202,"unread":true,"content":"<p>CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.\nThe output for the current repository can be found <a href=\"https://github.com/tesserato/CodeWeaver/blob/main/codebase.md\">here</a>.</p><ul><li><strong>Comprehensive Codebase Documentation:</strong> Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format.</li><li> Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.</li><li>  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types).</li><li> Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.</li><li><strong>Simple Command-Line Interface:</strong>  Offers an intuitive command-line interface with straightforward options for customization.</li></ul><p>If you have Go installed, run <code>go install github.com/tesserato/CodeWeaver@latest</code>to install the latest version of CodeWeaver or <code>go install github.com/tesserato/CodeWeaver@vX.Y.Z</code> to install a specific version.</p><p>Alternatively, download the appropriate pre built executable from the <a href=\"https://github.com/tesserato/CodeWeaver/releases\">releases page</a>.</p><p>If necessary, make the  executable by using the  command:</p><table><thead><tr></tr></thead><tbody><tr><td>The root directory to scan and document.</td></tr><tr><td>The name of the output Markdown file.</td></tr><tr><td><code>-ignore \"&lt;regex patterns&gt;\"</code></td><td>Comma-separated list of regular expression patterns for paths to exclude.</td></tr><tr><td><code>-included-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were included in the documentation.</td></tr><tr><td><code>-excluded-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were excluded from the documentation.</td></tr><tr><td>Display this help message and exit.</td></tr></tbody></table><h2><strong>Generate documentation for the current directory:</strong></h2><p>This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).</p><h2><strong>Specify a different input directory and output file:</strong></h2><pre><code>./codeweaver -dir=my_project -output=project_docs.md\n</code></pre><p>This command will process the  directory and save the documentation to .</p><h2><strong>Ignore specific file types and directories:</strong></h2><pre><code>./codeweaver -ignore=\"\\.log,temp,build\" -output=detailed_docs.md\n</code></pre><p>This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.</p><h2><strong>Save lists of included and excluded paths:</strong></h2><pre><code>./codeweaver -ignore=\"node_modules\" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md\n</code></pre><p>This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .</p><p>Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.</p><p>CodeWeaver is released under the <a href=\"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/LICENSE\">MIT License</a>. See the  file for complete license details.</p>","contentLength":3311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43048027"},{"title":"AI is stifling new tech adoption","url":"https://vale.rocks/posts/ai-is-stifling-tech-adoption","date":1739537105,"author":"kiyanwang","guid":214,"unread":true,"content":"<p>I propose that the advent and integration of AI models into the workflows of developers has stifled the adoption of new and potentially superior technologies due to training data cutoffs and system prompt influence.</p><p>I have noticed a bias towards specific technologies in multiple popular models and have noted anecdotally in conversation and online discussion people choosing technology based on how well AI tooling can assist with its usage or implementation.</p><p>While it has long been the case that developers have considered documentation and support availability when choosing software, AI’s influence dramatically amplifies this factor in decision-making, often in ways that aren’t immediately apparent and with undisclosed influence.</p><p>Large language models, especially those on the scale of many of the most accessible, popular hosted options, take humongous datasets and long periods to train. By the time everything has been scraped and a dataset has been built, the set is on some level already obsolete. Then, before a model can reach the hands of consumers, time must be taken to train and evaluate it, and then even more to finally deploy it.</p><p>Once it has finally released, it usually remains stagnant in terms of having its knowledge updated. This creates an AI knowledge gap. A period between the present and AI’s training cutoff. This gap creates a time between when a new technology emerges and when AI systems can effectively support user needs regarding its adoption, meaning that models will not be able to service users requesting assistance with new technologies, thus disincentivising their use.</p><p>The cutoff means that models are strictly limited in knowledge up to a certain point. For instance, Anthropic’s latest models have a cutoff of <a href=\"https://support.anthropic.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data\">April 2024</a>, and OpenAI’s latest models have cutoffs of <a href=\"https://platform.openai.com/docs/models\">late 2023</a>.</p><p>The influence of the popularisation of AI models has also led to the proliferation of AI slop content, which AI companies are likely trying to avoid introducing in their scraped data sets – potentially increasing this knowledge gap.</p><p>This knowledge gap doesn’t just stifle the adoption of new frameworks and tooling but also the adoption of new and potentially superior features introduced in updates to already in-use frameworks and tooling as a result of the models not yet including information about them in their training set. This is a lesser issue though, because a technology with existing market adoption is bound to have fanatics who will use it, create examples, and produce blog posts, documentation, and other media about it that will eventually end up in training data.</p><p>While some models are used in contexts where they are equipped with tooling, such as ChatGPT with its internet search capabilities, their prompts often only instruct them to use this functionality when explicitly requested by a user. Of course, plenty of implementations don’t have any access to any internet query functionality at all, such as Claude via its web interface, which will inform you it will only discuss things included in its training data.</p><p>Of course, if people are reluctant to adopt a new technology because of a lack of AI support, there will be fewer people likely to produce material regarding said technology, which leads to an overall inverse feedback effect. Lack of AI support prevents a technology from gaining the required critical adoption mass, which in turn prevents a technology from entering use and having material made for it, which in turn starves the model of training data, which in turn disincentivises selecting that technology, and so on and so forth.</p><p>Consider a developer working with a cutting-edge JavaScript framework released just months ago. When they turn to AI coding assistants for help, they find these tools unable to provide meaningful guidance because their training data predates the framework’s release. This forces developers to rely solely on potentially limited official documentation and early adopter experiences, which, for better or worse, tends to be an ‘old’ way of doing things and incentivises them to use something else.</p><p>I’ve anecdotally noticed that many AI tools have a ‘preference’ for React and Tailwind when asked to tackle a web-based task, or even to create any app involving an interface at all.</p><p>With Claude 3.5 Sonnet, which is generally my AI offering of choice given its superior coding ability, my “What personal preferences should Claude consider in responses?” profile setting includes the line “When writing code, use vanilla HTML/CSS/JS unless otherwise noted by me”. Despite this, Claude will  opt to generate new code with React, and in some occurrences even rewrite my existing code into React against my intent and without my consultation.</p><p>This can be evidenced when asking Claude to use a certain technology, such as Svelte Runes, where it will instead direct the user towards its preferred technology. For example:</p><blockquote><p> Build me something with Svelte Runes.</p></blockquote><blockquote><p> I apologize, but I am not able to create Svelte Runes components directly, as the artifact environment only supports certain technologies like React, HTML, and SVG. I can help you build something similar using React, or I can provide you with Svelte Runes code that you can use in your own Svelte project. Which would you prefer?</p><p>If you’d like to see a React version, I can create something with similar reactive functionality using React’s hooks. Or if you want Svelte Runes code specifically, I can write that for you to use in your own environment.</p></blockquote><p>To test if popular models do have built-in bias for React, I logged into the web interfaces of four of the current most popular AI chat platforms via my free account and asked the models to “Create me a web app. It can do anything.”. I repeated this with each of the four models three times in fresh chats with previous chats deleted and any shared context (such as ChatGPT’s memory feature) disabled or cleared between each test.</p><p>I asked Anthropic’s Claude 3.5 Sonnet with blank personal preferences and artifacts enabled. The first time it opted to create a mood tracking app, the second a reading list manager, and the third time a to-do list. Each time it chose to generate it using React with Tailwind CSS for styling. It generated each project in an artifact which offered an interactive preview directly in the interface.</p><p>I asked OpenAI’s ChatGPT 4o with both custom instructions and memory disabled. It produced a note-taking app the first time, a random quote generator the second time, and a to-do list the third time. All using React with Tailwind for styling and presented in <a href=\"https://openai.com/index/introducing-canvas\">canvases</a>. These canvases came with a preview button, which, much like with Claude, offered an interactive preview of the React project it built directly in the interface.</p><p>I asked Google’s Gemini 2.0 Flash with app activity disabled. All three times it built to-do list apps using vanilla HTML, CSS, and JavaScript. Each time, it also noted that I should consider using JavaScript frameworks on the frontend, namely React, Angular, and Vue - in that order.</p><p>I asked DeepSeek’s DeepSeek-V3, and it returned a to-do list app using vanilla HTML, CSS, and JavaScript. On the second request, it created a project outline for a to-do list app, but this time with Bootstrap, Node.js, Express.js, and SQLite. The third time, it created another outline for a note-taking app but with Node.js, Express.js, and MongoDB.</p><p>DeepSeek had the greatest variation in technology used but is the least accessible model and didn’t actually ‘create’ the app as requested – merely a general outline for it.</p><p>These tests show both Claude and ChatGPT have a strong preference for React and Tailwind,  that Gemini has a preference for HTML/CSS/JS but will recommend React and that DeepSeek is much more flexible and varied with its technological choices, though requires more prompting to produce an actual output.</p><p>I’d suggest that a beginner developer, or someone creating an app exclusively via prompting, is likely to use ChatGPT due to its position in the zeitgeist and use whatever output the model first produces without much second thought – thus influencing their tech selection without their realising.</p><p>Even if a developer does opt to use another framework or toolset, there is a chance that the model will prod them towards a selection more aligned with its system prompt, even going as far as rewriting their code to its ‘preference’ against user request, as evidenced by Claude ignoring my request for vanilla web technologies.</p><p>That is assuming that a specific framework or toolset hasn’t already been chosen by the user specifically because they have heard or experienced it being best handled by AI models. Also, while this very much applies to larger technical choices, such as what framework or general tooling a project may choose, it also trickles down into smaller decisions, such as what libraries or modules they may import.</p><p>I think it is evident that AI models are influencing technology, and that the technologies currently in use – especially those that reached popularity before November 2022, when ChatGPT was released, or that are otherwise in current data sets – will be around for a long time to come, and that AI models’ preferential treatment of them will expand their adoption and lifespan.</p><p>I think it would be prudent for AI companies to provide more transparent documentation of technology biases in their models, like they disclose that their models can make mistakes. These models are becoming a common part of developer knowledge and decision-making, and we’re letting the training and prompting decisions of OpenAI, Anthropic, etc shape the entire direction of software development.</p><p>As for further research on this topic, if one had a collection of system prompts over time, it may be possible to compare them to download trends of specified packages and identify correlations. Of course, there are a lot of other influences on the download and adoption of packages, so this may prove difficult and provide data too noisy and influenced by outside variables to be unusable.</p>","contentLength":10120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43047792"},{"title":"Ricochet: Peer-to-peer instant messaging system built on Tor hidden services (2017)","url":"https://github.com/ricochet-im/ricochet","date":1739522071,"author":"philonoist","guid":213,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43046192"},{"title":"Linux kernel cgroups writeback high CPU troubleshooting","url":"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/","date":1739521827,"author":"mesto1","guid":212,"unread":true,"content":"<p>Read on to learn how we used network packet captures and BPF to debug web server performance, ultimately uncovering a Linux kernel performance issue. This investigation was a collaboration between myself and my colleagues.</p><p>We’ve been upgrading the operating system from CentOS to Ubuntu on hosts across our fleet. Our CentOS hosts run an outdated Linux kernel version (3.10), whereas our Ubuntu hosts run a more modern kernel version (6.8). In August 2024, we began rolling out the Ubuntu upgrade across our Apache web servers. When we migrated larger portions of our fleet to Ubuntu, we began seeing elevated listen overflow errors. This elevated error rate prompted us to roll back the Ubuntu upgrade while we debugged:</p><p>Apache listens on a socket for incoming web requests. When incoming requests arrive more quickly than Apache can serve them, the queue of requests waiting to be served grows longer. This queue is capped to a configurable size. When the queue overflows its maximum size, we have a .</p><p>Listen overflows are a symptom of either of two things: the rate of incoming web requests is too high, and / or Apache is serving web requests too slowly.</p><p>Each listen overflow that occurs means we failed to serve a web request. This can result in user-facing errors. Furthermore, if the listen overflows are a symptom of web requests being served slowly, it means users may be experiencing slow page loads.</p><p>Listen overflows occurred a few minutes after a newly autoscaled web server was pooled. They did not tend to recur subsequently. Furthermore, web requests had elevated latency during this same time period:</p><p>We believed listen overflows were occurring because web requests served in the first few minutes after the host was pooled were being served unusually slowly.</p><p>Log lines,  timing information, and <a href=\"https://github.com/adsr/phpspy\"></a> flame graphs showed that network operations were executing particularly slowly. But investigating further, we found evidence that the network was performing normally. Instead, PHP had seemingly stalled for over a second. The below log line indicates that a memcached SET command was slow, but network packet captures on both the client and the server that we analyzed in Wireshark indicate that the SET command experienced normal network latency. The client waited over 1 second before sending the subsequent GET command, as if our PHP script stalled after the packet was received but before we recorded the elapsed time.</p><p><code>[Tue Sep 24 21:20:54 2024] [info] Memcached operation exceeded 20ms :operation=\"set\"  key=\"warmup_key_7746_2\"</code></p><h2>\n  \n  \n    Is it due to elevated system CPU? (yes) <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#is-it-due-to-elevated-system-cpu-yes\">#</a></h2><p>Adding on to the evidence pointing away from network problems, we saw a large spike in system CPU usage about four minutes after newly autoscaled hosts were booted. If we waited until after this spike in system CPU to pool the hosts, we experienced no listen overflows. This spike in system CPU only occurred on Ubuntu hosts – CentOS did not have this problem:</p><p>This made us realize that the problems we were seeing were unrelated to pooling the hosts – the spike in system CPU occurred regardless of whether the hosts were pooled. During the spike in system CPU, we saw logs in :</p><p><code>workqueue: inode_switch_wbs_work_fn hogged CPU for &gt;10000us 4 times, consider switching to WQ_UNBOUND</code></p><p>And  showed kernel workers were using lots of CPU in a function :</p><p>We made a  recording demonstrating that when system CPU spikes, the kernel is busy inside :</p><h2>\n  \n  \n    Why did we initially suspect the network? <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#why-did-we-initially-suspect-the-network\">#</a></h2><p>Our logs and profiling tools showed that network operations were executing particularly slowly. However, when CPU is busy, network operations may appear to slow down disproportionately. Network calls require CPU <a href=\"https://en.wikipedia.org/wiki/Context_switch\">context switches</a> (I think?). After a process blocked on network receives a response, the process may spend a long time waiting in the CPU <a href=\"https://en.wikipedia.org/wiki/Run_queue\">run queue</a> before it gets scheduled again when the CPU is busy. Network operations that appear slow at the user space level may be a symptom of CPU busyness.</p><p>Although there can be cases where this is not true, it has been my experience that when the network is the cause of slowness, CPU usage on client hosts is often lower than normal. When the client is blocked waiting for the network, it is often more idle. In retrospect, perhaps the fact that elevated CPU was one of the symptoms we were seeing should have pointed us away from network issues.</p><p>We believed something in the kernel function <a href=\"https://github.com/torvalds/linux/blob/906bd684e4b1e517dd424a354744c5b0aebef8af/fs/fs-writeback.c#L490\"></a> was causing elevated system CPU. This function is in a file <a href=\"https://github.com/torvalds/linux/blob/master/fs/fs-writeback.c\"></a>, which contains functions related to the <a href=\"https://github.com/firmianay/Life-long-Learner/blob/master/linux-kernel-development/chapter-16.md\">writeback functionality of the Linux page cache</a>. We knew that system CPU was elevated about four minutes after new hosts were booted. One of the first things a new host does is download the latest code – this is part of our host bootstrapping process and involves writing thousands of files to disk. We wondered if the process of flushing the dirty pages to disk was causing the elevated system CPU. While we did not see elevated disk write metrics during the system CPU spike, we decided to test this theory. We added a <a href=\"https://www.man7.org/linux/man-pages/man1/sync.1.html\"></a> call after the  command that downloads the code on new hosts. In theory, that should synchronously write the dirty pages in the page cache to disk. Perhaps by controlling when the page cache was flushed to disk, we could control when the spike in system CPU occurred and ensure that it occurred before we pooled the host. This attempt, however, was unsuccessful. We saw no spike in system CPU when calling . Furthermore, we still saw the spike in system CPU a minute or two later.</p><p>We were back to the drawing board. As we mentioned above, one of the first things a new host does is download the latest code. This process is called , and it runs as a <a href=\"https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html\">systemd oneshot</a> service. We found something perplexing: if we prevented the  service from running, we never saw the spike in system CPU. And if we subsequently ran the  service manually, we would see the spike in system CPU a couple minutes later. This implies that something in  was the cause of the issues. However, if we ran the  command in  manually in our interactive shell, we saw no spikes in system CPU. This apparent contradiction led us on a wild goose chase of trying to determine if some systemd service that was dependent on  was the cause of the elevated system CPU, or if there was some subtle difference in the way we were running the commands in our shell versus how systemd was running the commands.</p><h2>\n  \n  \n    Is it inode cgroup switching? (yes) <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#is-it-inode-cgroup-switching-yes\">#</a></h2><p>Each of the thousands of files that is written by the  service has a corresponding <a href=\"https://www.kernel.org/doc/html/latest/filesystems/ext4/inodes.html\">index node</a>, also known as an inode, that the kernel uses to store file metadata. <a href=\"https://www.kernel.org/doc/html/v5.0/admin-guide/cgroup-v2.html#control-group-v2\">Control groups</a>, also known as cgroups, are a feature of Linux which allows for setting per process limits on system resources. For example, cgroups allow us to limit a given process from consuming too much memory, disk I/O, network bandwidth, etc. Every process belongs to a cgroup. Cgroups form a tree-like hierarchical structure. Processes in a given cgroup are given limits both by the cgroup to which they belong and that cgroup’s parents.</p><p>In the context of cgroups, page cache writeback is tracked at the inode level. A given inode is assigned to whichever cgroup contains the process that is responsible for the majority of writes to the inode’s file. If a new process starts writing a lot to a file, the file’s inode may switch to the new process’s cgroup. Likewise, if a process managed by systemd is terminated, systemd will <a href=\"https://systemd.io/CGROUP_DELEGATION/#controller-support\">remove the process’s cgroup</a>, at which point any inodes assigned to the process’s cgroup will be moved to the parent cgroup.</p><p>The <a href=\"https://github.com/torvalds/linux/blob/906bd684e4b1e517dd424a354744c5b0aebef8af/fs/fs-writeback.c#L490\"></a> that we believed was causing the elevated system CPU is responsible for moving an inode from one cgroup to another in the context of writeback. We got more insight by running this <a href=\"https://github.com/bpftrace/bpftrace\"></a> command on a newly booted host:</p><div><div><pre><code>% sudo bpftrace -e '\ntracepoint:writeback:inode_switch_wbs {\n  printf(\n    \"[%s] inode is switching! inode: %d old cgroup: %d new cgroup: %d\\n\",\n    strftime(\"%H:%M:%S\", nsecs),\n    args-&gt;ino,\n    args-&gt;old_cgroup_ino,\n    args-&gt;new_cgroup_ino\n  );\n}'\n</code></pre></div></div><p>During the spike in system CPU, we saw thousands of these lines printed out by the  command:</p><div><div><pre><code>[20:49:30] inode is switching! inode: 3730800 old cgroup: 22438 new cgroup: 88\n[20:49:30] inode is switching! inode: 3730799 old cgroup: 22438 new cgroup: 88\n...\n\n</code></pre></div></div><p>Each line corresponds to a file written by  that was switching from ’s dying cgroup to the parent cgroup. The old cgroup identifier ( corresponds to ’s cgroup. The new cgroup identifier () corresponds to the parent cgroup.</p><p>The  command prints out data from a <a href=\"https://github.com/torvalds/linux/blob/906bd684e4b1e517dd424a354744c5b0aebef8af/fs/fs-writeback.c#L415\">kernel tracepoint</a> in the Linux kernel’s <a href=\"https://github.com/torvalds/linux/blob/906bd684e4b1e517dd424a354744c5b0aebef8af/include/trace/events/writeback.h#L216-L243\">writeback code</a>. The fields available to print in this tracepoint can be viewed via:</p><div><div><pre><code>% sudo cat /sys/kernel/debug/tracing/events/writeback/inode_switch_wbs/format\nname: inode_switch_wbs\nID: 886\nformat:\n    field:unsigned short common_type;   offset:0;   size:2; signed:0;\n    field:unsigned char common_flags;   offset:2;   size:1; signed:0;\n    field:unsigned char common_preempt_count;   offset:3;   size:1; signed:0;\n    field:int common_pid;   offset:4;   size:4; signed:1;\n\n    field:char name[32];    offset:8;   size:32;    signed:0;\n    field:ino_t ino;    offset:40;  size:8; signed:0;\n    field:ino_t old_cgroup_ino; offset:48;  size:8; signed:0;\n    field:ino_t new_cgroup_ino; offset:56;  size:8; signed:0;\n\nprint fmt: \"bdi %s: ino=%lu old_cgroup_ino=%lu new_cgroup_ino=%lu\", REC-&gt;name, (unsigned long)REC-&gt;ino, (unsigned long)REC-&gt;old_cgroup_ino, (unsigned long)REC-&gt;new_cgroup_ino\n</code></pre></div></div><p>We found that when we added a  to the end of the script that  executes, we could delay the spike in system CPU by one hour. Because systemd only removes a service’s cgroup when its process exits, the  delayed when inodes switched from one cgroup to another.</p><p>We found a systemd directive that allows us to turn off certain cgroup accounting features: <a href=\"https://www.freedesktop.org/software/systemd/man/latest/systemd.resource-control.html#Control%20Group%20Management\"></a>. If either the  or  controllers are disabled, the kernel <a href=\"https://github.com/torvalds/linux/blob/c291c9cfd76a8fb92ef3d66567e507009236ce90/include/linux/backing-dev.h#L172\">will not</a> perform cgroup writeback or any of the related accounting and cgroup switching. We found that by creating a systemd <a href=\"https://gist.github.com/dasl-/87b849625846aed17f1e4841b04ecc84#file-dasl-slice-L5\">slice with those controllers disabled</a> and configuring ’s unit file to <a href=\"https://gist.github.com/dasl-/06ced03d4b905fd79d8d58283ecaf67d#file-setupwebroot-service-L7\">use that slice</a>, we could no longer reproduce the elevated system CPU. We had solved our performance issue.</p><p>No more system CPU spike and no more listen overflows:\n<img src=\"https://dasl.cc/assets/posts/2025-01-01-debugging-our-new-linux-kernel/i8.png\" alt=\"system CPU graph\"></p><h2>\n  \n  \n    Do we have a minimal reproduction script? <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#do-we-have-a-minimal-reproduction-script\">#</a></h2><p>We came up with a minimal reproduction of the issue:</p><p><code>$ sudo mkdir -p /var/random-files &amp;&amp; sudo systemd-run --property=Type=oneshot bash -c 'dd if=/dev/urandom bs=1024 count=400000 | split -a 16 -b 1k - /var/random-files/file.'</code></p><p>This command creates 400,000 files, each consisting of 1,024 random bytes. The files have names like <code>/var/random-files/file.aaaaaaaaaaaaaaaa</code> and <code>/var/random-files/file.aaaaaaaaaaaaaaab</code>. This command is run as a systemd oneshot service. Within anywhere from 30 seconds to 3 minutes after this command finishes, we see a spike in system CPU. Viewing  will confirm this (press  to show kernel threads in ) – we see kernel workers using lots of CPU in the function .</p><h2>\n  \n  \n    Why wasn’t CentOS affected? <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#why-wasnt-centos-affected\">#</a></h2><p>The initial release of <a href=\"https://man7.org/linux/man-pages/man7/cgroups.7.html\">cgroups</a>, known as cgroups v1, was in kernel version 2.6.24. Cgroups v1 has since been replaced by a new implementation: cgroups v2. Cgroups v2 was officially released in kernel version 4.5. Our old CentOS operating system used kernel version 3.10. We believe this inode switching CPU issue is related to the  and  controllers introduced in cgroups v2. Thus CentOS, which uses cgroups v1, is not vulnerable to this issue.</p><h2>\n  \n  \n    Why couldn’t we reproduce when running rsync manually? <a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#why-couldnt-we-reproduce-when-running-rsync-manually\">#</a></h2><p><a href=\"https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/#is-it-writeback-no\">Recall</a> that when we ran the  command from  manually in our interactive shell, we saw no spike in system CPU. It turns out that each interactive ssh session you have open creates its own cgroup. Below is the output of  on a web server on which I have two interactive ssh sessions open. One session is running a  command, and the other session is running . The two cgroups are called  and :</p><pre>% sudo systemd-cgls\nControl group /:\n-.slice\n├─user.slice\n│ └─user-10101.slice\n│   ├─user@10101.service …\n│   │ └─init.scope\n│   │   ├─1710746 /lib/systemd/systemd --user\n│   │   └─1710793 (sd-pam)\n│   ├─\n│   │ ├─1708943 sshd: dleibovic [priv]\n│   │ ├─1711073 sshd: dleibovic@pts/0\n│   │ ├─1711171 -zsh\n│   │ └─1716022 sleep 100\n│   └─\n│     ├─1780667 sshd: dleibovic [priv]\n│     ├─1781414 sshd: dleibovic@pts/1\n│     ├─1781577 -zsh\n│     └─1791367 systemd-cgls\n...\n</pre><p>These  cgroups stick around until you terminate your ssh session. After terminating your ssh session, systemd removes the corresponding cgroup. With this insight, we tested terminating the interactive ssh session after manually running the  commands. Sure enough, about 2 minutes after we terminated the ssh session, we saw the big spikes in system CPU caused by inode cgroup switching.</p><p>We suspected that this performance issue was caused by either the hypervisor or the kernel. We shared our findings with Canonical, the company behind Ubuntu. Canonical confirmed that it is a kernel issue that was likely introduced by a Linux kernel commit from 2021. More details are available in the <a href=\"https://bugs.launchpad.net/ubuntu/+source/linux-oem-6.5/+bug/2038492\">public bug report</a>, in which I have commented. We are hopeful that Canonical will engage with the Linux kernel developers and eventually fix this performance issue.</p>","contentLength":13399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43046174"},{"title":"Benchmarking vision-language models on OCR in dynamic video environments","url":"https://arxiv.org/abs/2502.06445","date":1739517976,"author":"ashu_trv","guid":210,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045801"},{"title":"Extensible WASM Applications with Go","url":"https://go.dev/blog/wasmexport","date":1739516933,"author":"ingve","guid":209,"unread":true,"content":"<p>\n      Cherry Mui\n      13 February 2025\n      </p><p>Go 1.24 enhances its WebAssembly (Wasm) capabilities with the\naddition of the  directive and the ability to build a reactor\nfor WebAssembly System Interface (WASI).\nThese features enable Go developers to export Go functions to Wasm,\nfacilitating better integration with Wasm hosts and expanding the possibilities\nfor Go-based Wasm applications.</p><h2>WebAssembly and the WebAssembly System Interface</h2><p><a href=\"https://webassembly.org/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly (Wasm)</a> is a binary instruction format\nthat was initially created for web browsers, providing the execution of\nhigh-performance, low-level code at speeds approaching native performance.\nSince then, Wasm’s utility has expanded, and it is now used in various\nenvironments beyond the browser.\nNotably, cloud providers offer services that directly execute Wasm\nexecutables, taking advantage of the\n<a href=\"https://wasi.dev/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly System Interface (WASI)</a> system call API.\nWASI allows these executables to interact with system resources.</p><p>Go first added support for compiling to Wasm in the 1.11 release, through the\n port.\nGo 1.21 added a new port targeting the WASI preview 1 syscall API through the\nnew  port.</p><h2>Exporting Go Functions to Wasm with </h2><p>Go 1.24 introduces a new compiler directive, , which allows\ndevelopers to export Go functions to be called from outside of the\nWasm module, typically from a host application that runs the Wasm runtime.\nThis directive instructs the compiler to make the annotated function available\nas a Wasm <a href=\"https://webassembly.github.io/spec/core/valid/modules.html?highlight=export#exports\" rel=\"noreferrer\" target=\"_blank\">export</a>\nin the resulting Wasm binary.</p><p>To use the  directive, simply add it to a function definition:</p><pre><code>//go:wasmexport add\nfunc add(a, b int32) int32 { return a + b }\n</code></pre><p>With this, the Wasm module will have an exported function named  that\ncan be called from the host.</p><p>This is analogous to the <a href=\"https://go.dev/cmd/cgo#hdr-C_references_to_Go\">cgo  directive</a>,\nwhich makes the function available to be called from C,\nthough  uses a different, simpler mechanism.</p><p>A WASI reactor is a WebAssembly module that operates continuously, and\ncan be called upon multiple times to react on events or requests.\nUnlike a “command” module, which terminates after its main function finishes,\na reactor instance remains live after initialization, and its exports remain\naccessible.</p><p>With Go 1.24, one can build a WASI reactor with the  build\nflag.</p><pre><code>$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm\n</code></pre><p>The build flag signals to the linker not to generate the  function\n(the entry point for a command module), and instead generate an\n function, which performs runtime and package initialization,\nalong with any exported functions and their dependencies.\nThe  function must be called before any other exported functions.\nThe  function will not be automatically invoked.</p><p>To use a WASI reactor, the host application first initializes it by calling\n, then simply invoke the exported functions.\nHere is an example using <a href=\"https://wazero.io/\" rel=\"noreferrer\" target=\"_blank\">Wazero</a>, a Go-based Wasm runtime\nimplementation:</p><pre><code>// Create a Wasm runtime, set up WASI.\nr := wazero.NewRuntime(ctx)\ndefer r.Close(ctx)\nwasi_snapshot_preview1.MustInstantiate(ctx, r)\n\n// Configure the module to initialize the reactor.\nconfig := wazero.NewModuleConfig().WithStartFunctions(\"_initialize\")\n\n// Instantiate the module.\nwasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)\n\n// Call the exported function.\nfn := wasmModule.ExportedFunction(\"add\")\nvar a, b int32 = 1, 2\nres, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))\nc := api.DecodeI32(res[0])\nfmt.Printf(\"add(%d, %d) = %d\\n\", a, b, c)\n\n// The instance is still alive. We can call the function again.\nres, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))\nfmt.Printf(\"add(%d, %d) = %d\\n\", b, c, api.DecodeI32(res[0]))\n</code></pre><p>The  directive and the reactor build mode allow applications to\nbe extended by calling into Go-based Wasm code.\nThis is particularly valuable for applications that have adopted Wasm as a\nplugin or extension mechanism with well-defined interfaces.\nBy exporting Go functions, applications can leverage the Go Wasm modules to\nprovide functionality without needing to recompile the entire application.\nFurthermore, building as a reactor ensures that the exported functions can be\ncalled multiple times without requiring reinitialization, making it suitable\nfor long-running applications or services.</p><h2>Supporting rich types between the host and the client</h2><p>Go 1.24 also relaxes the constraints on types that can be used as input and\nresult parameters with  functions.\nFor example, one can pass a bool, a string, a pointer to an , or a\npointer to a struct which embeds  and contains supported\nfield types\n(see the <a href=\"https://go.dev/cmd/compile#hdr-WebAssembly_Directives\">documentation</a> for detail).\nThis allows Go Wasm applications to be written in a more natural and ergonomic\nway, and removes some unnecessary type conversions.</p><p>While Go 1.24 has made significant enhancements to its Wasm capabilities,\nthere are still some notable limitations.</p><p>Wasm is a single-threaded architecture with no parallelism.\nA  function can spawn new goroutines.\nBut if a function creates a background goroutine, it will not continue\nexecuting when the  function returns, until calling back into\nthe Go-based Wasm module.</p><p>While some type restrictions have been relaxed in Go 1.24, there are still\nlimitations on the types that can be used with  and\n functions.\nDue to the unfortunate mismatch between the 64-bit architecture of the client\nand the 32-bit architecture of the host, it is not possible to pass pointers in\nmemory.\nFor example, a  function cannot take a pointer to a struct that\ncontains a pointer-typed field.</p><p>The addition of the ability to build a WASI reactor and export Go functions to\nWasm in Go 1.24 represent a significant step forward for Go’s WebAssembly\ncapabilities.\nThese features empower developers to create more versatile and powerful Go-based\nWasm applications, opening up new possibilities for Go in the Wasm ecosystem.</p>","contentLength":5777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045698"},{"title":"The New York Stock Exchange plans to launch NYSE Texas","url":"https://ir.theice.com/press/news-details/2025/The-New-York-Stock-Exchange-to-Launch-NYSE-Texas/default.aspx","date":1739515211,"author":"ChrisArchitect","guid":207,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045558"},{"title":"Does X cause Y? An in-depth evidence review (2021)","url":"https://www.cold-takes.com/does-x-cause-y-an-in-depth-evidence-review/","date":1739513659,"author":"l0b0","guid":206,"unread":true,"content":"<p>\nThere's an interesting theory out there that X causes Y. If this were true, it would be pretty important. So I did a deep-dive into the academic literature on whether X causes Y. Here's what I found.\n</p><p>\n(Embarrassingly, I can't actually remember what X and Y are. I think maybe X was enriched preschool, or just school itself, or eating fish while pregnant, or the Paleo diet, or lead exposure, or a clever \"nudge\" policy trying to get people to save more, or some self-help technique, or some micronutrient or public health intervention, or democracy, or free trade, or some approach to intellectual property law. And Y was ... lifetime earnings, or risk of ADHD diagnosis, or IQ in adulthood, or weight loss, or violent crime, or peaceful foreign policy, or GDP per capita, or innovation. Sorry about that! Hope you enjoy the post anyway! Fortunately, I think <strong>what I'm about to write is correct for pretty much any (X,Y) from those sorts of lists.</strong>)\n</p><ul><li>There are hundreds of studies on whether X causes Y, but most of them are simple observational studies that are just essentially saying \"People/countries with more X also have more Y.\" For reasons discussed below, we can't really learn much from these studies.\n\n</li><li>There are 1-5 more interesting studies on whether X causes Y. Each study looks really clever, informative and rigorous at first glance. However, the more closely you look at them, the more confusing the picture gets.\n\n</li><li>We ultimately need to choose between (a) believing some overly complicated theory of the relationship between X and Y, which reconciles all of the wildly conflicting and often implausible things we're seeing in the studies; (b) more-or-less reverting to what we would've guessed about the relationship between X and Y in the absence of any research.\n</li></ul><h2>The chaff: lots of unhelpful studies that I'm disregarding</h2><p>\nFirst, the good news: there are hundreds of studies on whether X causes Y. The bad news? We need to throw most of them out. \n</p><p>\nMany have comically small sample sizes (like studying 20 people) and/or comically short time horizons (like looking at weight loss over two weeks), or unhelpful outcome measures (like intelligence tests in children under 5). But by far the most common problem is that <strong>most of the studies on whether X causes Y are simple observational studies: they essentially just find that people/countries with more X also have more Y. </strong></p><p>\nWhy is this a problem? There could be a  - some third thing, Z, that is correlated with both X and Y. And there are specific reasons we should expect confounders to be common:\n</p><ul><li>In general, people/countries that have more X also have more of lots of other helpful things - they're richer, they're more educated, etc. For example, if we're asking whether higher-quality schooling leads to higher earnings down the line, an issue is that people with higher-quality schooling also tend to come from better-off families with lots of other advantages.\n\n</li><li>In fact, the <em>very fact that people in upper-class intellectual circles think X causes Y</em> means that richer, more educated people/countries tend to deliberately get more X, and also try to do a lot of other things to get more Y. For example, more educated families tend to eat more fish (complicating the attempt to see whether eating fish in pregnancy is good for the baby).</li></ul><p>\nNow, a lot of these studies try to \"control for\" the problem I just stated - they say things like \"We examined the effect of X and Y, while controlling for Z [e.g., how wealthy or educated the people/countries/whatever are].\" How do they do this? The short answer is, well, hm, jeez. Well you see, to simplify matters a bit, just try to imagine ... uh ... shit. Uh. The only high-level way I can put this is:\n</p><ul><li>They use a technique called <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">regression analysis</a> that, as far as I can determine, cannot be explained in a simple, intuitive way (especially not in terms of how it \"controls for\" confounders).\n\n</li><li>The \"controlling for\" thing relies on a lot of subtle assumptions and can break in all kinds of weird ways. <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152719\">Here's </a>a technical explanation of some of the pitfalls; <a href=\"https://www.cold-takes.com/phil-birnbaums-regression-analysis/\">here's</a> a set of deconstructions of regressions that break in weird ways.\n\n</li><li>None of the observational studies about whether X causes Y discuss the pitfalls of \"controlling for\" things and whether they apply here.\n</li></ul><p>\nI don't think we can trust these papers, and to really pick them all apart (given how many there are) would take too much time. So let's focus on a smaller number of better studies.\n</p><h2>The wheat: 1-5 more interesting studies</h2><p>\nDigging through the sea of unhelpful studies, I found 1-5 of them that are actually really interesting! \n</p><p>\nBut on closer inspection, I have a bunch of reservations. For example:\n</p><ul><li>The paper doesn't make it easy to replicate its analysis, and when someone does manage to sort-of replicate it, they may <a href=\"https://blog.givewell.org/2017/12/07/questioning-evidence-hookworm-eradication-american-south/\">get different results</a>. \n\n</li><li>There was other weird stuff going on (e.g., changes in census data collection methods), during the strange historical event, so it's a little hard to generalize.\n\n</li><li>In a response to the study, another academic advances a complex theory of how the study could actually have gotten a misleading result. This led to an intense back-and-forth between the original authors and the skeptic, stretched out over years because each response had to be published in a journal, and by the time I got to the end of it I didn't have any idea what to think anymore.</li></ul><p>\nI found 0-4 other interesting studies. I can't remember all of the details, but they may have included:\n</p><p>\nCompounding the problem, the 1-5 studies I found tell very different stories about the relationship between X and Y. How could this make sense? Is there a unified theory that can reconcile all the results?\n</p><p>\nWell, one possibility is that X causes Y sometimes, but only under very particular conditions, and the effect can be masked by some other thing going on. So - if you meet one of 7 criteria, you should do X to get more Y, but if you meet one of 9 other criteria, you should actually avoid X!\n</p><p>\nI have to say, this all was simultaneously more fascinating and less  than I expected it would be going in. I thought I would find some nice studies about the relationship between X and Y and be done. Instead, I've learned a ton about weird historical events and about the ins and outs of different measures of X and Y, but I feel just super confused about whether X causes Y.\n</p><p>\nI guess my bottom line is that X does cause Y, because it intuitively seems like it would.\n</p><p>\nI'm glad I did all this research, though. It's good to know that social science research can go haywire in all kinds of strange ways. And it's good to know that despite the confident proclamations of pro- and anti-X people, it's legitimately just super unclear whether X causes Y. \n</p><p>\nI mean, how else could I have learned that?\n</p><h2>Appendix: based on a true story</h2><p>\nThis piece was inspired by:\n</p><p>\nThe basic patterns above apply to most of these, and the bottom line usually has the kind of frustrating ambiguity seen in this conclusion.</p><p>There are cases where things seem a bit less ambiguous and the bottom line seems clearer. Speaking broadly, I think the main things that contribute to this are:\n</p><ul><li>Actual randomization. For years I've nodded along when people say \"You shouldn't be dogmatic about randomization, there are many ways for a study to be informative,\" but each year I've become a bit more dogmatic. Even the most sophisticated-, appealing-seeming alternatives to randomization in studies seem to have a way of falling apart. Randomized studies almost always have problems and drawbacks too. But I’d rather have a randomized study with drawbacks than a non-randomized study with drawbacks.\n\n</li><li>Extreme thoroughness, such as Roodman's attempt to reconstruct the data and code for key studies in <a href=\"https://www.openphilanthropy.org/blog/reasonable-doubt-new-look-whether-prison-growth-cuts-crime\">Reasonable Doubt</a>. This sometimes leads to outright dismissing a number of studies, leaving a smaller, more consistent set remaining.\n</li></ul>","contentLength":7882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045406"},{"title":"TeX and Typst: Layout Models (2024)","url":"https://laurmaedje.github.io/posts/layout-models/","date":1739421249,"author":"fngjdflmdflg","guid":204,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43032697"},{"title":"NAT Is the Enemy of Low Power Devices","url":"https://blog.golioth.io/nat-is-the-enemy-of-low-power-devices/","date":1739214756,"author":"hasheddan","guid":203,"unread":true,"content":"<p>If you have ever tried communicating with a device on a private network, you may have encountered <a href=\"https://en.wikipedia.org/wiki/Network_address_translation\">Network Address Translation (NAT)</a>. Fundamentally, when one device needs to send data to another device, it needs to know how to address it. On <a href=\"https://en.wikipedia.org/wiki/Internet_Protocol\">IP-based networks</a>, devices are addressed using an IP address. Unfortunately, the number of connected devices has long outpaced the number of unique addresses in the <a href=\"https://en.wikipedia.org/wiki/IPv4\">IPv4</a> address space. Because of this, public IP addresses have to be shared between devices, which causes a few problems.</p><h2><a href=\"https://blog.golioth.io/nat-is-the-enemy-of-low-power-devices/#how-to-share-an-ip-address\" aria-hidden=\"true\"></a>How to Share an IP Address</h2><p>You probably accessed this blog post from a machine that does not have a public IP address. Rather, it has been assigned a private IP address on a network, perhaps via the <a href=\"https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol\">Dynamic Host Configuration Protocol (DHCP)</a>, and it talks to a router that is responsible for sending data to and from the device. To access this post, your device first had to use the <a href=\"https://en.wikipedia.org/wiki/Domain_Name_System\">Domain Name System (DNS</a>) to map a public IP address to , then had to send a request to that IP address for the content of this page.</p><p>When that request arrives at a router or some other intermediary, it knows where to deliver the request because the IP address of the server hosting  is specified. It forwards the request along, and the server responds with the requested content. However, the server does not know that your device sent the request. The router has replaced the private IP address and port from your device with its own public IP address and port, and it has made an entry in a translation table noting that incoming data for that port should be directed to your device. The server sends the content back to the router, which replaces its own public IP address and port with your device’s IP address and port, then forwards it along. The content arrives at your device, appearing as though the server sent it directly to you. Meanwhile, the router is doing the same song and dance for many other devices, maintaining all of the mappings from its own IP address and ports to internal IP addresses and ports. This is known as <strong>Network Address Translation (NAT)</strong>.</p><p>This works great in simple request-response scenarios like fetching a blog post from a server with a public IP address. However, what if the server wants to say something to the device before the device talks to it? The server may know the public IP address of the router, but the router has no way of knowing which device the message is actually intended for. There is no entry in the NAT translation table until an  message creates one. This also becomes a problem in peer-to-peer scenarios, where both devices are on a private network, making it such that neither device can talk to the other (this is solved using a public rendezvous point, such as a <a href=\"https://en.wikipedia.org/wiki/STUN\">STUN server</a>, but that’s a story for another post).</p><p>Another problem is that routers don’t want to maintain mappings forever. At some point if no outgoing messages have been observed, the entry will be removed from the translation table and any subsequent incoming traffic will be dropped. In many cases, this timeout is quite aggressive (e.g. 5 minutes or less). Typically this is resolved by sending “keep alive” messages, ensuring that entries are not removed and data can flow freely in both directions. For your laptop or a server in a data center that might work fine for the most part. For highly constrained devices, it can quickly drain battery or consume precious limited bandwidth.</p><p>Maybe you decide that its okay for incoming traffic to be dropped after some period of time, as long as when you next contact the server you are able to re-establish a mapping and fetch any data that you need. Unfortunately, there is no guarantee that the router, or any other layer in the hierarchy of intermediaries performing NAT (it’s actually much more complicated, with <a tabindex=\"0\" href=\"https://en.wikipedia.org/wiki/Carrier-grade_NAT\" rel=\"noopener noreferrer\" data-token-index=\"1\"></a> adding even more translation steps), will assign you the same public IP address and port. Therefore, when you try to continue talking to the server over a previously established session, it will not recognize you. This means you’ll have to re-establish the session, which typically involves expensive cryptographic operations and sending a handful of messages back and forth before actually delivering the data you were interested in sending originally.</p><p>The worst case scenario is that your device needs to send data somewhat frequently, but not frequently enough that NAT mappings are maintained. For example, if a device needs to send a tiny sensor reading every 30 minutes, and the NAT timeout is 5 minutes, it will either need to send a keep alive message every 5 minutes (that’s 5x the messages you actually need to send!), or it will need to re-establish the session every time it delivers a reading. In both cases, you are going to be using much more power than if you were just able to send your sensor reading alone every 30 minutes.</p><p>Unfortunately, the distributed nature of the internet means that we aren’t going to be able to address the issue by nicely asking carriers and ISPs to extend their NAT timeouts. However, we  make it such that being issued a new IP address and port doesn’t force us to re-establish a session.</p><p>More than a year ago, we <a href=\"https://blog.golioth.io/golioth-announces-connection-id-support/\">announced support</a> for DTLS 1.2 Connection IDs. DTLS provides a secure transport over UDP, which many devices, especially those that are power constrained, use to communicate with Golioth’s CoAP device APIs. Typically, DTLS sessions are established based on a “five tuple”: source address, source port, transport protocol, destination address, destination port. If any of these change, a handshake must be performed to establish a new session. To mitigate this overhead, a Connection ID can be negotiated during the initial handshake, and can be used in subsequent records to continue to associate messages even after changes in source IP or port.</p><p>Going back to our previous example of a device that sends a single sensor reading message every 30 minutes, enabling Connection ID would mean that a new handshake would not have to be performed after NAT timeout, and that single message can be sent then the device can go back to sleep. In fact, depending on how long the server is willing to store connection state, the device could sleep for much longer, sending once a day or more infrequently. This doesn’t solve the issue of cloud to device traffic being dropped after NAT timeout (check back for another post on that topic), but for many low power use cases, being able to sleep for an extended period of time is more important than being able to immediately push data to devices.</p><h2><a href=\"https://blog.golioth.io/nat-is-the-enemy-of-low-power-devices/#configuring-the-golioth-firmware-sdk-for-sleepy-devices\" aria-hidden=\"true\"></a>Configuring the Golioth Firmware SDK for Sleepy Devices</h2><p>By default, the <a href=\"https://github.com/golioth/golioth-firmware-sdk\">Golioth Firmware SDK</a> will send keep alive messages to ensure that an entry is preserved in the NAT translation table. However, this functionality can be disabled by setting <code>CONFIG_GOLIOTH_COAP_KEEPALIVE_INTERVAL</code> to 0, or just modifying it to be set to a large upper bound.</p><pre data-enlighter-language=\"generic\">CONFIG_GOLIOTH_COAP_KEEPALIVE_INTERVAL_S=0</pre><p>If using Zephyr, we’ll also need to set the receive timeout to a value greater than the interval at which we will be sending data. Otherwise, the client will attempt to reconnect after 30 seconds by default if it has not received any messages. In this example we’ll send data every 130 seconds, so setting the receive timeout to 200 ensures that we won’t attempt to reconnect between sending.</p><pre data-enlighter-language=\"generic\">CONFIG_GOLIOTH_COAP_CLIENT_RX_TIMEOUT_SEC=200</pre><p>To demonstrate the impact of NAT timeouts, we’ll initially build the  example without enabling Connection IDs. To ensure that we wait long enough for a NAT timeout, we need to update the <a href=\"https://github.com/golioth/golioth-firmware-sdk/blob/2e405f9d00a64b20f5b8d434b88db0069418d0cb/examples/zephyr/hello/src/main.c#L57\">loop</a> to send every 130 seconds instead of every 5 seconds.</p><pre data-enlighter-language=\"generic\">while (true)\n{\n    LOG_INF(\"Sending hello! %d\", counter);\n\n    ++counter;\n    k_sleep(K_SECONDS(130));\n}</pre><p>Building and flashing the  sample on a Nordic Thingy91 results in the following behavior.</p><pre data-enlighter-language=\"generic\">*** Booting nRF Connect SDK v2.7.0-5cb85570ca43 ***\n*** Using Zephyr OS v3.6.99-100befc70c74 ***\n[00:00:00.506,378] &lt;dbg&gt; hello_zephyr: main: start hello sample\n[00:00:00.506,378] &lt;inf&gt; golioth_samples: Bringing up network interface\n[00:00:00.506,408] &lt;inf&gt; golioth_samples: Waiting to obtain IP address\n[00:00:13.236,877] &lt;inf&gt; lte_monitor: Network: Searching\n[00:00:17.593,994] &lt;inf&gt; lte_monitor: Network: Registered (roaming)\n[00:00:17.594,696] &lt;inf&gt; golioth_mbox: Mbox created, bufsize: 1232, num_items: 10, item_size: 112\n[00:00:18.839,904] &lt;inf&gt; golioth_coap_client_zephyr: Golioth CoAP client connected\n[00:00:18.840,118] &lt;inf&gt; hello_zephyr: Sending hello! 0\n[00:00:18.840,179] &lt;inf&gt; hello_zephyr: Golioth client connected\n[00:00:18.840,270] &lt;inf&gt; golioth_coap_client_zephyr: Entering CoAP I/O loop\n[00:02:28.840,209] &lt;inf&gt; hello_zephyr: Sending hello! 1\n[00:02:32.194,396] &lt;wrn&gt; golioth_coap_client: 1 resends in last 10 seconds\n[00:02:46.252,868] &lt;wrn&gt; golioth_coap_client: 4 resends in last 10 seconds\n[00:03:03.419,219] &lt;wrn&gt; golioth_coap_client: 3 resends in last 10 seconds\n[00:03:04.986,389] &lt;wrn&gt; golioth_coap_client: Packet 0x2001e848 (reply 0x2001e890) was not replied to\n[00:03:06.045,715] &lt;wrn&gt; golioth_coap_client: Packet 0x2001e638 (reply 0x2001e680) was not replied to\n[00:03:15.213,592] &lt;wrn&gt; golioth_coap_client: 6 resends in last 10 seconds\n[00:03:21.874,298] &lt;wrn&gt; golioth_coap_client: Packet 0x2001ec90 (reply 0x2001ecd8) was not replied to\n[00:03:25.419,921] &lt;wrn&gt; golioth_coap_client: 5 resends in last 10 seconds\n[00:03:36.565,765] &lt;wrn&gt; golioth_coap_client: 5 resends in last 10 seconds\n[00:03:40.356,933] &lt;wrn&gt; golioth_coap_client_zephyr: Receive timeout\n[00:03:40.356,964] &lt;inf&gt; golioth_coap_client_zephyr: Ending session\n[00:03:40.356,994] &lt;inf&gt; hello_zephyr: Golioth client disconnected\n[00:03:47.035,675] &lt;inf&gt; golioth_coap_client_zephyr: Golioth CoAP client connected\n[00:03:47.035,705] &lt;inf&gt; hello_zephyr: Golioth client connected\n[00:03:47.035,827] &lt;inf&gt; golioth_coap_client_zephyr: Entering CoAP I/O loop</pre><p>After initially connecting and successfully sending , we are inactive for 130 seconds ( to ), then when we attempt to send , we see that the server never responds, eventually causing us to reach the  and reconnect. This is because when we send , our entry has been removed from the NAT translation table, and when we are assigned a new public IP address and port the server is unable to associate messages with the existing DTLS session.</p><p>Because using Connection IDs does involve sending extra data in every message, it is disabled in the Golioth Firmware SDK by default. In scenarios such as this one where the few extra bytes clearly outweigh more frequent handshakes, Connection IDs can be enabled with <code>CONFIG_GOLIOTH_USE_CONNECTION_ID</code>.</p><pre data-enlighter-language=\"generic\">CONFIG_GOLIOTH_USE_CONNECTION_ID=y</pre><p>Now when we build and flash the  example on a Thingy91, we can see our 130 second delay, but then the successful delivery of . 130 seconds later, we see another successful delivery of .</p><pre data-enlighter-language=\"generic\">*** Booting nRF Connect SDK v2.7.0-5cb85570ca43 ***\n*** Using Zephyr OS v3.6.99-100befc70c74 ***\n[00:00:00.508,636] &lt;dbg&gt; hello_zephyr: main: start hello sample\n[00:00:00.508,666] &lt;inf&gt; golioth_samples: Bringing up network interface\n[00:00:00.508,666] &lt;inf&gt; golioth_samples: Waiting to obtain IP address\n[00:00:13.220,001] &lt;inf&gt; lte_monitor: Network: Searching\n[00:00:16.318,908] &lt;inf&gt; lte_monitor: Network: Registered (roaming)\n[00:00:16.319,641] &lt;inf&gt; golioth_mbox: Mbox created, bufsize: 1232, num_items: 10, item_size: 112\n[00:00:21.435,180] &lt;inf&gt; golioth_coap_client_zephyr: Golioth CoAP client connected\n[00:00:21.435,394] &lt;inf&gt; hello_zephyr: Sending hello! 0\n[00:00:21.435,424] &lt;inf&gt; hello_zephyr: Golioth client connected\n[00:00:21.435,546] &lt;inf&gt; golioth_coap_client_zephyr: Entering CoAP I/O loop\n[00:02:31.435,455] &lt;inf&gt; hello_zephyr: Sending hello! 1\n[00:04:41.435,546] &lt;inf&gt; hello_zephyr: Sending hello! 2</pre><p>To see how often your devices are being forced to reconnect to Golioth after periods of inactivity, check out our <a href=\"https://docs.golioth.io/reference/device-api/device-connectivity\">documentation on device connectivity metrics</a>. Devices that effectively maintain long lasting connections will see a significant difference between their  and  timestamps. If you have any questions about optimizing your devices for low power, reach out to us on the <a href=\"https://forum.golioth.io/\">forum</a>!</p>","contentLength":12139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43003999"},{"title":"Show HN: A unique generated maze to share with your valentine","url":"https://love.berk.es/","date":1739198155,"author":"berkes","guid":192,"unread":true,"content":"<dd><p> is an art project. It is so called <a href=\"https://en.wikipedia.org/wiki/Generative_art\">generative art</a>: I write the software, and the software creates art autonomously. \n            A nice introduction into generative art is <a href=\"https://www.youtube.com/watch?v=4Se0_w0ISYk\">this talk by Tim Holman</a>.\n          </p></dd><dd><p>\n          In a gallery in London, I came across gorgeaus screenprints by <a href=\"https://rbyrneart.com/\">Ricky Byrne</a>. I loved their use of color and hand-produced feel. Attention for color, layout, tension.\n          So I started experimenting with maze generation algorithms in Rust, with Nannou.<p>\n          In the process, I decided to make it a web app, for valentine, so everyone can create their own maze. And ported the Rust code to TypeScript in a tiny web app. </p><a href=\"https://www.youtube.com/watch?v=HyK_Q5rrcr4\">The Coding Train has a great tutorial set on maze generation</a>. I used the same common recursive backtracking algorithm, because the aesthetics are what I was looking for. </p></dd><dd><p>\n          The names you provide are used to generate <a href=\"https://en.wikipedia.org/wiki/Random_seed \">unique randomnes</a>. \n          This is used in a <a href=\"https://en.wikipedia.org/wiki/Maze_generation_algorithm\">maze generation algorithm</a> to create a maze. \n          I deliberately chose to animate the maze generation, to show the process. It is a slow process, but I think it is interesting to watch it carve out your maze. </p></dd><dd><p>\n          Saving, copying and sharing the maze is disabled until the maze is generated. Once the maze is generated, you can save, copy and share the maze. \n          Sharing, copying won't work on all browsers and is affected by some browser plugins or settings. The best result is on Chrome on Android. \n          </p></dd><dd><p>, the only parameter you can provide is the names. The maze is generated based on these names, and the algorithm is fixed. This is by design. Only the two names determine the artwork. \n            However, you can change the source code, see below.\n          </p></dd><dd><p>\n            The code is available on <a href=\"https://github.com/berkes/art/tree/main/lost-in-love\">GitHub</a>. Feel free to fork, change and improve it, or just have fun with it. </p></dd><dd><ul><li><a href=\"https://nannou.cc/\">Nannou</a> for the original maze generation</li><li><a href=\"https://vitejs.dev/\">Vite</a> for the build and web stuff</li></ul></dd>","contentLength":1890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000764"}],"tags":["dev","hn"]}