{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Incremental Archival from Postgres to Parquet for Analytics","url":"https://www.crunchydata.com/blog/incremental-archival-from-postgres-to-parquet-for-analytics","date":1739554943,"author":"/u/gtobbe","guid":624,"unread":true,"content":"<p>PostgreSQL is commonly used to store event data coming from various kinds of devices. The data often arrives as individual events or small batches, which requires an operational database to capture. Features like <a href=\"https://www.crunchydata.com/blog/native-partitioning-with-postgres\">time partitioning</a> help optimize the storage layout for time range filtering and efficient deletion of old data.</p><p>The PostgreSQL feature set gives you a lot of flexibility for handling a variety of IoT scenarios, but there are certain scenarios for it is less suitable, namely:</p><ul><li>Long-term archival of historical data</li><li>Fast, interactive analytics on the source data</li></ul><p>Ideally, data would get automatically archived in cheap storage, in a format optimized for large analytical queries.</p><p>We developed two open source Postgres extensions that help you do that:</p><ul><li><a href=\"https://github.com/CrunchyData/pg_parquet\">pg_parquet</a> can export (and import) query results to the <a href=\"https://parquet.apache.org/\">Parquet</a> file format in object storage using regular COPY commands</li><li><a href=\"https://github.com/crunchydata/pg_incremental\">pg_incremental</a> can run a command for a never-ending series of time intervals or files, built on top of <a href=\"https://github.com/citusdata/pg_cron\">pg_cron</a></li></ul><p>With some simple commands, you can set up a reliable, fully automated pipeline to export time ranges to the columnar Parquet format in S3.</p><p>Then, you can use a variety of analytics tools to query or import the data. My favorite is of course <a href=\"https://www.crunchydata.com/blog/crunchy-data-warehouse-postgres-with-iceberg-for-high-performance-analytics\">Crunchy Data Warehouse</a>.</p><p>On any PostgreSQL server that has the pg_parquet and pg_incremental extensions, you can set up a pipeline that periodically exports data to in S3 in two steps.</p><p>The pg_incremental extension has a create_time_interval_pipeline function that will run a given command once the time interval has passed, with 2 timestamp parameters set to the start and end of the hour. We cannot directly use query parameters in a COPY command, but we can define a simple PL/pgSQL function that generates and executes a custom COPY command using the parameters.</p><pre><code>-- existing raw data table\ncreate table events (\n  event_id bigint not null generated by default as identity,\n  event_time timestamptz not null default now(),\n  device_id bigint not null,\n  sensor_1 double precision\n);\n\ninsert into events (device_id, sensor_1)\nvalues (297, 20.4);\n\n\ninsert into events (device_id, sensor_1)\nvalues (297, 20.4);\n\n-- define an export function that wraps a COPY command\ncreate or replace function export_events(start_time timestamptz, end_time timestamptz)\nreturns void language plpgsql as $function$\nbegin\n  execute format(\n    $$\n      copy (select * from events where event_time &gt;= %L and event_time &lt; %L)\n      to 's3://mybucket/events/%s.parquet' with (format 'parquet');\n    $$,\n    start_time, end_time, to_char(start_time, 'YYYY-MM-DD-HH')\n  );\nend;\n$function$;\n\n-- export events hourly from the start of the year, and keep exporting in the future\nselect incremental.create_time_interval_pipeline('event-export',\n  time_interval := '1 hour',                      /* export data by the hour                */\n  batched := false,                               /* process 1 hour at a time               */\n  start_time := '2025-01-01',                     /* backfill from the start of the year    */\n  source_table_name := 'events',                  /* wait for writes on events to finish    */\n  command := $$ select export_events($1, $2) $$   /* run export_events with start/end times */\n);\n\n</code></pre><p>By running these commands, Postgres will export all the data from the start of the year into hourly Parquet files in S3, and will keep doing so after every hour and automatically retry on failure.</p><p>To use pg_parquet Crunchy Bridge, you can add your S3 credentials for pg_parquet to your Postgres server via the dashboard under Settings -&gt; Data lake.</p><p>Once data is in Parquet, you can use a variety of tools and approaches to query the data. If you want to keep using Postgres, you can use <a href=\"https://www.crunchydata.com/products/warehouse\">Crunchy Data Warehouse</a> which has two different ways of working with Parquet data.</p><p>The simplest way to start querying Parquet files in S3 in Crunchy Data Warehouse is to use a lake analytics table. You can easily create a table for all Parquet files that match a wildcard pattern:</p><pre><code>create foreign table events_parquet ()\nserver crunchy_lake_analytics\noptions (path 's3://mybucket/events/*.parquet');\n</code></pre><p>You can then immediately query the data and the files get cached in the background, so queries will quickly get faster.</p><p>A downside of querying Parquet directly is that eventually we will have a lot of hourly files that match the pattern, and there will be some overhead from listing them for each query (listing is not cached). We also cannot easily change the schema later.</p><p>A more flexible approach is to import the Parquet files into an Iceberg table. Iceberg tables are also backed by Parquet in S3, but the files are compacted and optimized, and the table supports transactions and schema changes.</p><p>You can create an Iceberg table that has the same schema as a set of Parquet files using the definition_from option. You could also load the data using load_from, but we’ll do that separately.</p><pre><code>create table events_iceberg () using iceberg\nwith (definition_from = 's3://mybucket/events/*.parquet');\n</code></pre><p>Now we need a way to import all existing Parquet files and also import new files that show up in S3 into Iceberg. This is another job for pg_incremental. Following a similar approach as before, we create a function to generate a COPY command using a parameter.</p><pre><code>-- define an import function that wraps a COPY command to import from a URL\ncreate function import_events(path text)\nreturns void language plpgsql as $function$\nbegin\n  execute format($$copy events_iceberg from %L$$, path);\nend;\n$function$;\n\n-- create a pipeline to import new files into a table, one by one.\n-- $1 will be set to the path of a new file\nselect incremental.create_file_list_pipeline('event-import',\n   file_pattern := 's3://mybucket/events/*.parquet',\n   list_function := 'crunchy_lake.list_files',\n   command := $$ select import_events($1) $$,\n);\n\n-- optional: do compaction immediately\nvacuum events_iceberg;\n\n</code></pre><p>After running these commands, your data will be continuously archived from your source Postgres server into Iceberg in S3. You can then run fast analytical queries directly from Crunchy Data Warehouse, which uses a combination of parallel, vectorized query processing and file caching to speed up queries. You can additionally set up (materialized) views and assign read permissions to the relevant users.</p><p>No complex ETL pipelines required.</p><p>To give you a sense of the performance benefit of using Parquet, we loaded 100M rows into the source table, which got automatically mirrored in Parquet and Iceberg via our pipelines. We then ran a simple analytical query on each table:</p><pre><code>select device_id, avg(sensor_1) from events group by 1;\n</code></pre><p>The runtimes in milliseconds are shown in the following chart:</p><p>In this case the source server is a standard-16 instance (4 vcpus) on Crunchy Bridge, and the warehouse is a warehouse-standard-16 instance (4 vcpus). So, using Crunchy Data Warehouse we can analyze 100M rows in well under a second on a small machine, and get &gt;10x speedup with Iceberg.</p><p>The use of compression also means the size went from 8.9GB in PostgreSQL to 1.2GB in Iceberg using object storage.</p><p>With pg_parquet and pg_incremental, you can incrementally export data from PostgreSQL into Parquet in S3, and with Crunchy Data Warehouse you can process and analyze that data very quickly while still using PostgreSQL.</p><p>One of the nice characteristics of the approach described in this blog is that the pipelines are fully transactional. It means that every import or export step either fully succeeds or fails and then it will be retried until it does succeed. That’s how we can create production-ready pipelines with a few simple SQL commands.</p><p>Under the covers, pg_incremental keeps track of which time ranges or files have been processed. The bookkeeping happens in the same transaction as the COPY commands. So if a command fails because of an ephemeral S3 issue, the data will not end up being ingested twice or go missing. Having transactions takes away a huge amount of complexity for building reliable pipelines. There can of course be other reasons for pipeline failures that cannot be resolved through retries (e.g. changing data format), so it is still important to <a href=\"https://github.com/crunchydata/pg_incremental#monitoring-pipelines\">monitor</a> your pipelines.</p>","contentLength":8175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipg7oy/incremental_archival_from_postgres_to_parquet_for/"},{"title":"Siren Call of SQLite on the Server","url":"https://pid1.dev/posts/siren-call-of-sqlite-on-the-server/","date":1739551579,"author":"/u/sausagefeet","guid":626,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipevoh/siren_call_of_sqlite_on_the_server/"},{"title":"A better technique to split HTML into structured chunks while preserving the DOM hierarchy.","url":"https://github.com/carlosplanchon/betterhtmlchunking/","date":1739544345,"author":"/u/carlosplanchon","guid":625,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipc503/a_better_technique_to_split_html_into_structured/"},{"title":"Switching on Strings in Zig","url":"https://www.openmymind.net/Switching-On-Strings-In-Zig/","date":1739542983,"author":"/u/simon_o","guid":627,"unread":true,"content":"<p>Newcomers to Zig will quickly learn that you can't switch on a string (i.e. ). The following code gives us the unambiguous error message :</p><pre><code>color</code></pre><p>I've seen two explanations for why this isn't supported. The first is that there's ambiguity around string identity. Are two strings only considered equal if they point to the same address? Is a null-terminated string the same as its non-null-terminated counterpart? The other reason is that users of  [apparently] expect <a href=\"https://en.wikipedia.org/wiki/Branch_table\">certain optimizations</a> which are not possible with strings (although, presumably, these same users would know that such optimizations aren't possible with string).</p><p>Instead, in Zig, there are two common methods for comparing strings.</p><p>The most common way to compare strings is using  with :</p><pre><code>stdmem colorstdmem colorstdmem colorstdmem color</code></pre><p>The implementation for  depends on what's being compared. Specifically, it has an optimized code path when comparing strings. Although that's what we're interested in, let's look at the non-optimized version:</p><pre><code> T a balen  blenalen  aptr  bptra ba_elem b_elema_elem  b_elem</code></pre><p>Whether we're dealing with slices of bytes or some other type, if they're of different length, they can't be equal. Once we know that they're the same length, if they point to the same memory, then they must be equal. I'm not a fan of this second check; it might be cheap, but I think it's quite uncommon. Once those initial checks are done, we compare each element (each byte of our string) one at a time.</p><p>The optimized version, which  used for strings, is <a href=\"https://github.com/ziglang/zig/blob/5b9b5e45cb710ddaad1a97813d1619755eb35a98/lib/std/mem.zig#L720\">much more involved</a>. But it's fundamentally the same as the above with <a href=\"https://www.openmymind.net/SIMD-With-Zig/\">SIMD</a> to compare multiple bytes at once.</p><p>The nature of string comparison means that real-world performance is dependent on the values being compared. We know that if we have 100  branches then, at the worse case, we'll need to call  100 times. But comparing strings of different lengths or strings which differ early will be significantly faster. For example, consider these three cases:</p><pre><code> str1  10_000  str2  10_000 \n    _  stdmem str1 str2 str1  a\"  10_000 str2  a\"  10_000\n    _  stdmem str1 str2 str1  999_999 str2  1_000_000\n    _  stdmem str1 str2</code></pre><p>For me, the first comparison takes ~270ns, whereas the other two take ~20ns - despite the last one involving much larger strings. The second case is faster because the difference is early in the string allowing the  loop to return after only one comparison. The third case is faster because the strings are of a different length:  is returned by the initial  check.</p><p>The  takes an enum type and a string value and returns the corresponding enum value or null. This code prints \"you picked: blue\"</p><pre><code> std \n    red\n    blue\n    green\n    pink color  stdmetaColorInvalidChoicecolorred  stddebugblue  stddebuggreen  stddebugpink  stddebug</code></pre><p>If you don't need the enum type (i.e. ) beyond this check, you can leverage Zig's anonymous types. This is equivalent:</p><pre><code> std  color  stdmeta\n        red\n        blue\n        green\n        pinkInvalidChoicecolorred  stddebugblue  stddebuggreen  stddebugpink  stddebug</code></pre><p>It's  obvious how this should perform versus the straightforward  approach. Yes, we now have a  statement that the compiler can [hopefully] optimize, but  still has convert our input, , into an enum.</p><p>The implementation of  depends on the number of possible values, i.e. the number of enum values. Currently, if there are more than 100 values, it'll fallback to using the same  that we explored above. Thus, with more than 100 values it does the  check PLUS the switch. This should <a href=\"https://github.com/ziglang/zig/issues/3863\">improve in the future</a>.</p><p>However, with 100 or fewer values,  creates a comptime  which can then be used to lookup the value.  isn't something we've looked at before. It's a specialized map that buckets keys by their length. Its advantage over Zig's <a href=\"https://www.openmymind.net/Zigs-HashMap-Part-1/\">other hash maps</a> is that it can be constructed at compile-time. For our  enum, the internal state of a  would look something like:</p><pre><code>// keys are ordered by length\nkeys:     [\"red\", \"blue\", \"pink\", \"green\"];\n\n// values[N] corresponds to keys[N]\nvalues:   [.red, .blue, .pink, .green];\n\n// What's this though?\nindexes:  [0, 0, 0, 0, 1, 3];</code></pre><p>It might not be obvious how  is used. Let's write our own  implementation, simulating the above  state:</p><pre><code>str keys  values redbluepinkgreen indexes strlen  indexeslen index  indexesstrlenindex  keyslen key  keysindexkeylen  strlenstdmem key str valuesindex\n        index </code></pre><p>Take note that  are ordered by length. As a naive implementation, we could iterate through the keys until we either find a match or find a key with a longer length. Once we find a key with a longer length, we can stop searching, as all remaining candidates won't match - they'll all be too long.  goes a step further and records the index within  where entries of a specific length begin.  tells us where to start looking for keys with a length of 3 (at index 0).  tells us where to start looking for keys with a length of 5 (at index 3).</p><p>Above, we fallback to using  for any key which is the same length as our target string.  uses its own \"optimized\" version:</p><pre><code>a baptr  bptra ba_elem b_elema_elem  b_elem</code></pre><p>This is the same as the simple  implementation, minus the length check. This is done because the  within our  loop is only ever called for values with matching length. On the flip side, 's  doesn't use SIMD, so it would be slower for large strings.</p><p>In my own benchmarks, in general, I've seen little difference between the two approaches. It does seem like  is generally as fast or faster. It also results in more concise code and is ideal if the resulting enum is useful beyond the comparison.</p><p>You usually don't have long enum values, so the lack of SIMD-optimization isn't a concern. However, if you're considering building your own  at compile time with long keys, you should benchmark with a custom  function based on .</p><p>We could manually bucket those  branches ourselves, similar to what the  does. Something like:</p><pre><code>colorlenstdmem colorstdmem colorstdmem colorstdmem color</code></pre><p>Ughhh. This highlights the convenience of using  to generate similar code. Also, do remember that  quickly discards strings of different lengths, which helps to explain why both approaches generally perform similarly.</p>","contentLength":6154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipbna2/switching_on_strings_in_zig/"},{"title":"GitHub - openorch/openorch: AI app platform. A language-agnostic, distributed platform for building microservices-based AI backends.","url":"https://github.com/openorch/openorch","date":1739528271,"author":"/u/Sand4Sale14","guid":628,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ip7jlt/github_openorchopenorch_ai_app_platform_a/"},{"title":"Here's What Devs Are Saying About New GitHub Copilot Agent – Is It Really Good?","url":"https://favtutor.com/articles/github-copilot-agent/","date":1739523054,"author":"/u/ImpressiveContest283","guid":629,"unread":true,"content":"<p>With the introduction of AI, coding has never been easier. To simplify it further, GitHub Copilot is here to improve the working experience, helping code faster and better. </p><p>If <a href=\"https://favtutor.com/articles/github-copilot-workspace-insights/\">GitHub Copilot</a> has undergone a transition from being a mere passive suggestion engine for the code to being an active collaborator that simplifies development, Agent Mode takes it one step further. Think of it as an autonomous co-coder, a peer programmer who prods and pushes forward the work on your projects.</p><h2><strong>The New GitHub Copilot Agent Mode</strong></h2><p>Agent Mode is like having a co-coder within the programming team, prioritizing your development activity over that of any other person. It now allows Copilot to:</p><ul><li> It constantly improves and enhances the code it generates, always searching for optimal solutions.</li><li>On detection of an error, it automatically assumes the responsibility for spotting and correcting the source, thus killing debugging efforts.</li><li>It provides you with all terminal commands and facilitates running those commands for more speedy working.</li><li>It goes to infer subsequent underlying tasks beyond your instructions so all solutions would be comprehensive.</li></ul><p>An example would be tasking the agent to implement a web application for tracking marathon training. <a href=\"https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/?utm_source=chatgpt.com\" data-type=\"link\" data-id=\"https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/?utm_source=chatgpt.com\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Agent Mode can set up the project structure</a> alone; create the relevant components and even fix runtime errors to provide seamless development.</p><figure></figure><h3><strong>Auto Complete Coding Tasks with ease</strong></h3><p>With <a href=\"https://code.visualstudio.com/blogs/2025/02/12/next-edit-suggestions\" data-type=\"link\" data-id=\"https://code.visualstudio.com/blogs/2025/02/12/next-edit-suggestions\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Next Edit Suggestions (NES)</a>, GitHub Copilot has introduced a feature of code editing that does editing by predicting and suggesting modifications to existing code. NES accounts for the fact that coding often involves refining code already written. </p><p>This feature is now available for use along with Visual Studio Code, where developers can enable NES to receive intelligent edit recommendations that thereby improve coding efficiency and accuracy.</p><h3><strong>Rapid Coding and Beginner-Friendly</strong></h3><p><strong>By providing high-level instructions, Copilot’s Agent Mode generates the necessary code components, eliminating the need for manual coding. </strong></p><p>Moreover, Agent Mode iteratively improves the code, optimizing performance and ensuring functionality without human intervention. Beyond the initial instructions, Copilot infers additional necessary tasks, such as setting up game logic and user interfaces, to create a fully functional application.</p><h3><strong>GitHub Copilot Agent vs other coding assistants</strong></h3><p>Cursor is an advanced AI-driven coding tool aimed at boosting developer efficiency.</p><p>Copilot is designed to blend effortlessly into several integrated development environments (IDEs) such as Visual Studio Code and JetBrains and provide immediate inline code recommendations and auto-completion options.</p><p>On the other hand, Cursor operates as an independent AI-enhanced editor derived from Visual Studio Code, equipped with features like proactive code generation, intelligent auto-completion, and smart rewriting capabilities.</p><p>While many users commend Copilot for its seamless integration and intuitive user interface, Cursor stands out for providing a richer integration of AI within its ecosystem.</p><h3><strong>A Glimpse into the Future</strong>: </h3><p>Though Agent Mode is already a leap into revolution, GitHub’s Project Padawan hints at an even more exciting future. It aims to create a self-sufficient software engineering (SWE) agent capable of completing entire tasks without human intervention. </p><p>Just picture a future where you tell Copilot to fix GitHub issues and write whole modules. Features like Issue resolution and continuous learning could transform the very way that developers approach software engineering.</p><h2><strong>Divided Opinions: The other side of the coin</strong></h2><p>Although several developers have included the Copilot in their flow, that is not to say they did not face a fair share of challenges. Netizens on X and Reddit are divided over the new AI Agent and have pointed out how the tool creates code or content that sometimes feels generic or shallow. </p><p><strong>Importantly, the AI-generated output needs several thorough checks against project requirements and coding standards.</strong></p><p>The Agent Mode of GitHub Copilot is set to change the landscape of software development by relieving developers from mundane tasks such as debugging, documentation, or testing. This will give developers more time and energy to concentrate on higher-level innovation and creative problem-solving. Will it take over programmers or not is a topic up for another debate..</p>","contentLength":4399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ip6dts/heres_what_devs_are_saying_about_new_github/"}],"tags":["dev","reddit"]}