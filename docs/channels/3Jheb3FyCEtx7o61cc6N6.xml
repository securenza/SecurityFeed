<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit - NetSec</title><link>https://news.securenza.be</link><description></description><item><title>Quantifying Swiss Cheese, the Bayesian Way</title><link>https://stephenshaffer.io/quantifying-swiss-cheese-the-bayesian-way-b2b512472d85</link><author>/u/t0sche</author><category>netsec</category><pubDate>Sat, 1 Nov 2025 18:20:52 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[The Exploit Prediction Scoring System (EPSS) is a machine-learning model that publishes the likelihood of a CVE being exploited within the next thirty (30) days. Each CVE gets a likelihood value between 0 and 1 — the higher the score, the greater the chance of exploitation. (or Grouped EPSS) is the probability that  (1) vulnerability on a given asset will see exploitation activity in the next thirty (30) days. We calculate it using the classic “at least one” formula from probability theory. If each CVE  has probability EPSS, then the probability that none are exploited is the product of their non-exploitation probabilities (the inverse of the EPSS score). Subtracting from 1 gives us the probability of at least one CVE, which I call:This provides a baseline probability of exploitation for each asset — assuming no controls. It’s a “worst-case exposure” measure. So how do we factor in our security controls?Think of your organization’s defenses as layers of Swiss cheese. Each layer has some holes (no control is 100% effective), but multiple layers together can reduce the chance of an attacker slipping through all of them. This is the classic  of risk management. If the holes (control failures) don’t line up, the threat is stopped; only when all holes align does the bad outcome occur.This is where Bayesian inference enters the picture. Applying this to exploit likelihood means we can treat each control as a probabilistic filter that reduces the base exploitation likelihood from EPSS. To do this rigorously, we need a way to quantify how effective each layer is — and to update that belief over time.No model mirrors reality perfectly — but building a quantifiable starting point lets us iteratively align our assumptions with new evidence.Let’s dive into a few assumptions we need to establish.EPSS is representative of global exploitation pressure, a proxy for attacker behavior across the internetVulnerabilities with a CVE are not the only vulnerabilities in our software. EPSS covers only CVE-tagged vulnerabilities, a large but incomplete subset of all flaws. Still, it’s the only publicly available probabilistic dataset, making it the most practical foundation.Grouped EPSS (EPSS) is the public exploit pressure on this asset if it were on the public internet.Control Effectiveness rates assume that given the population of vulnerabilities, the control prevents exploitation % of the time. Controls are treated as independent layers — an assumption we can later test with telemetry.With those assumptions set, we can start quantifying how much each defensive slice of cheese actually helps.Modeling Control EffectivenessIn this context, I define control effectiveness as the probability that a given control prevents the exploitation of a vulnerability. Think of it as the defensive success rate for a specific layer.A network firewall might block 70% of exploit attempts.Assuming their effects are roughly independent, combined coverage could push actual likelihood well below the baseline.But how do we know those percentages?We usually don’t. Vendor claims are vague, telemetry can be incomplete, and empirical data is scarce. So we start with informed estimates — structured, measurable beliefs we can later update with data.One practical way to form our initial belief (the ) is to survey subject matter experts (SMEs) about control performance. Even if imperfect, this exercise builds a culture of quantitative reasoning and provides the starting point for Bayesian updating.That notion aside, we can construct a survey question using a scenario that accounts for our assumptions:Given an asset with 10 vulnerabilities present behind a well-configured firewall, how many does the firewall prevent the exploitation of?In this context, we can define well-configured as one that meets our internal standards (e.g., signatures current, traffic monitored, rule sets tuned).The answer options are integers between 0 and 10. Drawing on inspiration from How to Measure Anything in Cybersecurity Riskand , we ask twice: once for the median estimate (we directly ask our SMEs for an answer to the question and assume it’s the median value), and again for the 90th-percentile value — the number they’re 90% confident the true effectiveness is below.We then weight responses by self-rated expertise on a Likert scale (1 = novice, 5 = expert), producing both equal-weight and expertise-weighted models.Let’s say for demonstration purposes that we survey 12 SMEs. An example of those responses is below:In order to analyze this data, we turn to the Beta distribution. The Beta distribution is ideal for representing probabilities between 0 and 1 when we have uncertainty. The Beta distribution accepts two parameters:α (alpha) which is equal to the number of successes + 1β (beta) which is equal to failures + 1In simple terms, α and β shape the curve describing how confident we are about control success and failure rates.Our challenge is to infer α and β from SME p50 and p90 values. In Python, we can use  from  to reverse-engineer the Beta parameters that produce those percentiles.This function finds α and β whose cumulative distribution matches our SME-reported medians and 90th percentiles. By running this across all SME responses, we can build aggregate Beta distributions — both equally weighted and expertise-weighted versions — to visualize our shared belief about the control’s effectiveness and show the shape of our uncertainty.These distributions give us multiple values to pick for our central tendency single point value. In this scenario, I usually pick the lowest value in order to remain conservative with our judgments and overstate risk. Based on our example here, I would select a firewall control effectiveness rate for exploit prevention of 0.44, the expertise-weighted median value. The wide spread reminds us that uncertainty remains high until we gather more data.Updating our Beliefs: The Bayesian WayIntegrating Control Effectiveness with EPSSgOnce we have a control effectiveness rate, we can update our asset-level exploit likelihood by multiplying it by the inverse of our control effectiveness rate (our control failure rate).If an asset’s EPSS is 0.76 and our firewall effectiveness posterior mean is 0.44, then:That’s a 42% adjusted exploit likelihood — more realistic than assuming total exposure.Continuously Updating Control EffectivenessNow that we have our initial beliefs quantified, we can now update them with observations. Observation data can come in many forms, and for the firewall effectiveness example, we can likely look at our firewall logs to find exploit-related events. We could formulate a system that parses these logs and picks out the events that were successes and failures, depending on what type of logic an organization wants to implement.For our example here, let’s say we observe 15 successful exploit prevention events from our firewall logs, and 5 unsuccessful events in the next month. Using the reverse-engineered alpha and beta values from our expertise-weighted Beta distribution, we update our beliefs like so:Below is the resulting Beta distribution. The central tendency (our updated best estimate of control effectiveness) increased from 0.44 to 0.703, meaning we’ve gained confidence that the firewall blocks about 70.3% of exploit attempts. Our confidence interval also tightened.We can then turn around and make yet another update to our exploit likelihood on the asset (In practice, EPSS updates daily, so the asset’s baseline likelihood will shift as well.):From Static to Living ModelsThis process transforms our exploit likelihood model into a living system that evolves as evidence accumulates. Observation sources include:Firewall and EDR telemetry — blocked vs. successful exploit attemptsBreach and attack simulation tools like Red team or purple team exercisesIncident reports tied to control failuresAny feedback loop that distinguishes success from failure can feed the modelEach update shifts our posterior a little closer to reality. Over time, our understanding of “how holey our cheese really is” becomes quantifiable.A shared, continuously updated model lets all stakeholders align on reality and make smarter decisions about time, effort, and budget.In vulnerability management, our goal is to reduce exploit risk. By quantifying control effectiveness transparently, we strengthen trust in both the model and the decisions it supports.If you’re familiar with FAIR-CAM (Factor Analysis of Information Risk — Controls Analytics Model), you’ll recognize the conceptual overlap. FAIR-CAM formalizes how individual control functions combine to influence overall loss event frequency.I view what I outline here as  — a focused, quantitative slice of that same principle, applied specifically to exploit prevention. Instead of modeling every control family, we’re zooming in on how a single control’s effectiveness updates our belief in exploit likelihood through Bayesian inference. It’s a practical on-ramp for teams not yet ready for full FAIR-CAM implementation.The structure is identical in spirit: represents  (global exploit pressure). functions as  (how often the control stops that pressure). mirrors FAIR-CAM’s goal of adjusting control factors as evidence accumulates.In other words, this framework is an applied subset of FAIR-CAM — one that demonstrates how you can begin quantifying and updating control performance today, even before a full FAIR-CAM implementation.What I demonstrated here was the use of a single control’s effectiveness in reducing exploitation likelihood. However, organizations usually have multiple controls in place, and FAIR-CAM accounts for this. We can borrow that logic and apply it directly to this scenario by building additional control effectiveness models to update the exploit likelihood in succession:Exploit Vector Incident LikelihoodWhat if we could model the likelihood of an incident that includes exploitation within our environment? Where would we start, and how could we continuously update this model with observations? These questions point toward a vulnerability-to-incident pipeline, a frontier I am currently exploring.We can’t eliminate uncertainty — but we can measure it, update it, and communicate it.Bayesian updating gives us a disciplined way to evolve our beliefs as evidence accumulates. By pairing EPSS (our view of global exploit pressure) with quantified control effectiveness, we move from static assumptions to a dynamic, evidence-driven model.Each layer of Swiss cheese becomes a measurable probability curve — not just a metaphor, but a quantifiable defense system we can track, test, and improve.]]></content:encoded></item><item><title>open source CVE scanner for project dependencies. VSCode extension.</title><link>https://marketplace.visualstudio.com/items?itemName=abhishekrai43.vulscan-mcp-vscode</link><author>/u/FeelingResolution806</author><category>netsec</category><pubDate>Sat, 1 Nov 2025 14:32:34 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[Find and fix security vulnerabilities in your project dependencies - right inside VS Code!VulScan-MCP automatically scans your project dependencies for known security vulnerabilities (CVEs) and provides clear, step-by-step instructions to fix them. Just ask Copilot about security, and it handles the rest!🔍  - Checks NVD and OSV databases for latest vulnerabilities📦  - npm, pip, Maven, Go, Cargo, Composer, and more🎯  - No commands to remember, just ask naturally📝  - Get step-by-step remediation guidance🚫  - Never modifies your code automatically🌍  - Works on Windows, macOS, and LinuxOpen VS Code and install:Press  (Windows/Linux) or  (macOS)Search for "VulScan-MCP Security Scanner" - Required for MCP integration The extension automatically:Detects your Python installationInstalls required dependencies on first useRegisters the MCP server with CopilotWorks immediately - no configuration needed!Simply ask Copilot Chat about security:"Check for vulnerabilities"
"Scan my dependencies"
"Any security issues?"
The first time you use it, it may take a few seconds to install dependencies (requests library). After that, it's instant!After scanning, you'll get a detailed report like this:# VulScan-MCP Vulnerability Report

## Summary
- Total Dependencies Scanned: 87
- Vulnerable Dependencies: 2
- Manifest Files Found: 2

### Scanned Files:
- `package.json` at `/frontend/package.json`
- `requirements.txt` at `/backend/requirements.txt`

## Vulnerabilities Found

### HIGH Severity

#### lodash @ 4.17.15
- **Severity:** HIGH
- **CVEs Found:** 3 (OSV) + 2 (NVD)
- **Fix:** Upgrade to version 4.17.21 or later

WARNING: This fix requires a version upgrade. Test thoroughly 
in a staging environment before deploying to production.

### MEDIUM Severity

#### tslib @ ^2.3.0
- **Severity:** MEDIUM
- **CVEs Found:** 1 (NVD)
- **Fix:** Upgrade to version 2.6.0 or later

## Recommendations

1. **Prioritize HIGH and CRITICAL severity vulnerabilities**
2. **Test all updates in a staging environment first**
3. **Review changelogs before upgrading**
4. **Run your full test suite after updates**
5. **Monitor for new vulnerabilities regularly**
Supported Package Managers, , Simple Questions That WorkJust ask Copilot Chat naturally:"Check for vulnerabilities"The extension automatically activates - no need to mention "MCP" or "tool"! - Finds all dependency files in your project - Queries NVD and OSV databases for CVEs - Shows vulnerabilities grouped by severity - Provides clear remediation instructionsPowerShell or Command PromptPython from Microsoft Store or python.orgIntel and Apple Silicon (M1/M2/M3)Python via Homebrew or system PythonUbuntu, Debian, Fedora, ArchPython 3.11+ from package manager - All scanning happens on your machine - Your code stays private - Full transparency - Only checks public CVE databasesYour code never leaves your computer!"MCP server not available in Copilot"Ensure you have  of this extension installedReload VS Code window ( → "Developer: Reload Window")The server registers automatically - no settings.json configuration needed!Make sure  or  command works in your terminal:python --version  # or python3 --version
Ensure GitHub Copilot is installed and activeReload VS Code ( → "Reload Window")Check Python version:  (should be 3.11+)First scan may take 10-20 seconds while installing dependenciesCheck your internet connection (needed for CVE databases)Ensure you have dependency files (package.json, requirements.txt, etc.)Try scanning again - APIs may have rate limitsPowered by NVD, OSV, and the Model Context Protocol]]></content:encoded></item><item><title>r/netsec monthly discussion &amp; tool thread</title><link>https://www.reddit.com/r/netsec/comments/1olp81v/rnetsec_monthly_discussion_tool_thread/</link><author>/u/albinowax</author><category>netsec</category><pubDate>Sat, 1 Nov 2025 14:29:03 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[Questions regarding netsec and discussion related directly to netsec are welcome here, as is sharing tool links.Always maintain civil discourse. Be awesome to one another - moderator intervention will occur if necessary.Avoid NSFW content unless absolutely necessary. If used, mark it as being NSFW. If left unmarked, the comment will be removed entirely.If linking to classified content, mark it as such. If left unmarked, the comment will be removed entirely.Avoid use of memes. If you have something to say, say it with real words.All discussions and questions should directly relate to netsec.No tech support is to be requested or provided on r/netsec.As always, the content & discussion guidelines should also be observed on r/netsec.Feedback and suggestions are welcome, but don't post it here. Please send it to the moderator inbox.]]></content:encoded></item><item><title>EDR-Redir V2: Blind EDR With Fake &quot;Program Files&quot;</title><link>https://www.zerosalarium.com/2025/11/EDR-Redir-V2-Blind-EDR-With-Fake-Program-Files.html</link><author>/u/Cold-Dinosaur</author><category>netsec</category><pubDate>Sat, 1 Nov 2025 10:52:15 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[
  In previous articles, I demonstrated using Windows' bind link feature to block
  or redirect Antivirus/EDR from accessing their executable folder. You can
  review this article via the link:

  However, some EDRs provide good protection for their operating folders,
  resulting in failed bind link creation.

  This time, I will upgrade EDR-Redir to version V2. Of course, I will
  still use bind link technology, but in a completely different way.

  I will experiment with EDR-Redir V2 using Windows Defender on Windows 11. With
  this new approach, I'm quite confident it will work with many Antivirus/EDR
  solutions.
1. The Idea Behind Working With EDR-Redir V2
  When software is installed on Windows, it typically resides in a subfolder,
  such as , , ,
  and so on.

  Antivirus and EDR software are no exception; most of them are located in
  either  or . Windows Defender,
  however, is found in .

  Antivirus and EDRs, in order to protect their operating folders, typically
  prevent writing files there. However,
  they cannot stop file writing to their parent folder. For example, if
  they block writing to the  folder, they inadvertently
  prevent other software from being installed on the machine, which can cause
  significant inconvenience for users.
So why not think outside the box? Instead of creating a bind link to
  the EDR's folder, we could
  create a bind link to the Program Files folder, for example.

  When dealing with the Program Files folder, we encounter the issue of ensuring
  that other software, aside from the EDR, functions normally.
  The idea here is to create bind links so that a folder points back to
    itself.
  The steps to implement this idea are as follows (I will provide an example
  using the  folder):
Query all the folders within the Program Files folder.
    Create corresponding folders in a location you fully control
    ().
  
    Create bind links from the folders in Program Files that point to their
    corresponding folders in .
  
    Continue creating bind links from  that point to the
    corresponding folders in Program Files. This will create a loop, causing
    access to the folders in Program Files to circle back to themselves. Most
    importantly, we will not create a bind link for the EDR's folder at this
    stage.
  
    Create a bind link from Program Files to  to force the
    redirection of the EDR's folder through .
  
    At this point, you can perform DLL hijacking by dropping executable files
    that the EDR usually interacts with into , allowing you to
    leverage them to activate in place of the EDR.
  
  Once these steps are successfully completed, we may be able to effectively
  redirect even the most stubborn types of EDRs.

  2. Experimenting With EDR-Redir V2 Using Windows Defender
First, you can download  via the link below.Windows Defender on
  Windows 11 is located at C:\ProgramData\Microsoft\. Therefore, I will
  target this folder for the attack.
I will run  with the following parameters:
    EDR-Redir.exe C:\ProgramData\Microsoft c:\TMP\TEMPDIR
    "C:\ProgramData\Microsoft\Windows Defender"
   is the folder I need to redirect elsewhere.
 is the target folder. C:\ProgramData\Microsoft\Windows Defender is the exception folder; it
  will not have a link loop created to block
  Defender.

  During execution, EDR-Redir will print to the console information about which
  bind links will be created for easier monitoring. As shown in the results, I
  successfully redirected Windows Defender to C:\TMP\TEMPDIR.

  At this point, Windows Defender will always see the folder
   as the parent folder of its operating folder.

  Antivirus/EDR can only protect their operating folders; they cannot intervene
  in the parent folders of these directories. Once the parent folder is
  successfully attacked, the protection of the operating folders by the EDR
  becomes meaningless.

  When programming, many developers may not consider the possibility of a folder
  like Program Files being redirected. Therefore, I suspect that the list of
  EDRs affected by this technique will be quite extensive.
The defensive approach
  is to monitor the use of bind links with folders like Program Files to ensure
  they aren't tampered with by the hands of attackers.
Some books you should read to sharpen your cybersecurity skills, especially
    in offensive security:]]></content:encoded></item><item><title>Automating COM/DCOM vulnerability research</title><link>https://www.incendium.rocks/posts/Automating-COM-Vulnerability-Research/</link><author>/u/TangeloPublic9554</author><category>netsec</category><pubDate>Thu, 30 Oct 2025 20:24:07 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[COM (Component Object Model) and DCOM (Distrubuted COM) have been interesting components in Windows from a security perspective for many years. In the past, COM has been a target for many purposes. Not only have many vulnerabilities been discovered in COM, but it is also used for lateral movement or bypassing techniques.Because of this, many (security) research is already conducted in this area. So to look for new vulnerabilities or techniques, another approach would probably lead to better results (vulnerabilities). I couldn’t find any tooling/blogs related to fuzzing COM/DCOM, but correct me if I’m wrong. As fuzzing MS-RPC was proven to be a successful approach to discovering new vulnerabilities, I wondered if the same concept could be applied to COM/DCOM.I decided to write a fuzzer around the OleViewDotNet tool from James Forshaw. The tool is mainly known for it’s GUI (Graphical User Interface). However, it’s power really comes with working with the command line options, more on this topic later.This white paper describes how COM/DCOM works and what complications it has. In the next chapters, the white paper will describe how security research can be automated using the fuzzing approach. Since this approach comes with some problems, it describes how these problems were overcome (at least partially).The white paper continues to describe what capabilities the COM fuzzer has and discusses some examples. Finally, the white paper will share some results and discuss how the fuzzer can be improved.Essentially, COM solves a problem for developers. In the early days of Windows, software developers faced a major interoperability. Different applications were written in different languages (C, C++, Visual Basic, etc.). There was no standard way for programs or components to talk to each other or reuse code across applications. Developers often had to recompile or rewrite code just to use it in another program.COM (Component Object Model) is Microsoft’s technology for building binary software components that can interact with each other, regardless of the programming language they were created in.graph TD
    A1["C++ Component<br/>(e.g., Shape.dll)"]
    A2["Visual Basic App"]
    A3["Python Script"]
    A4["C# Application"]

    B1["COM Runtime<br/>(Component Object Model)"]
    B2["Common Binary Interface<br/>(IUnknown, Interfaces, GUIDs)"]

    A1 --> B2
    A2 --> B2
    A3 --> B2
    A4 --> B2

    B2 --> B1

    B1 --> C1["Interoperable Object Communication"]
    C1 --> C2["Language-Independent Reuse<br/>of Components"]
COM has a few core ideas that are necessary to get familiar with to understand how COM works. The first one are interfaces.A COM object exposes functionality through interfaces. Each interface is identified by a GUID (Globally Unique Identifier). COM Clients don’t care how the object is implemented since they just call methods via its interface. The interface ensures that the client and server communicate using the same rules. It uses the Interface Definition Language (IDL) to create an interface. Consider the following example interface:All COM interfaces inherit from a fundamental base interface called , which defines three methods:The  gets pointers to other interfaces and ,  manage the object’s lifetime.A COM Class is the actual implementation of one or more interfaces, and it is the blueprint for creating a COM object. It defines the functionality (methods and properties) available in the COM component. Every COM Class is uniquely identified by a Class ID (CLSID), which is a Globally Unique Identifier (GUID).A client application uses the CLSID to ask the COM runtime (via functions like ) to instantiate an object of that specific class. CLSIDs are registered under the  key in the Windows Registry, pointing to the location of the actual component (DLL or EXE).1
2
3
4
5
6
7
8
9
10
11
12
13
The ProgID (Programmatic Identifier) is a user-friendly, human-readable string that serves as an alias for a COM Class’s CLSID. It provides an easy-to-use name for creating a COM object, often used in scripting languages like VBScript’s . ProgID’s typically follows the pattern: <Program>.<Component>.<Version>. Example: .There are also version-independent ProgID’s that omit the version, example: . ProgIDs are registered under , and each ProgID key contains a subkey that maps it back to its corresponding CLSID.The AppID is a GUID that uniquely identifies a COM server (the executable or DLL that hosts one or more COM Classes) for configuration purposes. Essentially, it’s an identifier that groups multiple related COM classes.DCOM builds upon the core COM architecture by adding a network protocol layer, making remote object access feel like local access. DCOM primarily uses Remote Procedure Call (RPC) over network protocols like TCP/IP to facilitate communication between the client process and the object’s server process, which are on separate machines.Proxies and stubs are used to enable a client on one machine to call a method on an object on another machine. A Proxy object is created on the client machine. The client calls methods on this local proxy object. The proxy packages the method call arguments, sends them across the network using RPC (marshalling), and waits for a response.The client requests the object’s creation using the CLSID or ProgID. The local COM Service Control Manager (SCM) contacts the SCM on the remote machine to locate, authenticate, and launch the COM server process (EXE), which then creates the object.graph TD
    A[Client Application] --> B[COM Interface / Proxy]
    B -->|Local Call| C[Local COM Object<br>]
    
    B -->|Remote Call via RPC| D[Network Layer<br> which is RPC over TCP/IP]
    D --> E[Remote Machine<br>DCOM Server Process]
    E --> F[COM Object Stub]

    %% Labels
    subgraph Local_Machine[Local Machine]
        A
        B
        C
    end

    subgraph Remote_Machine[Remote Machine]
        E
        F
    end
When a COM class is defined, it represents a blueprint for creating COM objects. When a client application wants to use a COM object, it calls a function like , passing in the CLSID of the class and the interface it needs. This call is handled by the Service Control Manager (SCM), the COM service responsible for locating and activating the appropriate component.The SCM checks the registry to determine whether the COM server is local or remote, and whether it runs as a DLL or an executable. If it’s a DLL, the SCM loads it directly into the client’s process. If it’s an EXE, it starts the executable (if not already running) and establishes communication with it.sequenceDiagram
    participant Client
    participant SCM
    participant Server
    participant ClassFactory
    participant Object

    Client->>SCM: CoCreateInstance(CLSID)
    SCM->>Server: Load/start COM server (DLL or EXE)
    Server->>ClassFactory: DllGetClassObject(CLSID)
    ClassFactory->>Object: CreateInstance()
    Object-->>Client: Return interface pointer (e.g., IMyInterface*)
Using PowerShell reflection, we can create a COM object from a CLSID.1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Working with COM classes using the registry is quite inconvenient. This is where the tool OleViewDotNet comes in. This is a .net OLE/COM viewer and inspector to merge functionality of OleView.It allows you to find COM objects through a number of different views (e.g., by CLSID, by ProgID, by server executable), enumerate interfaces on the object and then create an instance and invoke methods. It also has a basic container to attack ActiveX objects to so you can see the display output while manipulating the data.The tool comes with a GUI that most people use when researching COM. However, the cmdlets accessible from PowerShell provide far more flexibility from a researching perspective.Most of the cmdlets of OleViewDotNet parse a COM database that contains a set of classes. These classes can be gathered using :1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Here we get the first 10 COM classes that  gathered. Now it’s going to get annoying if every time you want to look at some COM information you need to run the lengthy Get-ComDatabase command. That’s why a simple save and reload feature was implemented. Running the following command will write the current database out to the default database location:This is where things get more complicated, but it also is the reason the newer features of OleViewDotNet make a solid base for a fuzzer. In order to actually invoke COM procedures, we need a COM client to do so.As explained by James Forshaw: The NdrProxyInitialize function can be used to obtain the COM interface from its  structure by passing in the interface pointer to a proxy. Although this approach is not as flexible as a fully custom implementation, it provides a straightforward way to manage the transport layer without concern for platform or protocol differences. It can also operate with an existing COM object by querying the appropriate interface, extracting the buffer, and making calls to the remote server.In order to create a COM client for an interface, we first create a new COM object from a CLSID and gather its interface ID’s using:1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Next, we pick one of the interface ID’s, example 866738b9-6cf2-4de8-8767-f794ebe74f4e and create the client:Listing the members of the client object reveals its procedures:Finally, we can execute the procedure, for example  using:And this opens the calculator app.flowchart TD
  A([Start])
  A --> B[Create COM object<br/>]
  B --> C[Enumerate interfaces<br/>]
  C --> D[Pick an IID<br/>]
  D --> E[Create COM client for IID]
  E --> F[Inspect client members<br/>]
  F --> G[Invoke procedure]
  G --> H([Result: Calculator opens / remote call executed])

  %% Side-note about NdrProxyInitialize
  subgraph NOTE[ ]
    N1(["NdrProxyInitialize: obtain COM interface from a MIDL_STUB_MESSAGE"])
  end
  E -.-> N1
  style NOTE fill:#fff3cd,stroke:#e6b800,stroke-width:1px
  style A fill:#e3f2fd,color:#000
  style H fill:#e8f5e9,color:#000
IDispatch is a fundamental COM interface, that allows scripting languages (VB, PowerShell) and higher-level languages (.NET) to interact with COM objects that implement it without prior knowledge. It achieves this by exposing unified methods that describe and interact with the implementing object.  maps names of methods or properties to an integer value named DISPID.Our above example included the IDispatch interface:This is the reason our COM client was able to map the name of the methods/procedures, like . But not every COM class/object includes this interface. Proxies lose name information when compiled from MIDL to their C marshaled representation. Therefore, OleViewDotNet just generates placeholder names, for example, method names are of the form .1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
If the proxy is for a type that has a known definition, such as from a Windows Runtime type or a type library, then OleViewDotNet will try and automatically apply the names. While losing the procedure name context is a bummer, it still allows invoking the procedure and A LOT of classes do not support the IDispatch interface, which means that skipping them would reveal far less interesting results.With the previous chapters in mind and the power of OleViewDotNet, there lies an obvious solution to automating vulnerability research for COM; we loop over all classes it’s interfaces, create a COM client for it and invoke the procedures it holds. We feed the input parameters with randomized values and observe the result; fuzzing.The fuzzing approach by general looks as follows:flowchart TD
    A[Start: Define Target Program] --> B[Generate Inputs]
    B -->|Random / Mutated / Structured| C[Fuzzer Engine]
    C --> D[Feed Inputs to Program Under Test]
    D --> E[Program Execution]
    E -->|Crash / Exception / Hang?| F{Program Behavior}

    F -->|Yes| G[Log & Record Test Case]
    F -->|No| H[Continue Testing Loop]

    G --> I[Analyze Crash & Reproduce Bug]
    H --> B

    I --> J[Fix Vulnerability]
    J --> K[Improve Fuzzer or Add Regression Test]
    K --> B

    style A fill:#a2d2ff,stroke:#333,stroke-width:1px,color:#000
    style C fill:#bde0fe,stroke:#333,stroke-width:1px,color:#000
    style D fill:#ffc8dd,stroke:#333,stroke-width:1px,color:#000
    style F fill:#ffafcc,stroke:#333,stroke-width:1px,color:#000
    style I fill:#cdb4db,stroke:#333,stroke-width:1px,color:#000
Of course this was easier said than done, who would’ve thought. However, a large part of the MS-RPC-Fuzzer design could be reused for this project! First, we need a way to collect COM class information, the interfaces it holds and the interface it’s procedures. Each procedure has a definition with the input parameters it takes, example: Proc5(string p0, int p1, NtApiDotNet.Ndr.Marshal.NdrUnsupported p2).The first problem is parameters and a parameter its input value. This is also the most important factor for the fuzzer, because it will need to contain a value that will hopefully identify interesting COM classes. For fuzzing web applications this is a rather easy step, we can just always send strings. But for COM, there are more parameter types than just strings.We don’t need or even can provide each type with a value. The most essential types for the fuzzing are the primitives (Strings, Integers, etc.) and arrays. We can provide these with random values with different sizes or lengths.In summary, the COM fuzzer implement two functions: One function  will be responsible for extracting all parameters of a Method (procedure) and will call the other function  to provide it a value.Some types are “complex” like NtApiDotNet.Ndr.Marshal.NdrUnsupported and we cannot provide them a value. However, we will need to provide the procedure the right format parameter type if we want to fuzz it. For example, when a procedure takes a string input parameter and a complex input parameter, we still want to fuzz the string input for that procedure.To solve this, we can dynamically create an instance for the parameter using PowerShell reflection:The “complex” parameter type will most likely be provided with a value that the COM server will reject, because it expects some kind of value. However, this will provide us with the right parameter type so that we can still fuzz the other input parameters like strings.flowchart TD
    A[Call Format-DefaultParameters for a Method] --> B[Call Format-ParameterType for each parameter type]
    
    B --> F{Type is primitive?}
    F -- Yes --> G[Generate random value of appropriate size/length]
    F -- No --> H{Type is array?}
    
    H -- Yes --> I[Loop over array members]
    I --> J[Call Format-ParameterType recursively for each member]
    
    H -- No --> K{Type is complex}
    K -- Yes --> L[Create instance using PowerShell reflection]
    K -- No --> M[Skip or provide default/null value]

    G --> N[Return value to Format-DefaultParameters]
    J --> N
    L --> N
    M --> N
    N --> O[Assemble complete parameter set]
    O --> P[Use parameters for fuzzing input]

    style A fill:#eef,stroke:#333,stroke-width:1.5px,color:#000
    style P fill:#bfb,stroke:#333,stroke-width:1.5px,color:#000
    style L fill:#ffd,stroke:#333,stroke-width:1.5px,color:#000
    style M fill:#fcc,stroke:#333,stroke-width:1.5px,color:#000
Let’s take a step back. Before we can actually start fuzzing, we need to collect the required information to do so. The fuzzer implements a cmdlet  that can collect the COM class information from different input types; whole registry, COM database file, a specific CLSID or a list of CLSID’s.In the following example we tell  to extract the information for the CLSID 13709620-C279-11CE-A49E-444553540000:The required argument is , which tells the function where to export the results to. This is in the form of a JSON file. The following is a small part of the file that was generated using the above example:1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
Why exporting it to a JSON file instead of directly fuzzing from the parsed CLSID you may ask. The answer to that is efficiency and time.Creating an COM object isn’t always instant. Sometimes, the system is not able to create a COM object for the parsed CLSID and will error AFTER a timeout of around a minute.Some interfaces hold procedures that crash PowerShell, having a JSON file allows you to remove the interface from the file so that the fuzzer won’t fuzz that interface.In general, when the JSON file was created for a COM class, you can always use it to fuzz because nothing will change, which is the case with, for example, RPC endpoints. The COM class may have a different implementation on another version of Windows, or after an update. But in general it should work when copying and pasting it to another system.Windows (11) contains a lot of COM classes:While the tool is capable of fuzzing every class, it still would take a lot of time to do so. So it’s better to have a specific set of COM classes where to focus on. The tool can filter some classes in different contexts; Remotely accessible classes, classess initiated from services and interactive user COM objects.For example, we can point the  to collect COM classes initiated from services:This is also the reason the COM-fuzzer comes with 3 templates for COM classes that anyone can use to start fuzzing quickly. These are; ,  and .For some COM classes there is a bug when creating a COM object, after a specific amount of time, the PowerShell session crashes AFTER already having created the object. So imagine the following scenario; we parse 100 COM classes and COM class on index 18 causes the PowerShell session to crash, but it will do so after some time while the function for collecting the information is already at index 84. One more reason to export the results to a JSON file instead of directly fuzzing.During my research, I identified some of these classes and hard code blacklisted them in the  function. It could be that there are more COM classes/objects that show this behavior, so be aware of that when using the tooling.Let’s get back to the fuzzing process. Once we determined the parameter type, the fuzzer provides it with a value, either standalone or within an array. The function  can be parsed with parameters like minimal string length or minimal integer size. The function takes the parameter type as value and then uses random to generate the input. As an example, the case for a  parameter type is given:1
2
3
4
5
6
7
8
9
10
11
12
13
14
You may notice the  variable. This is an important deal for the fuzzer to know where our input landed and determine relationships.The canary method is used to expose an information leak by giving different versions of a sensitive document of several suspects and seeing which version gets leaked. In our case, for string parameter types, we send a recognizable string and attach a random value to it. Example:To find the information leak (know where our input lands in the background), we will need a tool that can monitor processes in the background of the Windows system. The ideal tool for this is Process Monitor. Process Monitor is an advanced monitoring tool for Windows that shows real-time file system, Registry and process/thread activity.While having Process Monitor listening in the background, we can start our fuzzer. When we apply a filter in Process Monitor that includes our recognizable part of the string e.g., , we can see where our fuzzing input landed and what kind of function calls are being made with our input.Process Monitor with canary as applied filterWhen you know which process ID (PID) is making the request, a next step would be to filter the specific PID and look for further calls it makes. A user can now export the Process Monitor results into a CSV file. The fuzzer provides the  cmdlet to import the data into Neo4j. More about this in a later chapter.Remotely invoking procedures is also possible (DCOM). Although you will need high privileges to actually invoke the COM call on the remote system, the fuzzer includes an option to do so. Using the  together with the credentials of the user that is allowed to invoke the COM call, it will fuzz COM on the remote system instead.This works by first parsing the user provided arguments to . This creates a credential object which then gets parsed to . By default,  uses the following authentication level configuration:AuthnSvc:        WinNT
AuthnLevel:      PKT_PRIVACY
ImpLevel:        IMPERSONATE
Capabilities:    None
Which should be fine for most tasks. To figure out if a COM class can be activated and executed remotely, OleViewDotNet’s cmdlet Format-ComSecurityDescriptor can help:1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
Perfect for discovering new lateral movement techniques ;).While the fuzzer is running, it will store the results into separate JSON files. These are allowed.json and denied.json depending on the returned message. The structure of the result file looks something like:1
2
3
4
5
6
7
8
9
10
11
12
13
14
It stores the input, the output and windows message according to the integer found in the output. But this json file can easily get as big as 50.000 lines. Also, from a JSON structure it is quite hard to see the relations. This calls for a solution that can provide us with tooling that makes analysis easy.The fuzzing results are obtained within standalone JSON files, so a user can do analysis however he likes. Showing the relation between COM class, interface, procedure, input and output is a effective way to get a better understanding about the COM implementation. For this, I chose to reuse the Neo4j wrapper from my MS-RPC-Fuzzer. I rewrote some of the functions to import our JSON files to a (remote) Neo4j database.The user can provide the JSON file and pipe it to the cmdlet :To query the data in Neo4j, the Cypher language is used to make the relations. The following Cypher is an example:This query looks for high privileged file operation function calls. What does this mean? I wrote a rule that when a high privileged identity, like  is making the function call, or it is impersonating this identity, and the function call contains , it will mark it as an  node.Viewing relationships in Neo4j with high privileged file operationsViewing this relationship is only possible because we combined both our fuzzing input and the Process Monitor results into the same Neo4j database. In the above example, a file is being written with our user input provided as NT AUTHORITY\LOCAL SERVICE.To do this, the user should first import the fuzzing results into Neo4j:Next, the user should export the Process Monitor results into a CSV file and import it to Neo4j:By default, the value for the canary is . But you can change this while fuzzing using the  argument with . Make sure to change the canary value for  as well if you changed this.In summary; the fuzzer has 3 phashes; inventarize (collecting information), fuzzing and analysis.graph TD
    User([User])

    %% Input and output styling
    classDef input fill:#d4fcd4,stroke:#2b8a3e,stroke-width:2px,color:#000;
    classDef output fill:#fff3cd,stroke:#ffbf00,stroke-width:2px,color:#000;

    %% Phase 1: Gather COM Data
    User --> A1[Get-ComServerData]
    A1 --> A2[Target or context specified]
    A2 --> A3[ComServerData.json]
    A3 --> B1[Invoke-ComFuzzer]

    %% Phase 2: Fuzzing
    B1 --> B2[log.txt Call History]
    B1 --> B3[allowed.json]
    B1 --> B4[denied.json]

    %% All fuzzer outputs used in Phase 3
    B3 --> C1[Import-DataToNeo4j]
    B4 --> C1

    %% Phase 3: Analysis
    C1 --> C2[Neo4j Database]
    C2 --> C3[Graph Visualization & Querying]

    %% Apply styling
    class A3 input;
    class B3,B4,B2 output;

    %% Labels for clarity
    subgraph Phase1 [Phase 1: Initialize COM]
        A1
        A2
        A3
    end

    subgraph Phase2 [Phase 2: Fuzzing]
        B1
        B2
        B3
        B4
    end

    subgraph Phase3 [Phase 3: Analysis]
        C1
        C2
        C3
    end
First, specify a target to . This can be the whole registry, a specific CLSID, a list of CLSIDs or a COM database file. This will output a JSON file , containing the classes their CLSIDs, interfaces and procedures, which the user can parse to the fuzzer. If there is no target specified, it will default to the entire registry.The fuzzer , takes the exported JSON file from the previous phase as required input. The fuzzer will output maximal 2 JSON files and one log file. It will write the COM calls before invoking them to , this way if there is a crash (BSOD), the user will know which call was responsible (last line).It will separate the fuzz results into 2 JSON files:Fuzzed inputs that lead to Access DeniedThe user can use the JSON files for analysis as he likes. However, the fuzzer has an option to import them into your Neo4j instance using the  cmdlet. The fuzzer has a data mapper that makes relations for the data, easy as that.The whole idea is to gain insights into COM/DCOM implementations that may be vulnerable using an automated approach and make it easy to visualize the data. By following this approach, a security researcher will hopefully identify interesting COM classes/implementations in such a time that would take a manual approach significantly more.While some very interesting COM classes were identified during my research, for now only CVE-2025-59253 got assigned. This shows that the tool is effective and can be used to discover new vulnerabilities. A big surface has not been properly analyzed. While COM/DCOM can lead to interesting bugs or vulnerabilities, it was never said that it reveals its secrets easily ;) (even with a fuzzing approach).COM (Component Object Model) and DCOM (Distributed COM) have been interesting components in Windows from a security perspective for many years. Diving into the implementation and internal workings of COM/DCOM was an interesting topic that I had on the planning for quite some time.Because of the design of the MS-RPC-Fuzzer, a large part of the COM-Fuzzer design could be recycled. While the fuzzer can reveal interesting COM/DCOM implementations, it is still blackbox fuzzing. This means that when we invoke a COM procedure, we are not sure what actually is going to happen. Using external tools such as Process Monitor while fuzzing helps to get a better understanding, but in the end, a researcher will still to reverse the actual implementation to know exactly what is going on. This fuzzer will hopefully identify the COM classes that are worth for further analysis, like reversing the server.I’m glad the fuzzing approach once again proves that it can be effective on different disciplines, like COM/DCOM, as CVE-2025-59253 was assigned during my research.While effective, the tool can be improved in future work. For example, the bug as described in chapter 3.2.3 COM objects and crashes is something that will probably lead to issues while gathering COM-data for many classes. At some time I plan to filter out more classes that lead to the problem and get to the root cause of this and hopefully fix it/work around it without needing a blacklist.While fuzzing, the PowerShell session can crash once again. I’ve tried to identify some of these classes for which this is the case and added them to  within the repository. The file can be parsed with the argument . While this works around the problem, it is again better to get to the root cause of this problem and fix it.Some procedures take long to finish/execute or even hang. While you can use the argument  for this with a specific procedure, it still is annoying. So in the future it will probably be wise to implement a time out. While this is tricky in PowerShell, the requirement to have OleViewDotNet installed beforehand makes this slightly easier (so the module doesn’t have to be important each time when working with jobs.)Finally, new approaches to fuzzing COM/DCOM implementations can be implemented in the future. Things like the  path argument or the  option can lead to new insights, pherhaps more of these options will be added in the future.OleView.NET (tyranid / James Forshaw) .NET reimplementation of Microsoft’s OLE/COM Object Viewer, useful for inspecting COM type libraries, registered classes, interfaces and typelib metadata when analyzing COM/RPC surfaces.COM-Fuzzer (Remco van der Meer) Fuzzing framework for exercising COM interfaces to discover parsing bugs, memory corruption, and unexpected behaviors in COM servers.MS-RPC-Fuzzer (Remco van der Meer) Fuzzer specialized for Microsoft RPC (MS-RPC) endpoints and automates generation and delivery of malformed RPC requests for stability and crash analysis. Real-time system activity monitor (file, registry, process/thread), indispensable for observing side-effects of fuzzing, RPC interactions, and runtime behavior on Windows hosts. Graph database and visualization platform, commonly used to model interfaces, call graphs and relationships discovered during reverse engineering and protocol analysis.Microsoft RPC – Wikipedia High-level overview of Microsoft RPC concepts, history and architecture, a good primer before digging into implementation details or tooling. Explanation of canary trap techniques for detection/attribution, useful when designing monitored testbeds or leak-detection mechanisms during research.]]></content:encoded></item><item><title>Can you break our pickle sandbox? Blog + exploit challenge inside</title><link>https://iyehuda.substack.com/p/we-may-have-finally-fixed-pythons</link><author>/u/valmarelox</author><category>netsec</category><pubDate>Thu, 30 Oct 2025 17:47:29 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How we found +2k vulns, 400+ secrets and 175 PII instances in publicly exposed apps built on vibe-coded platforms (Research methodology)</title><link>https://escape.tech/blog/methodology-how-we-discovered-vulnerabilities-apps-built-with-vibe-coding/</link><author>/u/PriorPuzzleheaded880</author><category>netsec</category><pubDate>Thu, 30 Oct 2025 15:53:10 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[Hey there,With Halloween around the corner, what’s scarier for organizations than vulnerabilities in their web applications?And it's even scarier when the development of these applications is in the hands of users not familiar with security practices.This year, the Escape research team has focused on a growing area of concern: . Purpose-built platforms like Lovable.dev, Base44.com, and Create.xyz have democratized application development, enabling non-developers to deploy full-stack applications without writing a single line of code. Just ask your colleagues on the marketing team what they've already deployed or experimented with under the hood! ;) As vibe-coded applications become more accessible, inexperienced users are creating fresh risks.Our research team analyzed over 5,600 publicly available applications and identified more than , , and  (including medical records, IBANs, phone numbers, and emails).Unlike other published articles on that topic, the goal of this research was to move beyond isolated case studies by identifying issues at scale that would otherwise require hours of manual work to uncover.You can review the complete results in our comprehensive report. Meanwhile, in this article, we'll show you the methodology that guided us to these impressive findings.To collect and analyze our dataset, multiple domain sources were leveraged, including official launch directories (e.g., launched.lovable.dev), Shodan indexing, Reddit communities, and manual crawling.First, we retrieved data from launched.lovable.dev to compile a dataset of 4,000 web applications. We then expanded our target set using additional sources, including lovable.dev, base44.com, vibe-studio.ai, bolt.new, create.xyz. Afterward, we performed subdomain enumeration on these domains to identify additional targets.Following a cross-source analysis of the aggregated dataset, we derived three independent fingerprinting methods for detecting Lovable-based web apps:After developing a fingerprint for the Lovable web app, we used Shodan to locate live instances of web apps that appear to be implemented with Lovable. To further augment the dataset, we scraped curated posts and comment threads from the Reddit communities r/lovable and r/base44.The resulting URLs were curated through a multi-stage process:Deduplication to remove redundant entriesAutomated reachability checks to exclude dead hosts: we filtered live assets by checking whether each main page returned an HTTP status code in the 200–399 rangeFiltering to distinguish between non-application landing pages and functioning deployments.Initial platform coverage included Lovable (~4,000+ applications discovered), Base44 (~159), Create.xyz (~449), Vibe Studio, and Bolt.new (smaller samples).The data collection was conducted as a one-time process. During collection, several limitations were deliberately imposed to ensure legal and ethical compliance. Domains that could be reasonably identified as educational or health-related were excluded, as typical users are not authorized to probe such systems. This exclusion reduced coverage but ensured adherence to established ethical norms of web crawling and security research.We acknowledge several sources of potential bias in our methodology: Reliance on launch directories, Shodan, and community postings may overrepresent applications that are actively promoted or easily discoverable, while underrepresenting private or restricted deployments. As data collection was performed once, the dataset represents a snapshot in time. Applications and platforms evolve rapidly; vulnerabilities may have been patched or newly introduced since collection. The dataset is heavily skewed toward Lovable deployments, with substantially fewer applications discovered for Base44, Create.xyz, Vibe Studio, and Bolt.new. This imbalance may disproportionately influence prevalence measurements.At the same time, focusing on domains openly accessible to anyone online gives us a useful window into how these applications are actually built and used in practice. This approach highlights the security habits (and mistakes) that most often appear in real deployments, especially when apps are created by people with little security background.Looking at this public-facing slice of the ecosystem helps us see not just isolated flaws, but broader patterns across sectors.After the dataset was curated, the next stage of the methodology focused on systematically mapping the attack surface of each application, i.e., extracting all hosts, web apps, and APIs exposed by the domains we found (further defined as “assets” fed into Escape’s Attack Surface Management scanner). Our goal was to build a structured model of each application’s externally visible footprint and then subject high-value pieces of that footprint to dynamic testing.As a platform, Escape is a collection of security scanners. A typical Escape’s ASM scanner is a tool that automates the identification of all exposed assets, correlates them, and helps prioritize which ones are most likely to be exploited. Its scanner structure can be seen as follows:The scanner first ingests the “assets”. In our case, these assets include hosts, web apps, APIs, and schemas. The scanner then proceeds through a multi-step process of validation and reachability checks, followed by fingerprinting and metadata collection (such as WAF, cloud provider, framework, and GeoIP). This process ensures that only valid, accessible assets are mapped and ready for further testing.Once the assets are identified, they are classified into the discovery phase. We relied on a layered discovery strategy to maximize coverage while minimizing intrusiveness:Domain and host discovery. Subdomain enumeration and passive indices were used to enumerate hosts associated with each base domain. As mentioned before, we validated reachability via benign HTTP(s) probes and excluded hosts that returned only generic landing pages.Web crawling and route enumeration. A headless-browser crawler rendered pages and followed links, collecting URL structures, JS bundles, and client-side routing artifacts.Static frontend analysis. JavaScript and HTML were parsed to extract embedded API endpoints, fetch/XHR/WebSocket URLs, inline tokens or keys (when present in the public bundle), and configuration objects that reveal backend schemas or third-party integrations.Endpoints discovered in JS, observed in network traces during crawl sessions, or exposed via documented routes were collated into service models. When available, open schema fragments (e.g., JSON responses illustrating resource shapes) were used to infer parameterization and access control points. We also performed API discovery by brute-forcing API paths, i.e., testing for common paths at scale and identifying API-like responses.By feeding the data collected during the discovery process into the scanner, we built a comprehensive, continuously updated map of each application's publicly visible footprint.While analyzing the structure of specifically Lovable websites, we came across the integration of Lovable and Supabase. In this structure, we specifically identified and targeted APIs integral to the application’s functionality that could be discovered and analyzed at scale.During our analysis, we also discovered that anonymous JWT tokens were exposed in the JavaScript bundles of the Lovable front end. These tokens were linked to PostgREST APIs as part of the Supabase backend integration. According to the documentation, Supabase “automatically generates a RESTful API from the database schema, allowing applications to interact with the PostgreSQL database through an interface, all from the browser.”However, while Supabase's default security rules are permissive for development, they leave important security gaps if not properly configured before going live. Specifically, Row-Level Security (RLS) policies must be implemented to ensure that only authorized users can access or modify specific rows in the database, such as ensuring that users can access only their own data. The issue arises when RLS is misconfigured between the API layer and the database. This creates security risks, as unauthorized access could occur if JWT tokens (used for authentication) are exposed in the frontend code.Therefore, while Lovable can assist in generating RLS policies, it is vital for users to manually review these policies (which can be a challenge for less experienced “vibe coders”).Introduction of Lightweight DAST Surface Scanning  or "Visage Surface Scanner"Given the structure of the integration between Lovable front-ends and Supabase backends via API, and the fact that certain high-value signals (exposed keys, for example, anonymous JWTs to APIs linking Supabase backends, client-side routes, embedded endpoints) only appear in frontend bundles or source output, we introduced a lightweight, read-only scan to harvest these artifacts and feed them back into the ASM inventory.We called this scanner the Visage Surface Scanner. Unlike previous versions, this scanner is less in-depth: it doesn't execute any actions on the web application or trigger processes. Instead, it analyzes source code and frontend responses to identify secrets or routes that can be added to the asset inventory in our Attack Surface Management tool through a feedback loop.The Visage Surface scanner was integrated into the Attack Surface Management (ASM) web app scanner, enabling us to scan Lovable web apps for vulnerabilities and identify exposed anonymous JWT tokens and Supabase API routes. These findings were then fed into the Escape API-focused Business Logic Security Testing Scanner, where they were analyzed for real-world security issues:After adding the discovered URLs to the ASM and running the Visage Surface Scan, we now discovered in total  assets in the ASM, composed of: schemas (found and generated via frontend traffic)These assets were then filtered and organized into an Attack Surface Management (ASM) per application within Escape’s platform that included:Web application entry points and client routesREST/GraphQL/WebSocket endpoints and their observed request/response shapesAuthentication and session management endpointsThird-party integrations and service endpoints (e.g., Supabase, analytics, storage)Security Testing and Dynamic AnalysisOnce the attack surface was extracted and modeled, we applied targeted security testing using in-house dynamic application security testing (DAST) techniques (see more info on the web app scanner here and the API scanner here). The objective was not to exhaustively exploit weaknesses, but to identify recurring classes of misconfigurations and vulnerabilities in a safe, controlled manner.We ran DAST scans in a “passive” mode, explicitly configured to avoid destructive operations, high-volume brute force attempts, or payloads that could disrupt target services. This design choice was made to respect ethical and legal boundaries and to minimize unintended impact on live deployments. While this conservative approach ensures safety, it also introduces an important bias: the results presented here are lower-bound estimates. Running Escape’s full scanning capabilities (e.g., injection payloads, deeper fuzzing, and aggressive brute-force) would almost certainly surface a larger volume of issues, including higher-severity vulnerabilities.All REST API endpoints passed our REST DAST scan, using the schema produced by the Visage Surface scanner and the credentials stored in the web application. We attempted to implement an automated registration agent for the web application to provision an account, execute a comprehensive scan, and forward the resulting authentication token to the REST DAST tool.Two important observations shaped the testing results:Most vulnerabilities were exposed without authenticationAcross platforms, critical weaknesses (e.g., exposed Supabase tokens, misconfigured APIs, and missing row-level security) were accessible directly through public endpoints. Tokens such as Supabase service keys were often trivially retrievable from frontend bundles, underscoring that many security issues in vibe-coded apps exist “in the open,” without requiring any privileged access. If we decided to go more in-depth, we could develop an AI-driven auto-authentication system that leveraged headless browser automation and agent-based orchestration.2. Results understate the true riskBecause scans were run in a passive mode, the findings reflect only a subset of exploitable issues. A more aggressive testing configuration would likely have uncovered additional vulnerabilities with greater impact. In this sense, our findings represent a conservative baseline rather than the full extent of the security risks present in these ecosystems.Data Cleanup and VerificationAfter running the ASM-driven DAST scans, the raw output contained a large volume of findings with the Escape platform, ranging from high-confidence exposures to some noisy signals. To ensure that only meaningful vulnerabilities and secret disclosures were included in our analysis, we applied a multi-stage cleanup and verification process.Deduplication and NormalizationDeduplication and normalization were performed automatically by the Escape platform, which consolidated findings across multiple discovery vectors.Verification of Exposed SecretsPattern-based filtering: Escape applied platform-specific rules to distinguish genuine credentials (e.g., API keys, Supabase tokens, environment variables) from placeholders or noise.False-positive reduction: Values resembling generic identifiers (UUIDs, hashes, opaque IDs) were automatically flagged and excluded if not usable as credentials.Safe live validation: Where legally and ethically permissible, exposed tokens were tested against non-destructive requests to public endpoints to verify validity. Tokens granting elevated privileges (particularly Supabase service role keys) were flagged as critical exposures due to the level of access they provided.Vulnerabilities suggesting missing or weak authentication were validated by replaying requests without tokens or with modified headers to confirm whether access restrictions were enforced.Manual spot-checks. A representative subset of findings was manually validated to assess the precision of automated classification and to calibrate severity scoring.It is important to note that the cleanup and verification process was intentionally conservative. Only findings that could be confirmed with high confidence were retained. As a result, the vulnerabilities and exposed secrets presented in this study represent a verified baseline rather than the full universe of potential issues.Mapping out the attack surface of these vibe-coded applications wasn’t easy, but it was necessary. By taking a comprehensive approach to identifying exposed assets and vulnerabilities, we uncovered risks that would otherwise go unnoticed. Our study reveals that these vulnerabilities are spreading across diverse websites, industries, and asset types. As the vibe coding phenomenon continues to explode, especially within modern tech industries, the challenge grows. Even though users may be increasingly tech-savvy, many still lack critical security awareness. As a result, the risks are only amplifying.Organizations must respond quickly to these threats by adopting best practices to protect their assets. The sooner they act, the better.]]></content:encoded></item><item><title>A Deep Dive Into Warlock Ransomware Deployed Via ToolShell SharePoint Chained Vulnerabilities</title><link>https://hybrid-analysis.blogspot.com/2025/10/a-deep-dive-into-warlock-ransomware.html</link><author>/u/CyberMasterV</author><category>netsec</category><pubDate>Thu, 30 Oct 2025 15:38:29 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[Warlock ransomware was deployed by exploiting the SharePoint vulnerabilities  and The malware includes a hostname verification mechanism that excludes designated systems from encryption, indicating self-preservation tacticsWarlock performs defense evasion by stopping a list of services and processes and removes volume shadow copies The ransomware encrypts files using a combination of the  algorithm and Warlock ransomware has been recently found being distributed through newly discovered SharePoint vulnerabilities. This malware represents the latest evolution in ransomware tactics, combining advanced encryption methods with targeted defense evasion techniques.As a result, we have conducted a comprehensive analysis of Warlock, examining both its initial behavior through sandbox environments and performing detailed static and dynamic analysis of samples in the wild. The findings reveal a methodical attack pattern designed to maximize damage while protecting itself from detection and removal.The ransomware exploits two critical SharePoint vulnerabilities ( and ) as its entry point, then deploys a multi-stage attack that includes terminating security services, removing recovery options, and implementing a hybrid encryption scheme using  and  algorithms.Perhaps most telling is Warlock's self-preservation mechanism—a hostname verification feature that deliberately avoids encrypting certain systems, suggesting a calculated self-preservation approach built by its operators.A Hybrid Analysis PerspectiveAs we can see in the Hybrid Analysis report, the ransomware appends its extension to the existing one:Figure 1 - Warlock ransomware’s extension identifiedFigure 2 reveals that the malware is looking to open and possibly stop multiple services related to backup, databases,  shadow copies, AntiVirus software, and so on.Figure 2 - Multiple services are targetedHybrid Analysis identifies that the sample implements the  algorithm for encryption using YARA rules (Figure 3).Figure 3 - ChaCha20 algorithm identifiedFigure 4 - CryptoPP library is statically linkedThe  API is utilized to empty the Recycle Bin in order to avoid possible file recovery from the location:Figure 5 - SHEmptyRecycleBinW API callA Deeper Dive Into WarlockThe process retrieves the command-line arguments and compares them with the following list: “” (doesn’t change the extension of the file passed as a parameter), “” (doesn’t create the ransom note) and “”.Figure 6 - Command-line arguments retrievalThe threat actor embedded a GUID in the code that will appear in all encrypted files. The ransomware also implements a check (skipping files encryption) for a placeholder that should be a hostname called “”.Figure 7 - Hard-coded informationThe malware hides the current window via a function call to  (0x0 = ):Figure 8 - Malware’s window is hidden is used to empty the Recycle Bin on all drives (0x7 = SHERB_NOCONFIRMATION | SHERB_NOPROGRESSUI | SHERB_NOSOUND):Figure 9 - Empty the Recycle BinWarlock mounts all unmounted volumes using the , , and  functions.Figure 10 - Mount all unmounted volumesThe ransomware stops a list of services (i.e. AntiVirus, backup, shadow copies) using the  method (0x1 = ). The entire list of services can be found in the Appendix.Figure 11 - Targeted services are stoppedThe executable stops a list of processes that might interfere with the encryption. The list of all processes can be found in the Appendix.Figure 12 - Targeted processes are killedVolume Shadow Copies DeletionThe ransomware deletes all volume shadow copies by calling the  function and then  on every shadow copy found (see Figure 13).Figure 13 - Delete volume shadow copies using COM interface is used to retrieve the drive type, which must be different than 0x1 () and 0x5 ():Figure 14 - GetDriveTypeW API callThe following files and directories will  be encrypted by Warlock Ransomware:Figure 15 - Skipped files and directoriesThe malware creates multiple threads that will handle the file encryption. Firstly, it appends the “.” extension to every file to be encrypted using :Figure 16 - Append the ransomware’s extension to encrypted filesThe ransomware uses  (CryptoPP library) and  for encrypting files. It calls  to generate 32 random bytes (session private key), computes the 32-byte session public key using , and then computes the 32-byte shared secret using the session private key and a hard-coded 32-byte public key. The  key is the SHA256 of the shared secret and the IV is equal to the first 8 bytes from the key. The entire workflow is highlighted in the figure below. The threat actor can recover the shared secret using the session public key that is written to the encrypted file and the secret private key that corresponds to the hard-coded public key.Figure 17 - Generate the shared secret using Curve25519Figure 18 - Hard-coded 32-byte public keyThe ransomware traverses the directories and encrypts the files using :Figure 19 - Open targeted file for encryptionFigure 20 - Write encrypted content to the fileA snippet of the  implementation is displayed in Figure 21.Figure 21 - ChaCha20 algorithmAn example of an encrypted file is displayed below. The footer contains the 32-byte session public key generated before and the hard-coded GUID already mentioned.Figure 22 - Footer contains the 32-byte session public key and GUIDThe ransom note called “” is dropped in every encrypted directory (Figure 23).Warlock Through the Eyes of Hybrid AnalysisThe Hybrid Analysis sandbox report reveals multiple key behavioral indicators of Warlock ransomware's functionality. The analysis identifies the ransomware's unique file extension and confirms its use of the ChaCha20 algorithm for file encryption. A significant indicator of malicious intent is the ransomware's systematic termination of backup and AntiVirus software services. The in-depth technical analysis provides crucial evidence from the dynamic analysis and describes the volume shadow copies deletion process, as well as every step of the complex file encryption workflow. Hybrid Analysis is a powerful platform for identifying and analyzing malware, whether mundane or highly sophisticated. It provides detailed context and information that can be investigated further during the dynamic analysis of the malware. For performing a more in-depth analysis of malware samples, you can download them by registering with a Hybrid Analysis account and becoming a vetted user.How to decrypt my data.txt"sql.exe", "oracle.exe", "ocssd.exe", "dbsnmp.exe", "synctime.exe", "agntsvc.exe", "isqlplussvc.exe", "xfssvccon.exe", "mydesktopservice.exe", "ocautoupds.exe", "encsvc.exe", "firefox.exe", "tbirdconfig.exe", "mydesktopqos.exe", "ocomm.exe", "dbeng50.exe", "sqbcoreservice.exe", "excel.exe", "infopath.exe", "msaccess.exe", "mspub.exe", "onenote.exe", "outlook.exe", "powerpnt.exe", "steam.exe", "thebat.exe", "thunderbird.exe", "visio.exe", "winword.exe", "wordpad.exe", "notepad.exe""vss", "sql", "svc$", "memtas", "mepocs", "sophos", "veeam", "backup", "GxVss", "GxBlr", "GxFWD", "GxCVD", "GxCIMgr", "DefWatch", "ccEvtMgr", "ccSetMgr", "SavRoam", "RTVscan", "QBFCService", "QBIDPService", "Intuit.QuickBooks.FCS", "QBCFMonitorService", "YooBackup", "YooIT", "zhudongfangyu", "sophos", "stc_raw_agent", "VSNAPVSS", "VeeamTransportSvc", "VeeamDeploymentService", "VeeamNFSSvc", "veeam", "PDVFSService", "BackupExecVSSProvider", "BackupExecAgentAccelerator", "BackupExecAgentBrowser", "BackupExecDiveciMediaService", "BackupExecJobEngine", "BackupExecManagementService", "BackupExecRPCService", "AcrSch2Svc", "AcronisAgent", "CASAD2DWebSvc", "CAARCUpdateSvc"]]></content:encoded></item></channel></rss>