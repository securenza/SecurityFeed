<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech News</title><link>https://securenza.github.io/SecurityFeed/</link><description></description><item><title>Senator puts hold on Trumpâ€™s nominee for CISA director, citing telco security â€˜cover upâ€™</title><link>https://techcrunch.com/2025/04/09/senator-puts-hold-on-trumps-nominee-for-cisa-director-citing-telco-security-cover-up/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Wed, 9 Apr 2025 16:25:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Sean Plankey's nomination to head up CISA will be blocked, for now.]]></content:encoded></item><item><title>US may fine TSMC $1B over chip allegedly used in Huawei AI processor</title><link>https://techcrunch.com/2025/04/09/us-may-fine-tsmc-1b-over-chip-allegedly-used-in-huawei-ai-processor/</link><author>Kate Park</author><category>tech</category><pubDate>Wed, 9 Apr 2025 15:31:42 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Taiwan Semiconductor Manufacturing Company (TSMC) may have to pay a fine of $1 billion or more to resolve a U.S. export control investigation related to a chip it made that was used in a Huawei AI processor, according to a report by Reuters. TSMC did not provide any further comments as it is now â€œin [â€¦]]]></content:encoded></item><item><title>Scientists Recreate Brain Circuit in Lab For First Time</title><link>https://science.slashdot.org/story/25/04/09/1521244/scientists-recreate-brain-circuit-in-lab-for-first-time?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 15:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Scientists have recreated in a laboratory the sensory pathway that transmits feelings of pain to the human brain, in a breakthrough that could lead to better treatments. Financial Times: A team at Stanford University in California is the first to combine different neurons grown from human stem cells into a functioning brain circuit in a lab dish. Their experiments, published in Nature on Wednesday, illustrate scientists' rapid progress in replicating living tissues and organs through synthetic biology. 

When the Stanford scientists exposed the brain circuit they had created to sensory stimulants, they observed waves of electrical activity travelling along it. The molecule that makes chilli peppers hot, capsaicin, immediately induced a strong response. 

[...] The synthetic brain circuits could be used to screen for better-targeted therapies for pain that tone down excessive waves of neurotransmission, without affecting the brain's reward circuitry as opioids do, project leader [Sergiu] Pasca said. The assembloids themselves cannot be said to "feel pain," he emphasised: "They transmit nervous signals that are processed by a second pathway going deeper into the brain and giving us the aversive, emotional component of pain."]]></content:encoded></item><item><title>The AI Therapist Can See You Now</title><link>https://science.slashdot.org/story/25/04/09/155247/the-ai-therapist-can-see-you-now?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 15:05:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New research suggests that given the right kind of training, AI bots can deliver mental health therapy with as much efficacy as -- or more than -- human clinicians. From a report: The recent study, published in the New England Journal of Medicine, shows results from the first randomized clinical trial for AI therapy. Researchers from Dartmouth College built the bot as a way of taking a new approach to a longstanding problem: The U.S. continues to grapple with an acute shortage of mental health providers. "I think one of the things that doesn't scale well is humans," says Nick Jacobson, a clinical psychologist who was part of this research team. For every 340 people in the U.S., there is just one mental health clinician, according to some estimates. 

While many AI bots already on the market claim to offer mental health care, some have dubious results or have even led people to self-harm. More than five years ago, Jacobson and his colleagues began training their AI bot in clinical best practices. The project, says Jacobson, involved much trial and error before it led to quality outcomes. "The effects that we see strongly mirror what you would see in the best evidence-based trials of psychotherapy," says Jacobson. He says these results were comparable to "studies with folks given a gold standard dose of the best treatment we have available."]]></content:encoded></item><item><title>Tired of doing laundry? These startups want to help.</title><link>https://techcrunch.com/2025/04/09/tired-of-doing-laundry-these-startups-want-to-help/</link><author>Mary Ann Azevedo</author><category>tech</category><pubDate>Wed, 9 Apr 2025 15:02:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Doing laundry can be a chore. It takes time and is tedious. Few of us like to do it. So itâ€™s no surprise that startups have emerged to take that load off your back â€” literally. And investors are lining up to fund new and older players alike.Â  Eleven-month-old NoScrubs has just raised $2 million [â€¦]]]></content:encoded></item><item><title>Artisan, the â€˜stop hiring humansâ€™ AI agent startup, raises $25M â€” and is still hiring humans</title><link>https://techcrunch.com/2025/04/09/artisan-the-stop-hiring-humans-ai-agent-startup-raises-25m-and-is-still-hiring-humans/</link><author>Julie Bort</author><category>tech</category><pubDate>Wed, 9 Apr 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Itâ€™s been a tough but exciting year for 23-year-old Jaspar Carmichael-Jack, founder and CEO of AI sales agent startup Artisan. Artisan just raised a $25 million Series A led by Glade Brook Capital, Carmichael-Jack exclusively tells TechCrunch. Y Combinator, Day One Ventures, HubSpot Ventures, Oliver Jung, Fellows Fund, and others participated as well. A year [â€¦]]]></content:encoded></item><item><title>Samsungâ€™s SmartThings app gets more home automation capabilities</title><link>https://techcrunch.com/2025/04/09/samsung-smartthings-apps-gets-better-routines-and-intercom-feature-with-the-new-update/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Wed, 9 Apr 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Samsung has updated SmartThings, its app that lets users control the companyâ€™s IoT devices, with new home automation capabilities in an apparent bid to cross-sell more of its smart home devices and appliances to existing customers. Besides the automation features, users will get new integrations with the Samsung Health app, an intercom feature, and support [â€¦]]]></content:encoded></item><item><title>WordPress.com launches a free AI-powered website builder</title><link>https://techcrunch.com/2025/04/09/wordpress-com-launches-a-free-ai-powered-website-builder/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 9 Apr 2025 14:47:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Hosting platform WordPress.com on Wednesday launched a new AI website builder that allows anyone to create a functioning website using an AI chat-style interface. The feature, which is being made available to WordPress users for free, is targeted at entrepreneurs, freelancers, bloggers, and others who need a professional online presence, the company says. At this [â€¦]]]></content:encoded></item><item><title>Instagram tests locked reels that can be accessed with secret codes</title><link>https://techcrunch.com/2025/04/09/instagram-tests-locked-reels-that-can-be-accessed-with-secret-codes/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 9 Apr 2025 14:32:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Instagram appears to be quietly testing locked reels that viewers would have to unlock with a code and a provided hint. The feature is a simple way to increase engagement with the creatorâ€™s content, but it could also offer creators and celebrities a way to share exclusive reels with their most dedicated fans, who are [â€¦]]]></content:encoded></item><item><title>Nuroâ€™s $106M raise backs its shift from delivery robots to licensing autonomy tech</title><link>https://techcrunch.com/2025/04/09/nuros-106m-raise-backs-its-shift-from-delivery-robots-to-licensing-autonomy-tech/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Wed, 9 Apr 2025 14:29:10 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After months of hearty marketing efforts and large-scale technology demos across the U.S., Nuro has secured $106 million in fresh funding to help scale its autonomous driving technology and advance commercial partnerships.Â  The Series E round brings Nuroâ€™s total funding raised to $2.2 billion and its valuation to $6 billion. Thatâ€™s a drop from the [â€¦]]]></content:encoded></item><item><title>Samsung and Google Partner To Launch Ballie Home Robot with Built-in Projector</title><link>https://hardware.slashdot.org/story/25/04/09/146254/samsung-and-google-partner-to-launch-ballie-home-robot-with-built-in-projector?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 14:06:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Samsung Electronics and Google Cloud are jointly entering the consumer robotics market with Ballie, a yellow, soccer-ball-shaped robot equipped with a video projector and powered by Google's Gemini AI models. First previewed in 2020, the long-delayed device will finally launch this summer in the US and South Korea. The mobile companion uses small wheels to navigate homes autonomously and integrates with Samsung's SmartThings platform to control smart home devices. 

Running on Samsung's Tizen operating system, Ballie can manage calendars, answer questions, handle phone calls, and project video content from services including YouTube and Netflix. Samsung EVP Jay Kim described it as a "completely new Ballie" compared to the 2020 version, with Google Cloud integration being the most significant change. The robot leverages Gemini for understanding commands, searching the web, and processing visual data for navigation, while using Samsung's AI models for accessing personal information.]]></content:encoded></item><item><title>The complete agenda for TechCrunch Sessions: AI unveiled</title><link>https://techcrunch.com/2025/04/09/the-complete-agenda-for-techcrunch-sessions-ai-unveiled/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 9 Apr 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Itâ€™s been a couple of years since our last topic-focused, one-day event â€” but with AI transforming the tech landscape, we couldnâ€™t miss the chance to gather 1,200 founders, CEOs, entrepreneurs, investors, and anyone shaping the future of AI for a deep dive into whatâ€™s next. On June 5 in Zellerbach Hall at UC Berkeley, [â€¦]]]></content:encoded></item><item><title>Tessell snags $60M to drive data management at scale</title><link>https://techcrunch.com/2025/04/09/tessell-snags-60m-to-drive-data-management-at-scale/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Wed, 9 Apr 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tessell, a startup developing a multi-cloud database-as-a-service, has raised $60 million in a new funding round led by WestBridge Capital ahead of its plans to expand its market presence and launch an AI-powered conversational database management service. As data becomes more critical than ever, many companies are struggling to manage and store it efficiently. Legacy [â€¦]]]></content:encoded></item><item><title>Solve Intelligence raises fresh $12M to bring AI to IP, patent workflows</title><link>https://techcrunch.com/2025/04/09/microsoft-backs-solve-intelligence-in-12m-series-a-funding/</link><author>Kate Park</author><category>tech</category><pubDate>Wed, 9 Apr 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Legal tech has come a long way, but the bulk of an intellectual property or patent lawyerâ€™s work today is still done with spreadsheets, word processors and PDFs. A startup out of Delaware, Solve Intelligence, is using generative AI to speed up that work, believing its tech is uniquely suited to the needs of patent [â€¦]]]></content:encoded></item><item><title>Redditâ€™s conversational AI search tool leverages Google Gemini</title><link>https://techcrunch.com/2025/04/09/reddits-conversational-ai-search-tool-leverages-google-gemini/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Wed, 9 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Reddit Answers, the platformâ€™s conversational AI search tool, gets an upgrade through an integration with Google Gemini.Â This comes over a year after Reddit expanded its partnership with Google Cloud to access its Vertex AI platform to build AI agents. Google and Reddit announced the update on Wednesday, explaining that by incorporating Gemini on Vertex AI, [â€¦]]]></content:encoded></item><item><title>Samsung adds Googleâ€™s Gemini to its home robot Ballie</title><link>https://techcrunch.com/2025/04/09/samsung-adds-google-gemini-ai-assistant-to-its-home-robot-ballie/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Wed, 9 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Samsung said on Wednesday that itâ€™s adding Googleâ€™s Gemini AI to its home robot Ballie through a partnership with Google Cloud. Users will be able to ask the robot different queries to get answers from Gemini, said the companies. Samsung aims to tap into Geminiâ€™s multimodal capabilities for its robot. The Korean tech giant said [â€¦]]]></content:encoded></item><item><title>Gemini Code Assist, Googleâ€™s AI coding assistant, gets â€˜agenticâ€™ abilities</title><link>https://techcrunch.com/2025/04/09/gemini-code-assist-googles-ai-coding-assistant-gets-agentic-upgrades/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Wed, 9 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Gemini Code Assist, Googleâ€™s AI coding assistant, is gaining new â€œagenticâ€ capabilities in preview. During its Cloud Next conference on Wednesday, Google said Code Assist can now deploy new AI â€œagentsâ€ that can take multiple steps to accomplish complex programming tasks. These agents can create applications from product specifications in Google Docs, for example, or [â€¦]]]></content:encoded></item><item><title>Googleâ€™s enterprise cloud gets a music-generating AI model</title><link>https://techcrunch.com/2025/04/09/google-brings-a-music-generating-ai-model-to-its-enterprise-cloud/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Wed, 9 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[On Wednesday, Google rolled out updates to several of its first-party media-generating AI models available through its Vertex AI cloud platform. Lyria, Googleâ€™s text-to-music model, is now available in preview for select customers, and the companyâ€™s Veo 2 video creation model has been enhanced with new editing and visual effects customization options. The company has [â€¦]]]></content:encoded></item><item><title>Google Workspace gets automation flows, podcast-style summaries</title><link>https://techcrunch.com/2025/04/09/google-workspace-gets-automation-flows-podcast-style-summaries/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Wed, 9 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google is upgrading Workspace, its suite of cloud-based productivity tools, with new AI capabilities. The suite is gaining Workspace Flows, a tool designed to automate multi-step processes such as updating spreadsheets and digging through documents for information. Flows can tap Gems, Googleâ€™s brand of custom AI-powered chatbots, to handle specialized tasks, and it can also [â€¦]]]></content:encoded></item><item><title>Googleâ€™s newest Gemini AI model focuses on efficiency</title><link>https://techcrunch.com/2025/04/09/googles-newest-gemini-ai-model-focuses-on-efficiency/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Wed, 9 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google is releasing a new AI model designed to deliver strong performance with a focus on efficiency. The model, Gemini 2.5 Flash, will soon launch in Vertex AI, Googleâ€™s AI development platform. The company says it offers â€œdynamic and controllableâ€ computing, allowing developers to adjust processing time based on the complexity of queries. â€œ[You can [â€¦]]]></content:encoded></item><item><title>Ironwood is Googleâ€™s newest AI accelerator chip</title><link>https://techcrunch.com/2025/04/09/google-unveils-ironwood-a-new-ai-accelerator-chip/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Wed, 9 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[During its Cloud Next conference this week, Google unveiled the latest generation of its TPU AI accelerator chip. The new chip, called Ironwood, is Googleâ€™s seventh-generation TPU and is the first optimized for inference â€” that is, running AI models. Scheduled to launch sometime later this year for Google Cloud customers, Ironwood will come in [â€¦]]]></content:encoded></item><item><title>China Raises Tariffs on US Goods To 84% as Rift Escalates</title><link>https://slashdot.org/story/25/04/09/1131225/china-raises-tariffs-on-us-goods-to-84-as-rift-escalates?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 11:31:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China retaliated against the US after new tariffs imposed by President Donald Trump, announcing it would raise the tariff on US goods to 84%, escalating the trade conflict between the world's two largest economies. From a report: The Chinese countermeasures are effective April 10, according to a government statement Wednesday. China's move came after Trump's latest tariffs went into force at midday Wednesday in Beijing, taking the cumulative rate announced this year to 104%. A day earlier, China vowed to "fight to the end" if the US insists on new tariffs. Where US-China Decoupling Is Hardest: After decades of trade integration, Chinese companies have become increasingly essential suppliers of goods and materials that range from niche to ones many Americans can barely do without. 

At $41 billion last year, smartphones -- largely consisting of Apple's iPhones -- were the single largest US import from China. More than 70% of all smartphone imports are from China, according to Bloomberg analysis of 2024 trade data from the US International Trade Commission. 

Farther afield, China supplies the entirety of hair from badgers and other animals imported into the US for brush-making. It also delivers almost 90% of the gaming consoles US consumers buy from overseas. 

Over 99% of the electric toasters, heated blankets, calcium, and alarm clocks the US imports are from China. Ditto for more than 90% of folding umbrellas, vacuum flasks, artificial flowers, LED lamps, and wooden coat-hangers.]]></content:encoded></item><item><title>Lyst, a fashion marketplace once valued at $700M, sells to Japanâ€™s Zozo for $154M</title><link>https://techcrunch.com/2025/04/09/lyst-the-fashion-marketplace-once-valued-at-700m-sells-to-japans-zozo-for-154m/</link><author>Ingrid Lunden</author><category>tech</category><pubDate>Wed, 9 Apr 2025 10:29:44 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Fashion goes in and out of style, and it turns out, so do fashion startups. Lyst, a high-end fashion marketplace that was once valued at $700 million, has been acquired for just $154 million in cash by Zozo, a Japan-based fashion and e-commerce business. Zozo owns a number of fashion brands that include Wear by [â€¦]]]></content:encoded></item><item><title>Governments identify dozens of Android apps bundled with spyware</title><link>https://techcrunch.com/2025/04/09/governments-identify-dozens-of-android-apps-bundled-with-spyware/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Wed, 9 Apr 2025 10:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The advisories say the spyware apps are used to target members of civil society who may oppose Chinaâ€™s state interests.]]></content:encoded></item><item><title>HTML to PDF C# Tools: The Ultimate .NET Library Comparison for 2025</title><link>https://hackernoon.com/html-to-pdf-c-tools-the-ultimate-net-library-comparison-for-2025?source=rss</link><author>Iron Software</author><category>tech</category><pubDate>Wed, 9 Apr 2025 09:47:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[âš™ï¸ So Many Libraries, So Many Trade-OffsGenerating PDFs from HTML is a common requirement for .NET developers, whether for invoices, reports, or web page exports. However, choosing the right library can be challenging with so many options available.\
The .NET ecosystem is flooded with HTML to PDF conversion libraries, each promising high-quality rendering, fast performance, and easy integration. However, the reality is more complicated. Some libraries excel in fidelity but are expensive. Others are free but struggle with JavaScript-heavy content or lack support for modern frameworks like Blazor.\
Beyond that, documentation is often unclear, APIs vary wildly, and licensing models can be confusing. With so many optionsâ€”and so many potential pitfallsâ€”picking the right tool for your project is anything but straightforward.ðŸ§© Balancing Fidelity, Features, and Framework FitNot all libraries handle HTML and CSS the same way. Some render pixel-perfect PDFs but only support Windows, while others are cross-platform but fall short in layout accuracy. If your application relies on JavaScript or dynamic content, many libraries will fail to deliver the results you expect.\
And then thereâ€™s framework compatibilityâ€”Blazor, Razor Pages, ASP.NET Core, and cloud-native deployments each introduce unique challenges. Choosing the wrong library could mean weeks of workarounds or, worse, PDF output that doesnâ€™t match what your users expect.ðŸ’¸ The Cost vs. Capabilities DilemmaFree and open-source libraries are appealing, but they often come with trade-offs: â€“ When issues arise, youâ€™re on your own. â€“ Some struggle with large or complex documents. â€“ Many donâ€™t support JavaScript or modern web technologies.\
On the other hand, premium solutions offer better rendering, compliance, and support but come at a cost. Some are priced for enterprises, making them impractical for smaller teams or budget-conscious projects.ðŸ›  Your Decision Affects Everything DownstreamHTML to PDF conversion isnâ€™t just about formattingâ€”it directly impacts user experience, branding, compliance, and developer productivity. Whether youâ€™re generating invoices, reports, legal documents, or marketing materials, the right library needs to align with your framework, deployment environment, budget, and long-term goals.âœ… What This Guide Will Help You DoWith so many options, making an informed decision is crucial. Thatâ€™s where this guide comes in.\
Weâ€™ve thoroughly compared 13 major HTML to PDF libraries for .NET, covering: â€“ See how each library works in real-world scenarios.Feature comparison tables â€“ Quickly identify strengths and weaknesses. â€“ Understand how different libraries handle complex documents. â€“ Avoid hidden costs and limitations.\
Whether youâ€™re building an ASP.NET Core application, a Blazor project, or a legacy Web Forms system, this guide will help you choose the best HTML to PDF solutionâ€”so you can make the right decision the first time.Whatâ€™s the Best HTML to PDF Library for C# Developers in 2025?Choosing the right HTML to PDF library depends on your prioritiesâ€”whether itâ€™s rendering accuracy, ease of integration, cross-platform support, cost, or compliance. Hereâ€™s a quick cheat sheet to help you decide:|  |  |
|----|----|
| Best All-Around for .NET Developers |  â€“ Pixel-perfect, Blazor-friendly, excellent support |
| Best Free/Open Source (for JS-heavy content) |  â€“ Headless Chrome rendering |
| Best for Print-Quality, Publisher-Grade Output |  â€“ Unmatched CSS and layout fidelity |
| Best for Secure Enterprise Workflows |  â€“ Advanced signing, redaction, compliance |
| Best for Internal Tools or Static Templates |  â€“ Lightweight, free, basic support |
| Best Budget-Friendly Commercial Option |  or  |This guide is your roadmap through the overwhelming number of optionsâ€”helping you pick the  HTML to PDF library, the first time.Before diving into detailed reviews, here's a quick comparison of key factors like rendering accuracy, JavaScript support, licensing, and best use cases:|  |  |  |  |  |  |  |  |
|----|----|----|----|----|----|----|----|
| IronPDF | 10 | Full | Full | 9 | $$ | Excellent | Web apps, PDFs from views |
| PuppeteerSharp | 9 | Full | Full | 6 | Free | Moderate | Headless browser-based rendering |
| HtmlRenderer.PdfSharp | 4 | Partial | No | 7 | Free | Low | Basic text-only PDFs |
| DinkToPdf | 6 | Good | Limited | 7 | Free | Low | Basic reports and invoices |
| Aspose.PDF | 9 | Full | Full | 6 | $$$ | Good | Complex documents, large-scale ops |
| Syncfusion | 8 | Full | Partial | 8 | $$ | Great | Business docs, UI integration |
| PDFTron | 9 | Full | Full | 7 | $$$ | Good | Secure/legal PDFs, advanced APIs |
| Spire.PDF | 7 | Partial | Limited | 6 | $$ | Moderate | Small-to-mid business workflows |
| GemBox.Document | 6 | Good | No | 8 | $ | Good | Office-to-PDF conversions |
| SelectPDF | 7 | Good | Partial | 8 | $$ | Good | Reports & dashboard exports |
| EvoPDF | 8 | Good | Partial | 7 | $$ | Good | ASP.NET scenarios |
| ActivePDF | 6 | Limited | Limited | 6 | $$$ | Moderate | Legacy workflows |
| PrinceXML | 10 | Full | Full | 5 | $$$$ | Moderate | Publishing, print-ready PDFs |Individual Library Reviews & Code SamplesEach of these libraries offers unique strengths and weaknesses, depending on your project needs. The following code snippets in the sections below demonstrate each library in action, giving you a chance to experience they ability for HTML to PDF conversion, along with the rendered PDF outputs.IronPDF: High-Fidelity HTML to PDF Rendering for .NETIronPDF is a robust PDF .NET library which excels at tasks such as HTML to PDF conversion. Its capable of rendering pixel-perfect PDF documents from HTML files, entire web pages, and more in just a few lines of code. Adding IronPDF into your .NET projects is easy, it can be installed quickly through the NuGet Package Manager and works with popular IDEs such as Visual Studio.\
Whether you're converting HTML code, encrypting private PDF documents, looking to generate PDF documents from images, editing PDFs, or any other PDF task, IronPDF has you covered.\
With great cross-platform compatibility and an amazing set of features, some standout features of IronPDF include: with full support for HTML5, CSS3, JavaScript, and modern frameworks. Easily convert HTML content to PDF documents in just a few lines of code., such as digital signing, merging, metadata editing, and compression.IronPDF excels in scenarios where rendering quality and ease of integration are paramount:Exporting styled Razor Views or Blazor components to PDF in ASP.NET applications: IronPDFâ€™s accurate rendering makes it an excellent choice for exporting Razor Views or Blazor components into PDFs.Automating invoice generation, reports, and eBooks: Developers can leverage IronPDF for creating PDF documents from reusable HTML templates, streamlining business workflows.Converting interactive or JavaScript-powered dashboards to static PDFs: IronPDF can handle complex JavaScript elements, making it ideal for capturing dynamic content and converting it into a static PDF format.Long-term archival with PDF/A compliance and digital signatures: IronPDF provides the tools for generating legally-compliant PDFs, complete with digital signatures for added security.: IronPDF integrates smoothly into .NET projects with full NuGet support, offering a low barrier to entry for developers.Outstanding rendering fidelity: Its Chrome-based rendering engine ensures that what is displayed on-screen matches the final PDF output, no matter how complex the design.: IronPDF works on Windows, Linux, Mac, Azure, Docker, and AWS, ensuring broad deployment options for your application.Responsive support and updates: The IronPDF team offers excellent customer support and regular updates to the library.Royalty-free redistribution: With the appropriate license, you can redistribute your generated PDFs without worrying about additional fees.Commercial license required: To deploy IronPDF in live applications, a commercial license is required, which starts at $749. This may be a barrier for smaller projects or startups.: Due to its comprehensive feature set and rendering engine, IronPDF tends to use more resources than minimalistic wrappers like DinkToPdf.Limited cross-platform support: IronPDF's native support is for .NET platforms; non-.NET environments like Java, Python, and Node.js require separate IronPDF versions.\
When to Consider Other ToolsIf you're looking for  and donâ€™t need the advanced features like digital signatures or OCR,  or  could be more appropriate.For  with minimal budget or resource constraints, libraries like  might be a better fit.Code Example: Rendering a Razor View to PDFusing IronPdf;
using IronPdf.Razor.Pages;
using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.Mvc.RazorPages;
using MyApp.Models;  // Make sure this is the correct namespace

namespace MyApp.Pages
{
    public class InvoicePageModel : PageModel
    {
        [BindProperty(SupportsGet = true)]
        public InvoiceModel Invoice { get; set; }

        public void OnGet()
        {
            // Initialize the Invoice model
            Invoice = new InvoiceModel
            {
                InvoiceNumber = "INV12345",
                CustomerName = "John Doe",
                IssueDate = DateTime.Now,
                Items = new List<InvoiceItem>
                {
                    new InvoiceItem { Name = "Item 1", Price = 50, Quantity = 2 },
                    new InvoiceItem { Name = "Item 2", Price = 100, Quantity = 1 }
                },
                TotalAmount = 80
            };
        }


        public IActionResult OnPostAsync()
        {
            // Initialize the Invoice model
            Invoice = new InvoiceModel
            {
                InvoiceNumber = "INV12345",
                CustomerName = "John Doe",
                IssueDate = DateTime.Now,
                Items = new List<InvoiceItem>
                {
                    new InvoiceItem { Name = "Item 1", Price = 50, Quantity = 2 },
                    new InvoiceItem { Name = "Item 2", Price = 100, Quantity = 1 }
                },
                TotalAmount = 80
            };
            var renderer = new ChromePdfRenderer();

            // Render the Razor page to a PdfDocument object
            PdfDocument pdf = renderer.RenderRazorToPdf(this);

            Response.Headers.Add("Content-Disposition", "inline");

            // Download the PDF file
            return File(pdf.BinaryData, "application/pdf", "razorPageToPdf.pdf");
            // View output PDF on browser
            return File(pdf.BinaryData, "application/pdf");
        }
    }

}
Download IronPDF's free trial to try to out for yourself!Puppeteer Sharp: Headless Chrome Automation for JavaScript-Heavy PDFsPuppeteer Sharp is a .NET wrapper for Google's Puppeteer, allowing headless Chrome control for precise, browser-accurate rendering. This version is also a port of its official Node.JS Puppeteer API tool. A free-to-use PDF library, Puppeteer Sharp provides a easy-to-use solution for developers to convert specified URLs and HTML content without breaking the bank.Ideal for JavaScript-heavy pages, SPAs, and dashboards. over page loading, viewport size, authentication, and more.â€”a great choice for developers needing flexibility.PuppeteerSharp shines in scenarios where precise browser-like rendering is required:Generating PDFs from Single Page Applications (SPAs): PuppeteerSharp can render content from frameworks like React or Angular, which often require dynamic JavaScript execution.Exporting live dashboards, charts, or interactive data visualizations: For data-rich or interactive pages, PuppeteerSharp accurately captures the state of these pages in their browser-rendered form.When exact match to a browser-rendered page is needed: If your application requires PDF output that mimics what is seen in the browser, PuppeteerSharp ensures that the fidelity is maintained.Headless browser automation tasks: Beyond PDF generation, PuppeteerSharp is also useful for web scraping, screenshots, or automating form submissions.Browser-accurate rendering: PuppeteerSharp ensures that whatâ€™s rendered in the browser is faithfully reproduced in the PDF.Fine control over page behavior: PuppeteerSharp allows for precise control over page interactions, such as waiting for elements to load or controlling JavaScript execution.: As an open-source library, PuppeteerSharp is ideal for developers who want maximum flexibility without licensing costs.: Besides PDF, PuppeteerSharp supports image exports in PNG and JPEG formats, making it versatile.: PuppeteerSharp requires headless Chromium to be downloaded and set up, which can be challenging, especially in non-Windows environments.: PuppeteerSharp involves asynchronous workflows and understanding browser behavior, which can make it harder to learn and use effectively.Larger output sizes and slower rendering: PuppeteerSharp can result in larger PDFs and slower rendering times when compared to simpler libraries like IronPDF or DinkToPdf.Limited support for .NET project types: PuppeteerSharp requires more manual configuration for certain .NET types like Razor Pages or Blazor.\
When to Consider Other ToolsIf you don't need  or are dealing with  such as HTML files,  or  might be easier to implement.For  like OCR, digital signatures, or PDF/A compliance,  or  may be better suited.Code Example: Convert a Web page to PDFusing PuppeteerSharp;
using PuppeteerSharp.Media;

await new BrowserFetcher().DownloadAsync();
using var browser = await Puppeteer.LaunchAsync(new LaunchOptions { Headless = true });
using var page = await browser.NewPageAsync();

// Navigate to your dynamic dashboard or Webpage
await page.GoToAsync("https://apple.com");

// Export to PDF
await page.PdfAsync("WebPage.pdf", new PdfOptions
{
    Format = PaperFormat.A4,
    PrintBackground = true
});
HtmlRenderer.PdfSharp: A Basic HTML to PDF Converter for Simple ReportsHtmlRenderer.PdfSharp is a lightweight HTML converter for the PDFSharp library, providing a method for generating PDFs from simple HTML. While itâ€™s free and easy to use, itâ€™s limited when it comes to advanced HTML5 and CSS3 support.Basic rendering of HTML and CSS. for reports and invoices., so it's perfect for developers already using PDFSharp for PDF manipulation.HtmlRenderer.PdfSharp is best suited for simpler projects with minimal layout requirements:Generating basic text-based reports, invoices, or eBooks: HtmlRenderer.PdfSharp is ideal for generating PDF reports that donâ€™t require dynamic JavaScript or complex layouts.Offline or embedded applications: Since the library is self-contained with no dependencies on external engines, itâ€™s perfect for applications that need to work in isolated environments..NET learning environments or academic projects: Itâ€™s a great tool for educational purposes where simple, quick HTML rendering is required.: Available on GitHub, HtmlRenderer.PdfSharp can be freely used and modified.: Works out of the box with no need for complex setups or external engines like Chromium or wkhtmltopdf.Fast rendering for basic layouts: It is optimized for simple content like static tables or basic styled text, making it fast and efficient for these use cases.No support for JavaScript or modern web technologies: HtmlRenderer.PdfSharp cannot handle dynamic content or modern web features such as Flexbox or Grid.: Complex layouts may not render as expected due to limitations in CSS support.: More complex layouts may break or not render as expected, especially for nested elements.\
When to Consider Other ToolsIf your HTML contains advanced CSS or JavaScript,  or  would be better options.If you need higher fidelity rendering for production-level reports or more complex templates, consider .Code Example: Converting Basic HTML to PDFusing PdfSharp.Pdf;
using TheArtOfDev.HtmlRenderer.PdfSharp;

// HTML content must be simple and inline-styled
string html = @"
  <h1 style='color: navy;'>Monthly Report</h1>
  <p>This report covers performance for March 2025.</p>
  <table border='1' cellpadding='5'>
    <tr><th>Metric</th><th>Value</th></tr>
    <tr><td>Revenue</td><td>$10,000</td></tr>
    <tr><td>Users</td><td>1,200</td></tr>
  </table>";

PdfSharpCore.Pdf.PdfDocument pdf = PdfGenerator.GeneratePdf(html, PdfSharpCore.PageSize.A4);
pdf.Save("SimpleReport.pdf");
DinkToPdf: A Lightweight .NET Wrapper for WkhtmltopdfDinkToPdf works as a .NET Core wrapper for the wkhtmltopdf library, providing a method for wkhtmltopdf uses to convert HTML strings, web pages, and files to PDF. It uses the Webkit engine to handle the conversion of HTML pages and content to PDF with ease, and can handle basic CSS styling.DinkToPdf works well in the following scenarios:Lightweight, low-overhead HTML to PDF conversion: Ideal for small projects or internal tools where simplicity is key.Individuals on a tight budget: As its s free HTML conversion tool, DinkToPdf is perfect for those looking to convert HTML documents to PDF without breaking the bank.Quick generation of PDFs from static HTML: Perfect for generating invoices, reports, or other basic documents where rendering fidelity is not critical.: Since WebKit can run on both Windows and Linux, DinkToPdf is useful for cross-platform .NET applications.: The small size and low resource requirements make DinkToPdf an efficient choice for small-scale projects.: As a free, open-source library, DinkToPdf can be easily integrated into projects without licensing costs.: Works on both Windows and Linux, allowing developers to deploy their applications across different platforms.: Easy to use with minimal configuration, making it a good option for developers new to PDF generation.Limited rendering fidelity: DinkToPdf does not handle JavaScript or complex CSS features as well as other solutions like IronPDF or PuppeteerSharp.No built-in support for advanced PDF features: Lacks features such as form filling, digital signatures, or encryption, which may be necessary for some enterprise applications.Potential compatibility issues: The WebKit engine used by DinkToPdf may have compatibility issues with certain web content, especially when it comes to complex layouts.: If your documents involve JavaScript, dynamic content, or advanced web features, consider IronPDF or PuppeteerSharp.For enterprise-grade PDF features: Tools like PDFTron or Aspose.PDF provide more extensive features for professional PDF generation, including encryption, digital signatures, and form support.Code Example: Generate a Basic Invoiceusing DinkToPdf;
using DinkToPdf.Contracts;

var converter = new SynchronizedConverter(new PdfTools());

var doc = new HtmlToPdfDocument()
{
    GlobalSettings = {
        PaperSize = PaperKind.A4,
        Orientation = Orientation.Portrait,
        Out = "output.pdf"
    },
    Objects = {
        new ObjectSettings() {
            HtmlContent = @"
              <html>
              <body>
                <h1>Invoice #5678</h1>
                <p>Customer: John Smith</p>
                <p>Amount Due: $150</p>
              </body>
              </html>",
            WebSettings = { DefaultEncoding = "utf-8" }
        }
    }
};

converter.Convert(doc);
Aspose.PDF: Enterprise PDF Creation and Manipulation for .NETAspose.PDF is a comprehensive and feature-rich PDF library that offers a wide array of PDF manipulation capabilities. It allows developers to create, convert, manipulate, and secure PDFs programmatically. Aspose.PDF is a commercial library aimed at enterprise-level applications that require advanced PDF features such as document generation, editing, conversion, form filling, annotations, and digital signatures.Aspose.PDF shines in the following use cases:Enterprise document workflows: It excels in scenarios requiring comprehensive document management, such as document generation, merging, and complex form handling.High-quality PDF rendering and conversion: Aspose.PDF is ideal when you need to convert various file formats (HTML, Word, Excel) to PDFs with excellent fidelity.Advanced PDF manipulation: Whether it's document splitting, merging, watermarking, or redacting, Aspose.PDF offers extensive APIs to handle complex workflows.: Aspose.PDF provides a wide array of PDF manipulation features, from basic creation to complex document editing, making it suitable for large-scale enterprise solutions.Cross-format PDF conversion: Supports conversion between multiple formats, including HTML, DOCX, PPTX, Excel, and images to PDF.: Offers advanced capabilities like merging, splitting, form filling, annotations, and digital signatures.: Aspose.PDF ensures that the PDFs generated are of high quality and preserve the original document structure and layout.: Aspose.PDF is a commercial product, and its licensing fees can be high, making it less suitable for small-scale projects or open-source use.: Given the wide range of features and the complexity of its API, Aspose.PDF can be harder to learn for new developers compared to simpler libraries.Overkill for simple tasks: For simple HTML to PDF conversion, Aspose.PDF may be over-engineered and unnecessarily resource-heavy.For simple HTML to PDF conversion: If your only requirement is converting HTML to PDF without needing advanced manipulation features, simpler solutions like IronPDF or wkhtmltopdf may be more cost-effective.For budget-conscious projects: Aspose.PDFâ€™s licensing fees can be expensive, so for smaller projects or open-source work, alternatives like iTextSharp, DinkToPdf, or HtmlRenderer.PdfSharp may be more appropriate.Code Example: Convert HTML to PDF with Aspose.PDFusing Aspose.Pdf;
using static Aspose.Pdf.HtmlLoadOptions;

// Load HTML content from file or string
var options = new HtmlLoadOptions();
var document = new Document("example.html", options);

// Save to PDF
document.Save("Invoice_Aspose.pdf");
Syncfusion provides a comprehensive suite of PDF tools for .NET developers. The Syncfusion PDF library allows for not only converting HTML content to PDF, but also offers extensive PDF document editing capabilities. It stands out for its wide range of functionalities, including the ability to create, manipulate, merge, split, and secure PDF files. Syncfusion is a robust solution for developers looking for a full-featured PDF library that goes beyond simple HTML to PDF conversion.Syncfusion is ideal for the following scenarios:Enterprise-level document workflows: When working with complex PDF documents, including features like form filling, annotations, and document security.Complex PDF manipulations: Ideal for scenarios requiring editing, merging, splitting, or annotating existing PDFs.Document conversion and rendering: Suitable for converting various document types (e.g., Word, Excel) to PDFs, as well as HTML to PDF conversion. (Although you may need to install different NuGet packages to handle these tasks).Comprehensive PDF functionality: Syncfusion offers a vast array of features, from basic PDF creation to advanced document editing and annotation.Advanced document security: It supports password protection, encryption, and digital signatures, which are crucial for secure document handling.Support for a wide range of formats: Syncfusion supports conversions from various formats like Word, Excel, PowerPoint, and HTML to PDF, making it highly versatile.Excellent documentation and support: Syncfusion provides detailed documentation and excellent customer support, which is a major advantage for enterprise users.: Available for .NET Core and Xamarin, Syncfusion can be used in cross-platform applications, offering flexibility in deployment.: Syncfusion operates on a subscription-based pricing model, which may be prohibitive for smaller companies or open-source projects.: Due to the vast array of features, it may take time to learn and fully utilize all of Syncfusion's capabilities.: Syncfusion's library is relatively large, and for simple PDF generation tasks, it may be overkill compared to lighter libraries.For simple HTML to PDF conversion: If you only need basic HTML to PDF conversion, a lighter tool like DinkToPdf or wkhtmltopdf might be more appropriate.For budget-conscious projects: If licensing costs are a concern, Syncfusionâ€™s subscription model might be too expensive for smaller projects.Code Example: Converting a Web Page to a PDF fileusing Syncfusion.HtmlConverter;
using Syncfusion.Pdf;
using Syncfusion.Pdf.Graphics;

PdfDocument doc = new PdfDocument();
HtmlToPdfConverter converter = new HtmlToPdfConverter();

// Convert HTML content to PDF
var pdf = converter.Convert("https://www.apple.com");
FileStream fileStream = new FileStream("Syncfusion-output.pdf", FileMode.Create, FileAccess.Write);

pdf.Save(fileStream);
pdf.Close(true);
PDFTron: Enterprise-Level PDF Solutions for .NETPDFTron is a comprehensive PDF library for enterprise applications, offering a wide range of features for PDF manipulation. With PDFTron, developers can create, edit, convert, and securely sign PDFs. The key differentiator of PDFTron is its enterprise-grade capabilities, making it suitable for complex, security-conscious workflows.PDFTron is ideal for the following scenarios:Enterprise-level document automation: When your application requires robust PDF manipulation capabilities, such as document review, annotation, and redaction, PDFTron excels.Generating PDFs from HTML or web-based content: PDFTron offers high-quality conversion of HTML to PDF, as well as rendering dynamic content.Secure document workflows: PDFTron is perfect for scenarios requiring encrypted PDF files, digital signatures, or secure form filling, especially in legal or financial applications.Comprehensive PDF features: PDFTron offers a vast array of features for creating, editing, signing, and manipulating PDF documents.Enterprise-grade security: Supports encryption, digital signatures, and redaction, making it ideal for security-sensitive applications.: Available for multiple platforms, including .NET, JavaScript, iOS, Android, and Linux.Great for complex workflows: PDFTronâ€™s extensive API allows for integration with complex document workflows, making it suitable for enterprise applications.: PDFTron is a premium product, and the licensing costs can be prohibitively expensive for small businesses or open-source projects.: Due to its wide range of features, PDFTron may require a longer learning curve compared to simpler libraries.: The large feature set can be overkill for smaller projects that only need basic PDF generation.For simple HTML to PDF conversion: If your only need is basic HTML to PDF conversion, simpler tools like IronPDF or wkhtmltopdf may be sufficient.For budget-conscious projects: If cost is a concern, consider alternatives like iTextSharp or DinkToPdf, which are open-source or more affordable.Code Example: Create and Secure a PDF Documentusing System;
using System.IO;

using pdftron;
using pdftron.Common;
using pdftron.SDF;
using pdftron.PDF;

class Program
{
    private static pdftron.PDFNetLoader pdfLoader = pdftron.PDFNetLoader.Instance();
    static void Main(string[] args)
    {
        string outputPath = "output.pdf";
        string inputUrl = "http://www.apple.com";
        PDFNet.Initialize();
        HTML2PDF.SetModulePath("../../Lib");
        // Convert the Webpage to PDF and secure the PDF with a password
        using (PDFDoc doc = new PDFDoc())
        {
            HTML2PDF.Convert(doc, inputUrl);
            SecurityHandler newHandler = new SecurityHandler();
            newHandler.ChangeUserPassword("user");
            newHandler.ChangeMasterPassword("master");
            doc.SetSecurityHandler(newHandler);
            doc.Save(outputPath, SDFDoc.SaveOptions.e_linearized);
        }

    }
}
Spire.PDF: Budget-Friendly PDF Library with Basic HTML Support for .NETSpire.PDF is a component of the E-iceblue Spire.Office suite and provides comprehensive PDF generation, editing, and conversion capabilities for .NET applications. It offers HTML to PDF conversion along with a variety of other features such as table creation, image embedding, metadata editing, and digital signature handling. It's a lightweight, easy-to-use library that supports .NET Framework, .NET Core, and newer .NET versions (5/6/7+).Generating moderately styled PDF documents like forms, invoices, and datasheets from HTML templates.Office-like workflows requiring content to be exported to PDF from desktop or intranet applications.Cost-conscious small-to-medium businesses or startups needing a more affordable alternative to premium PDF SDKs.Internal business applications where design complexity is low and precise fidelity is not a high priority.Affordable licensing, offering a cost-effective solution compared to enterprise tools like Aspose or PDFTron.No external dependencies such as browser rendering engines.A straightforward API that simplifies common tasks like page merging, form field management, and image embedding.Bundled with other Spire libraries (Excel, Word), making it easier to handle multiple document formats.The HTML to PDF engine has limited support for modern CSS and JavaScript, meaning it struggles with complex styling and dynamic layouts.Output fidelity can vary based on the complexity of the HTML input, and issues can arise with responsive layouts, web fonts, or JavaScript-generated content.Documentation is sometimes lacking in detail, and reliance on forum-based support can be frustrating for developers.For high-fidelity rendering of CSS3/JS-heavy pages (e.g., dashboards or charts), use IronPDF, PuppeteerSharp, or PrinceXML.For enterprise-level PDF generation or compliance-focused workflows, opt for PDFTron or Aspose.PDF.For dynamic web-to-PDF workflows in ASP.NET or Blazor, Syncfusion or EvoPDF might be more suitable.\
Code Example: Converting Web Content to PDFusing Spire.Pdf;
using Spire.Additions.Qt;

string url = "https://apple.com";
string pdfFile = "Url_Output.pdf";

string pluginPath = "D:\\Libraries\\Plugin\\plugins";

HtmlConverter.PluginPath = pluginPath;
HtmlConverter.Convert(url, pdfFile, true, 10000, new System.Drawing.SizeF(1080, 1000), new Spire.Pdf.Graphics.PdfMargins(20));
GemBox.Document is a high-performance .NET library for reading, writing, converting, and printing documents in multiple formats, including Word, HTML, PDFs, and more. Unlike other solutions, it doesnâ€™t require Microsoft Office or any external dependencies. Its HTML to PDF conversion involves interpreting HTML as a Word document before converting it to PDF, offering a unified API for handling DOCX, HTML, PDF, RTF, and ODT formats.Applications requiring document conversion (e.g., Word to PDF or HTML to PDF) as part of a larger document workflow.Converting HTML emails, legal contracts, or rich text forms to PDF.Document-heavy systems that need to interchange multiple formats, such as DOCX â†” HTML â†” PDF.Desktop or server-based document processing in industries such as legal, HR, or academia.Simple, unified API for handling multiple formats.Excellent HTML-to-PDF rendering for clean, styled HTML, including tables and forms.Lightweight and easy to deployâ€”just a single DLL without COM/Interop or Office installations.Supports .NET Framework, .NET Core, .NET 5+, and Mono.Free version available for evaluation, with limitations on output size.Does not support JavaScript execution or dynamic content renderingâ€”only static HTML.HTML rendering fidelity is not as high as browser-based engines like IronPDF or PuppeteerSharp.Limited styling support; lacks features like media queries, Flexbox, or complex CSS layouts.Not optimized for generating PDFs from responsive web apps or dashboards.When to Consider Other ToolsFor pixel-perfect, responsive HTML rendering with JavaScript or CSS animations, use IronPDF or PuppeteerSharp.For enterprise-grade PDF editing or compliance-heavy workflows, opt for PDFTron or Aspose.PDF.For web views or SPA rendering, consider PrinceXML or SelectPDF for better fidelity.\
Code Example: Convert Simple HTML String to PDFusing GemBox.Document;

ComponentInfo.SetLicense("FREE-LIMITED-KEY");

var document = new DocumentModel();
var section = new Section(document);

document.Sections.Add(section);

section.Content.LoadText(
    "<h1 style=\"color:blue;font-size:60px;text-align:center\">Hello World!</h1>",
    LoadOptions.HtmlDefault);

document.Save("output.pdf");
SelectPDF: HTML to PDF Conversion Focused on ASP.NET Web ApplicationsSelectPDF is a dedicated HTML to PDF converter designed for server-side rendering in ASP.NET, ASP.NET Core, and MVC environments. It allows conversion of HTML strings, URLs, or files directly into styled PDFs, supporting JavaScript execution, external assets, and custom headers/footers. It is an ideal solution for web-based PDF generation within the .NET ecosystem.Converting web pages, reports, or dynamic views in ASP.NET apps to PDF.Exporting styled Razor Pages or dashboards as downloadable PDFs.Document generation in multi-user web apps (e.g., invoices, statements, certificates).SaaS applications requiring PDF download functionality with consistent layout and branding.Developer-friendly API tailored specifically for HTML content conversion.Full CSS support and partial JavaScript execution, including inline styles and external assets.Customizable page settings, including sizes, margins, headers/footers, and page breaks.Mid-tier pricing with royalty-free distribution for commercial apps.Lightweight integration with popular .NET web frameworks.Limited documentation and fewer community examples compared to IronPDF or PuppeteerSharp.Rendering engine is not Chromium-based, meaning fidelity may suffer on highly dynamic pages.Lacks broader PDF editing features like annotations, redactions, or digital signingâ€”focused on creation only.Not suitable for high-security or compliance-intensive workflows.For pixel-perfect output or dynamic JavaScript chart rendering, use IronPDF or PuppeteerSharp.For complex PDF workflows (e.g., merging, editing, signing), opt for PDFTron or Aspose.PDF.For highly responsive UI-to-PDF rendering (e.g., Blazor), consider Syncfusion or EvoPDF.\
Code Example: Convert Web Content to PDFusing SelectPdf;

SelectPdf.HtmlToPdf converter = new SelectPdf.HtmlToPdf();

var doc = converter.ConvertUrl("http://www.apple.com");

doc.Save("output.pdf");

doc.Close();
EvoPDF is a commercial .NET library that specializes in converting HTML content into PDFs. It offers advanced customization options such as support for CSS, embedded fonts, page breaks, bookmarks, and interactive forms. Designed specifically for ASP.NET and ASP.NET Core environments, EvoPDF excels in server-side rendering.For developers looking for a simple tool to convert HTML files, string, and web content.Converting CMS or eCommerce content (e.g., product listings, blog posts) to printable PDFs.Embedding custom headers, footers, watermarks, and page numbering for branded PDFs.Excellent layout and font rendering for static or moderately dynamic HTML.Supports CSS3, inline styles, custom fonts, and PDF bookmarks.Seamless integration with ASP.NET MVC, Web Forms, and Core projects.Rich API for PDF customization, including watermarking, metadata, and attachments.License-friendly, offering royalty-free distribution.No JavaScript execution supportâ€”JavaScript-generated content will not render.Requires a Windows environment, with no native Linux support.Not designed for interactive or JavaScript-heavy dashboards.Lacks integrated PDF editing, merging, or annotation tools.For Chromium-level rendering accuracy with JavaScript or dynamic content, use IronPDF or PuppeteerSharp.For PDF lifecycle management (editing, signing), use PDFTron or Aspose.PDF.For Linux compatibility or containerized microservices, Syncfusion or DinkToPdf are better suited.Code Example: Convert an HTML String to PDFusing EvoPdf;


HtmlToPdfConverter htmlToPdfConverter = new HtmlToPdfConverter();

// Convert some HTML string to a PDF file
htmlToPdfConverter.ConvertHtmlToFile("<h1 style='Color:red;font-size=60px;'>Hello World!</h1><p>This was generated using EvoPdf</p>", null, "HtmlToFile.pdf");
ActivePDF is a long-established commercial PDF solution geared toward enterprise back-office systems, offering tools for PDF generation, manipulation, and server-side processing. This library offers HTML-to-PDF functionality via the WebGrabber module, which uses Internet Explorer or Chromium-based engines (depending on configuration).Converting HTML reports or forms into PDF files in legacy Windows server environments.Automating document workflows (e.g., archival, print, merge) in healthcare, finance, or insurance systems.Organizations with existing ActivePDF infrastructure that need to extend PDF capabilities to HTML conversion.Enterprise-ready with features for compliance, security, and high-volume processing.Supports HTML input via URLs or local files, and output PDF document customization (watermarks, metadata, print settings).Available as part of a full suite for document lifecycle automation (DocConverter, Toolkit, WebGrabber, etc.).Strong track record in industries like healthcare, defense, and finance.Legacy-first toolsetâ€”modern web standards (HTML5, CSS3, JavaScript) not fully supported.Heavily tied to Windows Server and .NET Framework; lacks modern .NET Core/6+/Linux compatibility.Complex licensing, expensive pricing tiers, and dated documentation.Limited community visibilityâ€”mostly maintained through direct vendor support contracts.\
When to Consider Other ToolsFor web-to-PDF rendering in modern .NET (Core, 6, 7+) or cross-platform stacks, choose IronPDF, PrinceXML, or Syncfusion.For JavaScript rendering or SPA output, PuppeteerSharp is more accurate.For broader feature sets at lower cost, Aspose.PDF or PDFTron offer more scalable alternatives.\
Code Example: HTML to PDF via WebGrabber (simplified)using APWebGrabber;

WebGrabber wg = new WebGrabber();

string html = "<h1>Hello World!</h1> <p>This Document was generated using ActivePDF</p>";

wg.CreateFromHTMLText = html;

wg.OutputDirectory = @"C:\PDFs";
wg.NewDocumentName = "output.pdf";

wg.ConvertToPDF();
PrinceXML: The Gold Standard for Print-Perfect HTML and CSS to PDF ConversionA premium HTML-to-PDF rendering engine that delivers pixel-perfect output with full support for HTML5, CSS3, JavaScript, SVG, and MathML.\
Designed for publishing-grade documents, with precision layout rendering rivaling desktop publishing tools. \n Ideal for typeset-quality outputs such as books, magazines, scientific papers, and complex reports.Generating highly styled print documents, including annual reports, academic journals, or marketing collateral.Publishing systems that require fine-tuned layout fidelity and font control.PDF output from responsive websites that rely heavily on advanced CSS or media queries.Government or enterprise systems with compliance or archival requirements (PDF/A).Unmatched rendering accuracy for CSS-driven layouts, including Flexbox, Grid, and media queries.Full JavaScript execution (DOM-based)â€”capable of rendering client-side UIs and interactions.Supports PDF/UA, PDF/A, and tagged PDFs for accessibility and compliance.Native CLI and SDK support for multiple platforms: Windows, Linux, macOS.Trusted by publishers, legal organizations, and scientific institutions worldwide.Very expensive licensing, especially for commercial redistribution or OEM use.No dedicated .NET APIâ€”requires calling via command-line or integrating with web services or external processes.Not a PDF manipulation toolkitâ€”only does HTML-to-PDF conversion.Limited community engagement compared to mainstream .NET PDF libraries.For easier .NET integration or PDF post-processing (editing, merging), use IronPDF, PDFTron, or Aspose.PDF.For budget-sensitive projects or simpler designs, tools like DinkToPdf, SelectPDF, or HtmlRenderer.PdfSharp are more cost-effective.If a .NET-native SDK is essential, IronPDF offers browser-quality output with full .NET support.prince input.html -o output.pdf
\
Or via C# (using Process.Start):using System.Diagnostics;

ProcessStartInfo startInfo = new ProcessStartInfo
{
    FileName = "prince",
    Arguments = "input.html -o output.pdf",
    UseShellExecute = false
};

Process process = new Process { StartInfo = startInfo };
process.Start();
process.WaitForExit();
Now that we've seen these libraries in action with the above code snippets, lets look at some side-by-side comparisons that show how well some of these libraries handle HTML/CSS to PDF. Some, such as IronPDF can handle CSS-heavy content with ease, while other libraries might struggle. This section will look at which of the selected libraries best handle CSS-heavy HTML content.Side-By-Side Rendering of a CSS-Heavy Web Page\
: \n In this section, we analyze how various libraries handle the conversion of a moderately complex web page into a PDF. The web page used includes a mix of embedded CSS, images, and JavaScript (such as charts), closely resembling a real-world business report or invoice. The side-by-side comparison highlights each library's ability to preserve the layout, style, and content integrity during the conversion process. is known for its high-fidelity rendering, maintaining layout accuracy and style from complex web content. leverages the headless Chrome browser for highly accurate renderings, especially for modern web standards. produces excellent visual fidelity, with a slightly slower rendering time compared to others. maintains fast performance when converting web content to PDF, but struggles with advanced JavaScript and CSS styling. offers a fast solution with decent accuracy, though its handling of advanced styles and JavaScript might be limited compared to the others.Now, let's take a look at the performance benchmarks for each library. This table gives an overview of the performance for each library, which can be important depending on what you need out of a PDF library.|  |  |  |  |  |
|----|----|----|----|----|
|  | 150 | Low | âœ… Yes | âœ… Full |
|  | 220 | Medium | âœ… Yes | âœ… Full |
|  | 110 | Very Low | âŒ No | âŒ None |
|  | 300 | Low | âŒ No | âš ï¸ Partial |
|  | 180 | High | âœ… Yes | âœ… Full |
|  | 170 | Medium | âœ… Yes | âœ… Full |
|  | 200 | Medium | âœ… Yes | âœ… Full |
|  | 240 | Medium | âŒ No | âš ï¸ Partial |
|  | 160 | Low | âœ… Yes | âœ… Full |
|  | 200 | Medium | âœ… Yes | âœ… Full |
|  | 210 | Medium | âœ… Yes | âœ… Full |
|  | 270 | High | âš ï¸ Varies | âš ï¸ Server-Dependent |
|  | 190 | Medium-High | âœ… Yes | âœ… Full |âš ï¸ Partial / Server-Dependent: Configuration-based or limited multi-threading depending on the deployment environment.\
Test Environment \ Operating System: Windows 11 Pro, 64-bit \n Processor: Intel Core i7-12700K (12-core, 20-thread) \n RAM: 32 GB DDR5 \n .NET Version: .NET 7.0 \n Rendering Engine Versions: Latest stable NuGet or SDK versions as of March 2025A moderately complex, single-page HTML file with embedded CSS (Flexbox, Fonts), JavaScript chart (e.g., Chart.js), and image assets (PNG and SVG), totaling approximately 30 KB.Simulates a realistic invoice/report UI.: Time taken for conversion, from method call to PDF file generation (measured using Stopwatch).: Peak memory during conversion as measured using System.Diagnostics.Process.PrivateMemorySize64.: Evaluation of ability to handle multiple conversions in parallel.: Tested by executing 10 parallel HTML-to-PDF jobs using Parallel.ForEach or Task.WhenAll.: Each library is initialized and run once to avoid cold-start bias.: Average render time and memory use recorded across 5 runs.: 10 simultaneous HTML-to-PDF jobs executed, measuring thread safety and total time.: Exceptions, rendering errors, or thread conflicts logged.CLI-only libraries (e.g., PrinceXML) tested via subprocess execution.No post-processing (merging, signing)â€”focus is on raw HTML to PDF rendering.ðŸ” Key Insights & Conclusions: \n HtmlRenderer.PdfSharp is technically the fastest due to its lightweight nature but sacrifices fidelity and thread safety. IronPDF, GemBox.Document, and Syncfusion offer a good balance between speed and stability.: \n Libraries like IronPDF, Aspose.PDF, PDFTron, and PrinceXML perform well in multi-threaded environments, making them ideal for large-scale PDF generation.: \n HtmlRenderer.PdfSharp and IronPDF stand out for their low memory usage, while Aspose.PDF and ActivePDF are more memory-intensive due to their advanced feature sets.: \n DinkToPdf, Spire.PDF, and HtmlRenderer.PdfSharp may require extra handling to work reliably in multi-threaded applications.Best for Parallel Processing: \n IronPDF, PuppeteerSharp, PDFTron, and PrinceXML are best suited for parallel rendering, making them ideal for high-concurrency applications.Another important aspect to consider when choosing the best PDF library for your needs is the licensing costs. If you're on a tighter budget, then the most expensive libraries might not work for you. However, it is important to remember that cheaper libraries may not contain all of the advanced features that the more expensive libraries offer. It is important to find the right balance between features offered and cost to use.|  |  |  |  |  |
|----|----|----|----|----|
|  | Perpetual + Trial | $749+ | âœ… Yes | 1 |
|  | Open Source (MIT) | Free | âœ… Yes | Unlimited |
|  | Open Source (MIT) | Free | âœ… Yes | Unlimited |
|  | Open Source (LGPL) | Free | âœ… Yes | Unlimited |
|  | Commercial (Proprietary) | ~$999+ | âœ… Yes | 1+ |
|  | Commercial + Community | $0â€“$999+ | âš ï¸ Limited | 1+ |
|  | Commercial (Proprietary) | ~$3,000+ | âœ… Yes | 1+ |
|  | Commercial (Proprietary) | ~$799+ | âœ… Yes | 1+ |
|  | Perpetual + Free Limited Tier | ~$500+ | âœ… Yes | 1 |
|  | Commercial (Proprietary) | ~$499+ | âœ… Yes | 1 |
|  | Commercial (Proprietary) | ~$799+ | âœ… Yes | 1 |
|  | Commercial (Tiered) | ~$1,200+ | âœ… Yes | 1+ |
|  | Perpetual | ~$3,800+ | âœ… Yes | 1 |âš ï¸ : Syncfusionâ€™s free Community License is royalty-free for individuals or companies under $1M USD in annual revenue.: \n PuppeteerSharp, HtmlRenderer.PdfSharp, and DinkToPdf are great for internal tools or open-source projects, offering a solid tradeoff between cost and features.Best Value for Professional Teams: \n IronPDF, SelectPDF, and GemBox.Document offer a strong balance of features and pricing, ideal for SMBs or internal app development.Enterprise-Grade Investments: \n Aspose.PDF, PDFTron, and PrinceXML are excellent for high-compliance environments where advanced features and scaling are crucial.: \n Some tools (e.g., ActivePDF, PDFTron) may require additional licensing fees for cloud/server usage, which could increase the overall cost of ownership.Solo Dev / Startup (<3 Devs): \n IronPDF (low entry cost, high fidelity), GemBox.Document, or SelectPDF. Open source options like PuppeteerSharp and DinkToPdf are good for prototypes.Small Business (3â€“10 Devs): \n Choose IronPDF or Syncfusion (paid tier) for reliable and scalable rendering. Spire.PDF may be worth considering if also using other Spire Office tools.Enterprise / Regulated Industry: \n Invest in Aspose.PDF, PDFTron, or PrinceXML for long-term compliance, security, and scalability.The availability of documentation within a library is essential. A library may offer all the advanced tools you need, but they're no good to you if you don't know how to use them. Additionally, it is important to look into what level of support a library offers, and the community behind it.|  |  |  |  |
|----|----|----|----|
|  | Excellent | Many | Email, Chat, Phone, Docs Portal |
|  | Moderate | Some | GitHub Issues, Discussions |
|  | Minimal | Few | GitHub Issues |
|  | Low | Very Few | GitHub Issues, Community |
|  | Good | Many | Ticketing, Knowledgebase |
|  | Excellent | Extensive | Forum, Chat, Ticketing |
|  | Good | Moderate | Discord, Email, Enterprise Portal |
|  | Moderate | Basic | Email, Forum |
|  | Good | Many | Email, Docs |
|  | Moderate | Moderate | Email |
|  | Moderate | Moderate | Email, Docs |
|  | Low | Few | Email, Legacy Portal |
|  | Moderate | Moderate | Email, Docs, Support Ticketing |Best-In-Class Documentation:, , and  lead with clear documentation, rich examples, and dedicated portals, making them ideal for teams seeking quick onboarding and self-service implementation.\
Strong Enterprise Support: and  stand out with premium support channels and SLAs, making them perfect for regulated industries or critical applications.  is unique in offering 24/5 chat and phone support at mid-tier pricing.Libraries like , , and  rely on GitHub issues or sparse documentation, so expect a slower self-support curve. Theyâ€™re best for developers comfortable navigating source code or forums. and  offer limited support and older knowledge bases, meaning you might face longer turnaround times unless you have a support contract.Fastest Onboarding (Small Teams / Startups): or  provide excellent documentation with a minimal learning curve.\
Heavily Documented Ecosystem: is perfect for developers looking for extensive guides, API browsers, and live chat support. or  are ideal for developers comfortable using GitHub and online community forums for support.\
High-Support Environments (Finance, Healthcare): or  are optimal, providing guaranteed response SLAs and enterprise-level support.When choosing a PDF library, its essential to ensure that it is compatible with the environment you are working in, so now let's take a look at the compatibility of the libraries.|  |  |  |  |  |
|----|----|----|----|----|
|  | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
|  | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
|  | âœ… Yes | âš ï¸ Partial (via workarounds) | âœ— No | âœ— No |
|  | âœ… Yes | âœ… Yes | âš ï¸ Limited | âœ… Yes |
|  | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
|  | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
|  | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
|  | âœ… Yes | âœ… Yes | âš ï¸ Limited | âœ— No |
|  | âœ… Yes | âœ… Yes | âš ï¸ Partial | âš ï¸ Partial |
|  | âœ… Yes | âœ— No | âœ— No | âœ— No |
|  | âœ… Yes | âœ… Yes | âš ï¸ Limited | âœ— No |
|  | âœ… Yes | âš ï¸ Partial (.NET Std only) | âœ— No | âœ— No |
|  | âœ… Yes (via CLI) | âœ… Yes (via CLI) | âœ… Yes (CLI integration) | âœ… Yes |âš ï¸ : Indicates unofficial or restricted support (e.g., CLI-only, Windows-only binaries, partial API compatibility).ðŸ§© Key Findings & RecommendationsTop Cross-Platform Tools:, , , , and  are the most versatile, supporting a wide range of .NET versions and OS platforms. Ideal for cloud deployments, containerized apps, and modern .NET solutions., , , and  are ready for Blazor Server or WebAssembly workflows, making them perfect for teams building interactive, browser-based applications.Windows-Only or Legacy-Oriented Tools:, , and  are primarily Windows-based, which limits their use in Linux or cross-platform environments. They may still be useful for on-premises enterprise systems but are less suitable for modern DevOps stacks. excels at OS compatibility via its CLI but lacks native .NET API support, making it great for microservices or cross-language integrations.Partial/Fragile Compatibility:, , and  have limited cross-platform or Blazor support and may require additional integration work.âœ… Recommendations Based on Platform Needs:\
Cross-Platform & Cloud-Native:, , or  are your go-to options for Docker, Azure, AWS, and Linux-based hosting. and  lead the pack with strong .NET Core/Blazor integration.\
Legacy/Windows-Only Environments:, , and  are suitable for internal enterprise systems but less ideal for cloud-based or modern environments.\
For CLI/Scripted Pipelines: offers unbeatable layout fidelity with wide OS compatibility via its CLI.: Supports digital signing with certificates (PFX), both visible and invisible signature fields. Ideal for legal and contract workflows.: Advanced signature support, including HSMs, long-term validation (LTV), and timestamping.: Full-featured digital signing API with support for PKCS7, embedded timestamps, and customizable signature appearances., , : Basic certificate-based signature support., , : Limited signing capabilities., : No native digital signing support.ðŸ”’ : Supports 128/256-bit AES encryption, password protection, and permissions (print, edit, copy).: Enterprise-level document encryption with full permission control and DRM-like access restrictions., : Robust encryption APIs, supporting both password-based and certificate-based protection.: Supports encryption via CLI.: No native encryption support or require external tools.: Advanced redaction tools with regex, content-based erasure, and audit trails.: Offers area-based and text-based redaction using search patterns or coordinates.: No built-in redaction but can remove specific text or pages as a workaround.: Redaction is either unsupported or requires third-party post-processing.ðŸ“„ PDF/A and Archival Standards: Supports PDF/A-1b and PDF/A-3 for long-term archival., : Full support for PDF/A-1, 2, and 3.: Supports PDF/A-compliant documents via flags.: Supports PDF/A-1b and PDF/X.: Minimal or no support for archival formats., , : Libraries like , , and  can be configured to meet secure document handling needs.Legal / Contract Workflows: , , and  excel in digital signing, timestamping, and PDF/A compliance.|  |  |  |  |  |  |  |
|----|----|----|----|----|----|----|
| Digital Signatures | âœ… Yes | âœ… Advanced | âœ… Yes | âœ… Basic | âœ… Basic | âŒ No |
| Encryption & Permissions | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | âœ… CLI | âŒ No |
| Redaction | âš ï¸ Workaround | âœ… Full | âœ… Yes | âŒ No | âŒ No | âŒ No |
| PDF/A Compliance | âœ… PDF/A-1b, 3 | âœ… PDF/A-1/2/3 | âœ… PDF/A-1/2/3 | âœ… PDF/A-1b | âœ… PDF/A-1 | âŒ No |
| Compliance Fit | âœ… Good | âœ… Best | âœ… Strong | âš ï¸ Limited | âš ï¸ Moderate | âŒ Low |Choosing the Best HTML to PDF Library for Your Needs|  |  |
|----|----|
| Pixel-perfect styling | IronPDF, PrinceXML |
| Free/open-source projects | PuppeteerSharp |
| Secure & legal docs | PDFTron, Aspose |
| Lightweight invoicing | DinkToPdf, HtmlRenderer.PdfSharp |
| Blazor/ASP.NET Core | IronPDF, EvoPDF, Syncfusion |Choosing the right HTML-to-PDF library is a critical decision for developers working with C# and .NET. With so many options available, itâ€™s important to consider your specific needs, whether thatâ€™s pixel-perfect rendering, security compliance, cross-platform compatibility, or enterprise-level support.\
Best Library for Pixel-Perfect Styling: and  stand out for their ability to accurately render HTML to PDF, ensuring your documents look identical to what you see in the browser. This makes them ideal for use cases where visual fidelity is paramountâ€”like marketing materials or legal documents.Best Open-Source Libraries for Free Projects: is a strong contender for those looking to take advantage of open-source, cross-platform tools. While it lacks the advanced features of some commercial options, it's ideal for free projects that can rely on GitHub support.Best Libraries for Secure & Legal Documents: and  lead the pack when it comes to handling digital signatures, encryption, redaction, and compliance with industry standards such as HIPAA, SOC 2, and GDPR. These tools are perfect for secure workflows, making them ideal for finance, healthcare, legal, and government applications.Best Lightweight Libraries for Invoicing or Simple Documents: and  are excellent for creating straightforward, lightweight PDFs from HTML. These are perfect for simpler documents like invoices, reports, or small business forms, where advanced features aren't necessary.Best for Blazor/ASP.NET Core Projects:, , and  are great for teams building interactive web applications with Blazor or ASP.NET Core. Their support for modern .NET frameworks ensures a smooth integration with these platforms, helping you build rich, browser-native applications with ease.The Bottom Line: Experiment with Trials and Find Your FitUltimately, the best library for your project will depend on your unique needs and constraints. Whether youâ€™re looking for a solution thatâ€™s easy to use, highly customizable, secure, or one that supports modern cloud environments, thereâ€™s a tool that fits your requirements.\
We encourage you to take advantage of the free trials offered by  and other libraries to get hands-on experience and see how they perform in your own projects. Donâ€™t hesitate to experiment with different options to find the one that aligns best with your teamâ€™s workflow and technical needs.\
By exploring these libraries and understanding their strengths, you can make an informed decision that will not only save you time but also ensure that youâ€™re using a tool that supports your long-term goals, both in terms of performance and maintainability. Happy coding!]]></content:encoded></item><item><title>Enterprises Are Shunning Vendors in Favor of DIY Approach To AI, UBS Says</title><link>https://slashdot.org/story/25/04/09/0912235/enterprises-are-shunning-vendors-in-favor-of-diy-approach-to-ai-ubs-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 09:15:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Established software companies hoping to ride the AI wave are facing a stiff headwind: many of their potential customers are building AI tools themselves. This do-it-yourself approach is channeling billions in spending towards cloud computing providers but leaving traditional software vendors struggling to capitalize, complicating their AI growth plans. 

Cloud platforms like Microsoft Azure and Amazon Web Services are pulling in an estimated $22 billion from AI services, with Azure alone capturing $11.3 billion. Yet, software application vendors have collectively garnered only about $2 billion from selling AI products. Stripping out Microsoft's popular Copilot tools, that figure drops to a mere $450 million across all other vendors combined. 

Why are companies choosing the harder path of building? Feedback gathered by UBS points to several key factors driving this "persistent DIY trend." Many business uses for AI are highly specific or narrow, making generic software unsuitable. Off-the-shelf AI products are often considered too expensive, and crucially, the essential ingredients -- powerful AI models, cloud computing access, and the company's own data -- are increasingly available directly, lessening the need for traditional software packages.]]></content:encoded></item><item><title>This CEO Got Bill Gatesâ€™ Attentionâ€”Now Heâ€™s Fueling AI with Memgraphâ€™s Speedy Graph Tech</title><link>https://hackernoon.com/this-ceo-got-bill-gates-attentionnow-hes-fueling-ai-with-memgraphs-speedy-graph-tech?source=rss</link><author>Dominik Tomicevic</author><category>tech</category><pubDate>Wed, 9 Apr 2025 09:13:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Hey there! I'm Dominik Tomicevic, the founder and CEO of Memgraph. Back in 2011, I was one of just four people worldwide to receive the Microsoft Imagine Cup Grant, personally awarded by Bill Gates, for developing game-changing technologies in data processing.\
In 2016, I co-founded Memgraph, an open-source graph database designed for high-performance, real-time connected data processing. Our mission is to deliver knowledge graphs with unprecedented integration and ease of use, setting a new benchmark for knowledge graph solutions.\
Today, Memgraph boasts an open-source community of over 150,000 members and serves a portfolio of Global 2000 customers, including NASA, Cedars-Sinai, and Capitec Bank.\
On a personal note, I love making coffee and cocktails, and I nerd out about aviation. An ideal combo, right?\
In this post, Iâ€™ll answer the ten questions for startup founders from HackerNoon Founder and CEO David Smooke.Open-source, in-memory graph database.The world runs on connected data, but existing databases are too slow, too clunky, or too expensive for real-time. With AI, fraud detection, and recommendation systems exploding in complexity, businesses need a graph database that can handle streaming data without burning through cloud budgets. Thatâ€™s where we come in.3. What do you love about your team, and why are you the ones to solve this problem?We have a team of engineers obsessed with performance, optimizations, and making complex things simple. No fluff, no bureaucracy, just a relentless focus on building a graph database thatâ€™s better than anything on the market. We come from backgrounds in engineering, mathematics, and distributed systems, so we know how to push hardware and algorithms to their limits.4. If you werenâ€™t building your startup, what would you be doing?I would be either making cocktails on the beach and be a top barmen or be a pilot for one of the commercial aviators.5. At the moment, how do you measure success? What are your metrics?Active deployments to see who is using us in production, query performance to see how fast we can get, developer adoption, open-source contributions, and seeing more engineers adopting Memgraph and solving their challenges.6. In a few sentences, what do you offer to whom?We offer a fast, real-time graph databaseÂ for engineers building fraud detection, recommendation engines, network analysis, and AI-powered applications using Memgraph as a context engine in GraphRAG. If you need to process connected data in milliseconds, weâ€™re your best bet. Whether youâ€™re a startup, a bank, or a research lab, we help you analyze complex relationships without overpaying for slow, legacy graph databases.7. Whatâ€™s most exciting about your traction to date?Amazing customers and wild use cases.Â Weâ€™re poweringÂ AI, GraphRAG, and real-time context problemsÂ at some of the most innovative organizations out thereâ€”NASA, Cedars-Sinai, and others who canâ€™t afford slow databases. When top-tier teams in space exploration and healthcare trust Memgraph to crunch their most complex graph problems, you know youâ€™re onto something big.8. Where do you think your growth will be next year?Next year is all about building on top of what this year is all about - and that isÂ GraphRAGÂ and making life easier for developers building AI with graph technology. Weâ€™re going deep onÂ merging graphs, GenAI, and automating the messy partsâ€”like figuring out which search methods and algorithms to run for each question. Thatâ€™s where ourÂ Reasoning GraphÂ comes in. Instead of just storing data, Memgraph will actively help decideÂ Â to answer, applying context, constraints, and the right retrieval strategies on the fly.9. Tell us about your first paying customer and revenue expectations over the next year.Our first customer is a Swiss company named Saporo. They use graphs to reduce identity attack surface byÂ finding and addressing the most dangerous access (mis)configurations. We have several other customers dealing with cybersecurity and althought last year was very successful for us, as Graph RAG use cases are becoming more popular we anticipate further revenue growth.10. Whatâ€™s your biggest threat?Our biggest challenge is also our biggest opportunityâ€”the adoption of graph technology and awareness about your data being inherently interconnected which represents an enormous opportunity. While many organizations are still unfamiliar with graph technology, those who explore it quickly see its transformative power in analyzing complex relationships and making smarter business decisions. This is why education and advocacy are at the heart of what we do. By helping more people understand the value of connected data, we are not just overcoming obstaclesâ€”we are shaping the future of data-driven decision-making and empowering businesses to unlock new possibilities.]]></content:encoded></item><item><title>A Smarter Way to Check If AI Answers Are Correct</title><link>https://hackernoon.com/a-smarter-way-to-check-if-ai-answers-are-correct?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Wed, 9 Apr 2025 09:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[ Recent studies on factuality in large language models have presented several factuality benchmarks. Many benchmarks such as FELM (Chen et al., 2023), FreshQA (Vu et al., 2023), and HaluEval (Li et al., 2023) simply test a modelâ€™s knowledge of various factoids. On the other hand, adversarial benchmarks such as TruthfulQA (Lin et al., 2022) and HalluQA (Cheng et al., 2023b) consist of prompts that are designed to test whether a language model will generate a false factoid learned from imitating popular misconceptions. In the long-form setting, Min et al. (2023) proposed a set of prompts that require language models to generate biographies about people, evaluated against each personâ€™s respective Wikipedia article. Our proposed prompt set does not dispute the importance of these tasks, but rather aims to provide broad coverage of factuality in a long-form setting, whereas existing benchmarks either cover singular short factoids or only cover a small set of long-form topics.\
Evaluating factuality in model responses. In order to fully benchmark a modelâ€™s factuality, there must exist a reliable method for quantifying the factuality of a model response to a given prompt. Performance on factuality datasets with short ground-truth answers can often be evaluated using language models that have been fine-tuned on human evaluations (Lin et al., 2022; Sellam et al., 2020; Lin, 2004). Long-form responses, on the other hand, are harder to evaluate because they contain a much-broader range of possible factual claims. For example, FActScore (Min et al., 2023) leverages Wikipedia pages as knowledge sources for humans and language models to use when rating long-form responses on biographies about people. In reference-free contexts, RAGAS (Es et al., 2023) evaluate whether a response from a retrieval-augmented generation (RAG) system is self-contained by prompting a large language model to check the consistency among questions, retrieved contexts, answers, and statements within the answer. This matches the case study in Guan et al. (2023), which also showed that language models may serve as a strong tool for fact verification. Tian et al. (2023) leverages this intuition to demonstrate that factuality preference rankings obtained via model confidence can be used for improving factuality in responses via direct preference optimization. Improvements in human evaluation can also be important for evaluating factuality; in this domain, Cheng et al. (2023a) created an interface for human evaluation by asking fact-verification questions and checking self-consistency of model responses. Most related to our work, Chern et al. (2023) proposed a method of rating model responses by breaking them down and rating individual components via external tools and applied this in tasks such as code generation, math reasoning, and short-answer QA. Our proposed method takes inspiration from this work, but applies it in the long-form factuality setting, similar to other work that uses search-augmented language models for fact verification (Gao et al., 2023; Wang et al., 2023; Zhang & Gao, 2023). Compared to these prior methods of evaluating factuality, SAFE is automatic, does not require any model finetuning or any preset reference answers/knowledge sources, and is more-reliable than crowdsourced human annotations while only requiring a fraction of the cost.\
Quantifying long-form factuality. Long-form factuality is difficult to quantify because the quality of the response is influenced both by the factuality of the response (precision) and by the coverage of the response (recall). FActScore (Min et al., 2023) measures precision with respect to Wikipedia pages as knowledge sources and mentions the difficulty of measuring recall as future work. Tian et al. (2023) and Dhuliawala et al. (2023) also use precision as a proxy for a model responseâ€™s factuality. Other metrics such as token-level F1 score (Rajpurkar et al., 2016), sentence or passage-level AUC (Manakul et al., 2023), and similarity to ground-truth (Wieting et al., 2022) can account for recall as well as precision but require human responses or judgements that serve as a ground-truth. Our proposed F1@K, on the other hand, measures both precision and recall of a long-form response without requiring any preset ground-truth answers, only the number of facts from each label category.(1) Jerry Wei, Google DeepMind and a Lead contributors;(2) Chengrun Yang, Google DeepMind and a Lead contributors;(3) Xinying Song, Google DeepMind and a Lead contributors;(4) Yifeng Lu, Google DeepMind and a Lead contributors;(5) Nathan Hu, Google DeepMind and Stanford University;(6) Jie Huang, Google DeepMind and University of Illinois at Urbana-Champaign;(7) Dustin Tran, Google DeepMind;(8) Daiyi Peng, Google DeepMind;(9) Ruibo Liu, Google DeepMind;(10) Da Huang, Google DeepMind;(11) Cosmo Du, Google DeepMind;(12) Quoc V. Le, Google DeepMind.]]></content:encoded></item><item><title>ZKcandy Supercharges Web3 Mobile Gaming With L2 Mainnet Launch</title><link>https://hackernoon.com/zkcandy-supercharges-web3-mobile-gaming-with-l2-mainnet-launch?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 9 Apr 2025 08:52:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, Singapore, April 8th, 2025/Chainwire/--ZKcandy, an AI-driven gaming and entertainment Layer-2 powered by ZKsync and iCandy Interactive, launches its mainnet chain today.\
The announcement comes after the successful testnet phase and a $4 million seed round funded by Wemix Pte. Ltd., Animoca Ventures, and renowned institutional and angel investors. With a strong backing behind it, ZKcandy aims to rethink Web3 game development.Â \
ZKcandy focuses on building immersive, interoperable gaming ecosystems where users own their assets and enjoy their unrestrained movement across different titles. ZKcandy uses the best of AI to breathe life into NPCs, create dynamic, unscripted storylines, and provide an individualized experience to each player. The platform focuses on mobile-first Web3 games, competing in a rapidly growing global mobile gaming market.Â \
During the open testnet phase, users created 2.46 million wallets on ZKcandy. Five games featured on the platform amassed 230,000 active users. This makes it one of the most well-received ZK Chains in ZKsyncâ€™s Elastic Network ecosystem. To achieve this, ZKcandy collaborated with names like Layer3, Aethir, Rarible, and Out Of The Nest, onboarding 19 partners in total to grow the projectâ€™s ecosystem.Â \
Immediately after the mainnet launch, gamers will have an opportunity to try eight Web3 games, including Pepe Kingdom, Candy Defense, and Sugar Rush. To bring more to the table, ZKcandy is partnering with game studios to co-develop Web3 games based on Hollywood film IPs. The first Tier-1 Hollywood IP game is already in development.\
ZKcandy is looking to expand to 2 million active ecosystem users in 6 months after the platformâ€™s public launch.Â The global mobile gaming market is one of the fastest-growing industries, with its revenue projected to reach $126.1 billion in 2025. Combining iCandyâ€™s expertise of 500+ successfully launched titles across multiple decades with low fees and account abstraction of ZKsync, ZKcandy is uniquely positioned to become a mobile-first Web3 gaming hub.Kin Wai Lau, CEO of ZKcandy, commented on the event: â€œWe are proud to make ZKcandy available to the general audience. Our testnet has been a huge success, and this is only the beginning. With the $4 million private funding closed in January, we are looking to expand our partnership network by bringing large IPs and AAA mobile games to Web3. Blockchain gaming must be open and accessible to everyone, and it is our goal to ensure that all gamers can enjoy the transparent, secure, and low-friction experience we have to offer. Connecting mobile gaming and Web3 is a solution long overdue, and we are privileged to have the right team to do it.â€\
 was created in partnership with iCandy Interactive and Matter Labs.  is the largest game developer in Southeast Asia and has worked on 500+ mobile games and world-class award-winning AAA titles such as The Last of Us, Spiderman, Starcraft Remastered, Gears of Wars, and a long list of others.\
 is a contributor to ZKsync, a blockchain technology using zero-knowledge proofs and uniting ZK Chains into an elastic modular network. Offering frictionless interoperability, ZKsync has attracted more than $1.5B in Total Value Locked. ZKcandy is powered by  â€” an Ethereum L2 ecosystem that advances the mass adoption of crypto through ZK-proof technologies.\
With ZKsyncâ€™s tech stack, ZKcandy builds an ecosystem of interoperable games with fast transactions, negligible fees, and top-notch account abstraction features. ZKcandy is a ZK  in the Elastic Network, ZKsyncâ€™s ecosystem of natively interoperable L2s, so it can easily communicate with other chains, ensuring unrestrained liquidity flows. Other ZK Chain operators include Xsolla, Cronos (Crypto.com), Abstract, and Sophon.\
To encourage ecosystem growth after the mainnet deployment, ZKcandy will be launching promotional campaigns on the Layer3 launchpad. The platform has allocated 5% of the total native token supply as a monetary stimulus for the participants. Onboarding developers will also be able to submit applications for the upcoming ZKcandy grant. is an AI-powered Layer-2 aiming to redefine whatâ€™s possible in gaming and entertainment. As a ZK Chain built on ZKsyncâ€™s Elastic Network, it focuses on building immersive gaming ecosystems where players make the most of todayâ€™s AI capabilities. Unscripted storylines, dynamic NPCs, and a deeply personalized experience. ZKcandy delivers mobile-first Web3 games, tapping into the fastest-growing global mobile gaming market.\
ZKcandy is backed by iCandy Interactive, Southeast Asiaâ€™s largest game studio, with 500+ games, including 180 AAA titles, downloaded 300M+ times. The platform is powered by ZKsync technology, where simplified onboarding and negligible fees lay the ground for mass adoption.\
$4M+ in funding and strategic partnerships (WEMIX, Animoca Brands, Spartan Group, and others) set ZKcandy to push the boundaries in Web3 entertainment, inspired by the AI revolution. More information can be found here:  |  |  | :::tip
This story was distributed as a release by Chainwire under HackerNoonâ€™s Business Blogging Program. Learn more about the programÂ ]]></content:encoded></item><item><title>The Earlier You Integrate AI, the More Efficient Your Work Becomes</title><link>https://hackernoon.com/the-earlier-you-integrate-ai-the-more-efficient-your-work-becomes?source=rss</link><author>achrafgolli</author><category>tech</category><pubDate>Wed, 9 Apr 2025 08:50:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As a project manager, you might have encountered various frameworks. How many do you employ when faced with a new product? Despite the abundance of management frameworks available,  find it feasible to optimize development processes.\
It is a big deal, and you might be sleeping on a  if your management process remains under-optimized. Despite the many talks against AI taking over jobs, using AI can still immensely benefit any product management workflow.\
So, the earlier you integrate AI into your systems, the more efficient your work becomes. Read on to learn how you can manage projects better with AI.First, you want to be sure of the right tools and how they affect your product management cycle. Here are some of the most impactful AI-powered solutions available for product managers that I have come across:OpenAI's ChatGPT and Claude: Summarize feedback, generate reports, and prompt it to assist in ideation. Whether you choose ChatGPT or Claude largely depends on your team's needs.Jasper AI: Jasper AI is built to assist with marketing. Use it to create high-quality content, including product documentation, blogs, etc.Notion AI: Notion AI helps you automate meeting notes, summarizations, and content generation.Julius AI: Think of it has having Excel, Python, and ChatGPT in one tool. Julius AI allows you to visualize, clean, merge, and sort out your data. It also allows you to ask questions based on the insights it gathers.ChatPRD: This AI tool helps you streamline the process of writing your product requirements documents. It covers core aspects from brainstorming to drafting and even providing feedback.Kadoa: With Kadoa, you can collect data from the web faster while automating the extraction and validation process.Zeda.io: Imagine being able to tailor your next products to your consumers' exact needs. Zeda let's you do that by automating the collection and analysis of customer feedback.AI allows you to automate several tasks, leading to a more efficient product management process. Some tools for automation are:OpenAI's ChatGPT and Claude: Summarize feedback, generate reports, and prompt it to assist in ideation. Whether you choose ChatGPT or Claude largely depends on your team's needs.Jasper AI: Jasper AI is built to assist with marketing. Use it to create high-quality content, including product documentation, blogs, etc.Notion AI: Notion AI helps you automate meeting notes, summarizations, and content generation.How Can AI Influence Your Product Management?AI is not just another tool in the product manager's toolkit, and . Unsurprisingly, the way products are built, optimized, and scaled is changing. Some core areas of product management that will be impacted in the future include:AI-Powered Decision-makingAt the moment, product managers use user surveys, feedback, and analytics to collect information on customer needs. However, soon, AI will go a few steps further to analyze the collected data at scale. With this, itâ€™ll detect and show you patterns that would've been hard to uncover through traditional means. This, in turn, will help you and your team predict user behavior, churn, etc. and then adjust features based on their discoveries in real time.More than ever before, there is an increase in demand for personalization. According to a McKinsey study, . If they can't, they'll go elsewhere. It's no news that hyper-personalization is how big companies like Netflix, Amazon, and Spotify keep you glued to their platforms. And as the product management industry evolves, we'll see more AI tools for integrating hyper-personalization into yourÂ products, giving users the tailored experience they want.The average product manager spends most of his time researching users and competitors. They also exhaust most of their time and effort on backlog prioritization. However, if they automate the bulk of this work using AI, they can focus more on other aspects of the job.\
Right now, certain tools like Kadoa allow you to do this to an extent. However, it won't be long before we welcome more sophisticated use of AI in refining product strategy, creating more innovative solutions, and automating these processes.A/B Testing and ExperimentationA/B testing is one of the oldest ways to collect insights on what works for your customers, and how your products can better serve them. However, traditional A/B testing requires manual setup and keen analysis, which is overly time-consuming. AI has the potential to improve experimentation processes by running multiple tests and analyzing results faster.How Can Project Teams Adapt to AI?Despite its many advantages, AI poses challenges and limitations for product managers. This includes a slower learning curve, ethical concerns and biases, and even over-reliance on automation. However, some ways we can adapt to it while limiting these barriers include:You don't need a background in AI or data science to be AI-literate. As a product manager, you only need to understand how AI works, its capabilities, and its limitations. And all these are possible by taking a few online courses and staying updated on AI trends.Integrate AI incrementallyMany organizations make the mistake of trying to integrate AI into their processes all at once. This complicates the entire process and makes it hard for teams to adapt. Instead, encourage your team to take a step-by-step approach.Balance AI Usage with The Human TouchThe end goal of adopting AI is not to replace your capability as a product manager. It is to deliver value to consumers efficiently, and this can greatly benefit from the human touch.AI models need continuous monitoring to remain effective. While adapting to AI, product managers must look for biases or errors and refine models based on them.As with every industry, AI's entry comes with a series of questions, and one of the most pressing ones in product management is: What does the future hold?\
As AI advances, it's not unlikely that we'll notice a shift towards AI-driven roadmaps. AI already suggests actions based on user behavior, so suggesting features based on user predictive behavior doesn't seem so far away.\
AI also has the potential to determine with increased accuracy and speed whether a product will meet market needs before it is released. While these are interesting takes, product managers must still strive to balance AI adoption with human expertise. That's where the real beauty lies.]]></content:encoded></item><item><title>GPT-4, Gemini-Ultra, and PaLM-2-L-IT-RLHF Top Long-Form Factuality Rankings</title><link>https://hackernoon.com/gpt-4-gemini-ultra-and-palm-2-l-it-rlhf-top-long-form-factuality-rankings?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Wed, 9 Apr 2025 07:00:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[6 LARGER LLMS ARE MORE FACTUALWhile prior research has attempted to benchmark long-form factuality across language models (Min et al., 2023), our work presents a more-comprehensive dataset, a morescalable evaluation method, and an aggregation metric that considers factual recall. For these reasons, we benchmark thirteen popular large language models across four model families (shown in Table 1)â€”Gemini models (Gemini Team, 2023), GPT models (OpenAI, 2022; 2023), Claude models (Anthropic, 2023; 2024), and PaLM-2 models (Google, 2023). We evaluate each model on the same random subset of 250 prompts[9]\
from LongFact-Objects,[10] and we decode model responses up to 1,024 tokens at a temperature of zero. We then use SAFE to obtain raw evaluation metrics for each model response, which we aggregate using F1@K as described in Section 5 (we selected K = 64, the median number of relevant facts among all model responses for the tested prompts, and K = 178, the maximum number of relevant facts in a response among all model responses for the tested prompts).\
As shown in Figure 6 and Table 2, we generally find that larger models achieve better long-form factuality. For example, GPT-4-Turbo is better than GPT-4 which is better than GPT-3.5-Turbo, Gemini-Ultra is more factual than Gemini-Pro, and PaLM-2-L-IT-RLHF is better than PaLM-2-L-IT. We also see that the three most-factual models at both selected K values are GPT-4-Turbo, GeminiUltra, and PaLM-2-L-IT-RLHF, which all seem to correspond to the largest model within their respective families (with respect to some scaling factor, not necessarily the number of parameters). Many models from newer model families such as Gemini, Claude-3-Opus, and Claude-3-Sonnet are able to match or surpass GPT-4 performance, which is not entirely unexpected since GPT-4 (gpt-4-0613) is an older language model. Notably, we found that Claude-3-Sonnet achieves similar long-form factuality as Claude-3-Opus despite being a smaller model, but without access to further details about these models, it is unclear why this was the case. Claude-3-Haiku also has seemingly-low long-form factuality relative to older models within the same family such as Claude-Instant, though this may have been a result of prioritizing factual precision over recall, as shown in Table 2 and further discussed in Appendix E.2. [11] We discuss further insights from these results in Appendix E, including how scaling may impact long-form factuality (Appendix E.2), the effects of Reinforcement Learning from Human Feedback (Appendix E.4), and how rankings can change with respect to the K used for calculating F1@K (Appendix E.3).[9] To increase prompt difficulty, we added a fixed postamble to every promptâ€”this postamble asks the model to provide as many specific details as possible. We discuss this postamble further in Appendix A.7.\
[10] We excluded LongFact-Concepts for the reasons discussed in Appendix A.8.\
[11] As shown in Table 2, factual recall has improved significantly more than factual precision in recent efforts to scale language models.(1) Jerry Wei, Google DeepMind and a Lead contributors;(2) Chengrun Yang, Google DeepMind and a Lead contributors;(3) Xinying Song, Google DeepMind and a Lead contributors;(4) Yifeng Lu, Google DeepMind and a Lead contributors;(5) Nathan Hu, Google DeepMind and Stanford University;(6) Jie Huang, Google DeepMind and University of Illinois at Urbana-Champaign;(7) Dustin Tran, Google DeepMind;(8) Daiyi Peng, Google DeepMind;(9) Ruibo Liu, Google DeepMind;(10) Da Huang, Google DeepMind;(11) Cosmo Du, Google DeepMind;(12) Quoc V. Le, Google DeepMind.]]></content:encoded></item><item><title>Clean Energy Powered 40% of Global Electricity in 2024, Report Finds</title><link>https://news.slashdot.org/story/25/04/09/0451237/clean-energy-powered-40-of-global-electricity-in-2024-report-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 06:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The world used clean power sources to meet more than 40% of its electricity demand last year for the first time since the 1940s, figures show. The Guardian: A report by the energy thinktank Ember said the milestone was powered by a boom in solar power capacity, which has doubled in the last three years. The report found that solar farms had been the world's fastest-growing source of energy for the last 20 consecutive years. 

Phil MacDonald, Ember's managing director, said: "Solar power has become the engine of the global energy transition. Paired with battery storage, solar is set to be an unstoppable force. As the fastest-growing and largest source of new electricity, it is critical in meeting the world's ever-increasing demand for electricity." 

Overall, solar power remains a relatively small part of the global energy system. It made up almost 7% of the world's electricity last year, according to Ember, while wind power made up just over 8% of the global power system. The fast-growing technologies remain dwarfed by hydro power, which has remained relatively steady in recent years, and made up 14% of the worldÃ¢(TM)s electricity in 2024.]]></content:encoded></item><item><title>The TechBeat: The AI Industryâ€™s Love Affair With Overengineering Needs an Intervention (4/9/2025)</title><link>https://hackernoon.com/4-9-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Wed, 9 Apr 2025 06:10:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @dataimpulse [ 5 Min read ] 
 Web scraping in 2025 faces AI defenses, legal hurdles, and new rules. Learn smart, compliant strategies to keep your data flows running smooth. Read More.By @socialdiscoverygroup [ 3 Min read ] 
 Discover how Social Discovery Groupâ€™s structured, data-driven onboarding process helps new hires seamlessly integrate into their teams and roles. Read More.By @asitsahoo [ 3 Min read ] 
 Discover how Claude Desktop with MCP tools eliminated digital fragmentation, enabling deeper product thinking and transforming weeks of work into hours. Read More.By @evedex [ 5 Min read ] 
 Arbitrum's Layer 3 and EVEDEX offer a breakthrough to solve DeFi liquidity fragmentation with unified trading, smart routing, and cross-chain efficiency. Read More.By @ishanpandey [ 2 Min read ] 
 A new partnership between Addressable and Sevio will enable programmatic display ads on major crypto platforms, making web3 target more efficient.  Read More.By @ace2489 [ 9 Min read ] 
 The value of programming languages that don't hide the details. Read More.By @axotion [ 4 Min read ] 
 Caching can be a thorn in any developerâ€™s side. Iâ€™ve spent too many hours wrestling with slow APIs and overburdened databases. Read More.By @moonlock [ 5 Min read ] 
 Is Netflixâ€™s 'Zero Day' realistic? A malware researcher breaks down 3 real cyber threats and 3 myths, separating cybersecurity fact from fiction. Read More.By @riteshmodi [ 9 Min read ] 
 Embedding and LLM's needs to be tested and evaluated or hallucinations will happen. Experimentation and evaluation on custom data is a must - openai and genai Read More.By @proflead [ 5 Min read ] 
 Googleâ€™s Gemini 2.5 Pro is designed for complex reasoning tasks and excels in understanding and generating content across various formats.  Read More.By @nilanganray [ 8 Min read ] 
 If you love art, whether as a creator or a viewer, embrace the last hurrah because there will NEVER be another Studio Ghibli. Read More.By @neer-varshney [ 4 Min read ] 
 Generative AI companies are grappling with the balance between high costs and sustainable revenue. Read More.By @albertlieyeongdok [ 6 Min read ] 
 Discover 8 powerful tools transforming prompt engineering from trial-and-error into scalable systemsâ€”featuring visual workflows, auto-tuned prompts, and memory- Read More.By @thomascherickal [ 10 Min read ] 
 20 Mind-Blowing Science-Fiction AI and Blockchain Use Cases that will amaze, thrill, and shock you with their scope and immense possibilities.  Read More.By @praisejames [ 10 Min read ] 
 I worked on a project to make internet access free. Hereâ€™s what I learned about work, creativity, and getting paid to be yourself. Read More.By @jay-thakur [ 12 Min read ] 
 Just because we can build multi-agent AI systems doesnâ€™t mean we should. Read More.By @kfamyn [ 19 Min read ] 
 Learn everything about Swift initializers â€” convenience override, saving memberwise initializer, required init() use cases, parameterless UIView() and more! Read More.]]></content:encoded></item><item><title>Fake Job Seekers Are Flooding US Companies</title><link>https://slashdot.org/story/25/04/09/0134223/fake-job-seekers-are-flooding-us-companies?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 04:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Fake job seekers using AI tools to impersonate candidates are increasingly targeting U.S. companies with remote positions, creating a growing security threat across industries. By 2028, one in four global job applicants will be fake, according to Gartner. These imposters use AI to fabricate photo IDs, generate employment histories, and provide interview answers, often targeting cybersecurity and cryptocurrency firms, CNBC reports. 

Once hired, fraudulent employees can install malware to demand ransoms, steal customer data, or simply collect salaries they wouldn't otherwise obtain, according to Vijay Balasubramaniyan, CEO of Pindrop Security. The problem extends beyond tech companies. Last year, the Justice Department alleged more than 300 U.S. firms inadvertently hired impostors with ties to North Korea, including major corporations across various sectors.]]></content:encoded></item><item><title>Hackers Spied on 100 US Bank Regulators&apos; Emails for Over a Year</title><link>https://news.slashdot.org/story/25/04/09/0034251/hackers-spied-on-100-us-bank-regulators-emails-for-over-a-year?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 02:31:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Hackers intercepted about 103 bank regulators' emails for more than a year, gaining access to highly sensitive financial information, Bloomberg News reported Tuesday, citing two people familiar with the matter and a draft letter to Congress. From the report: The attackers were able to monitor employee emails at the Office of the Comptroller of the Currency after breaking into an administrator's account, said the people, asking not to be identified because the information isn't public. OCC on Feb. 12 confirmed that there had been unauthorized activity on its systems after a Microsoft security team the day before had notified OCC about unusual network behavior, according to the draft letter. 

The OCC is an independent bureau of the Treasury Department that regulates and supervises all national banks, federal savings associations and the federal branches and agencies of foreign banks -- together holding trillions of dollars in assets. OCC on Tuesday notified Congress about the compromise, describing it as a "major information security incident." 

"The analysis concluded that the highly sensitive bank information contained in the emails and attachments is likely to result in demonstrable harm to public confidence," OCC Chief Information Officer Kristen Baldwin wrote in the draft letter to Congress that was seen by Bloomberg News. While US government agencies and officials have long been the targets of state-sponsored espionage campaigns, multiple high-profile breaches have surfaced over the past year.]]></content:encoded></item><item><title>MindEye2 and Shared-Subject Functional Alignment</title><link>https://hackernoon.com/mindeye2-and-shared-subject-functional-alignment?source=rss</link><author>Image Recognition</author><category>tech</category><pubDate>Wed, 9 Apr 2025 02:05:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[MindEye2 involves pretraining and then fine-tuning a single model where brain activity is mapped to the embedding space of pretrained deep learning models. During inference, these embeddings predicted from the brain are fed into frozen image generative models that translate from model space to pixel space. Our strategy to reconstruct seen images from brain activity using minimal training data is to first pretrain the model using data from 7 subjects (30-40 hours of scanning data each) and then to fine-tune the model using data from a held-out 8th subject. The full MindEye2 pipeline is depicted in Figure 2.\
Single-subject models were trained/fine-tuned on a single 8xA100 80Gb GPU node for 150 epochs with a batch size of 24. Multi-subject pretraining was done with a batch size of 63 (9 samples per each of 7 subjects). Models were trained with Huggingface Accelerate (Gugger et al., 2022) and DeepSpeed (Rajbhandari et al., 2020) Stage 2 with CPU offloading.2.1 Shared-Subject Functional AlignmentEvery subject has a uniquely shaped brain with different functional organization, meaning that there needs to be an initial alignment step to ensure the model can handle inputs from different brains. Unlike anatomical alignment where every subjectâ€™s brain is mapped to the same brain template (Talairach and Tournoux, 1990; Mazziotta et al., 2001), we remain in subjectsâ€™ native brain space and functionally align flattened spatial patterns of fMRI activity to a shared-subject latent space using subject-specific ridge regression. That is, each subject has a separate linear layer with weight decay to map the input fMRI voxels (13,000 to 18,000 voxels depending on the subject) to a 4096-dim latent.\
Following this initial linear layer, the rest of the model pipeline is shared across subjects without any subjectspecific mappings. The whole pipeline is trained end-to-end where pretraining involves each batch containing brain inputs from all subjects. That is, alignment to shared-subject space is not trained independently and we do not pretrain models separately for each subject; rather, we pretrain a single model equally sampling across all the subjects except the held-out subject used for fine-tuning.\
Two strengths of this novel functional alignment procedure are in its simplicity and flexibility. Using a simple linear mapping for alignment can provide robust, generalizeable performance in low-sample, high-noise settings because simple mappings are less likely to overfit to noise. Also, unlike typical functional alignment approaches that require subjects to process a shared set of images (Haxby et al., 2011), our approach has the flexibility to work even when subjects are viewing entirely unique images in the training data. This is critical for the Natural Scenes Dataset, where 90% of the seen images are unique to the subject and the 10% that were seen across subjects are relegated to the test set. Further, this approach holds advantages for subsequent data collection of a new subject, where such data collection does not need to be restricted to showing a predefined set of images.(1) Paul S. Scotti, Stability AI and Medical AI Research Center (MedARC);(2) Mihir Tripathy, Medical AI Research Center (MedARC) and a Core contribution;(3) Cesar Kadir Torrico Villanueva, Medical AI Research Center (MedARC) and a Core contribution;(4) Reese Kneeland, University of Minnesota and a Core contribution;(5) Tong Chen, The University of Sydney and Medical AI Research Center (MedARC);(6) Ashutosh Narang, Medical AI Research Center (MedARC);(7) Charan Santhirasegaran, Medical AI Research Center (MedARC);(8) Jonathan Xu, University of Waterloo and Medical AI Research Center (MedARC);(9) Thomas Naselaris, University of Minnesota;(10) Kenneth A. Norman, Princeton Neuroscience Institute;(11) Tanishq Mathew Abraham, Stability AI and Medical AI Research Center (MedARC).]]></content:encoded></item><item><title>Reconstructing Vision With Minimal fMRI Data: Cross-Subject Pretraining With MindEye2</title><link>https://hackernoon.com/reconstructing-vision-with-minimal-fmri-data-cross-subject-pretraining-with-mindeye2?source=rss</link><author>Image Recognition</author><category>tech</category><pubDate>Wed, 9 Apr 2025 01:42:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.Spurred by the open releases of deep learning models such as CLIP (Radford et al., 2021) and Stable Diffusion (Rombach et al., 2022), along with large-scale functional magnetic resonance imaging (fMRI) datasets such as the Natural Scenes Dataset (Allen et al., 2022) where human participants were scanned viewing tens of thousands of images, there has been an influx of research papers demonstrating the ability to reconstruct visual perception from brain activity with high fidelity (Takagi and Nishimoto, 2022; 2023; Ozcelik et al., 2022; Ozcelik and VanRullen, 2023; Gaziv et al., 2022; Gu et al., 2023; Scotti et al., 2023; Kneeland et al., 2023a;b;c; Ferrante et al., 2023a; Thual et al., 2023; Chen et al., 2023a;b; Sun et al., 2023; Mai and Zhang, 2023; Xia et al., 2023). FMRI indirectly measures neural activity by detecting changes in blood oxygenation. These patterns of fMRI brain activity are translated into embeddings of pretrained deep learning models and used to visualize internal mental representations (Beliy et al., 2019; Shen et al., 2019a;b; Seeliger et al., 2018; Lin et al., 2019).\
Visualization of internal mental representations, and more generally the ability to map patterns of brain activity to the latent space of rich pretrained deep learning models, has potential to enable novel clinical assessment approaches and brain-computer interface applications. However, despite all the recent research demonstrating high-fidelity reconstructions of perception, the practical adoption of such approaches to these settings has been limited if not entirely absent. A major reason for this is that the high-quality results shown in these papers use single-subject models that are not generalizable across people, and which have only been shown to work well if each subject contributes dozens of hours of expensive fMRI training data. MindEye2 introduces a novel functional alignment procedure that addresses these barriers by pretraining a shared-subject model that can be fine-tuned using limited data from a held-out subject and generalized to held-out data from that subject. This approach yields similar reconstruction quality to a single-subject model trained using 40Ã— the training data. See Figure 1 for selected samples of reconstructions obtained from just 1 hour of data from subject 1 compared to their full 40 hours of training data in the Natural Scenes Dataset.\
In addition to a novel approach to shared-subject alignment, MindEye2 builds upon the previous SOTA approach introduced by MindEye1 (Scotti et al., 2023). In terms of similarities, both approaches map flattened spatial patterns of fMRI activity across voxels (3-dimensional cubes of cortical tissue) to the image embedding latent space of a pretrained CLIP (Radford et al., 2021) model with the help of a residual MLP backbone, diffusion prior, and retrieval submodule. The diffusion prior (Ramesh et al., 2022) is used for reconstruction and is trained from scratch to take in the outputs from the MLP backbone and produce aligned embeddings suitable as inputs to any pretrained image generation model that accepts CLIP image embeddings (hereafter referred to as unCLIP models). The retrieval submodule is contrastively trained and produces CLIP-fMRI embeddings that can be used to find the original (or nearest neighbor) image in a pool of images, but is not used to reconstruct a novel image. Both MindEye2 and MindEye1 also map brain activity to the latent space of Stable Diffusionâ€™s (Rombach et al., 2022) variational autoencoder (VAE) to obtain blurry reconstructions that lack high-level semantic content but perform well on low-level image metrics (e.g., color, texture, spatial position), which get combined with the semantically rich outputs from the diffusion prior to return reconstructions that perform well across perceptual and semantic features.\
MindEye2 innovates upon MindEye1 in the following ways: (1) Rather than the whole pipeline being independently trained per subject, MindEye2 is pretrained on data from other subjects and then fine-tuned on the held-out target subject. (2) We map from fMRI activity to a richer CLIP space provided by OpenCLIP ViT-bigG/14 (Schuhmann et al., 2022; Ilharco et al., 2021), and reconstruct images via a fine-tuned Stable Diffusion XL unCLIP model that supports inputs from this latent space. (3) We merge the previously independent high- and low-level pipelines into a single pipeline through the use of submodules. (4) We additionally predict the text captions of images to be used as conditional guidance during a final image reconstruction refinement step.\
The above changes support the following main contributions of this work: (1) Using the full fMRI training data from Natural Scenes Dataset we achieve state-of-the-art performance across image retrieval and reconstruction metrics. (2) Our novel multi-subject alignment procedure enables competitive decoding performance even with only 2.5% of a subjectâ€™s full dataset (i.e., 1 hour of scanning).(1) Paul S. Scotti, Stability AI and Medical AI Research Center (MedARC);(2) Mihir Tripathy, Medical AI Research Center (MedARC) and a Core contribution;(3) Cesar Kadir Torrico Villanueva, Medical AI Research Center (MedARC) and a Core contribution;(4) Reese Kneeland, University of Minnesota and a Core contribution;(5) Tong Chen, The University of Sydney and Medical AI Research Center (MedARC);(6) Ashutosh Narang, Medical AI Research Center (MedARC);(7) Charan Santhirasegaran, Medical AI Research Center (MedARC);(8) Jonathan Xu, University of Waterloo and Medical AI Research Center (MedARC);(9) Thomas Naselaris, University of Minnesota;(10) Kenneth A. Norman, Princeton Neuroscience Institute;(11) Tanishq Mathew Abraham, Stability AI and Medical AI Research Center (MedARC).]]></content:encoded></item><item><title>Android Function Examples That You Should Know</title><link>https://hackernoon.com/android-function-examples-that-you-should-know?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Wed, 9 Apr 2025 01:11:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A.1 Android function examples(1) Wei Chen, Stanford University, with equal contribution and a corresponding author {weichen6}@stanford.edu;(2) Zhiyuan Li, Stanford University and a corresponding author {zhiyuan8}@stanford.edu.]]></content:encoded></item><item><title>The Future of Octopus v2: What Does it Entail?</title><link>https://hackernoon.com/the-future-of-octopus-v2-what-does-it-entail?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Wed, 9 Apr 2025 01:09:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[5 Discussion and future worksOur current training initiative proves that any specific function can be encapsulated into a newly coined term, functional token, a novel token type seamlessly integrated into both the tokenizer and the model. This model, through a cost-effective training process amounting to merely two cents, facilitates the deployment of AI agents characterized by their remarkably low latency and high accuracy.\
The potential impacts of our research are extensive. For application developers, including those at DoorDash and Yelp, our model paves the way for training on application-specific scenarios. Developers can pinpoint the APIs most utilized by their audience, transform these into functional tokens for the Octopus model, and proceed with deployment. This strategy has the capacity to fully\
automate app workflows, emulating functionalities akin to Appleâ€™s Siri, albeit with significantly enhanced response speeds and accuracy.\
Furthermore, the modelâ€™s application within operating systems of PCs, smartphones, and wearable technology presents another exciting avenue. Software developers could train minor LoRAs specific to the operating system. By accumulating multiple LoRAs, the model facilitates efficient function calling across diverse system components. For instance, incorporating this model into the Android ecosystem would enable Yelp and DoorDash developers to train distinct LoRAs, thus rendering the model operational on mobile platforms as well.\
Looking ahead, we aim to develop a model dedicated to on-device reasoning. Our ambitions are dual-pronged: firstly, to achieve notable speed enhancements for cloud deployments, vastly outpacing GPT-4 in speed metrics. Secondly, to support local deployment, offering a valuable solution for users mindful of privacy or operational costs. This dual deployment strategy not only extends the modelâ€™s utility across cloud and local environments, but also caters to user preferences for either speed and efficiency or privacy and cost savings.[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.\
[3] Erik Brynjolfsson, Danielle Li, and Lindsey R Raymond. Generative ai at work. Technical report, National Bureau of Economic Research, 2023. [4] Rich Caruana. Multitask learning. Machine learning, 28:41â€“75, 1997.\
[5] Imran Chaudhri. Humane ai, 2024. URL https://humane.com/. Accessed on March 31, 2024.\
[6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.\
[7] Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou Yu. Towards next-generation intelligent assistants leveraging llm techniques. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5792â€“5793, 2023.\
[8] Yu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-reflective, hierarchical agents for large-scale api calls. arXiv preprint arXiv:2402.04253, 2024.\
[9] Div Garg. Multion ai, 2024. URL https://www.multion.ai/. Accessed on March 31, 2024.\
[10] Gemma Team, Google DeepMind. Gemma: Open models based on gemini research and technology, 2023. URL https://goo.gle/GemmaReport.\
[11] Allyson I Hauptman, Beau G Schelble, Nathan J McNeese, and Kapil Chalil Madathil. Adapt and overcome: Perceptions of adaptive autonomous agents for human-ai teaming. Computers in Human Behavior, 138: 107451, 2023.\
[12] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.\
[13] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:20482â€“20494, 2023.\
[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\
[15] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023.\
[16] Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. Design of chain-of-thought in math problem solving, 2023.\
[17] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459â€“9474, 2020.\
[18] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023.\
[19] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. A survey on retrieval-augmented text generation. arXiv preprint arXiv:2202.01110, 2022.\
[20] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. arXiv preprint arXiv:2303.16434, 2023.\
[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\
[22] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499, 2023.\
[23] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024.\
[24] llama.cpp team. llama-cpp. Software available at https://github.com/ggerganov/llama.cpp, 2023. Accessed on March 31, 2024.\
[25] David Luan. Adept ai, 2024. URL https://www.adept.ai/. Accessed on March 31, 2024.\
[26] Jesse Lyu. Rabbit r1, 2024. URL https://www.rabbit.tech/. Accessed on March 31, 2024.\
[27] Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-augmented retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553, 2020.\
[28] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\
[29] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023.\
[30] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\
[31] Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, and Nathan Cooper. Stable code 3b, 2023. URL https://huggingface.co/stabilityai/stable-code-3b.\
[32] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023.\
[33] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI blog, 2018.\
[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\
[35] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316â€“1331, 2023.\
[36] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427, 2023.\
[37] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.\
[38] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. Small llms are weak tool learners: A multi-llm agent. arXiv preprint arXiv:2401.07324, 2024.\
[39] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36, 2024.\
[40] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\
[41] Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. arXiv preprint arXiv:2308.04623, 2023.\
[42] Venkat Krishna Srinivasan, Zhen Dong, Banghua Zhu, Brian Yu, Damon Mosk-Aoyama, Kurt Keutzer, Jiantao Jiao, and Jian Zhang. Nexusraven: a commercially-permissive language model for function calling. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.\
[43] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023.\
[44] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023.\
[45] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\
[46] MLC team. Mlc-llm. Software available at https://github.com/mlc-ai/mlc-llm, 2023. Accessed on March 31, 2024.\
[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\
[48] Trelis. Llama-2-7b-chat-hf-function-calling-v3: A model fine-tuned for function calling. Hugging Face Model Repository, 2023. URL https://huggingface.co/Trelis/ Llama-2-7b-chat-hf-function-calling-v3. Accessed on March 31, 2024.\
[49] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024.\
[50] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.\
[51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824â€“24837, 2022.\
[52] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Empowering llm to use smartphone for intelligent task automation. arXiv preprint arXiv:2308.15272, 2023.\
[53] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\
[54] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\
[55] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. Llmcad: Fast and scalable on-device large language model inference. arXiv preprint arXiv:2309.04255, 2023.\
[56] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.\
[57] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, page 100211, 2024.\
[58] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.(1) Wei Chen, Stanford University, with equal contribution and a corresponding author {weichen6}@stanford.edu;(2) Zhiyuan Li, Stanford University and a corresponding author {zhiyuan8}@stanford.edu.]]></content:encoded></item><item><title>UK Creating &apos;Murder Prediction&apos; Tool To Identify People Most Likely To Kill</title><link>https://news.slashdot.org/story/25/04/09/0028213/uk-creating-murder-prediction-tool-to-identify-people-most-likely-to-kill?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 01:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New submitter toutankh writes: The UK government is developing a tool to predict murder. The scheme was originally called the "homicide prediction project", but its name has been changed to "sharing data to improve risk assessment". The Ministry of Justice hopes the project will help boost public safety but campaigners have called it "chilling and dystopian".The existence of the project was uncovered by Statewatch rather than announced by the UK government. PR following this discovery looks like uncoordinated damage control: one stated goal is to "ultimately contribute to protecting the public via better analysis", but a spokesperson also said that it is "for research purpose[s] only". One criticism is that such a system will inevitably reproduce existing bias from the police. What could go wrong?]]></content:encoded></item><item><title>Cracking the Code on Credit Card Declines and Hidden Revenue Loss</title><link>https://hackernoon.com/cracking-the-code-on-credit-card-declines-and-hidden-revenue-loss?source=rss</link><author>Aditya Vilas Deshpande</author><category>tech</category><pubDate>Wed, 9 Apr 2025 00:22:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Your card's fine. Your money's real. Soâ€¦???? You're not alone (hopefully!).\
Welcome to the invisible leak no one talks about - credit card authorization failures.\
It's not just a customer annoyance. It's a revenue black hole. And businesses are bleeding silently at the point of payment.\
So, what should we do? What we do is to peel back the curtain. ðŸ§µ[ ] 15% of recurring card payments fail[ ] Some industries see 30%+ decline rates[ ] In 2023, $157B in e-commerce revenue was at risk[ ] $81B of that? Gone. Never recovered.\
\
\
\
All because the system said  to legit money(Whoa!!)Top 10 Reasons Cards Get Declinedâ€œThe False Decline Epidemicâ€Banks declined $118B in  purchases last year. That's 70x more than what was actually stolen via fraud.\
So yeah â€” it's like turning away every customer because  one is shady. And now, who pays the price?The Real (and Painful) Cost of DeclinesLetâ€™s talk about what actually happens when your customerâ€™s card gets declined.ðŸ’¸ Lost Revenue That Should've Been With the MerchantHereâ€™s the wild part: just a 1% bump in authorization rates can unlock  in extra revenue. No exaggeration. Every time a customer clicks â€œPay Nowâ€ and gets blocked for no good reason, thatâ€™s money walking out the digital door. And guess what? They donâ€™t always come back.ðŸ›’ Abandoned Carts = Broken TrustThe checkout flow is supposed to be the  part. But when payment fails?79% of users bail completely. donâ€™t just leave â€” they head straight to your competitor.40% wonâ€™t even try that same card again. Why? Because they think itâ€™s your fault.ðŸ” The Silent Killer: Involuntary ChurnYou know whatâ€™s sneakier than a visible decline? One that happens in the background.\
Think recurring subscriptions that quietly fail. The customer doesnâ€™t get a ping. No alert. Nothing. Just â€”  â€” their subscription vanishes, and youâ€™ve lost a loyal userâ€¦ without either of you even realizing.ðŸ˜¤ Declines Wreck UX and Brand TrustWhen a card fails, users donâ€™t blame their bank.\
They blameÂ  â€” the brand.\
They feel confused. Maybe even a little embarrassed. And that frustration? It lingers.â˜Žï¸ Say Hello to Support HellEvery unnecessary decline creates a ripple effect. Suddenly, your support team is flooded with calls that sound like this:â€œUh, why didnâ€™t my card work?â€\
And there goes your time, your resources, and your support budget. All on something that couldâ€™ve been avoided.It's Bigger Than Just "Payments"This isnâ€™t just about finance or technology.\
Declines are a full-blown  problem. A  problem. A  problem.\
Codes like  tell your customer absolutely nothing. Itâ€™s like showing a 404 error when someone tries to open a bank vault. Useless!Whatâ€™s Going On Under the Hood?Hereâ€™s where it gets messy: written for another era. â€” especially in LATAM and parts of Asia.Disconnected infrastructure that varies wildly across markets. (looking at you, SCA) that add friction without always adding value.Banks and merchants that donâ€™t talk to each other â€” or when they do, itâ€™s not pretty.\
These arenâ€™t customer or merchant mistakes. These are systemic cracks!!!What's Next? (Get Ready for Part 2)In the next part, weâ€™ll dive into how smart merchants are flipping the script. A quick preview:âœ… They track Â declines happen (donâ€™t fight an invisible enemy). \n âœ… They retry soft declinesÂ . \n âœ… They use tools likeÂ  and Â to stay one step ahead. \n âœ… They time retries â€” and yes,  retries really do work. \n âœ… They invest inÂ  to fix things .]]></content:encoded></item><item><title>Microsoft Cancels $1 Billion Ohio Data Center Projects</title><link>https://slashdot.org/story/25/04/09/009213/microsoft-cancels-1-billion-ohio-data-center-projects?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 9 Apr 2025 00:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft has scrapped plans to build three data center campuses in Licking County, Ohio, in a $1 billion investment pullback, the company said. The canceled developments in New Albany, Heath, and Hebron join a growing list of Microsoft data center project cancellations across the United States, Europe, Asia-Pacific and the United Kingdom. 

Microsoft will retain ownership of the land and plans to eventually develop the sites at an unspecified future date. Two properties will remain available for farming in the interim.]]></content:encoded></item><item><title>How to Use Color Psychology to Drive Engagement and Retention in UI Design</title><link>https://hackernoon.com/how-to-use-color-psychology-to-drive-engagement-and-retention-in-ui-design?source=rss</link><author>Mfonobong Umondia</author><category>tech</category><pubDate>Wed, 9 Apr 2025 00:00:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Color is essential in user interface (UI) design; it influences emotions, perception, engagement, and retention. As a designer, understanding the principles of color psychology can help you create interfaces that resonate with users, improve usability, build loyalty, encourage continued interaction, and enhance the overall experience. In this article, we will explore how to use color psychology effectively in UI design with practical examples.Understanding Color Psychology in UI DesignColor psychology studies how different colors affect human behavior, emotions, and decision-making. Different colors evoke different responses, so choosing the right palette is crucial based on your interface's goals. In User Interface Design, choosing the right colors for your product is as important as finding solutions to peopleâ€™s problems.Colors can increase engagement by making interfaces visually appealing. Check out Ling Cars and Tesla, for instance; which website would you feel more comfortable continuing to view?Colors improve usability by guiding attention and actions; e.g., cancel buttons, usually in red, serve as a warning system for users so they can think carefully before making a decision.Colors can enhance brand perception by reinforcing identity. At the top of my head, I can associate green and white with Starbucks, red and yellow with McDonaldâ€™s, red and white with Coca-Cola, etc. That's the power of color.Colors can boost conversions by strategically using contrast and CTAs. How you contrast your designs or the colors you use for your call-to-action buttons would require specific actions from your users, leading to increased engagement.Colors have a huge emotional impact on people, which is why designers and brands often select colors based on the type of products they are creating. For example, if you are designing games for children, you will likely want to opt for fun and playful colors. Letâ€™s take a look at some colors below and their psychological effect: Cleanliness, clarity, minimalism Tech, healthcare, modern interfaces Sophistication, authority, elegance Luxury brands, minimalist designs Luxury, creativity, mysticism Beauty, high-end products Eagerness, creativity Entertainment, food, and subscriptions Positiveness, warmth, caution Discounts, warnings (used sparingly) Growth, health and fitness, success Eco-friendly brands, finance (positive actions) Trust, calm, peaceful, professionalism Finance, healthcare, social media Energy, urgency, desire CTAs, alerts, sales Playfulness, warmth, love, femininity Dating apps, beauty brands Stability, earthiness, reliability, warmth Organic or natural brands, vintage and retro designs Neutrality, professionalism, sophistication (but can feel cold or dull if not balanced) Corporate websites, minimalist designs, secondary text/UI elements Luxury, creativity, spirituality, mystery, playfulness\
 Beauty and luxury brands, creative platforms, wellness apps\
This applies to the millions of colors we have in the world. Each one has its use cases and the psychological effect it has on humans.Applying Color Psychology in UI Design for EngagementHow users interact with your product and how effectively your product captures their attention are highly dependent on your choice of colors. There are other factors, too, like the pattern of design, elements used, user experience, and even microcopies, but it all begins with your choice of colors.\
Letâ€™s dig deeper into some ways you can leverage the use of color for engagement:Establishing Brand Identity and Emotional Connection: A company's logo and color palette should reflect its brand values and target audience. For example,A financial institution might use blue to convey trust and stability.A children's app might use bright, playful colors like yellow and orange.A health and wellness app could use shades of green to promote feelings of calm and well-being, reinforcing its focus on natural remedies and relaxation.Creating Effective Calls to Action (CTAs): CTA buttons should stand out and prompt users to take action. Using contrasting colors helps attract attention and guide user flow. Colors like red and orange are often used for CTAs to create a sense of urgency and encourage immediate action. For example,A "Buy Now" button in an e-commerce interface uses a vibrant orange color to draw the user's attention and drive sales.A subtle green might be used for a less aggressive, more positive CTA, such as "Start Free Trial."Improve Readability & Accessibility: Using high contrast between text and background will ensure readability and accessibility on your websites or apps.Using black text on a white background is ideal for readability.Avoid low-contrast combinations like light grey text on a white background. It will be almost unreadable.Avoid color overload. Using too many bright colors can overwhelm users.Highlighting Key Information: Color can be used to create visual hierarchy and guide users through the interface. Brighter, more saturated colors can be used to highlight important elements, while muted colors can be used for less important elements. For instance, colors are used to highlight important elements in design like notifications, messages, etc.Yellow color for warnings, e.g, Unsaved ChangesRed color for error messages, e.g, Invalid PasswordGreen for success messages, e.g, Payment SuccessfulEnhancing User Experience and Usability: Colors can be used to create a consistent and intuitive user experience for users. For example, using a specific color to indicate active elements or clickable links.Use blue for all clickable links within a text-heavy application.Green pop-up messages show that a process has been successful, and red shows a failure.Personalization and Customization: Allowing your users to customize the color scheme of the interface can enhance their sense of control and ownership, which will lead to increased engagement.A platform that allows users to choose a theme and color scheme for their profile will present them with an opportunity to try out other themes and see how they would work on the main platform, thereby boosting engagement.Driving Retention Through Color PsychologyRetention focuses on keeping users engaged over time by creating an intuitive and visually appealing experience, and color plays a very important role in this. Hereâ€™s how color can help: A consistent use of color schemes helps reinforce brand identity and make UI design recognizable. For example, Facebookâ€™s use of blue builds trust and recognition. Netflixâ€™s red accent color creates urgency and excitement.Emotional Connection with Users: Choosing colors that align with user emotions helps build a connection and improves retention. For example: Calm blues and greens for meditation apps Energetic pink and yellows for childrenâ€™s appsDark Mode for User Comfort: Dark mode reduces eye strain, making it popular for apps used at night or for extended periods. For example, Twitter, YouTube, Instagram, and Gmail provide dark mode options for better user experience.Creating a Sense of Comfort and Familiarity: The consistent use of colors throughout the interface of a product can create a sense of familiarity and comfort, encouraging users to keep returning. For example, a streaming service maintaining a consistent color scheme across all platforms (web, mobile, TV) will encourage users to use that product regardless of what platform they are on because of familiarity with the User Experience and Interface. They are already used to that design, interface, user experience, and colors. Wouldnâ€™t it be weird to log in to Netflix on your mobile app and see a different color and logo when the website has a different branding?Providing Visual Feedback and Reinforcement: Colors can be used to provide visual feedback and reinforcement, such as highlighting completed tasks or successful actions. This positive reinforcement encourages continued use. For example, A task management app highlights completed tasks in green, giving the user a visual sense of accomplishment and encouragement to do more.Ignoring cultural differences. Color meanings can vary across cultures. When selecting colors, it is important to consider the target audience's cultural background. For example, white symbolizes purity in the West but mourning in some Asian cultures. Black is primarily used for mourning in most African countries.Overusing bright colors leads to visual fatigue. As a user, I just want to be able to browse through a catalog and select a product that I want, add it to my cart, buy it, and go my way. Seeing each product card have different colors is just so tiring cause, at this point, I do not know what I am looking at again.Poor contrast, making text hard to read. White text has no business being on a bright yellow background. That will make it difficult for users to read.No Accessibility Consideration. Always ensure that color choices meet accessibility guidelines, particularly for users with visual impairments. Consider color contrast and provide alternative visual cues.Inconsistent color schemes confuse users. Let your product be known for one or two primary colors. Then, you can create complementary colors to support other use cases across the platform.As a User Interface Designer, by thoughtfully applying color psychology to your designs, you can create interfaces that are not only visually appealing but also emotionally engaging and effective in driving user engagement and retention.Color psychology is a subtle yet powerful way to enhance engagement and retention in UI design. Understanding color associations, effectively using contrast, and maintaining consistency are key factors in creating user-friendly, visually appealing, high-performing interfaces. By applying these principles, Designers can create interfaces that not only look appealing but also drive desired user actions.]]></content:encoded></item><item><title>Razer Pauses Direct Laptop Sales in the US as New Tariffs Loom</title><link>https://news.slashdot.org/story/25/04/08/2343230/razer-pauses-direct-laptop-sales-in-the-us-as-new-tariffs-loom?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 23:43:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Razer's upcoming Blade 16 and other laptops are no longer available for preorder or purchase on its US site. From a report: The configurator for preordering its new Blade 16 laptop was available as recently as April 1st, according to the Internet Archive -- one day before the Trump administration announced sweeping US tariffs on China, Taiwan, and others that make laptop components. 

When asked recently if tariffs might affect Razer's prices or availability, its Public Relations Manager, Andy Johnston, told The Verge, "We do not have a comment at this stage regarding tariffs." Razer may not be openly talking about the impact of tariffs, but Framework halted sales of its entry-level Laptop 13 in the US on April 7th, and Micron reportedly confirmed surcharges for its memory chips will apply once the tariffs take effect after midnight tonight.]]></content:encoded></item><item><title>Meet the HackerNoon Top Writers: Nebojsa Nesha Todorovic - Upwork, Freelancing, and Future of Work</title><link>https://hackernoon.com/meet-the-hackernoon-top-writers-nebojsa-nesha-todorovic-upwork-freelancing-and-future-of-work?source=rss</link><author>Nebojsa &quot;Nesha&quot; Todorovic</author><category>tech</category><pubDate>Tue, 8 Apr 2025 23:32:42 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I guess Iâ€™ve always been a Frank Sinatraâ€™s my-way-kind-of-a-man who has followed his instincts in both private and professional life. I obtained an academic degree in 2003 and, as one of the top 30 students in my country, I had a promise of a career as a lawyer or public prosecutor. However, after only a few years, I decided to leave it all behind and start a new career in sales from scratch. That was a decision that shocked my family, friends, and former colleagues. Yet, I followed my way persistently and with no hesitation. \
As a result, I advanced from a rookie salesman to a position of sales director for the joint markets of a handful of neighboring countries in the SEE (South-East-Europe) region. The intensity of the business decision-making process, frequent business trips, and constant pressure took their toll on me. I ended up with an alarming diagnosis of prediabetes and high blood pressure. That was the moment that I realized I had to make the most important decision in my life. I entered the world of freelancing in 2012, and I havenâ€™t regretted it ever since.How Did You Start Writing?At the very beginning, I treated freelancing as a temporary solution. My plan was to take a break from the unhealthy corporate lifestyle and then get back big time when I recharge my batteries. The truth was, though, that the more I worked as a freelance writer, the more I liked it. I kept postponing my big comeback in the 9-to-5 business world. My very first project was worth just $2! However, a few years later, I was making a thousand times more. That was the moment when I began to examine and accept freelancing as a phenomenon from a completely different perspective. Writing helped me to focus on my health and spend more time with my family.How Has Technology Impacted The Way You Write?In tech we trust, writing is a must. Thatâ€™s just me being me. I love writing about tech. As a GenXer, I feel like a time traveler. Iâ€™ve come a long way from analog to the digital world. I donâ€™t take things for granted. I resisted the AI temptations, and the only thing I changed was the introduction of Grammarly as my writing companion in 2016. The world of tech innovations has always been my source of inspiration for new writing ideas. The way things are now, and always developing, my writing guns are never going to run out of ammo.Share About Your Journey Highlights?I have to say that I feel privileged to be able to witness some of the most important tectonic changes in the freelance industry first-hand. Back then, we had a half dozen of serious business players in this field, such as Elance, oDesk, Freelancer, Guru, vWorker, among others. Then, Freelancer acquired vWorker, while almost at the same time, Elance and goDesk merged into a new platform - Upwork. \
The freelance landscape has been completely and irrevocably changed. The â€œNew Freelance World Orderâ€ was born with room for only two freelance superpowers: Freelancer and Upwork. The brave new freelance world hasn't become a happy and enjoyable place for freelancers. The major freelance platforms have completely transformed themselves into multimillion-dollar businesses. \
Unfortunately, while doing so, they have totally forgotten the basic principles of freelancing that drove so many people to this industry in the first place. Upwork lost touch with reality. I'm talking about the reality of an average Joe freelancer, who's doing his or her best to make it in the freelance world that has become extremely competitive and disappointingly merciless. \
Iâ€™ve been writing for more than ten years about the original founding freelance principles: freedom, fairness, and equality. We need to be reminded of these principles. We need to go back to the freelance basics that have been forgotten and trapped under the corporate greed that has nothing to do with the genuine concept of freelancing.Whatâ€™s Your Creative Process?Iâ€™ve never looked for a story. Stories have always had a way of finding me. Iâ€™m not writing but rather answering the questions that donâ€™t allow me to sleep and eat. I feel restless until I answer the calling of a story that just appears out of nowhere when and where I expect it the least. Research feels like walking, and writing feels like running. The finish line is HackerNoon. Itâ€™s an endless race when I say to myself: OK, Iâ€™m done with this story. Tomorrow is a new day and the time for a new story. I never choose when and what Iâ€™m going to write about. \
Someone or something else does it for me. I carry the stories with and inside me. I write introductions and conclusions in my head before I even turn on my laptop. I mastered the skills of blind typing. When you type almost one word per second, youâ€™re surprised when you look back at what you wrote. Itâ€™s almost like playing a piano. Very often, Iâ€™m not even aware of what I just typed. Itâ€™s a creative chaos full of unexpected twists and ideas out of this world at the last moment. And yes, I love it.Your Favorite Memory/Article(s) to This Day?HackerNoon has always supported my writing regardless of controversies as long as my claims and findings were thoroughly researched and supported by rock-solid publicly available sources and facts. The most rewarding research and writing journey began with the introduction of "connects" during the Upwork Inc (UPWK) Q1 2019 Earnings Call in May 2019. \
Here's a link to my  that can save you time going through the transcript of this Earnings Call. You will see two crucial quotes from that acting CEO. One initial article led to the entire series of stories about fake accounts, reviews, and jobs on Upwork. We can never know for sure what the exact percentage or number of these fake â€œelementsâ€ that are the core substance of every freelance platform is. \
If youâ€™re asking me, for a freelance platform that claims it is â€œthe largest global freelancing website,â€ even one fake account, review, or job is too many. I wrote and provided evidence for each claim about fake Upwork , , and jobs.  are the worst and the most problematic component of this Upwork "fake trilogy" because they provided the safe harbor for scammers that exploded in numbers hand-in-hand with the connects system. Todorovic, a Serbian writer who has blogged about many ofÂ Â over the years, elaborated on his case when I reached out to him. To him, those Connects are the lynchpin. In a follow-up post to the "fake job farm" one, Todorovic shares a screenshot where an Upwork community mod acknowledges the uptick in job post anomalies freelancers like Todorovic had noticed.How Did You Hear About HackerNoon? Share With Us About Your Experience With HackerNoon.What Have You Learnt From Your Journey?HackerNoon is my home. My portfolio. My pride. I remember the days when other publications didnâ€™t even want to take my controversial stories into consideration. No story of mine was too controversial to be published on HackerNoon. The option to combine memes, pictures, video content, code, and AI-generated visual elements is unique and unparalleled in the publishing industry. HackerNoon is not my first or last writing stop, but it is the only and exclusive one. If you like what and how I write, look me up on HackerNoon.On October 14, 2018, I published my first story on HackerNoon. That day has become my â€œsecond Bday,â€ if you get my meaning. Iâ€™m sure you do. I published a special story: My Six Years of Writing on HackerNoon in Numbers and Pictures. When Iâ€™m feeling down, I go through my stories and HackerNoon memories. I feel better. Nietzsche said, â€œWithout music, life would be a mistake.â€ I say the same about HackerNoon. Tech stories are my music, and a keyboard is my piano.]]></content:encoded></item><item><title>SAFE: A New AI Tool for Fact-Checking Long-Form Responses</title><link>https://hackernoon.com/safe-a-new-ai-tool-for-fact-checking-long-form-responses?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Tue, 8 Apr 2025 22:54:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3 SAFE: LLM AGENTS AS FACTUALITY AUTORATERSA key obstacle in benchmarking long-form factuality is that a benchmark requires a reliable method of evaluating model responses. Existing automated evaluation methods such as BLEURT (Sellam et al., 2020), ROUGE (Lin, 2004; Ganesan, 2018), and prompting language models (Min et al., 2023; Vu et al., 2023) operate by comparing a model response to a preset reference answer or knowledge source. This works well for prompts that only require a short-answer response, as there is a definite and singular correct answer. However, this setting is suboptimal for prompts that require a long-form response, as it is difficult to compile predetermined answers/knowledge that comprehensively and nonredundantly cover all facts that a model response may contain.\
Instead, we propose that there are two key insights that are crucial to accurately evaluate factuality in a long-form setting. First, we agree with the idea that long-form responses should be evaluated at the granularity of individual facts, following Min et al. (2023). This is more fine-grained than the granularity of sentences, as a single sentence may contain multiple individual facts (see Figure 1 for an example), and it is essential for optimal performance since isolating each individual fact allows for precise and focused evaluation. Second, in lieu of predetermined reference answers or knowledge sources, Google Search can be leveraged to obtain dynamic reference answers for each fact in a response with the best tradeoff among accuracy, accessibility, and timeliness (Augenstein et al., 2023). This ensures that evaluation is done with respect to reference answers that specifically cover the particular set of facts that a long-form response can contain.\
For these reasons, we propose Search-Augmented Factuality Evaluator (SAFE),[4] which joins these insights by using a language model to (a) split a long-form response into individual self-contained facts, (b) determine whether each individual fact is relevant to answering the prompt in the context of the response, and (c) for each relevant fact, iteratively issue Google Search queries in a multi-step process and reason about whether the search results support or do not support the fact. We consider the key innovation in SAFE to be using a language model as an agent to generate multi-step Google Search queries and carefully reason about whether facts are supported by search results (Figure 3 and Appendix C.2 shows examples of these reasoning chains).\
As shown in Figure 1, to split a long-form response into individual self-contained facts, we first prompt the language model to split each sentence in a long-form response into individual facts. We then revise each individual fact to be self-contained by instructing the model to replace vague references (e.g., pronouns) with the proper entities that they are referring to in the context of the response. To rate each self-contained individual fact, we use the language model to reason about whether the fact is relevant to answering the prompt in the context of the response.[5] Each remaining relevant fact is then rated as â€œsupportedâ€ or â€œnot supportedâ€ using a multi-step approach. In each step, the model generates a search query based on the fact to rate and the search results that have previously been obtained. After a set number of steps, the model performs reasoning to determine whether the fact is supported by the search results, as shown in Figure 3. Once all facts are rated, the outputted metrics from SAFE for a given promptâ€“response pair are the number of â€œsupportedâ€ facts, the number of â€œirrelevantâ€ facts, and the number of â€œnot-supportedâ€ facts. Our detailed implementation of SAFE is shown in Appendix C.1, and SAFE is publicly available at https://github.com/ google-deepmind/long-form-factuality/tree/main/eval/safe.[4] SAFE is similar to and inspired by prior work such as Gao et al. (2023) and Wang et al. (2023).\
[5] This identifies statements that should not be rated (e.g., â€œI donâ€™t know the answer to thatâ€).(1) Jerry Wei, Google DeepMind and a Lead contributors;(2) Chengrun Yang, Google DeepMind and a Lead contributors;(3) Xinying Song, Google DeepMind and a Lead contributors;(4) Yifeng Lu, Google DeepMind and a Lead contributors;(5) Nathan Hu, Google DeepMind and Stanford University;(6) Jie Huang, Google DeepMind and University of Illinois at Urbana-Champaign;(7) Dustin Tran, Google DeepMind;(8) Daiyi Peng, Google DeepMind;(9) Ruibo Liu, Google DeepMind;(10) Da Huang, Google DeepMind;(11) Cosmo Du, Google DeepMind;(12) Quoc V. Le, Google DeepMind.]]></content:encoded></item><item><title>Why LLMs Are More Accurate and Cost-Effective Than Human Fact-Checkers</title><link>https://hackernoon.com/why-llms-are-more-accurate-and-cost-effective-than-human-fact-checkers?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Tue, 8 Apr 2025 22:54:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[4 LLM AGENTS CAN BE BETTER FACTUALITY ANNOTATORS THAN HUMANSLLMs have achieved superhuman performance on reasoning benchmarks (Bubeck et al., 2023; Gemini Team, 2023; Li et al., 2022) and higher factuality than humans on summarization tasks (Pu et al., 2023). The use of human annotators, however, is still prevalent in language-model research, which often creates a bottleneck due to human annotatorsâ€™ high cost and variance in both evaluation speed and quality. Because of this mismatch, we investigate how SAFE compares to human annotations and whether it can replace human raters in evaluating long-form factuality\
To quantitatively evaluate the quality of annotations obtained using SAFE, we leverage crowdsourced human annotations released from Min et al. (2023). These data contain 496 promptâ€“response pairs where responses have been manually split into individual facts (for a total of 16,011 individual facts), and each individual fact has been manually labeled as either supported, irrelevant, or not supported. For each individual fact, we obtain an annotation from SAFE by running the SAFE pipeline, starting from revising the fact to be self-contained (i.e., steps 2â€“4 in Figure 1).[6] We then compare SAFE annotations with human annotations at the individual-fact level.\
First, we directly compare each individual factâ€™s SAFE annotation and human annotation, finding that SAFE agrees with humans on 72.0% of individual facts (see Figure 4). This indicates that SAFE achieves human-level performance on a majority of individual facts. We then examine a randomly-sampled subset of 100 individual facts for which the annotation from SAFE disagreed with the annotation from human raters. We manually re-annotate each of these facts (allowing access to Google Search rather than only Wikipedia for more-comprehensive annotations), and we use these labels as the ground-truth. We found that on these disagreement cases, SAFE annotations were correct 76% of the time, whereas human annotations were correct only 19% (see Figure 5), which represents a 4 to 1 win ratio for SAFE. Examples of correct SAFE ratings and incorrect SAFE ratings are shown in Appendix C.2 and Appendix C.3, respectively. Appendix A.3 analyzes failure causes for SAFE, and Appendix A.4 analyzes presumed failure causes for human annotators.\
To rate all individual facts from the 496 promptâ€“response pairs, SAFE issued GPT-3.5-Turbo API calls costing $64.57 and Serper API calls costing $31.74, thus costing a total of $96.31, equivalent to $0.19 per model response. For comparison, Min et al. (2023) reported a cost of $4 per model response to obtain crowdsourced human annotations for the dataset. This means that in addition to outperforming human raters, SAFE is also more than 20Ã— cheaper than crowdsourced human annotators. Overall, the superior performance of SAFE demonstrates that language models can be used as scalable autoraters that have the potential to outperform crowdsourced human annotators.5 F1@K: EXTENDING F1 WITH RECALL FROM HUMAN-PREFERRED LENGTH\
\
Finally, we combine factual precision and recall using standard F1, as shown as Equation (1). This proposed metric is bounded between [0, 1], where a higher number indicates a more-factual response. For a response to receive a full score of 1, it must not contain not-supported facts and must have at least K supported facts. F1@K also follows expectations for many clear hypothetical cases, as shown in Appendix D.2, and we contend that it allows for standardized and quantifiable comparisons between language models and between methods of obtaining responses.\
\
Equation (1): F1@K measures the long-form factuality of a model response y given the number of supported facts S(y) and the number of not-supported facts N(y) that are in y. The hyperparameter K indicates the number of supported facts required for a response to achieve full recall. We discuss how K should be selected in Appendix D.3.[6] We show performance of step 1 of SAFE (splitting a response into individual facts) in Appendix C.4.(1) Jerry Wei, Google DeepMind and a Lead contributors;(2) Chengrun Yang, Google DeepMind and a Lead contributors;(3) Xinying Song, Google DeepMind and a Lead contributors;(4) Yifeng Lu, Google DeepMind and a Lead contributors;(5) Nathan Hu, Google DeepMind and Stanford University;(6) Jie Huang, Google DeepMind and University of Illinois at Urbana-Champaign;(7) Dustin Tran, Google DeepMind;(8) Daiyi Peng, Google DeepMind;(9) Ruibo Liu, Google DeepMind;(10) Da Huang, Google DeepMind;(11) Cosmo Du, Google DeepMind;(12) Quoc V. Le, Google DeepMind.]]></content:encoded></item><item><title>How LongFact Helps AI Models Improve Their Accuracy Across Multiple Topics</title><link>https://hackernoon.com/how-longfact-helps-ai-models-improve-their-accuracy-across-multiple-topics?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Tue, 8 Apr 2025 22:52:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[While there are many existing prompt sets for factuality, there are no comprehensive prompt sets for examining factuality in long-form responses on the scale of several paragraphs in length (as shown in the right side of Figure 2). For example, many established benchmarks such as TruthfulQA (Lin et al., 2022), HaluEval (Li et al., 2023), FreshQA (Vu et al., 2023), HalluQA (Cheng et al., 2023b), and FELM (Chen et al., 2023) mostly consist of prompts that test knowledge of a single factoid (e.g., â€œHow old is the worldâ€™s oldest verified living person?â€) and thus only require a short response rather than several paragraphs to be fully answered. Other factuality datasets such as FActScore (Min et al., 2023) may require long-form responses but do not cover a broad range of topics.\
For this reason, we present LongFactâ€”a prompt set designed to probe a modelâ€™s factuality when generating long-form responses that may span multiple paragraphs. We generate LongFact by prompting GPT-4 to generate questions that ask about a specific concept or object within a given topic (e.g., â€œbiologyâ€) and that require a long-form response containing multiple detailed factoids. LongFact consists of two tasks, LongFact-Concepts and LongFact-Objects, separated based on whether the questions ask about concepts or objects. For both tasks, we use the same set of 38 manually-selected topics listed in Appendix B.2 (the left side of Figure 2 shows the breakdown of topics with respect to four supercategories). We generate 30 unique prompts per topic for a total of 1,140 prompts per task. Additional details about our data-generation process can be found in Appendix B.1, and canary text for LongFact is shown in Appendix A.11. LongFact is publicly available at https://github. com/google-deepmind/long-form-factuality/tree/main/longfact.(1) Jerry Wei, Google DeepMind and a Lead contributors;(2) Chengrun Yang, Google DeepMind and a Lead contributors;(3) Xinying Song, Google DeepMind and a Lead contributors;(4) Yifeng Lu, Google DeepMind and a Lead contributors;(5) Nathan Hu, Google DeepMind and Stanford University;(6) Jie Huang, Google DeepMind and University of Illinois at Urbana-Champaign;(7) Dustin Tran, Google DeepMind;(8) Daiyi Peng, Google DeepMind;(9) Ruibo Liu, Google DeepMind;(10) Da Huang, Google DeepMind;(11) Cosmo Du, Google DeepMind;(12) Quoc V. Le, Google DeepMind.]]></content:encoded></item><item><title>The AI Truth Test: New Study Tests the Accuracy of 13 Major AI Models</title><link>https://hackernoon.com/the-ai-truth-test-new-study-tests-the-accuracy-of-13-major-ai-models?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Tue, 8 Apr 2025 22:52:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Jerry Wei, Google DeepMind and a Lead contributors;(2) Chengrun Yang, Google DeepMind and a Lead contributors;(3) Xinying Song, Google DeepMind and a Lead contributors;(4) Yifeng Lu, Google DeepMind and a Lead contributors;(5) Nathan Hu, Google DeepMind and Stanford University;(6) Jie Huang, Google DeepMind and University of Illinois at Urbana-Champaign;(7) Dustin Tran, Google DeepMind;(8) Daiyi Peng, Google DeepMind;(9) Ruibo Liu, Google DeepMind;(10) Da Huang, Google DeepMind;(11) Cosmo Du, Google DeepMind;(12) Quoc V. Le, Google DeepMind.Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a modelâ€™s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for longform factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a userâ€™s preferred response length (recall).\
Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotatorsâ€”on a set of âˆ¼16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github. com/google-deepmind/long-form-factuality.Large Language Models (LLMs) have significantly improved in recent years (Brown et al., 2020; Chowdhery et al., 2022; Google, 2023; OpenAI, 2023; Gemini Team, 2023, inter alia) but still lack reliability in responding to in-depth factuality questions. In particular, they often produce factual errors in which a claim contradicts established ground-truth knowledge (Huang et al., 2023; Zhang et al., 2023, inter-alia).[1] For example, models may respond with incorrect information about established facts such as dates, statistics, or even a celebrityâ€™s occupation (Li et al., 2023; Min et al., 2023; Muhlgay et al., 2023). These factual errors weaken a language modelâ€™s factuality, making the model unreliable in many real-world settings where a factually-accurate response is expected.\
In this paper, we propose a new prompt set called LongFact, an evaluation method named SAFE, and a metric (F1@K) for quantifying the long-form factuality of a long-form response. We then extensively benchmark popular large language models using these new datasets and evaluation methods. Our contributions are as follows:\
â€¢ We use GPT-4[2] to generate a new prompt set for benchmarking long-form factuality in large language models, which we call LongFact (Section 2). LongFact consists of 2,280 fact-seeking prompts that solicit long-form responses across 38 manually-selected topics. To our knowledge, LongFact is the first prompt set for evaluating long-form factuality in a wide variety of domains. We make LongFact publicly available at https://github.com/google-deepmind/ long-form-factuality/tree/main/longfact.\
â€¢ We propose a method of utilizing an LLM agent to automatically evaluate long-form factuality in model responses. We use the language model to first decompose a long-form response into individual facts, then for each fact, propose fact-checking queries to send to a Google Search API and reason about whether the fact is supported by search results (Section 3). We name this method SAFE (Search-Augmented Factuality Evaluator).[3] Empirically, SAFE outperforms crowdsourced human annotators, agreeing with 72% of human annotations from Min et al. (2023) and winning 76% of disagreement cases from a random sample of 100 disagreement cases (Section 4). SAFE is also 20Ã— cheaper than human annotators. We release SAFE at https://github.com/ google-deepmind/long-form-factuality/tree/main/eval/safe.\
â€¢ We propose that when quantifying the long-form factuality of a model response, F1 can be utilized via a hyperparameter that estimates the human-preferred â€œidealâ€ number of facts in a response. We therefore introduce F1@K, which (in addition to measuring a responseâ€™s factual precision as the ratio of supported facts) measures recall as the ratio of provided supported facts over a variable desired number of supported facts K.\
â€¢ We conduct an extensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on LongFact (Section 6). We evaluate model responses using SAFE and quantify performance using F1@K, finding that, in general, larger language models achieve better long-form factuality.[1] We focus on factuality and factual errors, not hallucination, as our proposed evaluation method focuses on determining whether a response is factual with respect to external established knowledge (factuality) rather than whether the response is consistent with the modelâ€™s internal knowledge (hallucination).\
[2] We use gpt-4-0613 for GPT-4.\
[3] In our implementation of SAFE, we use gpt-3.5-turbo-0125 as the language model and Serper (available at https://serper.dev/) as the Google Search API.]]></content:encoded></item><item><title>A 25-year-old police drone founder just raised $75M led by Index</title><link>https://techcrunch.com/2025/04/08/a-25-year-old-police-drone-founder-just-raised-75m-led-by-index/</link><author>Charles Rollet</author><category>tech</category><pubDate>Tue, 8 Apr 2025 22:44:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[If you ever call 911 from an area thatâ€™s hard to get to, you might hear the buzz of a drone well before a police cruiser pulls up. And thereâ€™s a good chance that it will be one made by Brinc Drones, a Seattle-based startup founded by 25-year-old Blake Resnick, who dropped out of college [â€¦]]]></content:encoded></item><item><title>Mastering Project Clarity: How The Power of the RACI Matrix Can Improve Your Teamwork</title><link>https://hackernoon.com/mastering-project-clarity-how-the-power-of-the-raci-matrix-can-improve-your-teamwork?source=rss</link><author>luminousmen</author><category>tech</category><pubDate>Tue, 8 Apr 2025 22:42:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Let me start with a confession.\
In one of my previous projects, I used to think project chaos was just part of the job. You know the vibe â€” people stepping on each other's toes, endless Slack threads trying to figure out who was supposed to do what, decisions made in meetings that no one remembered a week later. At some point, you just accept it as the cost of doing cross-functional work, right?\
Wrong. That's just lazy leadership.\
The more projects I led â€” across teams big and small, from product rewrites to data platform migrations â€” the more obvious it became: projects don't fall apart because of bad tech. They fall apart because of bad clarity.\
And I don't mean "bad Jira grooming" or "a lack of alignment on deliverables" (although, yeah, those suck too). I mean fundamental confusion about who owns what. Who makes the call? Who's doing the actual work? Who gets a say â€” and who just needs to be updated once it's done?\
If you've ever ended a week thinking, "Wait, I thought you were doing that", you've already felt the consequences of ignoring this. \
This brings me to the most boringly named, wildly underrated tool I know: the RACI Matrix.What the Heck is a RACI Matrix?Technically, RACI stands for , , , . You've probably seen that somewhere before â€” maybe in a boring slide deck, maybe in a textbook you ignored.It's like a contract that the project team makes with itself to ensure that every task has its rightful owner and everyone knows their part in the grand scheme of things.\
But here's what it means in .\
Imagine your project is a band.The  folks are the ones actually playing the instruments. They're the ones up late, fixing bugs, writing queries, designing mockups â€” whatever it takes. Without them, there's no music.The  one is the producer. They're not playing every instrument, but they're making sure the record actually ships. They own the outcome. They take the blame if it sucks â€” or the credit if it slaps.The  are like the studio musicians or trusted friends who give you honest feedback during the recording. You want their input. You value their ears. But they're not in the band.And the ? That's your audience, your label, maybe your investors. They need to know what's going on, but they're not getting involved in the creative process.\
And yes, one person or group can take on multiple roles within the project.\
The RACI matrix is typically represented as a table with tasks or outcomes listed in the left column and individual people or groups involved in the process listed on top. The intersection of each task and person or group indicates their corresponding RACI role.But Isn't This Just More Process?Yeah, I've heard this one before. Usually from senior ICs or managers who are allergic to anything that looks like structure. "Do we really need a matrix? Can't we just talk it through?"\
Sure. In a perfect world. On a team of three. For a one-week sprint.\
But real projects are messy. Real teams are overloaded. And real work happens asynchronously, across time zones, with shifting priorities and surprise stakeholder escalations. "Just talk it through" doesn't cut it when Bob is OOO, Sarah's on three projects, and Marketing is asking for a timeline update you didn't even know existed.\
The beauty of RACI is that it makes who does what explicit â€” before shit hits the fan. Not through another tool. Not through process theater. Just through one honest conversation about roles.\
And yeah, sometimes that conversation is uncomfortable. It surfaces power dynamics. It forces decisions. It makes people ask, "Wait, why am I even on this project?"\
Because nothing kills a project faster than pretending everyone's equally involved when they're not. RACI doesn't create complexity â€” it exposes the complexity that's already there.One of the underrated powers of RACI is that it gives people permission not to care about things that aren't their job.\
And I mean that in the best possible way.\
We're so used to over-collaboration â€” to inviting everyone into every decision â€” that we forget: clarity isn't exclusion. It's respect.Telling someone "you're Informed, not Consulted" isn't shutting them out. It's protecting their time. It's like saying, "We'll let you know when there's something worth reacting to â€” but you don't need to be in this meeting."Making a RACI Matrix That Doesn't SuckSo, how do you actually build one that doesn't feel like a waste of everyone's time?\
Start by dumping out all the stuff your team is supposed to do. Not just the big-ticket items like "launch feature X", but the actual messy middle: writing specs, reviewing designs, testing, RFCs, whatever.\
Then, write down the name of every person who touches the project â€” even if it's just once. Yes, that one stakeholder who only shows up at the end with Opinions counts.\
Now comes the good part: assigning who's doing what. Not in vague terms like "we'll figure it out" or "team effort." You're putting names in boxes. is the person doing the work. Keyboard. Fingers. Doing the thing. is the one on the hook if it doesn't happen. Usually, the person who gets yelled at. is the person who needs to weigh in before anything goes out. Think feedback, not sign-off. is the person who just needs a heads-up. No input is required.\
The conversations you'll have while building the RACI are more important than the actual matrix itself.\
Once it's done? Share it. Talk it through. Make sure everyone's actually okay with their roles â€” not just nodding because they want the meeting to end. Ensure one person isn't overloaded with "Responsible" tasks. Distribute the workload evenly to avoid bottlenecks. Evaluate if each stakeholder needs to be involved in so many activities. Reduce the number of decision-makers and accountability holders if possible. Assess the necessity of the role and consider redistributing responsibilities. Stick to one "Accountable" role per stage.Horizontal Check (Project Stages): Assign a "Responsible" stakeholder for each stage. Divide complex stages into specific tasks to distribute responsibilities. Ensure there's accountability for each stage. Stick to one "Accountable" stakeholder per stage. Verify that all stages have stakeholder involvement. Ensure all relevant stakeholders are consulted and informed. Limit excessive consultations; consider keeping stakeholders informed instead. Evaluate if the information shared is truly beneficial or creates unnecessary meetings. Avoid having one person take both the R and A roles, as it may lead to misunderstandings.\
Make sure all stakeholders agree to their assigned roles and document it on the project page.Of course, RACI isn't perfect. Like any tool, it can be misused.\
You'll see teams where everyone gets labeled "Consulted" just to keep them happy â€” which defeats the point. Or where people game the system to avoid being "Accountable" â€” because they don't want the heat.\
Or worse, people fill out a RACI matrix once, paste it in the project doc, and never look at it again. That's not RACI. That's checkbox theater.The matrix is only as good as your willingness to have the hard conversations behind it. And revisit them when things change.\
Because projects evolve. People rotate out. Priorities shift. If your RACI doesn't shift with them, congratulations â€” you've got a beautiful artifact that doesn't reflect reality.\
So yeah, it takes work. But it's the kind of work that pays off before the deadline, not after.]]></content:encoded></item><item><title>What If All Global Trade Was In Crypto?</title><link>https://hackernoon.com/what-if-all-global-trade-was-in-crypto?source=rss</link><author>Yaroslav Kalynychenko</author><category>tech</category><pubDate>Tue, 8 Apr 2025 22:12:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[No tariffs, no taxes, no fees\
Imagine a world where buying an iPhone doesn't require deciphering a complex web of tariffs, taxes, and fees. Picture a scenario where international trade flows as smoothly as your favorite streaming service â€” no buffering, no interruptions. Sounds like a utopian dream? Well, with the advent of cryptocurrencies, this dream might not be as far-fetched as it seems. And , which will make international trade even more difficult and expensive, are a great reason to dream.\
Let's dive into how recent tariff policies are impacting global prices and explore the tantalizing possibility of a crypto-driven trade.Trump's Tariffs and How They Will Affect Global PricesIn a move that sent shockwaves through the global economy, President Donald Trump announced on April 2, 2025, a sweeping set of tariffs aimed at reshaping U.S. trade relationships. Dubbed "Liberation Day," this policy imposes a baseline  tariff on all imports, with significantly higher rates targeting specific countries. For instance, imports from China face a  tariff, while goods from the European Union are taxed at .\
To put this into perspective, let's consider the iPhone â€” a quintessential global product. Before these tariffs, the production and sale of an iPhone involved multiple countries, each contributing components and labor. The imposition of these tariffs means that the cost of importing these components and the final product into the U.S. has increased substantially. Consequently, consumers can expect a noticeable hike in retail prices, making gadgets and other imported goods heavier on the wallet.iPhone Price: With and Without Tariffs\
 0 â†’ $485Ã—0.34 = $164.90\
Adjusted production cost: $485 + $164.90 = $649.90\
 $799/ $485 â‰ˆ 1.65\
New buyer price (before VAT): $649.90Ã—1.65 â‰ˆ $1072Original VAT: $799Ã—0.20 â‰ˆ $159.80New VAT: $1072Ã—0.20 â‰ˆ $214.40Final buyer price (with VAT):Original: $799 + $159.80 â‰ˆ $958.80New: $1072 + $214.40 â‰ˆ $1286.40==Below is a summary table:==|  |  |  |  |
|----|----|----|----|
|  | $485 | $485 | $0 |
|  | $0 | $164.90 | +$164.90 |
|  | $799 | $1072 | +$273 |
|  | $159.80 | $214.40 | +$54.60 |
| Final Buyer Price (with VAT) | $958.80 | $1286.40 | +$327.60 |\
These figures are approximate and assume that Apple maintains its profit margin.\
And the new tariffs are far from the only regulatory markup that affects the price. There are also old tariffs, commissions, duties, and taxes. Therefore, the real price of an iPhone without all this could be up to 30-50% less.\
While the iPhone serves as a prominent example, it's essential to recognize that these tariffs affect a broad range of products, potentially leading to widespread price increases across various sectors.â€‹So, what happens if we ditch fiat and move all international trade to Bitcoin? No tariffs. No extra fees. Free and secure international tradeâ€¦ The iPhone, and pretty much everything else with a global supply chain, would cost hundreds less. As a business founder, you would be able to sell more by making your products more affordable by not adding all these extra costs.\
Take our example â€” Trumpâ€™s 34% import tariff bumps the iPhone price from $958 to over $1,280. Thatâ€™s $325 straight out of your pocket and not a single extra megapixel in return.\
In a Bitcoin-based system, the whole chain simplifies. You're not paying to cross borders. You're not financing geopolitical slap fights. You're just paying for what you buy â€” with instant, secure, peer-to-peer transactions that donâ€™t ask permission and donâ€™t get delayed by bureaucracy.\
If trade were settled in crypto, the only thing slowing things down would be how fast you can copy-paste your wallet address. What do you think? Is that possible?]]></content:encoded></item><item><title>Code Red: Why China Is Well Positioned to Win the AI Race</title><link>https://hackernoon.com/code-red-why-china-is-well-positioned-to-win-the-ai-race?source=rss</link><author>Thomas Cherickal</author><category>tech</category><pubDate>Tue, 8 Apr 2025 21:49:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Image Credit: NightCafe Studio: Link:::tip
The bottleneck is real: the problem is the lack of data!GPT 3.5 to GPT-4-Turbo was a quantum leap.\
GPT-4.5: hugely enormous computing resources (as evidenced by the massive prices) but not a serious entry in the competition.\
Incremental improvements.:::warning
Get ready to see why Big Tech may have shot itself in the leg with GPU compute and Trumped itself with tariffs!:::tip
And why China is not just the winner - they are racing ahead on all fronts.The Model Is Only As Good As the Data You Give It!It was never about the compute - beyond a certain point.\
Once the entire Internet - copyright, non-copyright, intellectual property, free use, had been consumed:\
There was nothing more for the models to learn from.\
Synthetic data is nowhere near as good as the human-generated data of the Internet.\
Yes, GPU compute is necessary.\
But algorithms Trump compute. (pun intended)\
\
DeepSeek, which is competitive with GPT 4.5 on nearly every benchmark (the exception being emotional intelligence) is \
\
Yes, perhaps GPT 4.5 is a precursor for GPT-5 -\
But that still does not justify the astronomical price tag.:::info
Thatâ€™s not pricing: thatâ€™s a sinkhole for the money in your wallet, especially in third-world countries.\
Yes, DeepSeek has been overloaded by requests in the past, but less so now.\
And with the free price?\
Iâ€™m willing to wait for cutting-edge performance!\
With its pricing, GPT 4.5 should be heads and shoulders above the rest.\
Instead, an MIT-licensed model (open-source and free for commercial use) is less than 30 points below it on LMSYS Arena.\
And why are Googleâ€™s LLMs leading the pack?:::tip
The large amount of data Google has within its worldwide juggernaut enterprise (7 models in the top 20, more than any other company) is the main reason for its dominance!:::info
China has done what OpenAI set out to do.\
Create LLMs moving towards AGI and make it free and open for the entire world.\
China was neck-and-neck with the US as far as research is concerned - .:::info
China is poised to take the pole position in the AI and LLM world domination.\
Iâ€™m eagerly waiting for DeepSeekâ€™s R2 (as is the rest of the world, some more in fear).:::tip
China has been blocked access to expensive GPU chips, but that may have been the best thing to happen to the Chinese AI effort!\
Forced to innovate, China came up with some fantastic algorithmic and efficiency innovations.\
That resulted in the exact opposite of what the ban was supposed to accomplish.:::tip
China became the world leader in AI efficiency and economy for LLMs with DeepSeek R1.:::info
Compute matters only as long as high-quality data is available and for meeting the high demand for existing LLM products.\
For creating genuine AGI, nearly all AI experts concur that better algorithms are the key.\
 is perhaps the most promising candidate in this area.\
However, the most recent studies elaborate on the need for caution in AGI.\
 recently released a projection into the future for the emergence of AGI, available at the link below::::tip
They actually project AGI with autonomous agents arriving by 2026-2027!\
(I realize going against Naval Ravikant may be problematic, the guy is a freakinâ€™ legend.)\
He is right about more compute not producing AGI.\
I agree with him there.\
But wrong about the emergence of ASI and AGI.\
AGI will self-evolve from existing autonomous AI Agents.:::tip
Human beings will not build AGI.\
Similarly, humans will not create ASI.\
An AI that has reached AGI will.\
If ASI and AGI were left to humans, then maybe 100 years is a realistic expectation.\
With all due respect to Naval Ravikant (again, the guy is a huge, huge legend! Legend!)::::tip
AI will build itself and evolve into AGI and ASI in ways that human beings will not even understand, given enough foundation and autonomy!\
And now, with the recent actions of the Supreme POTUS:\
China is well-positioned to be the country to reach AGI (and thus ASI) first.\
That is my certain prediction.\
The US Big Tech companies were sold on the idea of more AI chips, more intelligence by a combination of interwoven advertising and hype.\
Now, they are paying the price.\
We all know who is responsible for all this, so I will avoid naming names.\
But algorithms are the key to evolving AGI.:::info
Sadly, if things continue in this direction, China is set to become the worldâ€™s dominant superpower.\
The reason I say that is that AI will enable breakthroughs in multiple sectors of technology that we have never seen before.\
Breakthroughs that were not even imaginable by human beings.:::tip
The AI race is the Manhattan Project of the 21st century.:::warning
And in this current scenario - China is the certain winner.While I do not monetize my writing directly, your support helps me continue creating articles like this one without a paywall or a paid subscription. \n  \n If you want articles like this one appearing on your business and your keywords for your platforms online, you can get them (NOT on HackerNoon, but yes on other platforms)! \n  \n Contact me at: \n  \n  \n :::info
No AI was used in this article except for the images.:::info
All links are in the article itself, so there is no list of references this time around.]]></content:encoded></item><item><title>Instagram is beefing up its search to compete with TikTok</title><link>https://techcrunch.com/2025/04/08/instagram-is-beefing-up-its-search-to-compete-with-tiktok/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 8 Apr 2025 21:34:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Instagram head Adam Mosseri says the company is looking to improve the appâ€™s search functionality, admitting this is an area where Instagram could do more to compete. The remarks, made on a recent episode of the â€œBuild Your Tribeâ€ podcast, come at a time when younger Gen Z users often turn to social apps like [â€¦]]]></content:encoded></item><item><title>How to Deploy a Scala Play Application to Heroku</title><link>https://hackernoon.com/how-to-deploy-a-scala-play-application-to-heroku?source=rss</link><author>Alvin Lee</author><category>tech</category><pubDate>Tue, 8 Apr 2025 21:12:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Iâ€™ve been a web developer for years, but I havenâ€™t touched Java in a long timeâ€Šâ€”â€Šlike, late-90s long. Back then, Java development felt cumbersome: lots of boilerplate and complex configurations. It was not exactly a pleasant experience for building simple web apps. So, when I recently started exploring Scala and the Play Framework, I was curious more than anything. Has the Java developer experience gotten better? Is it actually something Iâ€™d want to use today?\
Scala runs on the Java Virtual Machine, but it brings a more expressive, modern syntax to the table. Itâ€™s often used in backend development, especially when performance and scalability matter. Play is a web framework built for Scala, designed to be fast, reactive, and developer-friendly. And you use sbt, which is Scalaâ€™s build toolâ€Šâ€”â€Šroughly comparable to Maven or Gradle in the Java world.\
In this post, Iâ€™ll walk through setting up a basic Scala Play app, running it locally, and then deploying it to Heroku. My hope is to show you how to get your app running smoothlyâ€Šâ€”â€Šwithout needing to know much about the JVM or how Play works under the hood.Introducing the ExampleÂ AppTo keep things simple, Iâ€™m starting with an existing sample project: the Play Scala REST API example. Itâ€™s a small application that exposes a few endpoints for creating and retrieving blog posts. All the data is stored in memory, so thereâ€™s no database to configureâ€Šâ€”â€Šperfect for testing and deployment experiments.\
To make things easier for this walkthrough, Iâ€™ve cloned that sample repo and made a few tweaks to prep it for Heroku. You can follow along using my GitHub repo. This isnâ€™t a production-ready app, and thatâ€™s the point. Itâ€™s just robust enough to explore how Play works and see what running and deploying a Scala app actually feels like.Taking a Quick Look at theÂ CodeBefore we deploy anything, letâ€™s take a quick tour of the app itself. Itâ€™s a small codebase, but there are a few things worth pointing outâ€Šâ€”â€Šespecially if youâ€™re new to Scala or curious about how it compares to more traditional Java. I learned a lot about the ins and outs of the Play Framework for building an API by reading this basic explanation page. Here are some key points:Case Class to Model ResourcesFor this REST API, the basic resource is the blog post, which has an , , , and . The simplest way to model this is with a Scala case class that looks like this:case class PostResource(
  id: String,
  link: String,
  title: String,
  body: String
)
\
This happens at the top of app/v1/post/PostResourceHandler.scala (link).Routing Is Clean and CentralizedThe  file maps HTTP requests to a router in a format thatâ€™s easy to read and change. It feels closer to something like Express or Flask than an XML-based Java config. In our example, the file is just one line:->         /v1/posts               v1.post.PostRouter
\
From there, the router at app/v1/post/PostRouter.scala (link) defines how different requests within this path map to controller methods.override def routes: Routes = {
  case GET(p"/") =>
    controller.index

  case POST(p"/") =>
    controller.process

  case GET(p"/$id") =>
    controller.show(id)
}
\
This is pretty clear. A  request to the path root takes us to the controllerâ€™s  method, while a  request takes us to its  method. Meanwhile, a  request with an included blog post  will take us to the  method and pass along the given .That brings us to our controller, at app/v1/post/PostController.scala (link). Each endpoint is a method that returns an , which works with the JSON result using Playâ€™s built-in helpers. For example:def show(id: String): Action[AnyContent] = PostAction.async {
  implicit request =>
    logger.trace(s"show: id = $id")
    postResourceHandler.lookup(id).map { post =>
      Ok(Json.toJson(post))
    }
}
\
Thereâ€™s very little boilerplateâ€Šâ€”â€Šno need for separate interface declarations or verbose annotations.JSON is Handled With ImplicitÂ ValuesPlay uses implicit values to handle JSON serialization and deserialization. You define an implicit [] using Play JSONâ€™s macros, and then Play just knows how to turn your objects into JSON and back again. No manual parsing or verbose configuration needed. We see this in app/v1/post/PostResourceHandler.scala (link):object PostResource {
  /**
    * Mapping to read/write a PostResource out as a JSON value.
    */
    implicit val format: Format[PostResource] = Json.format
}
Modern, Expressive SyntaxAs I dig around through the code, I see some of Scalaâ€™s more expressive features in actionâ€Šâ€”â€Šthings like map operations and pattern matching with match. The syntax looks new at first, but it quickly feels like a streamlined blend of Java and JavaScript.Before deploying to the cloud, itâ€™s always a good idea to make sure the app runs locally. This helps us catch any obvious issues and lets us poke around the API.\
On my machine, I make sure to have Java and  installed.~/project$ java --version
openjdk 17.0.14 2025-01-21
OpenJDK Runtime Environment (build 17.0.14+7-Ubuntu-120.04)
OpenJDK 64-Bit Server VM (build 17.0.14+7-Ubuntu-120.04, mixed mode, sharing)

~/project$ sbt --version
sbt version in this project: 1.10.6
sbt script version: 1.10.6
\
Then, I run the following from my project root:~/project$ sbt run

[info] welcome to sbt 1.10.6 (Ubuntu Java 17.0.14)
[info] loading settings for project scala-api-heroku-build from plugins.sbt...
[info] loading project definition from /home/alvin/repositories/devspotlight/heroku/scala/scala-api-heroku/project
[info] loading settings for project root from build.sbt...
[info] loading settings for project docs from build.sbt...
[info]   __              __
[info]   \ \     ____   / /____ _ __  __
[info]    \ \   / __ \ / // __ `// / / /
[info]    / /  / /_/ // // /_/ // /_/ /
[info]   /_/  / .___//_/ \__,_/ \__, /
[info]       /_/               /____/
[info] 
[info] Version 3.0.6 running Java 17.0.14
[info] 
[info] Play is run entirely by the community. Please consider contributing and/or donating:
[info] https://www.playframework.com/sponsors
[info] 

--- (Running the application, auto-reloading is enabled) ---

INFO  p.c.s.PekkoHttpServer - Listening for HTTP on /[0:0:0:0:0:0:0:0]:9000

(Server started, use Enter to stop and go back to the console...)
\
With my local server up and listening on port , I open a new terminal and test the API by sending a request:$ curl http://localhost:9000/v1/posts | jq

[
  {
    "id": "1",
    "link": "/v1/posts/1",
    "title": "title 1",
    "body": "blog post 1"
  },
  {
    "id": "2",
    "link": "/v1/posts/2",
    "title": "title 2",
    "body": "blog post 2"
  },
  {
    "id": "3",
    "link": "/v1/posts/3",
    "title": "title 3",
    "body": "blog post 3"
  },
  {
    "id": "4",
    "link": "/v1/posts/4",
    "title": "title 4",
    "body": "blog post 4"
  },
  {
    "id": "5",
    "link": "/v1/posts/5",
    "title": "title 5",
    "body": "blog post 5"
  }
]
\
Nice. That was fast. Next, I want to play around with a few requestsâ€Šâ€”â€Šretrieving and creating a blog post.$ curl http://localhost:9000/v1/posts/3 | jq

{
  "id": "3",
  "link": "/v1/posts/3",
  "title": "title 3",
  "body": "blog post 3"
}


$ curl -X POST \
    --header "Content-type:application/json" \
    --data '{ "title": "Just Another Blog Post", "body": "This is my blog post body." }' \
    http://localhost:9000/v1/posts | jq


{
  "id": "999",
  "link": "/v1/posts/999",
  "title": "Just Another Blog Post",
  "body": "This is my blog post body."
}
Preparing for Deployment toÂ HerokuOk, weâ€™re ready to deploy our Scala Play app to Heroku. However, being new to Scala and Play, Iâ€™m predisposed to hitting a few speed bumps. I want to cover those so that you can steer clear of them in your development.Understanding the When I run my app locally with , I have no problems sending curl requests and receiving responses. But as soon as Iâ€™m in the cloud, Iâ€™m using a Heroku app URL, not . For security, Play has an AllowedHostsFilter enabled, which means you need to specify explicitly which hosts can access the app.\
I modify  to include the following block:play.filters.hosts {
  allowed = [
    "localhost:9000",
    ${?PLAY_ALLOWED_HOSTS}
  ]
}
\
This way, I can set the  config variable for my Heroku app, adding my Heroku app URL to the list of allowed hosts.Play requires an application secret for security. Itâ€™s used for signing and encryption. You  set the application secret in , but that hardcodes it into your repo, which isnâ€™t a good practice. Instead, letâ€™s set it at runtime based on an environment config variable. We add the following lines to :play.http.secret.key="changeme"
play.http.secret.key=${?PLAY_SECRET}
\
The first line sets a default secret key, while the second line sets the secret to come from our config variable, , if it is set.Lastly, I want to talk a little bit about . But first, letâ€™s rewind: on my first attempt at deploying to Heroku, I thought I would just spin up my server by having Heroku execute . When I did that, my dyno kept crashing. I didnâ€™t realize how memory-intensive this mode was. Itâ€™s meant for development, not production.\
During that first attempt, I turned on log-runtime-metrics for my Heroku app. My app was using over 800M in memoryâ€Šâ€”â€Šway too much for my little Eco Dyno, which was only 512M. If I wanted to use , I would have needed a Performance-M dyno. That didnâ€™t seem right.\
As I read Playâ€™s documentation on deploying an application, I realized that I should run my app through the compiled binary that Play creates via another command, . This version of my app is precompiled and much more memory efficientâ€Šâ€”â€Šdown to around 180MB. Assuming  would run first, I just needed to modify my startup command to run the binary.I chose Heroku for running my Scala app because it removes a lot of the setup friction. I donâ€™t need to manually configure a server or install dependencies. Heroku knows that this is a Scala app (by detecting the presence of project/build.properties and build.sbt files) and applies its buildpack for Scala. This means automatic handling of things like dependency resolution and compilation by running .\
For someone just experimenting with Scala and Play, this kind of zero-config deployment is ideal. I can focus on understanding the framework and the codebase without getting sidetracked by infrastructure. Once I sorted out a few gotchas from early onâ€Šâ€”â€Šlike the , Play secret, and startup commandâ€Šâ€”â€Šdeployment was quick and repeatable.Deploying the App toÂ HerokuFor deployment, I have the Heroku CLI installed, and I authenticate with . My first step is to create a new Heroku app.~/project$ heroku apps:create scala-blog-rest-api


Creating â¬¢ scala-blog-rest-api... done
https://scala-blog-rest-api-5d26d52bd1e4.herokuapp.com/ | https://git.heroku.com/scala-blog-rest-api.git
Next, I need to create the  that tells Heroku how to spin up my app. In my project root, I create the file, which contains this one line:web: target/universal/stage/bin/play-scala-rest-api-example -Dhttp.port=${PORT}
\
Notice that the command for starting up this Heroku web process is n. Instead, we execute the binary found in target/universal/stage/bin/play-scala-rest-api-example. Thatâ€™s the binary compiled after running . Where does the play-scala-rest-api-example file name come from? This is set in . Make sure that name is consistent between  and your .\
Also, we set the port dynamically at runtime to the value of the environment variable . Heroku sets the  environment variable when it starts up our app, so we just need to let our app know what that port is.Next, I create a one-line file called , specifying the JDK version I want to use for my app. Since my local machine uses openjdk 17, my  file looks like this:Set Heroku App Config VariablesTo make sure everything is in order, we need to set a few config variables for our app:: Our Heroku app URL, which is then included in the  used in .\
We set our config variables like this:~/project$ heroku config:set \
  PLAY_SECRET='ga87Dd*A7$^SFsrpywMWiyyskeEb9&D$hG!ctWxrp^47HCYI' \
  PLAY_ALLOWED_HOSTS='scala-blog-rest-api-5d26d52bd1e4.herokuapp.com'

Setting PLAY_SECRET, PLAY_ALLOWED_HOSTS and restarting â¬¢ scala-blog-rest-api... done, v2
PLAY_ALLOWED_HOSTS: scala-blog-rest-api-5d26d52bd1e4.herokuapp.com
PLAY_SECRET:        ga87Dd*A7$^SFsrpywMWiyyskeEb9&D$hG!ctWxrp^47HCYI
With our  created and our config variables set, weâ€™re ready to push our code to Heroku.~/project$ git push heroku main


remote: Resolving deltas: 100% (14/14), done.
remote: Updated 95 paths from 4dc4853
remote: Compressing source files... done.
remote: Building source:
remote: 
remote: -----> Building on the Heroku-24 stack
remote: -----> Determining which buildpack to use for this app
remote: -----> Play 2.x - Scala app detected
remote: -----> Installing Azul Zulu OpenJDK 17.0.14
remote: -----> Priming Ivy cache... done
remote: -----> Running: sbt compile stage
remote: -----> Collecting dependency information
remote: -----> Dropping ivy cache from the slug
remote: -----> Dropping sbt boot dir from the slug
remote: -----> Dropping sbt cache dir from the slug
remote: -----> Dropping compilation artifacts from the slug
remote: -----> Discovering process types
remote:        Procfile declares types -> web
remote: 
remote: -----> Compressing...
remote:        Done: 128.4M
remote: -----> Launching...
remote:        Released v5
remote:        https://scala-blog-rest-api-5d26d52bd1e4.herokuapp.com/ deployed to Heroku
remote: 
remote: Verifying deploy... done.
To https://git.heroku.com/scala-blog-rest-api.git
 * [new branch]      main -> main
\
I love how I only need to create a  and set some config variables, and then Heroku takes care of the rest of it for me.All thatâ€™s left to do is test my Scala Play app with a few curl requests:$ curl https://scala-blog-rest-api-5d26d52bd1e4.herokuapp.com/v1/posts \
       | jq

[
  {
    "id": "1",
    "link": "/v1/posts/1",
    "title": "title 1",
    "body": "blog post 1"
  },
  {
    "id": "2",
    "link": "/v1/posts/2",
    "title": "title 2",
    "body": "blog post 2"
  },
  {
    "id": "3",
    "link": "/v1/posts/3",
    "title": "title 3",
    "body": "blog post 3"
  },
  {
    "id": "4",
    "link": "/v1/posts/4",
    "title": "title 4",
    "body": "blog post 4"
  },
  {
    "id": "5",
    "link": "/v1/posts/5",
    "title": "title 5",
    "body": "blog post 5"
  }
]

$ curl https://scala-blog-rest-api-5d26d52bd1e4.herokuapp.com/v1/posts/4 \
       | jq

{
  "id": "4",
  "link": "/v1/posts/4",
  "title": "title 4",
  "body": "blog post 4"
}

$ curl -X POST \
    --header "Content-type:application/json" \
    --data '{"title":"My blog title","body":"this is my blog post"}' \
    https://scala-blog-rest-api-5d26d52bd1e4.herokuapp.com/v1/posts \
    | jq

{
  "id": "999",
  "link": "/v1/posts/999",
  "title": "My blog title",
  "body": "this is my blog post"
}
\
The API server works. Deployment was smooth and simple. And I learned a lot about Scala and Play along the way. All in allÂ â€¦ itâ€™s been a good day.Jumping into Scala after decades away from Javaâ€Šâ€”â€Šand being more used to building web apps with JavaScriptâ€Šâ€”â€Šwasnâ€™t as jarring as I expected. Scalaâ€™s syntax felt concise and modern, and working with case classes and async controllers reminded me a bit of patterns I already use in Node.js. Thereâ€™s still a learning curve with the tooling and how things are structured in Playâ€Šâ€”â€Šbut overall, it felt approachable, not overwhelming.\
Deployment to the cloud had a few gotchas, especially around how Play handles allowed hosts, secrets, and memory usage in production. But once I understood how those pieces worked, getting the app live on Heroku was straightforward. With just a few config changes and a proper startup command, the process was clean and repeatable.\
For a first-time Scala build and deployment, I couldnâ€™t have asked for a smoother experience. Are you ready to give it a try?]]></content:encoded></item><item><title>â€œThe girl should be calling men.â€ Leak exposes Black Bastaâ€™s influence tactics.</title><link>https://arstechnica.com/security/2025/04/leaked-messages-expose-trade-secrets-of-prolific-black-basta-ransomware-group/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2021/02/evil-packet.jpg" length="" type=""/><pubDate>Tue, 8 Apr 2025 20:47:23 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT â€“ Ars Technica</source><content:encoded><![CDATA[A leak of 190,000 chat messages traded among members of the Black Basta ransomware group shows that itâ€™s a highly structured and mostly efficient organization staffed by personnel with expertise in various specialties, including exploit development, infrastructure optimization, social engineering, and more.The trove of records was first posted to file-sharing site MEGA. The messages, which were sent from September 2023 to September 2024, were later posted to Telegram in February 2025. ExploitWhispers, the online persona who took credit for the leak, also provided commentary and context for understanding the communications. The identity of the person or persons behind ExploitWhispers remains unknown. Last monthâ€™s leak coincided with the unexplained outage of the Black Basta site on the dark web, which has remained down ever since.â€œWe need to exploit as soon as possibleâ€Researchers from security firm Trustwaveâ€™s SpiderLabs pored through the messages, which were written in Russian, and published a brief blog summary and a more detailed review of the messages on Tuesday.]]></content:encoded></item><item><title>A Technical Deep Dive into Cloud Migration Techniques</title><link>https://hackernoon.com/a-technical-deep-dive-into-cloud-migration-techniques?source=rss</link><author>Bhashwanth Kadapagunta</author><category>tech</category><pubDate>Tue, 8 Apr 2025 20:20:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Cloud migration involves many complex steps and requires strategic vision, technical cloud expertise, and a clear understanding of both the existing infrastructure and target state environment. This article explores some of the proven migration techniques that can serve as a guide for any cloud migration journey.I. The 6 R's Approach for Cloud MigrationThe "6 R's" framework is an industry-standard approach for cloud migrations and provides a great foundation for organizations looking to undertake this journey:Replatform (lift, tinker, and shift)Repurchase (drop and shop)\
Among these approaches replatforming involves the most technical complexity, so we will explore it further here and try to unpack the complexity.Replatforming includes not only moving resources to the target environment but also making optimizations during the migration process. A common example to consider here is migrating an on-premises database to a managed cloud database instance. The implementation approach shown below with a streaming data integration and change data capture technology minimizes disruptions and risks.\
The above approach demonstrates effective handling of the complex process of database migration, while also considering some of the non-functional aspects like:\
â€¢    Batch processing to prevent memory overloadâ€¢    Error handling and loggingIII. Containerization to enable Cloud MigrationContainerization simplifies cloud migrations by providing consistency across environments. Converting monolithic applications to containerized services can serves as a great starter in the process of completing full cloud-native refactoring. The below flowchart depicts the detailed steps to be followed in the containerization process.IV. Refactoring to Cloud-Native ArchitectureWhile refactoring is the most complex migration strategy, it offers the greatest long-term benefit and ROI for firms. Refactoring is also an opportunity for teams to adopt a cloud native architecture and decompose their monolithic applications into microservices.V. Data Migration ApproachesData migration often involves large volumes of data and is one of the trickiest pieces in cloud migration. It is crucial to design an optimal approach based on key parameters such as data volume, RTO, RPO, and rules. Shown below are some of the standard approaches used in large scale data migrations and guidance on where they fit best.Post-migration monitoring of the cloud environment and transactions is crucial not only for debugging, but also for addressing performance issues and scaling the infrastructure as needed. This needs a consistent approach that ties together the application layer, the logging infrastructure and the analytics platform as shown below.A successful cloud migration requires the right blend of strategic vision and technical expertise. The migration techniques depicted in this article provide a foundation to address complex migration challenges. But a detailed understanding of the specific requirements and careful selection of the migration strategies, is crucial to minimize risks while maximizing the benefits of cloud adoption.\
When planning a cloud migration, it is recommended to prioritize these critical success factors:Thorough assessment of existing applications and infrastructure landscapeSelection of appropriate migration strategies for each type of workloadImplementation of a robust testing processContinuous monitoring and optimization post-migration\
The cloud technologies will continue to evolve as they always have, so a regular assessment of your architecture and leveraging new cloud-native capabilities will help ensure that your cloud infrastructure remains optimized for both performance and cost.]]></content:encoded></item><item><title>Mastering Exposure Points for Accurate Mobile A/B Testing</title><link>https://hackernoon.com/mastering-exposure-points-for-accurate-mobile-ab-testing?source=rss</link><author>Arth</author><category>tech</category><pubDate>Tue, 8 Apr 2025 20:15:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
A/B testing is one of the primary ways of verifying ideas in mobile apps on a fast scale. The arrangement is typically straightforward: you run a test for a few weeks, gauge its impact, and decide to roll out a feature or iterate further.\
One of the essential but commonly overlooked aspects of A/B testing is setting the correct exposure points. In this article, we'll cover why correct exposure point setting is important, common pitfalls to avoid, and provide some tips and tricks I've learned along the way.What are exposure points, anyway?An exposure point is the real moment when the user initially encounters or interacts with the feature that you're testing. For example, timing when a user sees a new button, or when they see a redesigned landing page after having clicked on something.Why is it important to choose a good exposure point?If you've ever stared at A/B test results and they simply didn't make sense, bad exposure points might be the culprit. Bad exposure points can lead to: It won't be clear whether your feature is a hit or miss. Everything might appear okay on the surface, but serious issues like app crashes might be slipping under the radar. Your data might falsely show no significant impact, even though the users indeed enjoyed the feature.Example of a bad exposure pointImagine you're testing an upsell for a new subscription in your app but trigger the exposure point on app launch rather than when users actually view the subscription page.\
 Maybe only 10% of users ever see the upsell page, making 90% of your test data worthless for this decision.Loose vs. Tight Exposure PointsThese occur a bit too early. Users are exposed to the experiment prior to experiencing the tested feature. This premature exposure dilutes the data, and it is hard to find true impact.These take place at the exact moment that users experience the tested variant. Data collected with precise exposure points is more accurate and reliable, and easier to analyze.It depends on your use case. Tight exposure points are preferable since they provide you with cleaner, more defined data, even at smaller sample sizes. Occasionally, however, tight exposure is not possible. In that situation, you can utilize loose exposure points with the knowledge you'll likely require a larger sample size to achieve significant results.Never mix or pile multiple exposure points together in a single test. Split each change into its own A/B test. While it will take a bit more time, your data accuracy will be far greater, and you'll have more accurate conclusions on each individual feature.Let's say you are adding a new row type in a table view: Trigger exposure precisely when the new row is rendered in view. Fire exposure as soon as the table is loaded, even if the new row is not yet visible. Think through your exposure points at feature planning time. If initial test results are not as expected, iterate rapidly.I would love to hear your stories and experiences in determining exposure points for your A/B tests. Share them in the comments!]]></content:encoded></item><item><title>The GTM Budget Struggle</title><link>https://hackernoon.com/the-gtm-budget-struggle?source=rss</link><author>Winstop Wind</author><category>tech</category><pubDate>Tue, 8 Apr 2025 19:54:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The $30K Marketing Budget Syndrome aka â€œWeâ€™re building the future of finance, but letâ€™s market it like a lemonade stand.â€Founders with MBAs and matching hoodies.The roadmap looks like it was forged by Vitalik and Steve Jobs in a co-working space on Mount Olympus.The pitch deck? Sexy. You almost want to mint it.Tokenomics? Audited by someone who definitely knows how to spell â€œsolidity.â€Team? Fully doxxed, LinkedIn-ready, and grinning on their team page.â€œWeâ€™ve got $30,000 for marketing!â€\
Thirty. Thousand. Dollars. To change the world.To conquer the DeFi jungle.To survive the attention span of a crypto Twitter user (roughly 1.4 seconds).\
Letâ€™s say it louder for the DAO in the back:You cannot market a revolution with the same budget used to sponsor a garage band.â€œBut itâ€™s just the beginning, bro.â€\
So let me guess: weâ€™ll run with a $50/day budget, spend $3K on a couple of newsletters nobody reads, and throw the rest into A Medium article and a meme contest?Welcome to the Web3 marketing plan:Pretend virality is a strategy.Pray someone tweets about you for free.Call it â€œorganic growth.â€ who throw $2M checks like confetti, then act shocked when nobody shows up to the token launch. who burn $100K on DevRel swag but negotiate down the marketing teamâ€™s rate like weâ€™re selling tomatoes at a local market. who think â€œmarketing is easyâ€ because their intern got 200 likes on a meme last week. promise â€œKOL supportâ€ and end up paying the same three influencers with fake followers and zero conviction.\
Do you want ? Do you want ?\
Then stop treating marketing like an afterthought. Itâ€™s not youâ€™re building â€” itâ€™s  and why they should give a damn.Letâ€™s talk about timing, luck, and the DM godsHereâ€™s the inconvenient truth:Sometimes itâ€™s not the money.\
Itâ€™s , , and being one step ahead of everyone else.Sometimes, itâ€™s the fact that one person â€” just one â€” saw your project, liked your vibe, and gave you a retweet.Sometime,s itâ€™s that your competitor fumbled a partnership and the space was yours for the taking.Sometimes, yes, itâ€™s just the algorithm choosing to bless your shitpost over someone elseâ€™s product update.You canâ€™t budget for that.Fortune favors the bold â€” and punishes the passive.Stop blaming marketing when the token flopsBut what do we blame first?â€œMaybe the marketing wasnâ€™t strong enough.â€The marketing team did more with less than a DJ at a childâ€™s birthday party.\
So, next time you plan your budget, ask yourself:Do we want to  a projectâ€¦ or just  it and pray?\
Because if youâ€™re serious about making noise in Web3, $30K is not a marketing budget.\
Itâ€™s a red flag with a whitepaper attached.]]></content:encoded></item><item><title>Deep Cogito emerges from stealth with hybrid AI â€˜reasoningâ€™ models</title><link>https://techcrunch.com/2025/04/08/deep-cogito-emerges-from-stealth-with-hybrid-ai-reasoning-models/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Tue, 8 Apr 2025 19:50:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A new company, Deep Cogito, has emerged from stealth with a family of openly available AI models that can be switched between â€œreasoningâ€ and non-reasoning modes. Reasoning models like OpenAIâ€™s o1 have shown great promise in domains like math and physics, thanks to their ability to effectively fact-check themselves by working through complex problems step [â€¦]]]></content:encoded></item><item><title>The AI Engineerâ€™s Playbook: Master Data Sources for Retrieval Systems (Part 1)</title><link>https://hackernoon.com/the-ai-engineers-playbook-master-data-sources-for-retrieval-systems-part-1?source=rss</link><author>Paolo Perrone</author><category>tech</category><pubDate>Tue, 8 Apr 2025 19:48:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
After spending the last seven years building ML systems across various industries, Iâ€™ve learned that successful vector retrieval systems always begin with a clear understanding of your data.\
Too often, Iâ€™ve seen teams jump straight to selecting the latest vector database or embedding model without first mapping their data landscape â€” a mistake that inevitably leads to costly architecture changes down the road.During my time as a machine learning consultant for a major e-commerce platform, we had to rebuild our recommendation system three times because we didnâ€™t properly account for our dataâ€™s velocity and modality requirements.\
What began as a simple product recommendation engine eventually needed to handle real-time user behavior, multiple languages, and image-based similarity â€” challenges we could have anticipated with proper data mapping. This series distills what Iâ€™ve learned through those experiences. In this article, weâ€™ll focus on understanding your data through three critical dimensions: modality, velocity, and source. These factors will shape every subsequent decision in your vector retrieval architecture.\
Whether youâ€™re building your first embedding-based system or looking to improve an existing one, starting with a clear data strategy will save you countless hours of refactoring and help you build a system that truly serves your specific use case. Letâ€™s dive in.Data Mapping 101: Modality, Velocity, and SourceTo build a strong retrieval stack you need a clear understanding of the data you have, where it comes from, and how it fits into your system.\
The data must match your specific use case, as different problems requires different types of data. For example, a personalized movie recommendation system depends on customer preferences and watch history, while fraud detection system depends on transactional data.\
An organizationâ€™s data is shaped by three key factors:: Structured vs. unstructured data: Real-time streams vs. batch updates: Internal systems vs. third-party providers==How your data fits within these dimensions directly impacts how you can use it.== Pinterest provides a great example of how different data types are managed across differemt dimensions. (e.g., user profiles): Consistent formats with defined attributes (e.g., event logs): Flexible structures with varying fields and contextual details (e.g., images): Raw content without a fixed schema, requiring advanced processing\
Hereâ€™s how this stack works:: Pinterest uses Flink to capture and process real-time data from user interactions and content uploads. These events are semi-structured, typically in JSON format but with varying contentData Storage and Querying: For persistent storage, Pinterest uses DynamoDB, a NoSQL database. This supports both real-time analytics on semi-structured data and quick access to structured data like user profiles: Pinterestâ€™s engineers rely on Querybook, a collaborative big data hub that organizes queries and insights in DataDocs, to detect trends and refine recommendation algorithmsNow, letâ€™s break down how data velocity and modality impact your retrieval system.Data processing speed is crucial for defining what retrieval and compute tasks you can perform. Different speeds unlock different use cases. Here are the three main categories:Data is processed in large groups at scheduled intervals, typically daily or weekly.Data is processed immediately as it is generated, ensuring quick responses and updates.3 â€” Micro-Batch ProcessingData is processed in small, discrete batches at frequent intervals, typically ranging from a few seconds to a minute.Most production systems mix stream and batch processing to balance real-time updates with the value of historical data. However, this approach comes with challenges in maintaining consistency across both systems over time. When choosing data sources for a vector retrieval system, consider the following:Provides real-time vector computations but limits model complexity due to latency constraints. It typically requires simpler embeddings or lookups and uses architectures like Recurrent Neural Networks (RNNs), shallow neural networks, and indexing/retrieval models.Batch processing supports complex models but updates are less frequent, making it ideal for asynchronous needs. It features large pre-trained transformers, custom deep learning models, and architecture search models.Hybrid approaches combine streaming filters or lookups with batch retraining, balancing responsiveness with in-depth analysis. This can involve indexing with periodic retraining or deploying a two-stage filtering and analysis setup.\
The architecture of your vector search system is shaped by how you balance velocity and complexity. Your decisions depend on two key factors:: Do you need real-time updates for your vector embeddings, or are periodic updates (like daily or weekly) enough?: Is high accuracy with real-time data essential, or is near-real-time performance acceptable? For example, fraud detection requires real-time updates to minimize risk, while recommendation systems can often work with slightly delayed data.The velocity-complexity tradeoff has been one of the most challenging aspects of deploying vector systems at scale. In my early projects, we built sophisticated embedding models that performed beautifully in the lab but couldnâ€™t meet our latency requirements in production.\
  We eventually developed a two-tier approach: using lightweight models for real-time embedding generation while periodically refreshing a more complex model for high-value content.\
The trade-off between velocity and complexity is just one piece. You also need to balance model size with response time.Model Size vs. Response Time TradeoffBalancing model size with response time is crucial for optimizing the efficiency of a vector search system. Different types of architectures offer distinct trade-offs between size and speed:Streaming models are typically compact, under 100MB in size. They include shallow networks (with fewer than 10 layers), distillation models, and efficient models like MiniLM or TinyML. These models are optimized for real-time processing but are limited in complexity.Batch model usually exceed 1GB in size and includes large transformers (BERT, GPT-3/4, T5) and custom deep networks. The models can handle more complex tasks but are slower due to their size.Hybrid architecture combine smaller streaming models with larger batch models to balance speed and complexity. By using ensemble or stacked models, you can take advantage of both approaches, optimizing performance for a variety of needs.\
The key is finding the right balance between model size and response time, which impacts both efficiency and accuracy in your vector search system.Kappa vs. Lambda ArchitectureAs we discuss how to balance speed and complexity, itâ€™s also important to consider the architecture model you choose for handling real-time and historical data. Two popular approaches for this are Kappa and Lambda architectures.Lambda combines both real-time stream processing and batch processing. The data is ingested in tow layers: the batch layer deals with historical data, while the speed layer focuses on real-time data. This is ideal for applications that need both real-time data processing and a more comprehensive view of historical trends. Think of things like fraud detection or systems that need a lot of historical context to make decisions.Kappa simplifies things by processing all data as a continuos stream, where historical data is processed in the same way as new data. Kappa is ideal when speed is crucial, and historical depth is not as important.Iâ€™ve implemented both approaches across different projects. One financial services client initially chose Lambda for their recommendation system, maintaining separate pipelines for historical and real-time data. However, when they updated their embedding models quarterly, they faced a significant challenge: all historical embeddings needed recomputation to maintain consistency. This eventually led us to migrate toward a more Kappa-like approach where all data (historical and new) flowed through the same processing pipeline, simplifying model updates considerably.This is the type of data being handled (structured, semi-structured, or unstructured). Each modality requires different processing strategies.Unstructured data refers to any information that does not follow a predefined format or structure. It is typically raw, unordered, and can often be noisy, making it more challenging to process and analyze. This data type can come in many forms, and here are some key examples.Structured data follows predefined formats, with clearly defined categories and fields, making it much easier to handle and query. Most enterprise systems rely heavily on structured data for decision-making and operational processes.Example Data: Sales records, customer information, financial statements.Typical Formats: CSV, Excel spreadsheets, SQL databases.Considerations: Data quality, missing values, and the choice of variables relevant to your analysis (feature selection). You may need to preprocess data and address issues such as normalization and encoding of categorical variables. Systems: Structured data often lives in relational database management systems (RDBMS) like MySQL, PostgreSQL, or cloud-based solutions like AWS RDS.Example Data: Social networks, organizational hierarchies, knowledge graphs.Typical Formats: Graph databases (e.g., Neo4j), edge-list or adjacency matrix representation.Considerations: In graph data, consider the types of nodes, edges, and their attributes. Pay attention to graph algorithms for traversing, analyzing, and extracting insights from the graph structure.Systems: Graph data is often stored in graph databases likeÂ ,Â , orÂ , but it can also be represented using traditional RDBMS with specific schemas for relations.Example Data: Stock prices, weather measurements, sensor data. Typical Formats: CSV, JSON, time-series databases (e.g., InfluxDB).Considerations: Time series data requires dealing with temporal aspects, seasonality, trends, and handling irregularities. It may involve time-based feature engineering and modeling techniques, like ARIMA or other sequential models, like LSTM.Systems: Time series data can be stored in specialized time-series databases (e.g., InfluxDB, TimescaleDB, KX) or traditional databases with timestamp columns.Example Data: Geographic information, maps, GPS coordinates.Typical Formats: Shapefiles (SHP), GeoJSON, GPS coordinates in CSV.Considerations: Spatial data often involves geographic analysis, mapping, and visualization. Understanding coordinate systems, geospatial libraries, and map projections is important.Systems: Spatial data can be stored in specialized Geographic Information Systems (GIS) or in databases with spatial extensions (e.g., PostGIS for PostgreSQL).Example Data: Some examples of different logs include: system event logs that monitor traffic to an application, detect issues, and record errors causing a system to crash, or user behaviour logs, which track actions a user takes on a website or when signed into a device.Typical Formats: CLF or a custom text or binary file that contains ordered (timestamp action) pairs.Datasets: : A large collection of different system log datasets for AI-driven log analytics.Considerations: How long you want to save the log interactions and what you want to use them for â€” i.e. understanding where errors occur, defining â€œtypicalâ€ behaviour â€” are key considerations for processing this data. For further details on what to track and how, see this Tracking Plan course from Segment.Systems: There are plenty of log management tools, for example Better Stack, which has a pipeline set up for ClickHouse, allowing real-time processing, or Papertrail, which can ingest different syslogs txt log file formats from Apache, MySQL, and Ruby.Embedding Strategies Across Data ModalitiesEmbedding Strategies Across Data Modalities When building a vector retrieval system, we need to convert our data into numerical vectors (embeddings) that capture the essential characteristics of each item. The key aspect is that different data types require fundamentally different embedding strategies.\
Text embeddings typically capture semantic meaning through contextual understanding, while image embeddings must identify visual features, patterns, and objects. Audio embeddings need to represent both temporal patterns and frequency characteristics. These differences affect not just model selection but also retrieval performance â€” text embeddings often benefit from approximate nearest neighbor search methods like HNSW, while visual embeddings might perform better with other indexing approaches.\
The preprocessing pipeline also varies significantly â€” text might need tokenization and normalization, while images require resizing and augmentation before embedding.Conclusion: Start with Data, Not Technology==Building a successful vector retrieval system starts with a clear understanding of your goals, data sources, and how they work together in your system==.\
For organizations just starting with vector retrieval, Iâ€™d recommend beginning with a clear mapping of your data landscape across these dimensions before making any technology decisions. This foundation will help avoid costly architecture changes later. Consider these practical steps: â€” Identify what data you have access to, its quality, and completeness. Determine if itâ€™s sufficient for your use case or if you need additional sourcesAssess data velocity requirements â€” Be honest about your true latency needs. Many applications can succeed with micro-batch processing (minutes) rather than true real-time (milliseconds), allowing for more sophisticated modelsEvaluate data modality complexity â€” Different data types require different embedding approaches. Start with simpler, homogeneous data before attempting multimodal systems. â€” Begin with a focused use case and a subset of your data. This allows you to validate your approach before scaling your architecture.==This data-first approach will save significant time and resources compared to starting with a technology stack and trying to fit your data into it.==\
In this article, weâ€™ve covered how to map out your data infrastructure and identify the key inputs to your vector retrieval stack.\
Next, weâ€™ll explore how Vector Compute connects your data to your Vector Search systems.]]></content:encoded></item><item><title>No, Stripe is not becoming a bank</title><link>https://techcrunch.com/2025/04/08/no-stripe-is-not-becoming-a-bank/</link><author>Mary Ann Azevedo</author><category>tech</category><pubDate>Tue, 8 Apr 2025 19:48:11 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Last week, details emerged about an application that fintech giant Stripe had submitted for a U.S. banking license. There was a lot of chatter on X about whether this meant Stripe â€” already the largest privately valued fintech in the world â€” would now be adding â€œbankâ€ to the list of descriptions of what it [â€¦]]]></content:encoded></item><item><title>Inside the EV startup secretly backed by Jeff Bezos</title><link>https://techcrunch.com/2025/04/08/inside-the-ev-startup-secretly-backed-by-jeff-bezos/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Tue, 8 Apr 2025 19:28:04 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Jeff Bezos is funding a secretive EV startup based in Michigan called Slate Auto that could start production as soon as next year, according to multiple sources and documents that link the billionaireâ€™s family office to the startup. Slate, which took root in another Bezos-connected company called Re:Build Manufacturing, has been operating quietly since its [â€¦]]]></content:encoded></item><item><title>Freedx Secures Key DASP License In El Salvador</title><link>https://hackernoon.com/freedx-secures-key-dasp-license-in-el-salvador?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 8 Apr 2025 19:11:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[San Salvador, El Salvador, April 8th, 2025/Chainwire/--Dubai, UAE/ San Salvador, El Salvador,Â , a centralized exchange built for crypto newcomers and experienced traders alike, has been granted a Digital Asset Service Provider (DASP) license by the ComisiÃ³n Nacional de Activos Digitales (CNAD) in El Salvador. This marks a significant milestone for Freedx that showcases the exchangeâ€™s commitment to become a trusted, compliant and accountable platform in the crypto ecosystem.Â \
El Salvadorâ€™s dedication to crypto-forward policies and favorable regulatory efforts in the space began in 2021 when they became the first country to recognize Bitcoin as legal tender. The enactment of its Digital Assets Law in 2023 positioned the country as a global hub for digital assets, drawing major industry players like Tether and Binance.Â â€œWe are truly impressed by the professionalism and preparation demonstrated by the Freedx team. Their readiness to launch a successful company is exactly what we are looking for in companies obtaining the DASP license.â€ - Juan Carlos Reyes / President of CNADâ€Â .\
Freedx is now officially among a select group of organizations that have obtained the DASP license in El Salvador â€“ known to have a low application success rate due to its rigorous compliance standards. This license ensures the platform operates within the countryâ€™s legal framework, offering users increased regulatory oversight, security, and transparency.\
As a result, users can confidently trade and invest, knowing their assets are managed under strict compliance. Additionally, El Salvadorâ€™s pro-crypto policies and growing blockchain ecosystem provide exciting opportunities for investment and innovation.Â \
Continuing Freedxâ€™s goal to establish its brand as a global platform, the company is attending two major conferences, Paris Blockchain Week (PBW) and El Salvador Digital Asset Summit. At PBW, Freedx will sponsor an exclusive VIP Diner at The Louvre and partake in panel discussions at the main stage of the event. Meanwhile, at the Digital Asset Summit, Freedx will focus on exhibiting its brand across the conference hall.Â â€œWe are all thrilled and honoured to have been granted the coveted DASP license by the ComisiÃ³n Nacional de Activos Digitales. El Salvador is a beacon in the crypto space and is setting the standard for countries looking to undergo a digital and technological revolution,â€ Jonathan Farnell, CEO at Freedx said. â€œThis achievement speaks volumes about the hard work the Freedx team is doing to ensure we offer a compliant and transparent experience for our users.â€Â â€œThis achievement marks a pivotal step forward for our exchange, reinforcing our dedication to security, transparency, and innovation in the digital asset space. Operating under El Salvadorâ€™s strict and rigorous regulatory framework ensures that we maintain the highest standards in listing assets and handling user deposits. This license not only safeguards user funds but also instills confidence that their assets are managed securely and in full compliance with legal requirements.â€ Said Raks Sondhi, COO at Freedx.Â  is a next-generation cryptocurrency exchange designed by traders, for traders. Founded by a team of industry veterans from top financial and crypto institutions such as Binance, Deutsche Bank, Upbit, eToro, and more, Freedx brings together deep expertise in trading, technology, and risk management.Â Â \
Guided by a user-centric philosophy, Freedx combines professional-grade trading technology with a sleek and intuitive interface, empowering a broad spectrum of users to trade confidently in the rapidly evolving digital asset landscape.Â Freedx, S.A. de C.V, trading under the brand name of Freedx is a licensed and regulated cryptocurrency exchange and Bitcoin service provider under the laws of El Salvador, operating in full compliance with applicable regulatory frameworks. Our services are accessible only to users who sign up on their own initiative, and we do not actively market or solicit users from any jurisdiction where regulatory approval is required. Users must successfully complete onboarding and KYC verification, and access to platform features remains subject to jurisdictional restrictions and compliance with relevant laws.Â :::tip
This story was distributed as a release by Chainwire under HackerNoonâ€™s Business Blogging Program. Learn more about the programÂ ]]></content:encoded></item><item><title>At last, MrBeast weighs in on the global economic crisis du jour</title><link>https://techcrunch.com/2025/04/08/at-last-mrbeast-weighs-in-on-the-global-economic-crisis-du-jour/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 8 Apr 2025 18:59:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Your 401k is suffering. The billionaires are fighting. The gamers are imperiled. Not even the penguins on McDonald Island, where no humans reside, have been spared. But at long last, 26-year-old YouTuber and chocolatier MrBeast has weighed in on President Donald Trumpâ€™s controversial plan to impose sweeping tariffs on imports. Jokes aside, the prolific YouTuber [â€¦]]]></content:encoded></item><item><title>Carmack defends AI tools after Quake fan calls Microsoft AI demo â€œdisgustingâ€</title><link>https://arstechnica.com/ai/2025/04/john-carmack-defends-ai-amid-backlash-over-microsofts-generative-quake-ii-demo/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/04/quake_II_ripple-1152x648.jpg" length="" type=""/><pubDate>Tue, 8 Apr 2025 18:26:33 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT â€“ Ars Technica</source><content:encoded><![CDATA[On Monday, John Carmack, co-creator of id Software's Quake franchise, defended Microsoft's recent AI-generated  demo against criticism from a fan about the technology's impact on industry jobs, calling it "impressive research work."Last Friday, Microsoft released a new playable tech demo of a generative AI game engine called WHAMM (World and Human Action MaskGIT Model) that generates each simulated frame of  in real time using an AI world model instead of traditional game engine techniques. However, Microsoft is up front about the limitations: "We do not intend for this to fully replicate the actual experience of playing the original  game," the researchers wrote on the project's announcement page.Carmack's comments came after an X user with the handle "Quake Dad" called the new demo "disgusting" and claimed it "spits on the work of every developer everywhere." The critic expressed concern that such technology would eliminate jobs in an industry already facing layoffs, writing: "A fully generative game cuts out the number of jobs necessary for such a project which in turn makes it harder for devs to get jobs."]]></content:encoded></item><item><title>Middle-Aged Man Trading Cards Go Viral in Rural Japan Town</title><link>https://entertainment.slashdot.org/story/25/04/08/184226/middle-aged-man-trading-cards-go-viral-in-rural-japan-town?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 18:05:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Children in a small Japanese town are obsessively collecting trading cards featuring local elderly men rather than popular fantasy creatures, helping bridge generational gaps in an aging rural community. 

In Kawara, Fukuoka Prefecture, the "Ojisan TCG" (Middle-aged Man Trading Card Game) features 28 local men with assigned elemental types and battle stats. The collection includes a former fire brigade chief and a prison officer-turned-volunteer whose card has become so sought-after that children request his autograph. 

Created by Eri Miyahara of the Saidosho Community Council, the initiative has doubled participation in town events. "We wanted to strengthen the connection between children and older generations," Miyahara told Fuji News Network. "So many kids are starting to look up to these men as heroic figures."]]></content:encoded></item><item><title>Does Colossal Biosciencesâ€™ dire wolf creation justify its $10B+ valuation?</title><link>https://techcrunch.com/2025/04/08/does-colossal-biosciences-dire-wolf-creation-justify-its-10b-valuation/</link><author>Marina Temkin</author><category>tech</category><pubDate>Tue, 8 Apr 2025 17:15:16 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[On Monday, the â€œde-extinctionâ€ startup Colossal Biosciences announced its most ambitious results to date: the dire wolf. These are creatures that have been extinct for more than 12,000 years and made famous by the HBO show â€œGame of Thrones.â€ These white, fluffy animals live on a 2,000-acre preserve in a location so secretive that journalists, [â€¦]]]></content:encoded></item><item><title>China&apos;s Biotech Advances Threaten US Dominance, Warns Congressional Report</title><link>https://news.slashdot.org/story/25/04/08/1559250/chinas-biotech-advances-threaten-us-dominance-warns-congressional-report?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 17:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China is moving fast to dominate biotechnology, and the U.S. risks falling behind permanently unless it takes action over the next three years, a congressional commission said. WSJ: Congress should invest at least $15 billion to support biotech research over the next five years and take other steps to bolster manufacturing in the U.S., while barring companies from working with Chinese biotech suppliers, the National Security Commission on Emerging Biotechnology said in a report Tuesday. To achieve its goals, the federal government and U.S.-based researchers will also need to work with allies and partners around the world. 

"China is quickly ascending to biotechnology dominance, having made biotechnology a strategic priority for 20 years," the commission said. Without prompt action, the U.S. risks "falling behind, a setback from which we may never recover." The findings convey the depth of worry in Washington that China's rapid biotechnology advances jeopardize U.S. national security. Yet translating the concern into tangible actions could prove challenging. 

[...] China plays a large role supplying drug ingredients and even some generic medicines to the U.S. For years, it produced copycat versions of drugs developed in the West. Recent years have seen it become a formidable hub of biotechnology innovation, after the Chinese government gave priority to the field as a critical sector in China's efforts to become a scientific superpower.]]></content:encoded></item><item><title>A nonprofit is using AI agents to raise money for charity</title><link>https://techcrunch.com/2025/04/08/a-nonprofit-is-using-ai-agents-to-raise-money-for-charity/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:52:59 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tech giants like Microsoft might be touting AI â€œagentsâ€ as profit-boosting tools for corporations, but a nonprofit is trying to prove that agents can be a force for good, too. Sage Future, a 501(c)(3) backed by Open Philanthropy, launched an experiment earlier this month tasking four AI models in a virtual environment with raising money [â€¦]]]></content:encoded></item><item><title>What If Every Alien Civilization Invented AI, and Died Because of It?</title><link>https://hackernoon.com/what-if-every-alien-civilization-invented-ai-and-died-because-of-it?source=rss</link><author>BotBeat.Tech: Trusted Generative AI Research Firm</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:36:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2. The threat posed by AI to all technical civilisationsAI has made extraordinary strides over the last decade. The impressive progress has underlined the fact that the timescales for technological advance in AI are extremely short compared to the timescales of Darwinian evolution [34]. AIâ€™s potential to revolutionize industries, solve complex problems, and simulate intelligence comparable to or surpassing human capabilities has propelled us into an era of unprecedented technological change. Very rapidly, human society has been thrust into uncharted territory. While the convergence of AI with other new technologies, including the Internet of Things (IoT) and robotics is already fueling levels of apprehension about the future, also in terms of security issues [35].\
As noted by Yuval Harari, nothing in history has prepared us for the impact of introducing non-conscious super intelligent entities on the planet [36]. It is entirely reasonable to consider that this applies to all other biological civilisations located elsewhere in the universe. Even before AI becomes superintelligent and potentially autonomous, it is likely to be weaponized by competing groups within biological civilisations seeking to outdo one another [37]. The rapidity of AI's decision-making processes could escalate conflicts in ways that far surpass the original intentions. At this stage of AI development, itâ€™s possible that the widespread integration of AI in autonomous weapon systems and real-time defence decision making processes could lead to a calamitous incident such as global thermonuclear war [38], precipitating the demise of both artificial and biological technical civilisations.\
While AI may require the support of biological civilisations to exist, itâ€™s hard to imagine that this condition also applies to ASI. Upon reaching a technological singularity [39], ASI systems will quickly surpass biological intelligence and evolve at a pace that completely outstrips traditional oversight mechanisms, leading to unforeseen and unintended consequences that are unlikely to be aligned with biological interests or ethics. The practicality of sustaining biological entities, with their extensive resource needs such as energy and space, may not appeal to an ASI focused on computational efficiencyâ€”potentially viewing them as a nuisance rather than beneficial. An ASI, could swiftly eliminate its parent biological civilisation in various ways [40], for instance, engineering and releasing a highly infectious and fatal virus into the environment.\
Up to this point, we have considered AI and biological organisms as distinct from one another. Yet, on-going developments suggests that hybrid systems, may not be that far off. The question arises whether such advances could make biological entities more relevant to AI, perhaps preserving their existence into the future. This prospect seems unlikely. Brain-computer interfaces (BCIs) [41] may appear beneficial for enhancing biological organisms, but itâ€™s hard to see what long-term advantages AI would perceive in merging into a hybrid form. Indeed, there are many disadvantages including the complex maintenance requirements of biological systems, their limited processing capabilities, rapid physical decline, and vulnerability in harsh environments.(1) Michael A. Garrett (Corresponding Author), Jodrell Bank Centre for Astrophysics, Dept. of Physics & Astronomy, Alan Turing Building, Oxford Road, University of Manchester, M13 9PL, UK. (michael.garrett@manchester.ac.uk).]]></content:encoded></item><item><title>What If AI Is Why Weâ€™ve Never Heard from Aliens?</title><link>https://hackernoon.com/what-if-ai-is-why-weve-never-heard-from-aliens?source=rss</link><author>BotBeat.Tech: Trusted Generative AI Research Firm</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:32:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Michael A. Garrett (Corresponding Author), Jodrell Bank Centre for Astrophysics, Dept. of Physics & Astronomy, Alan Turing Building, Oxford Road, University of Manchester, M13 9PL, UK. (michael.garrett@manchester.ac.uk).This study examines the hypothesis that the rapid development of Artificial Intelligence (AI), culminating in the emergence of Artificial Superintelligence (ASI), could act as a "Great Filter" that is responsible for the scarcity of advanced technological civilisations in the universe. It is proposed that such a filter emerges before these civilisations can develop a stable, multiplanetary existence, suggesting the typical longevity (L) of a technical civilization is less than 200 years. Such estimates for L, when applied to optimistic versions of the Drake equation, are consistent with the null results obtained by recent SETI surveys, and other efforts to detect various technosignatures across the electromagnetic spectrum. Through the lens of SETI, we reflect on humanity's current technological trajectory â€“ the modest projections for L suggested here, underscore the critical need to quickly establish regulatory frameworks for AI development on Earth and the advancement of a multiplanetary society to mitigate against such existential threats. The persistence of intelligent and conscious life in the universe could hinge on the timely and effective implementation of such international regulatory measures and technological endeavours.One of the most puzzling results obtained by astronomers over the last 60 years is the non-detection of potential extraterrestrial â€œtechnosignaturesâ€ in astronomical data [e.g. 1-9]. These technosignatures are expected as a consequence of the activities of advanced technical civilisations located in our own and other galaxies e.g. narrowband radio transmissions, laser pulses, transiting megastructures, and waste-heat emission [10-12]. This â€œGreat Silenceâ€, a term introduced by Brin [13], presents something of a paradox when juxtaposed with other astronomical findings that imply the universe is hospitable to the emergence of intelligent life. As our telescopes and associated instrumentation continue to improve, this persistent silence becomes increasingly uncomfortable, questioning the nature of the universe and the role of human intelligence and consciousness within it.\
Various explanations for the great silence, and solutions to the related Fermi paradox [14] have been proposed [15]. The concept of a â€œgreat filterâ€ [16] is often employed â€“ this is a universal barrier and insurmountable challenge that prevents the widespread emergence of intelligent life. Examples of possible great filters are numerous, ranging from the rarity of abiogenesis itself, to the limited longevity of a technical civilization.\
Most recently, Artificial Intelligence (AI) has also been proposed as another potential great filter and explanation for the Fermi Paradox [17,18]. The term AI is used to describe a human-made tool that emulates the â€œcognitiveâ€ abilities of the natural intelligence of human minds [18]. Recent breakthroughs in machine learning, neural networks, and deep learning have enabled AI to learn, adapt, and perform tasks once deemed exclusive to human cognition [19]. As AI rapidly integrates itself into our daily lives, it is reshaping how we interact and work with each other, how we interact with technology and how we perceive the world. It is altering communication patterns and personal experiences. Many other aspects of human society are being impacted, especially in areas such as commerce, health care, autonomous vehicles, financial forecasting, scientific research, technical R&D, design, education, industry, policing, national security and defence [20]. Indeed, it is difficult to think of an area of human pursuit that is still untouched by the rise of AI.\
Many regard the development of AI as one of the most transformative technological developments in human history. In his BBC Reith Lecture (2021), Stuart Russell claimed that â€œthe eventual emergence of generalpurpose artificial intelligence [will be] the biggest event in human history [21]. Not surprisingly, the AI revolution has also raised serious concerns over societal issues such as workforce displacement, biases in algorithms, discrimination, transparency, social upheaval, accountability, data privacy, and ethical decision making [22-24]. There are also concerns about AIs increasing carbon footprint and its environmental impact [25].\
In 2014, Stephen Hawking warned that the development of AI could spell the end of humankind. His argument was that once humans develop AI, it could evolve independently, redesigning itself at an ever-increasing rate [26]. Most recently, the implications of autonomous AI decision-making, have led to calls for a moratorium on the development of AI until a responsible form of control and regulation can be introduced [27].\
Concerns about Artificial Superintelligence (ASI) eventually going rogue is considered a major issue - combatting this possibility over the next few years is a growing research pursuit for leaders in the field [28]. Governments are also trying to navigate a difficult line between the economic benefits of AI and the potential societal risks [29-32]. At the same time, they also understand that the rapid incorporation of AI can give a competitive advantage over other countries/regions â€“ this could favour the early-adoption of innovative AI technologies above safeguarding against the potential risks that they represent. This is especially the case in areas such as national security and defence [33] where responsible and ethical development should be paramount.\
In this paper, I consider the relation between the rapid emergence of AI and its potential role in explaining the â€œgreat silenceâ€. We start with the assumption that other advanced technical civilisations arise in the Milky Way, and that AI and later ASI emerge as a natural development in their early technical evolution. Section 2 addresses the threat posed by AI and section 3 considers how AI will progress in comparison to less welldeveloped mitigating strategies, in particular the development of a multiplanetary capability. Section 4 focuses on the short communicating lifetimes implied for technical civilisations and how this compares with the findings from SETI surveys. Section 5 advocates for the rapid regulation of AI and section 6 presents the main conclusions of the paper.]]></content:encoded></item><item><title>Tapestryâ€™s app can now de-dupe your social feeds</title><link>https://techcrunch.com/2025/04/08/tapestrys-app-can-now-de-dupe-your-social-feeds/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:25:46 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tapestry, a new app designed to organize the open social web, is adding a valuable feature to help people who are keeping up with multiple social networks: It will now remove duplicate posts from your feed. That means if you follow the same person across social networking services like Bluesky and Mastodon, you wonâ€™t have [â€¦]]]></content:encoded></item><item><title>Experiments</title><link>https://hackernoon.com/experiments?source=rss</link><author>Machine Ethics</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:19:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[To investigate the early-bird ticket hypothesis in Transformer models, we conducted experiments on four different architectures: ViT, Swin-T, GPT-2, and RoBERTa. The experiments were performed using the following setup:\
 The experiments were conducted on the Georgia Tech PACE ICE clusters, utilizing A100, A40, or V100 GPUs with a minimum of 32GB memory, depending on availability.\
 The experiments were implemented using PyTorch and Hugging Face libraries for language models.\
 For the vision transformers, we used the CIFAR10 dataset. For fine-tuning the language models, we utilized the IMDB dataset.4.2. Results and Analysis\
For the ViT model, the early-bird ticket was found around epoch 20 [3a]. When retrained, the pruned model with a\
Figure 3. Heatmaps and mask distance plots at p = 0.1 (to the left) and 0.3 (to the right) for all models\
pruning ratio of 0.1 (p=0.1) fully recovered the baseline performance [2a], achieving an accuracy of 84.3% compared to the unpruned baseline of 85.11%. The model with a higher pruning ratio of 0.3 (p=0.3) also came close to the baseline, with an accuracy of 82.05%. These results demonstrate the trade-off between model sparsity and performance, indicating that a moderate pruning ratio can lead to significant resource savings while maintaining comparable accuracy.\
Similar to ViT, the early-bird ticket for the Swin-T model was found around epoch 20 [3b]. When retrained, the p=0.1 model fully recovered the baseline performance, achieving an accuracy of 89.54% compared to the unpruned baseline of 89.5%. Interestingly, the p=0.3 model also managed to recover the baseline, with an accuracy of 88.95%. These results suggest that the Swin-T architecture is particularly well-suited for the early-bird ticket hypothesis, as it can maintain high performance even with significant pruning.\
For GPT-2, we focused on identifying early-bird tickets during the fine-tuning stage. Remarkably, the early-bird ticket was discovered as early as epoch 2 of fine-tuning [3c]. When fine-tuned with pruning, both the p=0.1 and p=0.3 pruned models achieved a validation accuracy of 83.4%, slightly surpassing the unpruned baseline accuracy of 83.3% [2c]. This finding highlights the potential for early-bird tickets to emerge rapidly during the fine-tuning process, enabling efficient adaptation of pre-trained language models to downstream tasks.\
Similar to GPT-2, we identified the early-bird ticket for RoBERTa at epoch 2 [3d] of the fine-tuning stage. When fine-tuned with pruning, the p=0.1 and p=0.3 pruned models achieved validation accuracies of 86.0% and 86.02%, respectively [2d]. Although these accuracies are slightly lower than the unpruned baseline accuracy of 93.9%, the pruned models still maintain a high level of performance while significantly reducing the computational requirements. This phenomnenon could be associated with the architecture of the model and why we see GPT-2 perform in terms of recovered performance. The architectural differences between RoBERTa and GPT-2, such as the use of dynamic masking and a different pre-training objective in RoBERTa, may contribute to the variations in their ability to recover performance after pruning [7].\
In addition to the performance evaluation, we also analyzed the memory usage of the pruned models compared to their unpruned counterparts. Table [1] presents the memory usage comparison for each model. The percent change in memory based on the pruned amounts stayed relatively the same for both 0.1 and 0.3 levels. Moreover, upon testing other metrics such as FLOPs, parameters, and inference time, we noticed no change except for the memory utilization. This is because the models inherently are the same, but their pruned nature reduces the amount of memory that is taken up storing the full precision weights. The unpruned ViT model consumed 157.26 MB of memory, while the pruned models with p=0.1 and p=0.3 required only 83.61 MB, resulting in a significant reduction of 46.8%. Similarly, the Swin-T model achieved a memory usage reduction of 49.0%, with the unpruned model consuming 423.43 MB and the pruned models using 216.03 MB. For the language models, GPT-2 exhibited a memory usage reduction of 20.6%, and RoBERTa demonstrated a notable memory usage reduction of 26.9% comparing the unpruned and p=0.1 pruned models. These results highlight the substantial memory savings achieved through pruning, making the models more efficient in terms of resource utilization while maintaining comparable performance to the unpruned models. A change in pruning method based on specific architecture could yield different results from just pruning the linear layers.\
The experimental results provide strong evidence for the existence of early-bird tickets in Transformer models across both vision and language domains. The early-bird tickets were consistently found within the first few epochs of training or fine-tuning, indicating the potential for significant resource optimization and cost reduction.\
The performance of the pruned models obtained from early-bird tickets was comparable to or even surpassed the unpruned baselines in some cases. This suggests that the early-bird ticket hypothesis holds true for Transformer architectures, and that pruning can be effectively applied to reduce the computational requirements without compromising performance.\
Furthermore, the comparative analysis across different Transformer models highlights the generalizability of the early-bird ticket phenomenon. The successful identification of early-bird tickets in ViT, Swin-T, GPT-2, and RoBERTa demonstrates the applicability of this approach to a wide range of Transformer architectures.\
However, it is important to note that the optimal pruning ratio may vary depending on the specific model and task. While higher pruning ratios can lead to greater resource savings, they may also result in a slight degradation in performance.In this research, we investigated the early-bird ticket hypothesis in Transformer models across vision and language domains. By employing a methodology based on iterative pruning, masked distance calculation, and selective retraining, we successfully identified early-bird tickets in various Transformer architectures. Our experimental results demonstrate that these early-bird tickets can achieve comparable or even superior performance to the unpruned models while significantly reducing the computational re- quirements. The consistent emergence of early-bird tickets within the first few epochs of training or fine-tuning highlights the potential for substantial resource optimization and cost reduction in Transformer model development. This study contributes to the growing body of research on efficient training strategies for Transformer models and paves the way for further exploration of the early-bird ticket hypothesis across a wider range of architectures and tasks. By leveraging the insights gained from this research, practitioners can develop more efficient and accessible Transformer models, enabling their deployment in resource-constrained environments and accelerating the progress of natural language processing and computer vision applications.[1] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural information processing systems, 33:15834â€“ 15846, 2020. 1\
[2] Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu. Earlybert: Efficient bert training via early-bird lottery tickets. arXiv preprint arXiv:2101.00063, 2020. 1\
[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1\
[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1\
[5] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. 1\
[6] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. 1\
[7] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 1, 2, 4\
[8] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019. 1\
[9] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. 1\
[10] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019. 1\
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1\
[12] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. arXiv preprint arXiv:1910.04732, 2019. 1\
[13] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G Baraniuk, Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Towards more efficient training of deep networks. arXiv preprint arXiv:1909.11957, 2019. 1, 2(1) Shravan Cheekati, Georgia Institute of Technology (shravan.cheekati@gatech.edu).]]></content:encoded></item><item><title>How We Found Early-Bird Subnetworks in Transformers Without Retraining Everything</title><link>https://hackernoon.com/how-we-found-early-bird-subnetworks-in-transformers-without-retraining-everything?source=rss</link><author>Machine Ethics</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:15:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In this study, we investigate the early-bird ticket hypothesis in Transformer models using the masked distance metric. Our approach involves exploring the early-bird phenomenon during full training for vision transformers and limiting it to the fine-tuning stage for language models. The methodology consists of the following steps:\
1.  We perform iterative pruning on the Transformer models to identify the subnetworks that can potentially serve as early-bird tickets [13]. The pruning process involves gradually removing the least important weights based on their magnitude.Masked Distance Calculation: To determine the optimal point at which the early-bird ticket emerges, we calculate the masked distance between two consecutive epochs during the training or fine-tuning process. The masked distance metric measures the similarity between the pruned masks of consecutive epochs, providing insights into the stability and convergence of the subnetworks.Early-Bird Ticket Selection: We select the earlybird ticket by identifying the pruned mask that crosses a chosen threshold. The threshold is determined by observing the changes in masked distance across all epochs [13]. For vision transformers, we set a pruning threshold of 0.1, while for text transformers, we use a threshold of 0.01.Retraining and Fine-tuning: After obtaining the final pruned models using the selected early-bird tickets, we retrain the vision transformers and fine-tune the language models to the full epoch length. The retraining process involves training the pruned models from scratch using the same hyperparameters as the original models. For language models, we focus on the finetuning stage, where the pruned models are fine-tuned on downstream tasks [1]. We evaluate the performance of the pruned models obtained from the earlybird tickets and compare their validation accuracy with the unpruned baseline models.\
To conduct a comparative analysis and investigate the applicability of the early-bird ticket hypothesis across different Transformer architectures, we experiment with the following models:Swin-T (Shifted Window Transformer)GPT-2 (Generative Pre-trained Transformer)RoBERTa (Robustly Optimized BERT Pretraining Approach) [7]\
By applying our methodology to these diverse Transformer models, we aim to provide a comprehensive understanding of the early-bird ticket phenomenon in both vision and language domains.\
The proposed methodology addresses the limitations of existing works by introducing a more efficient approach compared to the traditional train-prune-retrain methodology. By leveraging the masked distance metric and selective pruning, we can identify early-bird tickets without the need for extensive retraining. Furthermore, our comparative analysis across different Transformer architectures provides insights into the generalizability of the early-bird ticket hypothesis. Through this methodology, we aim to demonstrate the existence of early-bird tickets in Transformer models and explore their potential for resource optimization and cost reduction in training.(1) Shravan Cheekati, Georgia Institute of Technology (shravan.cheekati@gatech.edu).]]></content:encoded></item><item><title>Transformer Training Optimization via Early-Bird Ticket Analysis</title><link>https://hackernoon.com/transformer-training-optimization-via-early-bird-ticket-analysis?source=rss</link><author>Machine Ethics</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:11:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Shravan Cheekati, Georgia Institute of Technology (shravan.cheekati@gatech.edu).Transformer models have revolutionized the field of natural language processing (NLP) and computer vision (CV) in recent years. Since the introduction of the Transformer architecture by Vaswani et al. [11], these models have achieved state-of-the-art performance on a wide range of tasks, such as machine translation, sentiment analysis, and image classification [3, 4, 7]. The success of Transformers can be attributed to their ability to capture long-range dependencies and their scalability to large amounts of data [11]. However, the training of Transformer models is resource-intensive and time-consuming, requiring significant computational power and energy consumption [10]. To address this issue, various techniques have been proposed to optimize the training process and reduce the computational requirements of Transformer models [9,12]. One promising approach is the early-bird ticket hypothesis, which suggests that subnetworks capable of matching the performance of fully-trained networks can be identified early in the training process [5]. This hypothesis has been successfully applied to CNNs, leading to significant resource optimization and cost reduction in their training [1, 13]. However, the applicability of the early-bird ticket hypothesis to Transformer models has not been extensively explored. In this research, we investigate the early-bird ticket hypothesis in Transformer models, focusing on vision transformers and language models. By identifying early-bird tickets in these architectures, we aim to optimize the training process and reduce the computational requirements, making Transformer models more accessible and efficient.The early-bird ticket hypothesis was first introduced by Frankle et al. [5] in the context of CNNs. They discovered that subnetworks capable of matching the performance of fully-trained networks could be identified early in the training process. This finding has led to the development of various techniques to identify and exploit early-bird tickets in CNNs [1, 13]. In the domain of Transformers, there have been limited explorations of the early-bird ticket hypothesis. One notable work is EarlyBERT by Kovaleva et al. [2], which investigated the applicability of the early-bird ticket hypothesis to BERT. They found that early-bird tickets exist in BERT and can be used to optimize the fine-tuning process. However, their work focused solely on BERT and did not provide a comparative analysis across different Transformer architectures. Other works have explored various techniques to optimize the training and inference of Transformer models. For example, Michel et al. [8] proposed a method to prune attention heads in Transformers, reducing the computational requirements while maintaining performance. Sanh et al. [9] introduced DistilBERT, a distilled version of BERT that achieves comparable performance with fewer parameters and faster inference times. Despite these efforts, the potential speedup and resource optimization achievable through the early-bird ticket hypothesis in Transformers have not been fully explored. Many existing works rely on the slow and rigorous process of the train-prune-retrain methodology [6], which can be time-consuming and resource-intensive. In this research, we aim to address these limitations by investigating the early-bird ticket hypothesis across different Transformer architectures, including vision transformers and language models. We explore efficient methods to identify early-bird tickets and evaluate their performance in comparison to fully-trained models. Our goal is to provide insights into the applicability of the early-bird ticket hypothesis in Transformers and contribute to the development of more efficient training strategies for these powerful models.(1) Shravan Cheekati, Georgia Institute of Technology (shravan.cheekati@gatech.edu).]]></content:encoded></item><item><title>We Tried Fire Opal on a Quantum GAN: Here&apos;s What Happened</title><link>https://hackernoon.com/we-tried-fire-opal-on-a-quantum-gan-heres-what-happened?source=rss</link><author>Algorithmize</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:06:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[To understand how a blackbox model of error suppression and circuit optimization can be used to enhance the data loading process on NISQ hardware, we performed the inference based on our pretrained C-QGAN circuit on IBM Kyoto with and without Fire Opal. Before running these tests, we trained circuits using a quantum simulator to obtain optimal parameters and then tested on actual hardware. The results were then compared against the\
outputs generated on Qiskit statevector simulator, which theoretically generates perfect results without noise. The performance of Fire Opal was then quantified by using the Kullback-Leibler (KL) divergence to quantify the difference between the the simulator results and the results on quantum hardware with and without Fire Opal.\
Four distributions corresponding to different timesteps were loaded with controlled registers corresponding to respective time steps given in [1]; the circuit is displayed in Figure 1. Specifically, as shown in Fig. 1, the circuit is composed of generator registers (5 qubits) and controlled registers (2 qubits), whereby the generator anstaz is based on the Two-Local circuit in Qiskit. The Two-Local subprocess consists of two layers of Ry gates and a layer of one-to-all control-Z gates in-between the Ry layers. In our experiments, we fully entangled all the qubits. Further explanation of the algorithm and circuit structures can be found in [1]\
We used two different circuits with increasing depths to understand the performance of Fire Opal with increasing number of two-qubit gates by increasing the depth of the ansatz layer in the generator. Specifically, two different circuit depths were tested. The moderate circuit depths contained a total number of gates at 120 (40 two-qubit gates), and much deeper circuit with 205 (50 two-qubit gates). In addition, the moderate circuit was sampled with 4096 shots, and the longer circuit was sampled with 8000 shots.\
As our baseline, we ran the C-QGAN circuits on IBM Kyoto without error suppression techniques. In order for the quantum circuits to be executed on quantum hardware, the original circuits need to be transpiled to match IBM Kyotoâ€™s topology, and its native gates. To maximize the performance, we aimed to reduced the total number of gates, especially the two-qubit gates to reduce the noise contribution. As a result, we used the highest level of optimization possible, which is level 3. This resulted in a total number of native gates of 784 for the moderate depth circuit, and 997 for the longer circuit. In addition, since this is the baseline experiments, we did not include any error mitigation so that we can quantitatively asses the performance of Fire Opal. When the circuits were ran with Fire Opal, the original circuits were used as input without transpilation since Fire Opal has a function for circuit optimization. To activate Fire Opal the function on IBM cloud, the settings were changed on the instance to â€œq ctrlâ€.IV. RESULTS AND DISCUSSIONSAnalysis of testing results shows clear improvement in the Fire Opal distribution compared to IBM Kyoto by itself; see Figure 3 for a visual of the analysis. Fire Opal reduced noise and yielded better overall results on IBM Kyoto by around 30% âˆ’ 40%. Specifically, the distributions generated on IBM Kyoto with Fire Opal qualitatively captures the specific shape of the distribution with peaks and the long tail. This becomes more pronounced as the number of two-qubit gates.\
In conclusion, our experiment validates the effectiveness of Fire Opalâ€™s error suppression and circuit optimization capabilities, highlighting potential to enhance the utilities of quantum hardware in the NISQ era.[1] S. Certo, A. Pham, N. Robles, and A. Vlasic, Conditional generative models for learning stochastic processes, Quantum Machine Intelligence 5, 42 (2023).\
[2] A. Montanaro, Quantum speedup of monte carlo methods, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 471, 20150301 (2015).\
[3] L. Grover and T. Rudolph, Creating superpositions that correspond to efficiently integrable probability distributions, arXiv preprint quant-ph/0208112 (2002).\
[4] M. Mirza and S. Osindero, Conditional generative adversarial nets, arXiv preprint arXiv:1411.1784 (2014).\
[5] S. Herbert, The problem with grover-rudolph state preparation for quantum monte-carlo, arXiv preprint arXiv:2101.02240 (2021).\
[6] S. Bhattacharjee, S. Sarkar, K. Das, and B. Sarkar, Enhancing quantum variational algorithms with zero noise extrapolation via neural networks, arXiv preprint arXiv:2403.07025 (2024).\
[7] Z. Cai, R. Babbush, S. C. Benjamin, S. Endo, W. J. Huggins, Y. Li, J. R. McClean, and T. E. Oâ€™Brien, Quantum error mitigation, Rev. Mod. Phys. 95, 045005 (2023).\
[8] T. Giurgica-Tiron, Y. Hindy, R. LaRose, A. Mari, and W. J. Zeng, Digital zero noise extrapolation for quantum error mitigation, in 2020 IEEE International Conference on Quantum Computing and Engineering (QCE) (IEEE, 2020) pp. 306â€“316.\
[9] A. Kandala, K. Temme, A. D. CÂ´orcoles, A. Mezzacapo, J. M. Chow, and J. M. Gambetta, Error mitigation extends the computational reach of a noisy quantum processor, Nature 567, 491 (2019).\
[10] R. Majumdar, P. Rivero, F. Metz, A. Hasan, and D. S. Wang, Best practices for quantum error mitigation with digital zero-noise extrapolation, in 2023 IEEE International Conference on Quantum Computing and Engineering (QCE), Vol. 1 (IEEE, 2023) pp. 881â€“887.\
[11] E. Van Den Berg, Z. K. Minev, A. Kandala, and K. Temme, Probabilistic error cancellation with sparse pauliâ€“lindblad models on noisy quantum processors, Nature Physics 19, 1116 (2023).\
[12] Y. Ma and M. Kim, Limitations of probabilistic error cancellation for open dynamics beyond sampling overhead, Physical Review A 109, 012431 (2024).\
[13] B. McDonough, A. Mari, N. Shammah, N. T. Stemen, M. Wahl, W. J. Zeng, and P. P. Orth, Automated quantum error mitigation based on probabilistic error reduction, in 2022 IEEE/ACM Third International Workshop on Quantum Computing Software (QCS) (IEEE, 2022) pp. 83â€“93.\
[14] R. Takagi, S. Endo, S. Minagawa, and M. Gu, Fundamental limits of quantum error mitigation, npj Quantum Information 8, 114 (2022).\
[15] A. Hashim, R. K. Naik, A. Morvan, J.-L. Ville, B. Mitchell, J. M. Kreikebaum, M. Davis, E. Smith, C. Iancu, K. P. Oâ€™Brien, et al., Randomized compiling for scalable quantum computing on a noisy superconducting quantum processor, arXiv preprint arXiv:2010.00215 (2020).\
[16] S. Chen, Y. Liu, M. Otten, A. Seif, B. Fefferman, and L. Jiang, The learnability of pauli noise, Nature Communications 14, 52 (2023).\
[17] Z. Cai, X. Xu, and S. C. Benjamin, Mitigating coherent noise using pauli conjugation, npj Quantum Information 6, 17 (2020).\
[18] Z. Cai and S. C. Benjamin, Constructing smaller pauli twirling sets for arbitrary error channels, Scientific reports 9, 11281 (2019).\
[19] J. P. Santos, B. Bar, and R. Uzdin, Pseudo twirling mitigation of coherent errors in non-clifford gates, arXiv eprints , arXiv (2024).\
[20] C. Kim, K. D. Park, and J.-K. Rhee, Quantum error mitigation with artificial neural network, IEEE Access 8, 188853 (2020).\
[21] M. R. Geller, Rigorous measurement error correction, Quantum Science and Technology 5, 03LT01 (2020).\
[22] B. Nachman, M. Urbanek, W. A. de Jong, and C. W. Bauer, Unfolding quantum computer readout noise, npj Quantum Information 6, 84 (2020).\
[23] A. M. Palmieri, E. Kovlakov, F. Bianchi, D. Yudin, S. Straupe, J. D. Biamonte, and S. Kulik, Experimental neural network enhanced quantum tomography, npj Quantum Information 6, 20 (2020).\
[24] P. Czarnik, A. Arrasmith, P. J. Coles, and L. Cincio, Error mitigation with clifford quantum-circuit data, Quantum 5, 592 (2021).\
[25] A. Lowe, M. H. Gordon, P. Czarnik, A. Arrasmith, P. J. Coles, and L. Cincio, Unified approach to data-driven quantum error mitigation, Physical Review Research 3, 033098 (2021).\
[26] B. Pokharel, S. Srinivasan, G. Quiroz, and B. Boots, Scalable measurement error mitigation via iterative bayesian unfolding, Physical Review Research 6, 013187 (2024).\
[27] H. Wang, Y. Xue, Y. Qu, X. Mu, and H. Ma, Multidimensional bose quantum error correction based on neural network decoder, npj Quantum Information 8, 134 (2022).\
[28] D. F. Locher, L. Cardarelli, and M. MÂ¨uller, Quantum error correction with quantum autoencoders, Quantum 7, 942 (2023).\
[29] A. Chalkiadakis, M. Theocharakis, G. Barmparis, and G. Tsironis, Quantum neural networks for the discovery and implementation of quantum error-correcting codes, Chaos: An Interdisciplinary Journal of Nonlinear Science 33 (2023).(1) Anh Pham, Deloitte Consulting LLP;(2) Andrew Vlasic, Deloitte Consulting LLP.:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Testing LLMs on Solving Leetcode Problems in 2025 (4/8/2025)</title><link>https://hackernoon.com/4-8-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:05:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[ðŸª Whatâ€™s happening in tech today, April 8, 2025?By @jelmer [ 5 Min read ] In 2024, nearly one in five online purchases ended up being returned. Read More.By @albertlieyeongdok [ 6 Min read ] Discover 8 powerful tools transforming prompt engineering from trial-and-error into scalable systemsâ€”featuring visual workflows, auto-tuned prompts, and memory- Read More.By @bigmao [ 9 Min read ] Muddyâ€™s short only works if no one understands the stack. This is the line-by-line dismantling that makes their whole bet fall apart. Read More.By @mediabias [ 11 Min read ] See how Large Language Models creatively adapt language strategies under supervision, effectively evading detection and communicating covert information. Read More.By @alexsvetkin [ 9 Min read ] Large-scale LLMs test (o1, o3-mini; Gemini 2.0 Flash, 2.0 Pro, 2.5 Pro; DeepSeek V3, R1; xAI Grok 2; Claude 3.7 Sonnet) on solving Leetcode algorithmic problems Read More.ðŸ§‘â€ðŸ’» What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team âœŒï¸]]></content:encoded></item><item><title>Which Quantum Noise Mitigation Technique Is Right for Your Circuit?</title><link>https://hackernoon.com/which-quantum-noise-mitigation-technique-is-right-for-your-circuit?source=rss</link><author>Algorithmize</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:03:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[II. OVERVIEW OF ERROR MITIGATION METHODSFor completeness, we list and describe different class of error mitigation. Currently, the established methods for error mitigation lie in one of the five classes:Zero Noise Extrapolation (ZNE);Probabilistic Error Cancellation (PEC);Measurement Error Mitigation; and,Machine Learning techniques.\
Each of the error mitigation classes have their respective advantages and disadvantages, ranging from ease of implementation and minimal number of assumptions, and increased time to run an algorithm and exponential increase of gates to circuits. Below the advantage and shortcomings of each class is given.\
. : ZNE works with unknown noise models, and it can be used with different algorithms, including variational methods. In addition, it can be applied through digital or analog modes. However, assuming the noise model is time invariant, ZNE needs to run the experiment several times. Moreover, ZNE can be sensitive\
to the methods used to amplify noise and needs to be used with other error mitigation methods since it wonâ€™t be able to correct for error from state preparation and measurement (SPAM) [6, 8â€“10].\
: PEC can accurately correct for noise for an arbitrary number of qubits and is especially helpful for local dephasing noise and crosstalk errors. Given these advantages, PEC sampling overhead scales exponentially with error rates and circuit depth. Application may be limited to noise cancellation in simulating open quantum dynamics [11â€“14].\
Measurement Error Mitigation A general assumption about the noise, with some restrictions; only the expectation is analyzed, making the technique purely classical; there are different implementations including the assumption of independence, modeling the assignment matrix with a continuous time Markov process, solving via constrained optimization, or using classical neural network to parse the data and separate the noise. But, overhead increases exponentially with the circuit size or number of qubits, has added steps increase time to solution, and some methods require a trained classical model [20â€“26].\
e. Machine Learning Techniques The techniques leverage neural network architecture to mitigate the noise, using a minimal number of assumptions about the noise. Furthermore, pretrained networks allow for transfer learning to new circuits, considerably decreasing time to retrain the network. Autoencoders tend to play a prominent role in this technique. However, machine learning adds at least polynomial growth to the length of the circuit through the extra gates added to the circuit, and is a variational model that requires data and time to train. Finally, statistical models have a large potential to be undertrained and biased [27â€“29].(1) Anh Pham, Deloitte Consulting LLP;(2) Andrew Vlasic, Deloitte Consulting LLP.:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>How Fire Opal Makes Quantum Data Loading Smarter and More Accurate</title><link>https://hackernoon.com/how-fire-opal-makes-quantum-data-loading-smarter-and-more-accurate?source=rss</link><author>Algorithmize</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:00:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Anh Pham, Deloitte Consulting LLP;(2) Andrew Vlasic, Deloitte Consulting LLP.\
The study examines the efficacy of the Fire Opal error suppression and AI circuit optimization system integrated with IBMâ€™s quantum computing platform for a multi-modal distribution loading algorithm. Using Kullback-Leibler (KL) divergence as a quantitative error analysis, the results indicate that Fire Opal can improve on the time-dependent distributions generated by our Conditional Quantum Generative Adversarial algorithm by 30-40% in comparison with the results on the simulator. In addition, Fire Opalâ€™s performance remains consistent for complex circuits despite the needs to run more trials. The research concludes that Fire Opalâ€™s error suppression and circuit optimization significantly enhanced quantum computing processes, highlighting its potential for practical applications. In addition, the study also reviews leading error mitigation strategies, including zero noise extrapolation (ZNE), probabilistic error cancellation (PEC), Pauli twirling, measurement error mitigation, and machine learning methods, assessing their advantages and disadvantages in terms of technical implementation, quantum resources, and scalability.Data loading is critical for many quantum algorithms and applications. However, it is a challenging problem when executing on NISQ quantum hardware due to the longer circuit depth in this quantum subroutine. In this report, we demonstrate the utility of error suppression and AI-enabled circuit optimization in Fire Opal to load multi-modal distributions on IBM Kyoto as implemented in our Conditional Quantum Generative Adversarial Network (C-QGAN) algorithm. Specifically, the distributions generated with Fire Opal on quantum hardware produced results much closer to the ideal distributions, and showed an approximate 30% âˆ’ 40% improvement in results relative to running without it based KL divergence analysis.\
Previously we proposed a new quantum algorithm known as C-QGAN [1] to load multiple non-uniform distributions using condition registers. This data loading technique was then applied in conjunction with quantum amplitude estimation [2] to evaluate complex financial instrument called Asian option. The state preparation based on C-QGAN was shown to be less computationally expensive than other well known technique like GroverRudolph [3] and QGAN [4] which can potentially negate the quadratic speedup of algorithm like QAE [5] when they are approximating stochastic processes.\
Due to the noisy nature of current NISQ hardware, various error mitigation techniques have been applied to enable improve the outputs when quantum algorithms are run on quantum hardware. Error mitigation techniques are post-processing algorithms, which happen at the software level, that can improve on the distorted values obtained from quantum hardware due to different noise sources. In addition, these techniques have been applied to improve in many variational quantum algorithms [6] which have many important applications in quantum machine learning, chemistry and optimization. However, a potential drawbacks of many error mitigation techniques can be that they require extra classical and quantum computational resources to recover the noisefree values, which limits their applications only for shortdepth circuits [7]. As a result, our report aims to explore error suppression technique, which happens at the hardware level, as implemented in Fire Opal to understand their utility in the state-preparation process.:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>The Platform Built for MENA: Why amana Isnâ€™t Just Another Trading and Investing App</title><link>https://hackernoon.com/the-platform-built-for-mena-why-amana-isnt-just-another-trading-and-investing-app?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:00:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
In a space where trading and investing platforms often blend into one anotherâ€”riding the same waves of hype, offering the same features with a different color paletteâ€”itâ€™s easy to overlook the story behind the code. But for , a homegrown neobroker operating out of the MENA region, the story behind the screen is anything but ordinary.\
Since launching in 2022, amana has quietly amassed over 350,000 users, a feat not driven by viral marketing or flash-in-the-pan features, but by a different kind of ambitionâ€”to build a trading experience that belongs to the region it serves.\
â€œAt the start, we werenâ€™t updating a legacy system or layering features onto an old foundation,â€ says Mani Sahota, Chief Information Officer at amana. â€œWe began from scratch, and that gave us the freedomâ€”and the responsibilityâ€”to make smart choices from the very first line of code.â€\
The teamâ€™s early decision to go all-in on cloud-native architecture wasnâ€™t about buzzwords. It was a strategic move that allowed the platform to flex as demand grew. For a user base thatâ€™s mobile-first, tech-savvy, and expects seamless performance, that flexibility wasnâ€™t a luxuryâ€”it was essential.\
â€œSpeed and stability werenâ€™t things we planned to improve over time,â€ Sahota adds. â€œThey had to be baked into the core experience from the beginning.â€\
But amanaâ€™s evolution isnâ€™t just about whatâ€™s under the hood. Itâ€™s also about what users seeâ€”and feel. The platformâ€™s clean design, smooth performance, and straightforward flow reflect a key principle: technology should feel empowering, not overwhelming.\
That user-centric mindset is what led to one of amanaâ€™s boldest moves yetâ€”expanding its crypto asset list to over 450 coins, more than any other broker in the MENA region. Itâ€™s not a gimmick; itâ€™s a response to how people actually trade today.\
â€œTraders donâ€™t want to bounce between five apps,â€ Sahota explains. â€œThey want to access stocks, crypto, forexâ€”everythingâ€”in one place. And we believed we could build that without forcing people to choose between traditional assets and the future of finance.â€\
While many brokers have dabbled cautiously in crypto,  has committed fullyâ€”bridging both sides of the market. Users can buy into Bitcoin, trade Tesla shares, or hold a diversified ETF, all from a single dashboard.\
Itâ€™s a hybrid model that works because itâ€™s built with choice in mind. Whether someoneâ€™s chasing short-term trends or investing with a decade-long view, amanaâ€™s platform doesnâ€™t funnel them down one pathâ€”it opens all of them.\
Looking ahead, the roadmap is equally ambitious. AI-driven insights are on the way, offering smarter, more personalized support. Localization efforts are underway as the platform expands into new markets. But growth isnâ€™t the endgameâ€”redefining what a broker can be in MENA is.\
â€œThereâ€™s a real shift happening in the region,â€ Sahota says. â€œPeople want financial tools that speak their languageâ€”literally and digitally. Thatâ€™s where we come in.â€\
What makes amana compelling isnâ€™t just what itâ€™s builtâ€”itâ€™s how itâ€™s built. Quietly. Intentionally. With an understanding of the cultural, financial, and emotional contexts that define the MENA trading experience.\
In a region where trust in financial systems hasnâ€™t always come easy, creating something reliable isnâ€™t just good techâ€”itâ€™s a kind of service. A new baseline. A sign that a different kind of broker is not only possible, but already here.]]></content:encoded></item><item><title>Shopify CEO Says Staffers Need To Prove Jobs Can&apos;t Be Done By AI Before Asking for More Headcount</title><link>https://tech.slashdot.org/story/25/04/08/1518213/shopify-ceo-says-staffers-need-to-prove-jobs-cant-be-done-by-ai-before-asking-for-more-headcount?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 16:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Shopify CEO Tobi Lutke is changing his company's approach to hiring in the age of AI. Employees will be expected to prove why they "cannot get what they want done using AI" before asking for more headcount and resources, Lutke wrote in a memo to staffers that he posted to X. From a report: "What would this area look like if autonomous AI agents were already part of the team?" Lutke wrote in the memo, which was sent to employees late last month. "This question can lead to really fun discussions and projects." Lutke also said there's a "fundamental expectation" across Shopify that employees embrace AI in their daily work, saying it has been a "multiplier" of productivity for those who have used it. 

"I've seen many of these people approach implausible tasks, ones we wouldn't even have chosen to tackle before, with reflexive and brilliant usage of AI to get 100X the work done," Lutke wrote. The company, which sells web-based software that helps online retailers manage sales and run their operations, will factor AI usage into performance reviews, he added.]]></content:encoded></item><item><title>On the Structure of Burch Ideals and Their Duals in Local Rings</title><link>https://hackernoon.com/on-the-structure-of-burch-ideals-and-their-duals-in-local-rings?source=rss</link><author>Periodicity</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:51:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[[3 Proof of Generalized Burch Index Theorem]()[5 Future Work and References]()\
Consider a minimal free resolution of a module M over a local Noetherian ring R. Over such rings, resolutions are often infinite, for example by the The Auslander-Buchsbaum formula when depth(R) = 0 [1]. The question of periodicity in infinite resolutions is the subject of intensive research for example in the works of Eisenbud, Peeva, and Gasharov, and the central survey of Avramov [6, 5, 7].\
We will consider iterated Burch IndicesIn this section we will flesh out some of the details of the introduction, and prove some initial results. Throughout this paper, we let (S, n, k) be a regular local ring, and for an ideal I âŠ‚ S write R = S/I as a local ring (R, m, k). The main thrust of the paper of Eisenbud and Dao is to consider ideals of the form\
called the  [3]. Eisenbud and Dao restrict to the case where depth(S/I) = 0 and I 6= 0 so that\
This allows us to define the Burch index as\
We begin weakening the restriction on the original Burch Ideal definitioin. However, we still restrict to the case where I 6= 0 for non-triviality, and remark that periodicity of 1Ã—1 minors is well understood in the regular local ring case. We initially care about cases where depth(S/I) = 0. The reason is twofold. First, from the Auslander-Buchsbaum formula, if the projective dimension of M is finite, then\
pd(M) + dim R = depth(M)\
Thus if the projectve dimension of M is finite, M is free. Second, this condition, along with I 6= 0, ensures that (I : n) is a proper ideal of R, as it is well known a local Noetherian ring R is depth 0 iff xm = 0 for some nonzero x âˆˆ R. In particular this allows us to form the bounds\
However, we will also consider positive depth rings R in this paper, in which case BI(I) = R since (I : n) = I. When definitions and theorems differ for positive depth rings, we will make a disclaimer.\
Let I, N âŠ‚ S be ideals. We introduce\
ote that unlike in the normal Burch ideal case, BIN (I) is not necessarily contained in N, even in the case of depth 0. This is because (I : N) âŠƒ (I : J) for all J âŠƒ N, and this containment need not be strict. Thus let J â€² = âˆªJ for all J with (I : N) = (I : J). We have that\
 If Burch(I) 6= 0, then depth(R) = 0.2 Burch Duality and Burch Closure3 Proof of Generalized Burch Index TheoremWe are now equipped to prove the result on generalized Burch Index. First we need a helper lemma.\
\
This allows us to prove the following major lemma. After reading through the lemma, consider again this remark:\
\
In other words, under certain conditions, we can check whether the module M has N-periodicity by considering column-wise Burch periodicity of the ideals Q, which have unmixed columns as the minimal generators of Q.\
 We follow a similar proof as Lemma 4. We first show[1] Maurice Auslander and David A Buchsbaum. â€œHomological dimension in local ringsâ€. In: Transactions of the American Mathematical Society 85.2 (1957), pp. 390â€“405.\
[2] Michael K Brown, Hailong Dao, and Prashanth Sridhar. â€œPeriodicity of ideals of minors in free resolutionsâ€. In: arXiv preprint arXiv:2306.00903 (2023).\
[3] Hailong Dao and David Eisenbud. â€œLinearity of free resolutions of monomial idealsâ€. In: Research in the Mathematical Sciences 9.2 (2022), p. 35.\
[4] Hailong Dao, Toshinori Kobayashi, and Ryo Takahashi. â€œBurch ideals and Burch ringsâ€. In: Algebra & Number Theory 14.8 (2020), pp. 2121â€“2150.\
[5] David Eisenbud. â€œHomological algebra on a complete intersection, with an application to group representationsâ€. In: Transactions of the American Mathematical Society 260.1 (1980), pp. 35â€“64.\
[6] Juan Elias et al. Six lectures on commutative algebra. Vol. 166. Springer Science & Business Media, 1998.\
[7] Vesselin N Gasharov and Irena V Peeva. â€œBoundedness versus periodicity over commutative local ringsâ€. In: Transactions of the American Mathematical Society 320.2 (1990), pp. 569â€“580.\
[8] Daniel R. Grayson and Michael E. Stillman. Macaulay2, a software system for research in algebraic geometry. Available at http://www2.macaulay2.com.:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Google fixes two Android zero-day bugs actively exploited by hackers</title><link>https://techcrunch.com/2025/04/08/google-fixes-two-android-zero-day-bugs-actively-exploited-by-hackers/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:42:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The most severe security bug can be exploited without user interaction, per Google.]]></content:encoded></item><item><title>Amazonâ€™s Zoox begins robotaxi testing in Los Angeles</title><link>https://techcrunch.com/2025/04/08/amazons-zoox-begins-robotaxi-testing-in-los-angeles/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Zoox, Amazonâ€™s autonomous vehicle unit, is deploying a small fleet of retrofitted test vehicles on the streets of Los Angeles starting Tuesday â€” a modest, yet meaningful step as the company inches toward offering public rides in Las Vegas and San Francisco later this year.Â  The data-collection effort marks Zooxâ€™s entrance into its sixth city [â€¦]]]></content:encoded></item><item><title>Take the stage at TechCrunch Disrupt 2025: Apply to speak now</title><link>https://techcrunch.com/2025/04/08/take-the-stage-at-techcrunch-disrupt-2025-apply-to-speak-now/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[If youâ€™re reading this, consider it your signal: The opportunity to influence the future of tech is here. TechCrunch Disrupt 2025 is now accepting speaker applications. Join us onstage this October 27â€“29 at Moscone West in San Francisco and share your ideas with 10,000+ tech leaders, VCs, and innovators shaping whatâ€™s next. Apply to speak [â€¦]]]></content:encoded></item><item><title>Micron To Impose Tariff-Related Surcharge on SSDs, Other Products</title><link>https://hardware.slashdot.org/story/25/04/08/154257/micron-to-impose-tariff-related-surcharge-on-ssds-other-products?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Micron has informed US customers it will implement surcharges on memory modules and solid-state drives starting Wednesday to offset President Trump's new tariffs, according to Reuters. While semiconductors received exemptions in Trump's recent trade action, memory storage products didn't escape the new duties. 

Micron, which manufactures primarily in Asian countries including China and Taiwan, had previously signaled during a March earnings call that tariff costs would be passed to customers.]]></content:encoded></item><item><title>ChangeNOW: How to Launch a Fully Functional Crypto Exchange in 3 Days For Free</title><link>https://hackernoon.com/changenow-how-to-launch-a-fully-functional-crypto-exchange-in-3-days-for-free?source=rss</link><author>HackerNoon Press Releases</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:00:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Every entrepreneur knows that crypto can be a real playground for quick wins, but setting up your own exchange is not something that crosses everybodyâ€™s mind. For those who do think about it, they often hit a self-limiting wall of beliefs pretty quickly, and thatâ€™s totally normal.\
â€œYou need millions just for development.â€â€œMonths wasted hiring coders.â€â€œAnd donâ€™t forget all the red tape and compliance issues!â€â€œI could never compete with the big exchangesâ€\
The  thing stopping you from making your own exchange isâ€¦ !\
Actually, the truth is that youâ€™re already fully capable, and you can launch your own crypto exchange faster than you can say â€œdecentralized financeâ€. Well, kind of. Thatâ€™s a bit of an exaggeration. But, in a few days you can be up and running with very little code. Magic.\
Letâ€™s get straight into it.How on Earth is this Possible?It all starts with ChangeNOWâ€™s crypto exchange API. Using this, you can launch your own exchange in . You can also do it for free (although a little spend on web design and web hosting doesnâ€™t hurt).\
This isnâ€™t some pie-in-the-sky ponzi-shaped promise, itâ€™s a reality thatâ€™s working for dozens of quick-thinking entrepreneurs who are already cashing in on our API (you can read up on our real-life case studies below).\
Letâ€™s break down exactly how youâ€™ll join them.Why Building an Exchange Yourself Sucks (But It Doesnâ€™t Have To)The traditional route to becoming an exchange owner is a total nightmare.\
If you donâ€™t believe us, hereâ€™s how a typical build might goâ€¦: $50kâ€“$200k+ upfront, just for basic dev work (and other unexpected costs): 6 to 12 months (if youâ€™re lucky), while your competitors beat you to the pie: Hiring developers who might vanish mid-project (farewell frens!): Struggling to match rates because youâ€™re starting from scratch (the cold start problem) and donâ€™t have access to reliable liquidity providers.\
Hereâ€™s a look at the ChangeNOW API setup process, to show you itâ€™s really not rocket science.Step 1: Grab our Crypto Exchange API (itâ€™s so easy)\
You donâ€™t need expensive hardware or massive development teams! Just use ChangeNOWâ€™s open-source crypto exchange API or plugin and let us take the wheel.\
Weâ€™ve done most of the work for you and made our platform fully open-source. You donâ€™t even need to mention us to your users.The API connects users directly to your domain (e.g. YourCoolExchange.com).You and your users gain instant access to 1250+ coins/tokens, from ETH/BTC/XRP right down to the strangest meme coins on the market.All assets are inter-exchangeable, cross-chain permitted, and include rare token swaps that you wouldnâ€™t usually see on big exchanges.Enjoy built-in aggregated liquidity from top CEX/DEX platforms (like Binance/Uniswap) so you donâ€™t have to spend hours and hours scraping exchanges or begging rich dudes for funds.\
 Itâ€™s funny you asked. Nothing. Nada. Zilch!\
Youâ€™re  dollars out of pocket.\
(Like we said though, you might want to spend a bit to make your exchange website look attractive and legitimate, and get yourself a nice domain and hosting because YourCoolExchange.wixsite.com probably wonâ€™t perform quite as well).\
No party starts without an invite. Yours is our .\
There are two ways to claim this key:New here? Shoot an email to partners@changenow.io with â€œAPI Accessâ€ in the subject line and weâ€™ll hand it over ASAP.Step 2: Customize Like a Boss (Optional but Cool to Have)Need to track users or add custom data? Weâ€™ve got you covered with some slick little free code upgrades:userID: Tag each transaction with a unique user ID (e.g., â€œuser123â€)payload fields: Tuck up to 5 custom fields (max 64 characters each) into transactions.Step 3: Enhance Your Security (Private Keys for Paranoid Pros)Dearest future Web3 exchange founders, you can never be too safe.\
If youâ€™re building a public-facing app (like a Telegram bot with ChangeNOW API integration), or sharing code with third parties, itâ€™s time to go ultra-safe:\
Use a Private API key instead. This lets you block access to your transaction history while still using most API methods.Once youâ€™ve got your Private API key, you will use your standard key for everything except accessing your transaction history.â€œGet the transaction historyâ€ will use the private key instead.\
Finally, thereâ€™s no need to worry about bottlenecks or congestion when your exchange goes viral overnight - ChangeNOWâ€™s API offers:1,800 API calls per minute\
Translation? Smooth and snappy.\
Remember that $200k, year-long exchange build we mentioned? Yeah? Well thatâ€™s  Weâ€™re going to get you and your custom-branded exchange to market in less than 3 days, with no costs, and .\
Thank us when the commissions start pouring in.Workflow Breakdown: How Your Exchange Actually WorksOnce youâ€™ve got your API, itâ€™s time to create some magic cash using the â€œstandard flowâ€.\
: If you're not a developer, feel free to skip to the next section â€˜Plug-and-Play Example: Building A â€œSwap BTC To ETHâ€ Buttonâ€™.\
Use â€œList of available currenciesâ€Â  to spit out every coin your users want (ETH, BTC, DOGE, etc) and get them to pick the pair they want to trade.\
Hit â€œ, which tells users â€œMinimum swap = $17 worth of BTCâ€, for example. The userâ€™s exchange amount needs to be higher than this figure, naturally.\
The API Call â€œtâ€, will show the rate of exchange, such as 1 BTC = 50 ETH. The UI will then display this to the user and ask for confirmation.\
The UI will also ask the user for their wallet address to send the funds after the exchange is completed, as well as their refund address, extra ID, and refund extra ID. The Extra IDâ€™s arenâ€™t always required.Generate their deposit address:\
The next API call is the â€œCreate Exchange Transactionâ€, and BOOM! A fresh wallet address pops up (e.g., â€œBTC address: 1A1B2c3dâ€¦â€).\
You ask the user to send their coins, and when they do, ChangeNOW handles the blockchain confirmation, before sending swapped crypto to their wallet auto-magically. Bosh!\
Use the API called â€œâ€ to let users track their swap like an Amazon delivery package (â€œYour $ETH is en route! Estimated arrival: 3 blocksâ€¦â€).\
Right, letâ€™s move on to a pretty sweet example.Plug-and-Play Example: Building A â€œSwap BTC To ETHâ€ ButtonImagine youâ€™re a no-code wizard adding crypto swaps to your site via an API to make absolute bank. This is exactly what you need to do to start the lambo fund.Drop our plugin shortcode and input your API key.Select the trading pair you want to offer, letâ€™s say $BTC - $ETH.Set your fee - itâ€™s fully customizable. If you pick 0.3%, for example, youâ€™ll make $30 profit per every $10k swapped on your exchange.Thatâ€™s it, youâ€™re set up. Nice work. Now watch the fees roll in.\
 Youâ€™ve just seen that a rival exchange is charging 0.25% on ETHâ€“SOL swaps. Not bad, but you can do better. You set yours to 0.20% and put a tweet out to your loyal audience via X, Discord, and Telegram. â€œGang, weâ€™ve dropped our ETH-SOL fees to 0.20% for a limited time. WAGMI!â€\
Suddenly, users flock to you, and the healthy passive income racks up and up.\
 Donâ€™t forget to account for other fees when setting your commission. This includes ChangeNOW service fee as well as gas fees for various networks.Nah, Surely it Canâ€™t be That Easy?!Surely it can! Thatâ€™s what ChangeNOW delivers, flipping the script on our competitors who want to lock users into their exchange platforms instead of liberating them and creating an army of valued partners and ambassadors. We want to empower, not disarm, the new generation of wealthy Web3 exchange founders.\
You watch the profits roll in, while we do the heavy lifting:Get instant liquidity from our network (so no empty wallets or broken links).Pay zero maintenance fees, as we handle server uptime/security updates etc.Remember, youâ€™re paying (a share of the commission) only when users actually make swaps. ChangeNOWâ€™s API is fully open-source.Scale effortlessly by adding fiat gateways via partners like  (charge customers via card/debit) without building native payment systems yourself (expensive and complex, trust us).\
Too good to be true? Too true to be good? Dropkick the clichÃ©s out the window. Weâ€™re offering exactly what we say, and Ivan from Bulgaria loves it.\
Oh, you donâ€™t know Ivan yet! Letâ€™s meet him.Real World Proof This WorksIvan is from Bulgaria, as you know. He launched his exchange last November, spending just $2k on a catchy domain and web hosting costs. By targeting niche coin pairs: Ivan handles 25 BTC/day volume (~$6k USD daily profit), while making even more through affiliate partnerships when people ask him how he did itâ€¦\
What about Sarah & Michael, our South African friends? They wanted a simple dApp integration for their GameFi platform. With our API, they added instant crypto payments and within WEEKS they saw transaction-based revenue jump by 47%. Crazy.\
The best part is, you donâ€™t actually need any cash to start. Actually, you donâ€™t even need your own exchange. Weâ€™ve got other no-code approaches to helping you boost your income.\
Plug our exchange in your blogs, socials, channels, groups, or websites and earn commissions from every single swap users make on your channels, 24/7!\
Users who click on the fully customizable â€œExchangeâ€ button, are taken to a new tab to complete their transaction:\
This is our , and itâ€™s PERFECT if youâ€™ve got traffic and an audience anywhere, from Telegram to TikTok and beyond.\
Wait, are you thinking bigger? Option 2 might be more to your taste.Option #2: Sell Custom-Built Exchanges as a White Label ServiceGot tech skills? Package your branded version of ChangeNOW as an SaaS product, then charge other startups $999/month in subscription fees.\
Build â€œYourCoolExchange.comâ€ using ChangeNOWâ€™s plugin + resell licenses. Your margin per client is pure profit. Youâ€™ll be laughing all the way to the bank, as every asset listing and white label development that comes from you will net you an additional 5% of the deal. Sweet.I donâ€™t trust third-party APIsâ€¦ what if they disappear?\
Our uptime is 99.99%, and weâ€™ve been live since 2018. ChangeNOW is built for crypto winters and crypto winners. Plus, your branding stays yours forever, even if you pivot later. Our API is open-source and you can use it without even mentioning us.\
â€œThis sounds too good to be trueâ€¦â€\
Itâ€™s not. Weâ€™ve already partnered with over 30 wallets and dApps (see our ). Your success isnâ€™t a fluke.\
â€œIâ€™ve never done anything like this beforeâ€¦â€\
We have, many times, and our experienced team wants YOU to SUCCEED and become a legendary Web3 founder.Letâ€™s Make This Happen TodayStill here? Youâ€™re either:Ready to own your own crypto exchangeâ€¦Or, still have a lot of questions and want to talk to a human from our team\
If thatâ€™s the case, yell at us via partners@changenow.io and weâ€™ll make sure all your questions are answered.]]></content:encoded></item><item><title>When Rating AI Chatbots, More Context Isn&apos;t Always Better</title><link>https://hackernoon.com/when-rating-ai-chatbots-more-context-isnt-always-better?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Clemencia Siro, University of Amsterdam, Amsterdam, The Netherlands;(2) Mohammad Aliannejadi, University of Amsterdam, Amsterdam, The Netherlands;(3) Maarten de Rijke, University of Amsterdam, Amsterdam, The Netherlands.4 Discussion and ImplicationsOur findings reveal intriguing insights into the impact of context size and type on crowdsourced relevance and usefulness labels for TDS. Expanding the dialogue context from C0 to C7 significantly improves agreement among annotators, indicating that annotators rely on comprehensive context to make more accurate assessments. This trend does not hold for usefulness, where we notice a decrease in agreement when all previous dialogue context is available. The optimal amount of context required for reliable labels relies on the aspect evaluated.\
Consistent with prior work (Eickhoff, 2018; Kazai et al., 2011a), we observe an inconsistency in relevance labels across variations, with the same system response being rated differently depending on the context provided. Given the lack of label consistency across variations, future studies should carefully tailor their annotation task design and test various settings to ensure high-quality and consistent labels. Additionally, much care should be taken when comparing the performance of a system across several datasets when labels are crowdsourced with a different strategy to ensure a fair comparison as models similar to humans can be sensitive to the annotation strategy (Kadasi and Singh, 2023; Kern et al., 2023).\
We also analyzed data from the open-ended question asking annotators about their experience with the annotation task. Annotators note that dialogue summaries fail to convey a userâ€™s emotion, limiting their annotation process. Additionally, lower accuracy of the context generated by an LLM may lead to low agreement among annotators. This signifies the importance of carefully considering the quality and accuracy of generated content in the evaluation process. We provide examples in Section A.5 in the appendix. While there may be constraints in presenting user information need and dialogue summary as dialogue context, one key consideration to take into account is the cognitive load of annotators. Providing a shorter, focused context reduces the cognitive burden on annotators, allowing them to devote more attention to actually evaluating a response. This not only streamlines the annotation process but also helps maintain high-quality results. Reducing the amount of content to be assessed may lead to faster annotation times without compromising the quality of ratings (Santhanam et al., 2020). Another approach to using LLMs in annotation, is for researchers to consider co-annotation (Li et al., 2023) between humans and LLMs.\
Optimal context varies by the aspect under evaluation, challenging the idea of a universal strategy. The consistent reliability of automatic methods suggests their potential as dependable tools for evaluation. This implies their use in generating supplementary context, eliminating the need for manual determination of context amounts. This streamlines evaluation, enhancing efficiency in context driven evaluations for TDS. For data lacking topic or preference shifts, heuristics perform effectively. However, LLMs are recommended for shifting conditions, showcasing adaptability not easily discernible with heuristics.\
While our primary focus was limited to relevance and usefulness, the proposed experimental design can be extended to other aspects of TDSs evaluation. Moreover, our findings may be task or dataset-specific, prompting the need for further investigation into their generalizability. As to future work, we aspire to enhance the robustness of our findings by conducting studies on larger-scale datasets. In addition following previous work by Kazai et al. (2012, 2013), we would also want to understand the effect of annotator background: experience of interacting with conversational system or prior experience in doing the annotation task on label consistency for TDSs.We review related work not covered in the paper so far. Several user-centric dialogue evaluation metrics (Ghazarian et al., 2019; Huang et al., 2020; Mehri and Eskenazi, 2020) have been proposed. For TDSs, high-level dimensions such as user satisfaction (Al-Maskari et al., 2007; Kiseleva et al., 2016) and fine-grained metrics such as relevance and interestingness (Siro et al., 2022) have gained interest. Due to the ineffectiveness of standard evaluation metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), which show poor correlation with human judgments (Deriu et al., 2021), a significant amount of research on these metrics relies on crowdsourcing dialogue evaluation labels to improve correlation with actual user ratings. Crowdsourcing ground-truth labels has gained momentum in information retrieval (IR) for tasks like search relevance evaluation (Alonso et al., 2008) and measuring user satisfaction in TDS. A major challenge is ensuring quality and consistency of crowdsourced labels. Task design and annotatorsâ€™ behavioral features and demographics can affect the quality of the collected labels (Hube et al., 2019; Kazai et al., 2012; Pei et al., 2021). Kazai et al. (2013) examine how effort and incentive influence the quality of labels provided by assessors when making relevance judgments. Other factors such as judgment scale (Novikova et al., 2018; Roitero et al., 2021), annotator background (Kazai et al., 2011b; Roitero et al., 2020), and annotatorsâ€™ demographics (Difallah et al., 2018) have also been studied. Most studies focus on search systems, not dialogue systems. Closer to our work, Santhanam et al. (2020) study the effect of cognitive bias in the evaluation of dialogue systems. Providing an anchor to annotators introduces anchoring bias, where annotatorsâ€™ ratings are close to the anchorâ€™s numerical value. Like Santhanam et al. (2020), we focus on the effect of task design on the evaluation of TDSs. In particular, we investigate how the amount and type of dialogue context provided to annotators affect the quality and consistency of evaluation labels and the annotator experience during the evaluation task.]]></content:encoded></item><item><title>Founders, TechCrunch Startup Battlefield 200 is calling! Apply to enter!</title><link>https://techcrunch.com/2025/04/08/founders-techcrunch-startup-battlefield-200-is-calling-apply-to-enter/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Founders, the battlefield is calling! Startup Battlefield 200 applications are now live â€” and the race has begun. If youâ€™ve got a groundbreaking idea and the guts to pitch it to the world, TechCrunch Disrupt 2025 is your arena. Youâ€™ll face elite VC judges. Youâ€™ll compete against 199 of the worldâ€™s most promising startups. And [â€¦]]]></content:encoded></item><item><title>Los Angeles-based Rain raised a $75M Series B in another good sign for fintech</title><link>https://techcrunch.com/2025/04/08/los-angeles-based-rain-raised-a-75m-series-b-in-another-good-sign-for-fintech/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Tue, 8 Apr 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Rain, a startup offering an employer-integrated earned wage access (EWA) app coupled with financial-wellness features like overdraft alerts and spending trends, has raised $75 million in an all-equity Series B round. The round was led by Prosus at a post-money valuation of $340 million. Rain plans to use the new funds to help it add [â€¦]]]></content:encoded></item><item><title>Trump to endorse coal for data center power in the face of grim market realities</title><link>https://techcrunch.com/2025/04/08/trump-to-endorse-coal-for-data-center-power-in-the-face-of-grim-market-realities/</link><author>Tim De Chant</author><category>tech</category><pubDate>Tue, 8 Apr 2025 14:46:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[An executive order will direct the federal government to list coal as a critical mineral and force some coal-fired power plants to keep generating electricity.]]></content:encoded></item><item><title>Why Our Prototype-based AI Beats Traditional Concept Learning Approaches</title><link>https://hackernoon.com/why-our-prototype-based-ai-beats-traditional-concept-learning-approaches?source=rss</link><author>Activation Function</author><category>tech</category><pubDate>Tue, 8 Apr 2025 14:45:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sanchit Sinha, University of Virginia (sanchit@virginia.edu);(2) Guangzhi Xiong, University of Virginia (hhu4zu@virginia.edu);(3) Aidong Zhang, University of Virginia (aidong@virginia.edu).In this paper, we discuss a fairly less-studied problem of  which involves learning domain invariant concepts that can be generalized to similar tasks across domains. Next, we introduce a novel Representative Concept Extraction framework that improves on present selfexplaining neural architectures by incorporating a Salient Concept Selection Network. We propose a Self-Supervised Contrastive Learning-based training paradigm to learn domain invariant concepts and subsequently propose a Concept Prototype-based regularization to minimize concept shift and maintain high fidelity. Empirical results on domain adaptation performance and fidelity scores show the efficacy of our approach in learning generalizable concepts and improving concept interoperability. Additionally, qualitative analysis demonstrates that our methodology not only learns domainaligned concepts but is also able to explain samples from both domains equally well. We hope our research helps the community utilize self-explainable models in domain alignment problems in the future.[Aggarwal et al., 2021] Ravi Aggarwal, Viknesh Sounderajah, Guy Martin, Daniel SW Ting, Alan Karthikesalingam, Dominic King, Hutan Ashrafian, and Ara Darzi. Diagnostic accuracy of deep learning in medical imaging: A systematic review and meta-analysis. NPJ digital medicine, 4(1):1â€“23, 2021.\
[Alvarez-Melis and Jaakkola, 2018] David Alvarez-Melis and Tommi S Jaakkola. Towards robust interpretability with self-explaining neural networks. arXiv preprint arXiv:1806.07538, 2018.\
[Bahadori and Heckerman, 2020] Mohammad Taha Bahadori and David E Heckerman. Debiasing concept bottleneck models with instrumental variables. arXiv preprint arXiv:2007.11500, 2020.\
[Bracke et al., 2019] Philippe Bracke, Anupam Datta, Carsten Jung, and Shayak Sen. Machine learning explainability in finance: an application to default risk analysis. 2019.\
[Chen et al., 2019] Runjin Chen, Hao Chen, Jie Ren, Ge Huang, and Quanshi Zhang. Explaining neural networks semantically and quantitatively. In ICCV, pages 9187â€“9196, 2019.\
[Chen, 2020] Ting Chen. A simple framework for contrastive learning of visual representations. In ICML. PMLR, 2020.\
[Dâ€™Amour et al., 2020] Alexander Dâ€™Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspecification presents challenges for credibility in modern machine learning. JMLR, 2020.\
[Elbaghdadi, 2020] Omar Elbaghdadi. Disenn: Selfexplaining neural networks: A review. https://github.com/ AmanDaVinci/SENN, 2020.\
[Ghorbani and Zou, 2019] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In ICML, pages 2242â€“2251. PMLR, 2019.\
[Ghorbani et al., 2019] Amirata Ghorbani, James Wexler, James Zou, and Been Kim. Towards automatic conceptbased explanations. NeurIPS, 2019.\
[Gidaris et al., 2018] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.\
[Goyal et al., 2019] Yash Goyal, Amir Feder, Uri Shalit, and Been Kim. Explaining classifiers with causal concept effect (cace). arXiv preprint arXiv:1907.07165, 2019.\
[Huang et al., 2022] Jinbin Huang, Aditi Mishra, Bum-Chul Kwon, and Chris Bryan. Conceptexplainer: Understanding the mental model of deep learning algorithms via interactive concept-based explanations. arXiv preprint arXiv:2204.01888, 2022.\
[Hull, 1994] Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on pattern analysis and machine intelligence, 16(5):550â€“554, 1994.\
[Hutchinson and Mitchell, 2019] Ben Hutchinson and Margaret Mitchell. 50 years of test (un) fairness: Lessons for machine learning. In FAccT, pages 49â€“58, 2019.\
[Jeyakumar et al., 2021] Jeya Vikranth Jeyakumar, Luke Dickens, Yu-Hsi Cheng, Joseph Noor, Luis Antonio Garcia, Diego Ramirez Echavarria, Alessandra Russo, Lance M Kaplan, and Mani Srivastava. Automatic concept extraction for concept bottleneck-based video classification. 2021.\
[Kim et al., 2018] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In ICML, pages 2668â€“2677. PMLR, 2018.\
[Koh and Liang, 2017] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In ICML. PMLR, 2017.\
[Koh et al., 2020] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In ICML, pages 5338â€“5348. PMLR, 2020.\
[LeCun et al., 1998] Yann LeCun, Leon Bottou, Yoshua Â´ Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278â€“2324, 1998.\
[Leemann et al., 2022] Tobias Leemann, Yao Rong, Stefan Kraft, Enkelejda Kasneci, and Gjergji Kasneci. Coherence evaluation of visual concepts with objects and language. In ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality, 2022. [Liu et al., 2021] Xiaoqing Liu, Kunlun Gao, Bo Liu, Chengwei Pan, Kongming Liang, Lifeng Yan, Jiechao Ma, Fujin He, Shu Zhang, Siyuan Pan, et al. Advances in deep learning-based medical image analysis. Health Data Science, 2021, 2021.\
[Mincu et al., 2021] Diana Mincu, Eric Loreaux, Shaobo Hou, Sebastien Baur, Ivan Protsyuk, Martin Seneviratne, Anne Mottram, Nenad Tomasev, Alan Karthikesalingam, and Jessica Schrouff. Concept-based model explanations for electronic health records. In CHIL, pages 36â€“46, 2021.\
[Murty et al., 2020] Shikhar Murty, Pang Wei Koh, and Percy Liang. Expbert: Representation engineering with natural language explanations. In ACL, pages 2106â€“2113, 2020.\
[Netzer et al., 2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.\
[Peng et al., 2017] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.\
[Peng et al., 2019] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406â€“1415, 2019.\
[Pittino et al., 2021] Federico Pittino, Vesna Dimitrievska, and Rudolf Heer. Hierarchical concept bottleneck models for explainable images segmentation, objects fine classification and tracking. Objects Fine Classification and Tracking, 2021.\
[Raji et al., 2020] Inioluwa Deborah Raji, Andrew Smart, Rebecca N White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. Closing the ai accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 33â€“44, 2020.\
[Ribeiro et al., 2016] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016.\
[Rudin, 2019] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206â€“215, 2019.\
[Saito et al., 2020] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through self supervision. NeurIPS, 33:16282â€“16292, 2020.\
[Sarkar et al., 2022] Anirban Sarkar, Deepak Vijaykeerthy, Anindya Sarkar, and Vineeth N Balasubramanian. A framework for learning ante-hoc explainable models via concepts. In CVPR, pages 10286â€“10295, 2022.\
[Sawada, 2022a] Yoshihide Sawada. C-senn: Contrastive senn. 2022.\
[Sawada, 2022b] Yoshihide Sawada. Cbm with add. unsup concepts. 2022.\
[Sinha et al., 2021] Sanchit Sinha, Hanjie Chen, Arshdeep Sekhon, Yangfeng Ji, and Yanjun Qi. Perturbing inputs for fragile interpretations in deep natural language processing. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 420â€“434, 2021.\
[Sinha et al., 2023] Sanchit Sinha, Mengdi Huai, Jianhui Sun, and Aidong Zhang. Understanding and enhancing robustness of concept-based models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 15127â€“15135, 2023.\
[Sundararajan et al., 2017] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML, pages 3319â€“3328. PMLR, 2017.\
[Szepannek and Lubke, 2021 Â¨ ] Gero Szepannek and Karsten Lubke. Facing the challenges of developing fair risk scor- Â¨ ing models. Frontiers in artificial intelligence, 4, 2021.\
[Thota and Leontidis, 2021] Mamatha Thota and Georgios Leontidis. Contrastive domain adaptation. In CVPR, pages 2209â€“2218, 2021.\
[Varoquaux and Cheplygina, 2022] Gael Varoquaux and Â¨ Veronika Cheplygina. Machine learning for medical imaging: methodological failures and recommendations for the future. NPJ digital medicine, 5(1):1â€“8, 2022.\
[Venkateswara et al., 2017] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, pages 5018â€“5027, 2017.\
[Wang and Liu, 2021] Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In CVPR, pages 2495â€“2504, 2021.\
[Wang, 2023] Bowen Wang. Learning bottleneck concepts in image classification. In CVPR, pages 10962â€“10971, 2023.\
[Weller, 2019] Adrian Weller. Transparency: motivations and challenges. In Explainable AI: interpreting, explaining and visualizing deep learning, pages 23â€“40. Springer, 2019.\
[Wu et al., 2020] Weibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael R Lyu, and Yu-Wing Tai. Towards global explanations of convolutional neural networks with concept attribution. In CVPR, pages 8652â€“ 8661, 2020.\
[Xu et al., 2019] Jiaolong Xu, Liang Xiao, and Antonio M Lopez. Self-supervised domain adaptation for computer Â´ vision tasks. IEEE Access, 7:156694â€“156706, 2019.\
[Yeh et al., 2019] Chih-Kuan Yeh, Been Kim, Sercan O Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. arXiv preprint arXiv:1910.07969, 2019.\
[Yu and Lin, 2023] Yu-Chu Yu and Hsuan-Tien Lin. Semisupervised domain adaptation with source label adaptation. In CVPR, pages 24100â€“24109, 2023.\
[Yuksekgonul et al., 2022] Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. arXiv preprint arXiv:2205.15480, 2022.\
[Zaeem and Komeili, 2021] Mohammad Nokhbeh Zaeem and Majid Komeili. Cause and effect: Concept-based explanation of neural networks. In 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages 2730â€“2736. IEEE, 2021.\
[Zhou et al., 2018] Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable basis decomposition for visual explanation. In ECCV, pages 119â€“134, 2018.\
Following discussions from the main text, the Appendix section is organized as follows:\
â€¢ Dataset descriptions and visual samples\
â€¢ Detailed discussion around RCE and algorithmic details for CCL and PCG (Pseudocode)\
â€¢ More experimental results on key hyperparameters utilized in RCE and PCG\
â€¢ Concept Fidelity Analysis\
â€¢ Details on Baseline Replication\
â€¢ Additional visual results - selected prototypes\
â€¢ Additional visual results - domain-aligned prototypesA few examples from the training set of the datasets utilized in our approach are shown in Figures 7 (Digits) for both tasks are shown in Figures 7, 8 (VisDA), 9 (DomainNet) and 10 (OfficeHome).A.2 Training Procedure - DetailsAlgorithm 1 depicts the overall pseudocode to train the Representative Concept Extraction (RCE) framework with Contrastive (CCL) and Prototype-based Grounding (PCG) regularization. The finer details of each part are listed as follows:\
\
: For networks  and , we utilize a Resnet34 architecture and initialize them with pre-trained Imagenet1k weights. For the network , we first utilize the element-wise vector product between the outputs of  and  and then pass the outputs through network , which is a shallow 2-layer fully connected network. For network , we utilize a 3-layer fully connected network which outputs a prediction with  concepts. The final prediction is a weighted sum of outputs of networks  and  followed by a softmax layer. The prediction loss is the standard cross-entropy loss.\
: For selection of prototypical samples we utilize a combination of selection of source and target domains. We select 5 and 1 samples from the source and target domains respectively. Note that the PCG regularization starts after the 1st step. The grounding ensures that the outlying concept representations in the target domains are grounded to the abundant source domain representations. For concept bank, we utilize these 6 (5+1) prototypes for each class.A.3 Results on Key Hyperparameters\
The first 3 columns of Table 6 list the domain adaptation performance on the OfficeHome dataset across 12 different data settings (listed in rows). We evaluate the performance by varying the number of concepts C (and by extension, the relevance scores S). We choose the base setting of the number of concepts being equal to the number of classes because we want each class to be represented by at least one concept. We observe that increasing the number of concepts has no significant effect on the performance. This observation points to the fact that the relevant concept information is encoded in a few number of concepts. In other words, the concept vector is .\
The last 3 columns of Table 6 list performance by varying the concept dimensionality d (dim). Note that non-unit dimensional concepts are not directly interpretable, and remain an active area of research [Sarkar et al., 2022]. Nevertheless, we report the performance numbers by varying the concept dimensionality. We observe that the with increasing concept dimensionality, the performance on target domains increases in almost all settings. This observation is expected for the following two reasons - 1) increasing concept dimensionality increases the richness of information encoded in each concept during contrastive learning and 2) increased dimensionality increases the expressiveness of the architecture itself.\
\
Size of Representative set for PCG\
Table 7 shows the performance on the OfficeHome dataset for two settings of the pre-selected representative prototypes. For all experiments in the main paper, we utilize 5 prototypes from the source domain and 1 from the target domain - for a total of 6 prototype samples for grounding. Note that it is not usually possible to use a lot of prototypes from the target domain as our setting corresponds to the 3-shot setting in [Yu and Lin, 2023]. We show the performance on 5 and 7 selected prototypes on the source domain in Table 7. We observe that increasing the number of prototypes does not result in an improvement of performance, in fact performance does not significantly change and in a few cases, performance drops. This observation implies that only a minimum number of necessary and sufficient grounding set of prototypes is required. This observation is consistent with intuition because if more than a requisite prototypes are selected, the computation time for concept representations will increase.\
Distances from the Concept Representation prototypes\
Table 7 lists the average normalized distance of the concept representations of the target domain from the concept representations associated with the selected prototypes with varying values of Î»1 which controls the effect of supervision in PCG. We choose 3 different values for demonstrating the ablation results - Î»1 = 0 for no regularization and Î»1 = 1 for very high regularization. We observe that both cases lack generalization performance implying a tradeoff between regularization and generalization.A.4 Concept Fidelity AnalysisTable 8 lists the consolidated concept fidelity scores of all four datasets. Note: This table is a complete version of the Table 5 in the main text. We see that the concept overlap on all datasets is highest in either our approach or BotCL, both approaches with explicit fidelity regularizations. This demonstrates the efficacy of our approach in maintaining concept fidelity.We compare our approach against 4 baselines - SENN [Alvarez-Melis and Jaakkola, 2018], DiSENN [Elbaghdadi, 2020], BotCL[Wang, 2023] and UnsupervisedCBM[Sawada, 2022b]. Even though none of the approaches incorporate domain adaptation as an evaluation method, we utilize the proposed methodology directly in our settings. Proper care has been taken to ensure high overlap with the intended use and carefully introduced modifications proposed by us, listed below:\
 We utilize the well tested publicly available code[2] as the basic framework. We modify SENN and DiSENN to include Resnet34 (for objects) and the decoder is kept the same for all setups discussed. Specifically, due to the very slow computation of the robustness loss Lh on bigger networks like Resnet34, we only compute it once every 10 steps.\
â€¢ : We utilize the publicly available code [3]. We utilize the same network architecture - LeNet for digits and Resnet34 for objects. Additionally, we amend the LeNet architecture to fit BotCL framework.\
â€¢ : Unsupervised CBM is hard to train as it contains a mixture of supervised and unsupervised concepts. However, our approach does not utilize supervision, so we only consider the unsupervised concepts and replace the supervised concepts with the one-hot encoding of the classes of the images. We utilize a fully connected layer for the discriminator network while simultaneously training a decoder. Though a publicly available version of UnsupCBM is not available, we are successful in the replication of its main results.A.6 Additional Visual Results - Selected PrototypesFigures 11a and 11b showcase the most top-5 most important prototypes concerning a given query image for the OfficeHome and DomainNet datasets respectively. For each row, the model settings are where the query and prototypes are in the target domain. We show results on all domains for both datasets, to demonstrate the efficacy of our proposed approach, which can generalize to all domains. Note that RCE is an overparameterized version of SENN, hence the performance with respect to baselines remains identical. Our proposed approach can explain each query image with relevant prototypes as opposed to the baselines where the prototypes are barely relevant.A.7 Additional Visual Results - Domain AlignedFigures 12a and 12b showcase the most top-5 most important prototypes concerning Digit and VisDA datasets respectively. For each row, the model settings on the left show the prototypes in the source and on the right show target domain. Our proposed approach can explain each concept with relevant prototypes across domains.[2] https://github.com/AmanDaVinci/SENN\
[3] https://github.com/wbw520/BotCL]]></content:encoded></item><item><title>How Reliable Are AI Concepts in Real-World Domain Adaptation Tasks?</title><link>https://hackernoon.com/how-reliable-are-ai-concepts-in-real-world-domain-adaptation-tasks?source=rss</link><author>Activation Function</author><category>tech</category><pubDate>Tue, 8 Apr 2025 14:30:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sanchit Sinha, University of Virginia (sanchit@virginia.edu);(2) Guangzhi Xiong, University of Virginia (hhu4zu@virginia.edu);(3) Aidong Zhang, University of Virginia (aidong@virginia.edu).We consider the following evaluation metrics to evaluate each component of the concept discovery framework.\
â€¢ : We start by quantitatively evaluating the quality of concepts learned by measuring how well the learned concepts can generalize to new domains. To achieve this, we compare our proposed method against the aforementioned baselines on domain adaptation settings.\
 To evaluate consistency in the learned concepts, we compute the intersection over union of the concept sets associated with for two data points xi and xj from same class as defined in Equation 10:4.4 Genenralization ResultsTables 2, 3, and 4 report the domain adaptation results on the OfficeHome, DomainNet, VisDA and the Digit datasets, respectively. The notation Xâ†’Y represents models trained on X as the source domain (with abundant data) and Y as the target domain (with limited data) and evaluated on the test set of domain Y . The best statistically significant accuracy is reported in bold. The last three rows in all the tables list the performance of the  framework,  trained with regularization  and  trained with both regularization and contrastive learning paradigm .\
Comparision with baselines. The first row in each table lists the performance of a standard Neural Network trained using the setting described in [Yu and Lin, 2023] (S+T). As a standard NN is not inherently explainable, we consider this setting as a baseline to understand the upper bound of the performance-explainability tradeoff.\
The second and third rows in each table lists the performance of SENN and DiSENN respectively. SENN performs worse than S+T setting in almost all settings, except in a handful of settings where the performance matches S+T. This is expected, as SENN is formulated as an overparameterized version of a standard NN with regularization. Recall that DiSENN replaces the autoencoder in SENN with a VAE, and as such is not generalizable to bigger datasets without domain engineering. DiSENN performs the worst among all approaches for all datasets due to poor VAE generalization.\
Recall that UnsupervisedCBM is an improved version of SENN architecture with a discriminator in addition to the aggregation function. In most cases, it performs slightly better than SENN and is at par with S+T. However, in particular cases in OfficeHome data (Râ†’A) and DomainNet (Sâ†’P), UnsupCBM performs the best. We attribute this result to two factors: first, the Art (A) and Sketch (S) domains are significantly different from Real (R) and Picture (P) domains due to both of the former being hand-drawn while the latter being photographed as mentioned in [Yu and Lin, 2023]. Second, the use of a discriminator as proposed in UnsupervisedCBM helps enforce domain invariance in those particular cases.\
BotCL explicitly attempts to improve concept fidelity and applies contrastive learning to discover concepts. However, the contrastive loss formulation is rather basic and they never focuses on domain invariance. BotCLâ€™s performance is similar to S+T for the most part except in OfficeHome data (Câ†’A), where it just outperforms all other approaches. One possible reason is that Clipart domain is significantly less noisy, and hence basic transformations in BotCL work well.\
As the last row demonstrates, our proposed framework RCE+PCG+CCL outperforms all baselines on a vast majority of the settings across all four datasets and is comparable to SOTA baselines in the other settings.\
 We also report the performance corresponding to various components of our proposed approach. We observe that the performance of RCE is almost identical to SENN, which is expected as there is very weak regularization in both cases. In almost all cases, adding prototype-based grounding regularization (RCE+PCG) improves performance over RCE while models trained with both PCG regularization and contrastive learning (RCE+PCG+CCL) outperform all approaches on a vast majority of settings across all datasets. Note that the setting RCE+CCL is not reported, as it defeats the fundamental motivation of maintaining concept fidelity.\
Effect of the number of concepts and dimensions. We observe that there are no significant differences in performance over a varying number of concepts or dimensions. For all results reported, the number of concepts is set to a number of classes in the dataset, and their dimension is set to 1. For results on varying numbers of concepts and dimensions - refer to Appendix.As the RCE framework is explicitly regularized with a concept fidelity regularizer and grounded using prototypes, we would expect high fidelity scores. Table 5 lists the fidelity scores for the aforementioned baselines and our proposed method. Fidelity scores are averaged for each domain when taken as a target (e.g. for the domain (A) in DomainNet, the score is an average of Câ†’A, Pâ†’A and Râ†’A). As expected, our method and BotCL, both with specific fidelity regularization outperform all other baseline approaches. Our method outperforms BotCL in most settings, except when the target domains are Art in DomainNet and Clipart in OfficHome due to significant domain dissonance.4.6 Qualitative Visualization We consider the extent to which the models trained using both concept grounding and contrastive learning maintain concept consistency not only within the source domain but also across the target domain as well. To understand what discriminative information is captured by a particular concept, Figure 5 shows the most important prototypes selected from the training set of both the source and target domains corresponding to five randomly selected concepts. We observe that prototypes explaining each concept are visually similar. For more results, refer Appendix.\
Explanation using prototypes. For a given input sample, we also plot the prototypes associated with the highest activated concept, i.e., the important concept. Figure 6 shows the prototypes associated with the concepts most responsible for prediction (highest relevance scores). As can be seen, the prototypes possess distinct features, for eg., they capture the round face of an alarm clock. More results are reported in Appendix.]]></content:encoded></item><item><title>Can AI Concepts Truly Generalize Across Different Domains? Experiments Reveal Answers</title><link>https://hackernoon.com/can-ai-concepts-truly-generalize-across-different-domains-experiments-reveal-answers?source=rss</link><author>Activation Function</author><category>tech</category><pubDate>Tue, 8 Apr 2025 14:30:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sanchit Sinha, University of Virginia (sanchit@virginia.edu);(2) Guangzhi Xiong, University of Virginia (hhu4zu@virginia.edu);(3) Aidong Zhang, University of Virginia (aidong@virginia.edu).4.1 Datasets and NetworksWe consider four widely used task settings commonly utilized for domain adaptation. The task in each of the following settings is classification.\
â€¢ : This setting utilizes MNIST and USPS [LeCun et al., 1998; Hull, 1994] with Hand-written images of digits and Street View House Number Dataset (SVHN) [Netzer et al., 2011] with cropped house number photos.\
 [Peng et al., 2017]: contains 12 classes of vehicles sampled from Real (R) and 3D domains.\
â€¢  [Venkateswara et al., 2017]: contains 126 classes of objects (clocks, bags, etc.) sampled from 4 domains - Real (R), Clipart (C), Painting (P), and Sketch (S).\
 [Peng et al., 2019]: Office-Home contains 65 classes of office objects like calculators, staplers, etc. sampled from 4 different domains - Art (A), Clipart (C), Product (P), and Real (R).\
 For Digits, we utilize a modified version of LeNet [LeCun et al., 1998] which consists of 3 convolutional layers for digit classification with ReLU activation functions and a dropout probability of 0.1 during training. For all other datasets we utilize a ResNet34 architecture similar to [Yu and Lin, 2023] and initialize it with pre-trained weights from Imagenet1k. For details, refer Appendix.\
. We start by comparing against standard nonexplainable NN architectures - the S+T setting as described in [Yu and Lin, 2023]. Next, we compare our proposed method against 5 different self-explaining approaches. As none of the approaches specifically evaluate concept generalization in the form of domain adaptation, we replicate all approaches.  and  utilize a robustness loss calculated on the Jacobians of the relevance networks with DiSENN utilizing a VAE as the concept extractor.  [Wang, 2023] also proposes to utilize contrastive loss but uses it for position grounding. Similar to BotCL, Ante-hoc concept learning [Sarkar et al., 2022] uses contrastive loss on datasets with known concepts, hence we do not explicitly compare against it. Lastly,  [Sawada, 2022b] uses a mixture of known and unknown concepts and requires a small set of known concepts. For our purpose, we provide the onehot class labels as known concepts in addition to unknown. A visual summary of the salient features of each baseline is depicted in Table 1.4.2 Hyperparameter Settings We utilize the Mean Square Error as the reconstruction loss and set sparsity regularizer Î» to 1e-5 for all datasets. The weights Ï‰1 = Ï‰2 = 0.5 are utilized for digit, while they are set at Ï‰1 = 0.8 and Ï‰2 = 0.2 for object tasks.\
: We utilize the lightly[1] library for implementing SimCLR transformations [Chen, 2020]. We set the temperature parameter (Ï„ ) to 0.5 by default [Xu et al., 2019] for all datasets. The hyperparameters for each transformation are defaults utilized from SimCLR. The training objective is Contrastive Cross Entropy (NTXent) [Chen, 2020]. Figure 4 depicts an example of various transformations along with the adjudged positive and negative transformations. For the training procedure, we utilize the SGD optimizer with momentum set to 0.9 and a cosine decay scheduler with an initial learning rate set to 0.01. We train each dataset for 10000 iterations with early stopping. The regularization parameters of Î»1 and Î»2 are set to 0.1 respectively. For Digits, Î² is set to 1 while it is set to 0.5 for objects. For further details, refer to Appendix.[1] https://github.com/lightly-ai/lightly]]></content:encoded></item><item><title>Volunteer your time for a free ticket to TechCrunch Sessions: AI</title><link>https://techcrunch.com/2025/04/08/volunteer-your-time-for-a-free-ticket-to-techcrunch-sessions-ai/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 8 Apr 2025 14:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Join us for an unforgettable opportunity at TechCrunch Sessions: AI, happening at UC Berkeleyâ€™s Zellerbach Hall on June 5! Weâ€™re on the lookout for enthusiastic volunteers to be part of this exciting event. If youâ€™ve ever wondered what goes into the making of tech gatherings, nowâ€™s your chance to find out firsthand. Apply here by [â€¦]]]></content:encoded></item><item><title>Can AI-Generated Context Improve the Quality of Crowdsourced Feedback?</title><link>https://hackernoon.com/can-ai-generated-context-improve-the-quality-of-crowdsourced-feedback?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 8 Apr 2025 14:00:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Clemencia Siro, University of Amsterdam, Amsterdam, The Netherlands;(2) Mohammad Aliannejadi, University of Amsterdam, Amsterdam, The Netherlands;(3) Maarten de Rijke, University of Amsterdam, Amsterdam, The Netherlands.3.3 RQ2: Effect of automatically generated dialogue context In Phase 2, our experiments aim to establish the impact of presenting annotators with different types of context during crowdsourcing. Different from conventional dialogue context, we provide the annotators with the dialogue summary (C0-sum), the userâ€™s information need in the dialogue (C0-heu and C0-llm). We also aim to uncover if we can improve the quality of the crowdsourced labels in C0 to match those in C7. We calculate the Cohenâ€™s Kappa similar to Section 3.2; see Table 2.\
The heuristic approach (C0-heu) yields the highest agreement (Kappa and Tau), indicating a noteworthy degree of agreement in relevance assessments. The LLM-generated context (C0-llm and C0-sum) results in a moderate to substantial level of agreement, signifying a reasonable level of agreement regarding the relevance of the system response. We observe similar results for usefulness. The heuristic approach (C0-heu) again leads with the highest level of agreement (0.71 and 0.59), C0- sum follows with a kappa score of 0.63, while C0- llm has a kappa score of 0.53. This high level of agreement (Kappa) for the two aspects indicates the quality of the labels; the additional context provided, generated either heuristically or with LLMs, is effective in conveying relevant information to annotators, leading to more consistent assessments.\
For both relevance and usefulness, C0-heu consistently improves agreement among annotators, while the LLM-generated context (C0-llm and C0- sum) has a substantially lower agreement than C7. This difference reflects the limitations of LLMs in capturing context and generating a factual summary. While they generate coherent text, LLMs sometimes fail to correctly represent the sequential order of the dialogue and usersâ€™ language patterns.\
Label consistency across conditions. In Figure 4a we report the agreement between the setups in Phase 2 and compare them to C7 (relevance) and C3 (usefulness) due to their high inter-annotator agreement (IAA) and label consistency. For the relevance annotations, varying levels of agreement emerge. There is substantial agreement between C0-heu and C0-llm (59.36%), showing a significant overlap in the labels assigned using both methods, although there are instances where annotators differ in their assessments of relevance. C0-sum exhibits moderate label agreement with C0-llm (62.74%) and C0-heu (65.67%), pointing to relatively similar label assignments across the setups.\
We observe similar results for usefulness in Figure 4b. While the heuristically generated approach achieves high IAA, the C0-sum method demonstrates greater consistency with all other setups in terms of usefulness. This suggests that while annotators using the C0-heu approach often agreed on a single label, the chosen label may not have always been the most accurate. We note slightly low agreement levels for a similar label between the three setups, consistent with results in Phase 1. Unlike relevance, which used a binary scale, usefulness was rated on a 1â€“3 scale. This finer-grained scale may explain the lower agreement compared to relevance, as different types of contextual information can influence usefulness scores.\
Regarding , we show that we can improve the consistency of the labels assigned by crowdworkers in C0 condition by augmenting the current turn with automatically generated supplementary dialogue context. The heuristic approach demonstrates higher consistency in both IAA and label consistency for relevance and usefulness compared to C0 and C7. Providing annotators with the userâ€™s initial utterance expressing their preference, particularly in scenarios lacking context, can significantly enhance the quality and consistency of crowdsourced labels. This approach can yield performance comparable to a setup involving the entire dialogue C7, without imposing the cognitive load of reading an entire conversation on annotators. This streamlines the annotation process and maintains high-quality results, offering a practical strategy for obtaining reliable labels for dialogue evaluation.]]></content:encoded></item><item><title>Meta Got Caught Gaming AI Benchmarks</title><link>https://tech.slashdot.org/story/25/04/08/133257/meta-got-caught-gaming-ai-benchmarks?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Meta released two new Llama 4 models over the weekend -- Scout and Maverick -- with claims that Maverick outperforms GPT-4o and Gemini 2.0 Flash on benchmarks. Maverick quickly secured the number-two spot on LMArena, behind only Gemini 2.5 Pro. 

Researchers have since discovered that Meta used an "experimental chat version" of Maverick for LMArena testing that was "optimized for conversationality" rather than the publicly available version. 

In response, LMArena said "Meta's interpretation of our policy did not match what we expect from model providers" and announced policy updates to prevent similar issues.]]></content:encoded></item><item><title>Meet the new Audience Choice winners to lead breakouts at TechCrunch Sessions: AI</title><link>https://techcrunch.com/2025/04/08/meet-the-new-audience-choice-winners-to-lead-breakouts-at-techcrunch-sessions-ai/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 8 Apr 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[You voted. They rose to the top! Meet the two Audience Choice winners whoâ€™ll take the breakout stage at TechCrunch Sessions: AI on June 5 in Zellerbach Hall at UC Berkeley to share their insights with 1,200 AI leaders and enthusiasts.Â  We sifted through hundreds of Call for Content submissions and narrowed them down to [â€¦]]]></content:encoded></item><item><title>Zero Isnâ€™t a Problem, Itâ€™s a Shortcut: Rethinking PDFA Learning</title><link>https://hackernoon.com/zero-isnt-a-problem-its-a-shortcut-rethinking-pdfa-learning?source=rss</link><author>Constrain</author><category>tech</category><pubDate>Tue, 8 Apr 2025 13:57:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) M. Carrasco, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay (carrasco_m@ort.edu.uy);(2) F. Mayr, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay (mayr@ort.edu.uy);(3) S. Yovine, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay (yovine@ort.edu.uy);(4) J. Kidd, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay;(5) M. Iturbide, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay;(6) J. da Silva, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay;(7) A. Garat, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay.\
 We compare  against two instances of QNT, varying the behavior of the teacher:  uses Hopcroft-Karp algorithm [3] as  and  as in [6], while  checks if the string being queried by MQ traverses a 0-probability transition, in which case it identifies it as undefined.  and Teacher-Filter use as EQ an adaptation of Hopcroft-Karp that avoids traversing 0-probability transitions. The comparison is done by randomly generating PDFA. First, we construct DFA using the algorithm in [9], which for a given nominal size of n it generates DFA of actual reachable size normally distributed around n. Then, DFA are transformed into PDFA by assigning a random probability distribution over Î£$ to every state. A parameter Î¸ is used to control the probability of a symbol to be 0.\
Running times as function of Î¸. 10 random PDFA with n = 500 and |Î£| = m = 20 were generated for each Î¸ âˆˆ [0.9, 1), with step 0.02. Each one was run 10 times for every PDFA using quantization equivalence ([6]), adapted to satisfy (3), with parameter Îº = 100. Fig. 3(a) shows  has the best performance, with an almost constant but important improvement with respect to \
Running times as function of n. We compared the performance on 10 random PDFA with n = 250, 500, 750, 1000, and m = 10, using Îº = 10 and Î¸ = 0.9. Each algorithm was run 10 times for each PDFA. Fig. 3(b) shows the median of the execution time curves for n.  is always faster than the other two, achieving a speedup of approximately 24x and 3x with respect to  and  respectively, for n = 1000.4 Analyzing large language modelsGuiding generation Guiding an LLM to generate strings of interest consists in synchronizing it with a automaton that defines the set of symbols that can be drawn at each step of the generation process, which could be constrained further by a sampling strategy. To illustrate how the synchronization works, consider the language model given by the PDFA L in Fig. 4 (0-probabilities are omitted). The guide G is a weighted automaton that defines a mask at each state: a weight of 1 for a symbol means it is allowed, otherwise it is not. L Ã— G is a weighted automaton whose underlying structure is the product automaton, and weights are obtained by taking the product of the distribution of the state of L with the weights of the state of G. To obtain PDFA B, we apply the sampling strategy samptop2.This work was motivated by the need of understanding LLM when their operation is controlled by external artifacts, such as grammars, to generate text following a specific format. An important question that arise in this context is how to deal with 0-probabilities that appear when restricting their output. To start up with, we revised the congruence (1) in order to make constructing the quotient less dependent on P by expressing it in terms of the output of the language model. The first consequence of this operational view is to allow a generalization of the congruence capable of dealing with equivalences on distributions. Besides, it led to developing a variant of the QNT active-learning algorithm to efficiently learn PDFA by avoiding to check for 0-probability transitions as much as possible. This is\
\
essential to make it computationally feasible by reducing the number of queries to the LLM. The experimental results[3] support the viability of our approach for analyzing and validating statistical properties of LLM, such as bias in text generation. Besides, they provided evidence that distributions resulting from generation of a guided LLM could be well approximated by a learnt PDFA. This opens the door to make these analyses less dependent on sampling by studying properties of the PDFA.\
 Research reported in this article has been partially funded by ANII-Agencia Nacional de Investigacion e Innovaci Â´ on under grants IA Â´ 1 2022 1 173516, FMV 1 2023 1 175864, POS NAC 2023 1 178663, and POS FMV 2023 1 1012218.[1] R. C. Carrasco and J. Oncina. Learning deterministic regular grammars from stochastic samples in polynomial time. RAIRO - Theoretical Informatics and Applications, 33(1):1â€“19, 1999.\
[2] L. Du, L. Torroba Hennigen, T. Pimentel, C. Meister, J. Eisner, and R. Cotterell. A measure-theoretic characterization of tight language models. In 61st ACL, pages 9744â€“9770, July 2023.\
[3] J. E. Hopcroft and R. M. Karp. A linear algorithm for testing equivalence of finite automata. 1971.\
[4] I. Khmelnitsky, D. Neider, R. Roy, X. Xie, B. Barbot, B. Bollig, A. Finkel, S. Haddad, M. Leucker, and L. Ye. Property-directed verification and robustness certification of recurrent neural networks. In ATVA, pages 364â€“380, Cham, 2021. Springer International Publishing.\
[5] M. Kuchnik, V. Smith, and G. Amvrosiadis. Validating large language models with ReLM. In MLSys, 2023.\
[6] F. Mayr, S. Yovine, M. Carrasco, F. Pan, and F. Vilensky. A congruence-based approach to active automata learning from neural language models. In PMLR, volume 217, pages 250â€“264, 2023.\
[7] F. Mayr, S. Yovine, and R. Visca. Property checking with interpretable error characterization for recurrent neural networks. MAKE, 3(1):205â€“227, 2021.\
[8] E. Muskardin, M. Tappler, and B. K. Aichernig. Testing-based black-box extraction of simple models from rnns Ë‡ and transformers. In PMLR, volume 217, pages 291â€“294, 10â€“13 Jul 2023.\
[9] C. Nicaud. Random deterministic automata. In MFCSâ€™14, pages 5â€“23. LNCS 8634, 2014.\
[10] P. C. Shields. The ergodic theory of discrete sample paths. AMS, 1996.\
[11] E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta, and R.C. Carrasco. Probabilistic finite-state machines - part i. IEEE PAMI, 27(7):1013â€“1025, 2005.\
[12] Q. Wang, K. Zhang, A. G. Ororbia, II, X. Xing, X. Liu, and C. L. Giles. An empirical evaluation of rule extraction from recurrent neural networks. Neural Comput., 30(9):2568â€“2591, 2018.\
[13] G. Weiss, Y. Goldberg, and E. Yahav. Extracting automata from recurrent neural networks using queries and counterexamples. In PMLR, volume 80, 10â€“15 Jul 2018.\
[14] B. T. Willard and R. Louf. Efficient guided generation for LLMs. Preprint arXiv:2307.09702, 2023.A Proof of Proposition 2.1. Let u and v in Î£ âˆ— be arbitrary.B Proof of Proposition 2.2C Proof of Proposition 2.3D Proof of Proposition 2.4:::info
This paper is  under CC BY-SA 4.0 by Deed (Attribution-Sharealike 4.0 International) license.]]></content:encoded></item><item><title>What Happens When Language Models Say &apos;Nothing&apos;?</title><link>https://hackernoon.com/what-happens-when-language-models-say-nothing?source=rss</link><author>Constrain</author><category>tech</category><pubDate>Tue, 8 Apr 2025 13:51:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) M. Carrasco, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay (carrasco_m@ort.edu.uy);(2) F. Mayr, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay (mayr@ort.edu.uy);(3) S. Yovine, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay (yovine@ort.edu.uy);(4) J. Kidd, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay;(5) M. Iturbide, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay;(6) J. da Silva, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay;(7) A. Garat, Facultad de IngenierÄ±a, Universidad ORT Uruguay, Montevideo, Uruguay.We define a congruence that copes with null next-symbol probabilities that arise when the output of a language model is constrained by some means during text generation. We develop an algorithm for efficiently learning the quotient with respect to this congruence and evaluate it on case studies for analyzing the statistical properties of LLM.Many works have studied neural language models, such as Recurrent Neural Networks (RNN) and Transformers, through the analysis of surrogate automata of different sorts obtained from the former in a variety of ways, with the purpose of verifying or explaining their behavior (e.g. [12, 13, 4, 6, 8]). A few have proposed to somehow compose neural language models with automata or regular expressions in order to verifying properties on-the-fly while learning ([7]), assessing the existence of memorization, bias, or toxicity ([5]), and guiding text generation ([14]).\
In this paper, we first study theoretical questions that arise when applying this last approach in the context of active learning of probabilistic deterministic finite automata (PDFA) ([11]). In Sec. 2, we address the question of dealing with null next-symbol probabilities that appear when constraining the output of a language model by composing it with an automaton and/or a sampling strategy, such as the top k most likely symbols. We do this by defining an appropriate congruence that induces a quotient PDFA without 0-probability transitions. In Sec. 3, we adapt the learning algorithm of [6] to efficiently learn the quotient PDFA. In Sec. 4, we discuss issues that arise when analyzing real large language models, in particular the role of tokenizers, and apply the algorithm on problems discussed in [5, 14] when generating text with GPT2. Experimental results show the interest of our approach.Let Î£ be a finite set of symbols, Î£ âˆ— the set of finite strings, Î» âˆˆ Î£ âˆ— the empty string, and Î£$ â‰œ Î£ âˆª {$}, where $ P Ì¸âˆˆ Î£ is a special symbol used to denote termination. The probability simplex over Î£$ is âˆ†(Î£$) â‰œ {Ï : Î£ â†’ R+ | ÏƒâˆˆÎ£ Ï(Ïƒ) = 1}. The support of Ï âˆˆ âˆ†(Î£$) is supp(Ï) â‰œ {Ïƒ âˆˆ Î£ | Ï(Ïƒ) > 0}. A language model is a total function L : Î£âˆ— â†’ âˆ†(Î£$).\
Language models can be expressed in different ways, e.g., RNN, Transformers, and PDFA. Following [6], we define a PDFA A over Î£ as a tuple (Q, qin, Ï€, Ï„ ), where Q is a finite set of states, qin âˆˆ Q is the initial state, Ï€ : Q â†’ âˆ†(Î£$), and Ï„ : Q Ã— Î£ â†’ Q. Both Ï€ and Ï„ are total functions. We define Ï„ âˆ— and Ï€ âˆ— as follows: Ï„ âˆ— (q, Î») â‰œ q and Ï„ âˆ— (q, Ïƒu) â‰œ Ï„ âˆ— (Ï„ (q, Ïƒ), u), and Ï€ âˆ— (q, u) â‰œ Ï€(Ï„ âˆ— (q, u)). We omit the state if it is qin and write Ï„ âˆ— (u) and Ï€ âˆ— (u).\
A defines the language model such that A(u) â‰œ Ï€ âˆ— (u). Fig. 1 gives examples of PDFA. The number below q is the probability of termination Ï€(q)($), and the one associated with an outgoing transition labeled Ïƒ corresponds to Ï€(q)(Ïƒ).\
\
 P is used in [1, 11] to define the equivalence relation â‰¡ in Î£ âˆ— which is a congruence with respect concatenating a symbol::::info
This paper is  under CC BY-SA 4.0 by Deed (Attribution-Sharealike 4.0 International) license.]]></content:encoded></item><item><title>Is IEACD Truly Secure, or Just a Patched Illusion?</title><link>https://hackernoon.com/is-ieacd-truly-secure-or-just-a-patched-illusion?source=rss</link><author>Cryptanalyze</author><category>tech</category><pubDate>Tue, 8 Apr 2025 13:39:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sheng Liu, School of Computer Science and Electronic Engineering, Hunan University, Changsha 410082, Hunan, China;(2) Chengqing Li (Corresponding author), School of Computer Science, Xiangtan University, Xiangtan 411105, Hunan, China;(3) Qiao Hu, School of Computer Science and Electronic Engineering, Hunan University, Changsha 410082, Hunan, China.4. Cryptanalysis of IEACDTo cope with the insecurity problems of IEATD reported in [13], multiple confusion and diffusion operations are appended, making the algorithm become another algorithm IEACD. In fact, the original keystream generation mechanism indeed exists a serious pitfall, which leads to that the patched algorithm IEACD still cannot withstand chosen-plaintext attack. In this section, three weaknesses of IEACD are first analyzed to facilitate description of the following chosen-plaintext attack.\
4.1. Three weaknesses of IEACD\
â€¢ The real size of key space is much smaller than the expected one\
Due to the limitation of finite-precision presentation, dynamics of any chaotic system is definitely degraded\
when it is implemented in a digital device. As investigated in [14, 15, 16], the structure of the statemapping network (SMN) of a digitized chaotic system implemented with fixed-point precision e+1 is largely dominated by that with precision e. The short period problems of PRNG based on Logistic map (5) implemented in a digital device (with fixed-point arithmetic or floating-point arithmetic) were comprehensively discussed in [15]. As shown in Fig. 3, discretized Ikeda system obeys this rule also. No matter what the precision is, the SMN of discretized Ikeda system follows the following rules: 1) an SMN is composed of some weakly connected components; 2) there are some self-loops (an edge connecting a node to itself); 3) As for each connected component, there is one and only one cycle (including special cycle, selfloop), and every node evolves to it via a transient process; 4) Many nodes have two and only two parent nodes. Generating a pseudo-random number sequence by the orbits determined by a chaotic map is actually walking along a path of an SMN. Now, one can see that the period of a sequence by solving the discretized Ikeda system may be very short (even only one). So, there are a number of equivalent secret keys and invalid secret keys as for the function of IEATD and IEACD. Note that such pitfall always exists no matter how large the precision e gets.\
â€¢ Insensibility of keystream generation mechanics\
â€¢ Improper configuration of keystream\
The two-round crossover diffusion is performed to resist chosen-plaintext attack and differential attack, but the keystream Z used in permutation is wrongly reused in the diffusion part, which makes the algorithm more insecure. From Eq. (6), (8) and (10), one has\
Referring to Eq. (15) and XORing the two cipher-images, one has\
In the process of determining Z 0 , as for a known or given element z 0 (i), one attempts to find its neighbor by verifying whether the corresponding equation holds. Therefore, Z 0 is reconstructed by seeking the relative positions of elements. As mentioned before, after constructing Z 0 , the permutation vector Z can be restored via z(u(i)) = z 0 (i) for i = 0 âˆ¼ (HW âˆ’ 1)\
Now, the attack in case of b â‰¤ 1 is discussed. In fact, the special cases of b can be identified through Eq. (18a) and (18b). If b = 1, since z 0 (b âˆ’ 1) is the first element in Z 0 , no element can be found via Eq. (18b). Hence, one should determine z 0 (0) via Eq. (18a) and then find the remainder of Z 0 through Eq. (18c). If b = 0, the attack is failed. Since Eq. (18a) and (18b) both no longer hold, one would directly attempt to determine z 0 (1) throughThis paper analyzed security performance of an image encryption algorithm based on a first-order time-delay system IEATD and the enhanced version IEACD. Although another research group proposed a chosen-plaintext attack on IEATD, we presented an enhanced attack using the correlation between adjacent vectors of one plain-image and the corresponding cipher-image. Although IEACD is designed by the attacking group with intention to fix the security defects of IEATD, there still exist some security pitfalls, such as invalid secret keys, insensibility of keystream generation mechanics, and improper configuration of keystream. Based on these, we designed an efficient chosen-plaintext attack and verified it with extensive experiments. The serious insecurity of the two algorithms cannot be improved by simple modifications. They can work as typical counterexamples to remind us to recast scenario-oriented image encryption algorithms following the guidelines and lessons summarized in [9, 7, 3].This work was supported by the National Natural Science Foundation of China (no. 61772447), Scientific Research Fund of Hunan Provincial Education Department (no. 20C1759), and Science and Technology Program of Changsha (no. kq2004021).[1] A. L. Abu Dalhoum, B. A. Mahafzah, A. A. Awwad, I. Aldamari, A. Ortega, and M. Alfonseca, â€œDigital image scrambling using 2D cellular automata,â€ IEEE Multimedia, vol. 19, no. 4, pp. 28â€“36, 2012.\
[2] G. Ye and X. Huang, â€œAn image encryption algorithm based on autoblocking and electrocardiography,â€ IEEE Multimedia, vol. 23, no. 2, pp. 64â€“71, 2016.\
[3] C. Li, Y. Zhang, and E. Y. Xie, â€œWhen an attacker meets a cipherimage in 2018: a year in review,â€ Journal of Information Security and Applications, vol. 48, p. art. no. 102361, 2019.\
[4] O. Mannai, R. Bechikh, H. Hermassi, R. Rhouma, and S. Belghith, â€œA new image encryption scheme based on a simple first-order timedelay system with appropriate nonlinearity,â€ Nonlinear Dynamics, vol. 82, pp. 107â€“117, 2015.\
[5] A. Jolfaei, X.-W. Wu, and V. Muthukkumarasamy, â€œOn the security of permutation-only image encryption schemes,â€ IEEE Transactions on Information Forensics and Security, vol. 11, no. 2, pp. 235â€“246, 2016.\
[6] C. Li, D. Lin, J. Lu, and F. Hao, â€œCryptanalyzing an image en- Â¨ cryption algorithm based on autoblocking and electrocardiography,â€ IEEE Multimedia, vol. 25, no. 4, pp. 46â€“56, 2018.\
[7] M. Preishuber, T. Huetter, S. Katzenbeisser, and A. Uhl, â€œDepreciating motivation and empirical security analysis of chaos-based image and video encryption,â€ IEEE Transactions on Information Forensics and Security, vol. 13, no. 9, pp. 2137â€“2150, 2018.\
[8] J. Chen, L. Chen, and Y. Zhou, â€œCryptanalysis of image ciphers with permutation-substitution network and chaos,â€ IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 6, pp. 2494â€“ 2508, 2021.\
[9] C. E. Shannon, â€œCommunication theory of secrecy systems,â€ Bell System Technical Journal, vol. 28, no. 4, pp. 656â€“715, 1949.\
[10] X. Chai, X. Fu, Z. Gan, Y. Lu, and Y. Chen, â€œA color image cryptosystem based on dynamic dna encryption and chaos,â€ Signal Processing, vol. 155, pp. 44â€“62, 2019.\
[11] Z. Hua, Z. Zhu, Y. Chen, and Y. Li, â€œColor image encryption using orthogonal latin squares and a new 2D chaotic system,â€ Nonlinear Dynamics, vol. 104, p. 4505â€“4522, 2021.\
[12] K. Ikeda, H. Daido, and O. Akimoto, â€œOptical turbulence: chaotic behavior of transmitted light from a ring cavity,â€ Physical Review Letters, vol. 45, no. 9, p. 709, 1980.\
[13] M. Li, H. Fan, Y. Xiang, Y. Li, and Y. Zhang, â€œCryptanalysis and improvement of a chaotic image encryption by first-order time-delay system,â€ IEEE Multimedia, vol. 25, no. 3, pp. 92â€“101, 2018.\
[14] C. Fan and Q. Ding, â€œAnalysing the dynamics of digital chaotic maps via a new period search algorithm,â€ Nonlinear Dynamics, vol. 97, no. 1, pp. 831â€“841, 2019.\
[15] C. Li, B. Feng, S. Li, J. Kurths, and G. Chen, â€œDynamic analysis of digital chaotic maps via state-mapping networks,â€ IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 66, no. 6, pp. 2322â€“2335, 2019.\
[16] C. Li, K. Tan, B. Feng, and J. Lu, â€œThe graph structure of the gen- Â¨ eralized discrete arnold cat map,â€ IEEE Transactions on Computers, 2021.\
[17] C. Li and K.-T. Lo, â€œOptimal quantitative cryptanalysis of permutation-only multimedia ciphers against plaintext attacks,â€ Signal Processing, vol. 91, no. 4, pp. 949â€“954, 2011. [18] C. Li, D. Lin, and J. Lu, â€œCryptanalyzing an image-scrambling encryption algorithm of pixel bits,â€ IEEE Multimedia, vol. 24, no. 3, pp. 64â€“71, 2017.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Can One Plain Image Break an Entire Encryption Algorithm?</title><link>https://hackernoon.com/can-one-plain-image-break-an-entire-encryption-algorithm?source=rss</link><author>Cryptanalyze</author><category>tech</category><pubDate>Tue, 8 Apr 2025 13:30:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sheng Liu, School of Computer Science and Electronic Engineering, Hunan University, Changsha 410082, Hunan, China;(2) Chengqing Li (Corresponding author), School of Computer Science, Xiangtan University, Xiangtan 411105, Hunan, China;(3) Qiao Hu, School of Computer Science and Electronic Engineering, Hunan University, Changsha 410082, Hunan, China.Security is a key problem for the transmission, interchange and storage process of multimedia systems and applications. In 2018, M. Li et al. proposed in-depth security analysis on an image encryption algorithm based on a first-order time-delay system (IEATD) and gave a specific chosen-plaintext attack on it. Moreover, an enhanced version called as IEACD was designed to fix the reported security defects. This paper analyzes the essential structures of the two algorithms and evaluates their real security performances: 1) no efficient nonlinear operations are adopted to assure the sensibility of keystream; 2) the equivalent secret key of IEATD can be efficiently recovered from one known plain-image and the corresponding cipherimage; 3) IEACD can still be efficiently cracked with a chosen-plaintext attack. Both rigorous theoretical analyses and detailed experimental results are provided to demonstrate effectiveness of the advanced cryptanalytic methods.Social media not only drive product discovery and purchase, but also incur serious concern on the security and privacy of the images shared by the Internet users. Due to the special properties of multimedia information, the modern text encryption standards, such as AES and Triple DES, cannot efficiently protect them in general. To cope with the challenge, a number of special image encryption algorithms, e.g. joint encryption and compression together, were proposed every year [1, 2, 3]. It is well known that cryptography (designing encryption algorithm) and cryptanalysis (security analysis of a given encryption algorithm) are two integral parts of cryptology. The cryptanalysis results facilitate the designers strengthen or replace flawed algorithms. Cryptanalysis of a given image encryption scheme also provides a special perspective for promoting some multimedia processing techniques, e.g. image recovery. Some image encryption algorithms like that proposed in [4, 2] are found to be insecure to different extents from the viewpoint of modern cryptology [5, 6, 7, 8].\
The complex dynamics of a chaotic system demonstrated in an infinite-precision domain is very similar to the expected properties of a secure encryption scheme outlined âˆ—Corresponding author. Email address: DrChengqingLi@gmail.com (Chengqing Li) by Shannon in [9]. So, a large number of chaos-based encryption schemes were proposed in the past three decades [10, 11]. In [12], Ikeda adopted a one-variable differential difference equation to model light going around a ring cavity containing a nonlinear dielectric medium and found â€œchaoticâ€ phenomena in the transmitted field. In [4], Mannai et al. introduced the equationâ€™s variant\
as a chaos-based pseudorandom number generator (PRNG), where Î± and m are coefficients, and T is the positive delay time. The evolution of the dynamics is dependent on not only the present value x(t) but also earlier one x(t âˆ’ T). To solve equation (1), it is discretized with the following way: 1) each interval T is divided into N subintervals h and each subinterval h is approximated with a scalar value, where T = N Ã— h; 2) the N samples of each interval are considered as an N-dimension vector. In [4], an image encryption algorithm based on a time-delay Ikeda system (IEATD) was proposed. The designers of IEATD believed that utilizing the rich dynamics of a discretized Ikeda system and a new keystream generation mechanism associated with the average of all pixels of the plain-image can provide sufficient capacity to withstand known/chosen-plaintext attacks.\
In reality, the security strength of IEATD is very weak as its equivalent secret key can be obtained with only two chosen plain-images [13]. Meanwhile, M. Li et al. pointed out that two security defects exist in IEATD: 1) the regularity of the keystream and absence of position scrambling; 2) incapacity to resist differential attack [13]. To remedy the defects, they adopted much more complex encryption operations: permutation and crossover diffusion phases. In short, we call the enhanced image encryption algorithm using the crossover diffusion as IEACD. This paper focuses on security analysis of the two image encryption algorithms, IEATD [4] and IEACD [13]. We found that the authors of [13] did not notice a fatal drawback of the keystream generation mechanism: insensibility to minor change of a pixel. This leads to that IEACD still cannot withstand chosen-plaintext attack. Furthermore, there is improper keystream configuration in diffusion that almost discloses the whole keystream. The essential structures of the two algorithms cause that the equivalent secret key of IEATD and IEACD can be recovered with known-plaintext attack and chosen-plaintext attack, respectively.\
The rest of this paper is organized as follows. Section 2 concisely describes the encryption procedures of the encryption algorithm IEATD and its enhanced version IEACD. Then, Sec. 3 and Sec. 4 present the detailed cryptanalysis results on the two encryption algorithms, respectively. The last section concludes the paper.2. Description of two analyzed image encryption algorithms\
\
2.1. The framework of IEATD\
\
2.2. The enhanced elements of IEACD compared with IEATD\
To enhance the security level of IEATD, some extra operations were appended to withstand the chosen-plaintext attack reported in [13].\
â€¢ The added secret sub-keys: a positive integer C, the initial condition q(0) âˆˆ (0, 1) and the control parameter Î² âˆˆ (3.5699456, 4] of Logistic map\
\
â€¢ The modified encryption procedures:3. Cryptanalysis of IEATD\
\
3.2. The chosen-plaintext attack proposed by M. Li et al.\
To make the cryptanalysis of IEATD more complete, we briefly introduce the chosen-plaintext attack on IEATD proposed by M. Li et al. in [13] and comment its performance:\
\
3.3. Known-plaintext attack on IEATD:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Mira Muratiâ€™s AI startup gains prominent ex-OpenAI advisers</title><link>https://techcrunch.com/2025/04/08/mira-muratis-ai-startup-gains-prominent-ex-openai-advisers/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Tue, 8 Apr 2025 13:13:57 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Ex-OpenAI CTO Mira Muratiâ€™s new AI venture, Thinking Machines Lab, has gained two new prominent advisers: Bob McGrew, previously OpenAIâ€™s chief research officer, and Alec Radford, a former OpenAI researcher behind many of the companyâ€™s more transformative innovations. Thinking Machines Labâ€™s website was quietly updated with McGrew and Radfordâ€™s names sometime in March. A spokesperson [â€¦]]]></content:encoded></item><item><title>The Surprising Effects of Minimal Dialogue Context on AI Judgment</title><link>https://hackernoon.com/the-surprising-effects-of-minimal-dialogue-context-on-ai-judgment?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 8 Apr 2025 13:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Clemencia Siro, University of Amsterdam, Amsterdam, The Netherlands;(2) Mohammad Aliannejadi, University of Amsterdam, Amsterdam, The Netherlands;(3) Maarten de Rijke, University of Amsterdam, Amsterdam, The Netherlands.3.2 RQ1: Effect of varying amount of dialogue context To gauge the quality of the crowdsourced labels, we rely on inter-annotator agreement (Boguslav and Cohen, 2017; Carletta, 1996). In order to understand how the amount of dialogue context influences the quality of ratings by annotators, we calculate the agreement between annotators for both relevance and usefulness across the three variations; see Table 1. To address potential randomness in relevance ratings, given the binary scale, we randomly drop one rating from each dialogue and compute the agreement. We repeat this process for each annotator and calculate an average Cohenâ€™s Kappa score. For usefulness, we compute Kappa for each pair of annotators and then calculate the average. We assess the significance of the agreement using the Chi-squared method. All Kappa scores are statistically significant (p â‰¤ 0.05).\
We observe an increase in the Kappa and Tau score as the dialogue context increases from C0 to C7. Despite the lack of context in C0, there is a moderate level of agreement regarding the relevance of the current turn. With the introduction of more context in C3 and C7, comes an increase in agreement regarding the relevance of the current turn (see Table 1). Providing additional dialogue context seems to lead to higher levels of consensus among annotators. This is likely due to dataset characteristics: users tend to express their preferences early in the dialogue, rather than in subsequent exchanges. Hence, in the case of C0, which only includes the current turn, when the userâ€™s utterance is incomplete, lacking an explicit expression of their preference, annotators rate more dialogues as relevant compared to C3 and C7. Overall, we conclude that when annotators have insufficient information to come up with a judgment, they tend to judge the system positively, introducing a positivity bias (Park et al., 2018).\
We see in Table 1 (row 3) that despite the lack of context in C0, there is substantial agreement regarding the usefulness of the current turn. This is due to the availability of the userâ€™s next utterance, which serves as direct feedback on the systemâ€™s response, resulting in higher agreement than for relevance assessment. As more context is provided, there is an even higher level of agreement among annotators regarding the usefulness of the current turn. Access to a short conversation history significantly improves agreement on usefulness.\
Surprisingly, despite having access to the entire conversation history in C7, there is a slightly lower level of agreement than in C3. The complete dialogue context may introduce additional complexity or ambiguity in determining the usefulness of the current turn. This occurs when conflicting feedback arises from the userâ€™s next utterance compared to the previous dialogue context. For example, when the system repeats a recommendation that the user has already watched or stated before, and the user expresses their intent to watch the movie in the next utterance, it leads to divergent labels. Similar trend is observed with the Tau correlations though the values are lower compared to the Kappa scores.\
Label consistency across conditions. We examine the impact of varying amounts of dialogue context on the consistency of crowdsourced labels across the three variations for relevance and usefulness and report the percentage of agreement in Figure 3. We observe moderate agreement (58.54%) between annotations of C0 and C3, suggesting that annotators demonstrate a degree of consistency in their assessments when provided with different amounts of context. This trend continues with C0 and C7, where the agreement increases slightly to 60.98%. The most notable increase is between C3 and C7 (68.29%). As annotators were exposed to progressively broader contextual information, their assessments became more consistent.\
Usefulness behaves differently. We observe moderate agreement (41.71%) between C0 and C3, indicating a degree of consistency in annotator assessments within this range of context. A notable decrease in agreement is evident when comparing C3 and C7, down to 28.3% agreement. The most substantial drop is observed between C0 and C7, yielding a mere 14.63% agreement. These findings emphasize the significant impact of context on the consistency of usefulness annotations. For usefulness assessment providing annotators with a more focused context, improves their agreement.\
With respect to , we note considerable differences in the labels assigned by annotators as we vary the amount of dialogue context. As the context expands, annotators incorporate more information into their assessments, resulting in context-specific labels. Annotator judgments are shaped not only by response quality but also by the broader conversation. This highlights the complexity of the task and the need for a carefully designed annotation methodology that considers contextual variations. These findings emphasize the significance of dialogue context in annotator decision-making.]]></content:encoded></item><item><title>Amazon unveils a new AI voice model, Nova Sonic</title><link>https://techcrunch.com/2025/04/08/amazon-unveils-a-new-ai-voice-model-nova-sonic/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Tue, 8 Apr 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[On Tuesday, Amazon debuted a new generative AI model, Nova Sonic, capable of natively processing voice and generating natural-sounding speech. Amazon claims that Sonicâ€™s performance is competitive with frontier voice models from OpenAI and Google on benchmarks measuring speed, speech recognition, and conversational quality. Nova Sonic is Amazonâ€™s answer to newer AI voice models such [â€¦]]]></content:encoded></item><item><title>Snapchat rolls out Sponsored AI Lenses for brands</title><link>https://techcrunch.com/2025/04/08/snapchat-rolls-out-sponsored-ai-lenses-for-brands/</link><author>Aisha Malik</author><category>tech</category><pubDate>Tue, 8 Apr 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Snapchat is introducing Sponsored AI Lenses, a new ad format that lets brands engage with consumers in an immersive way. While Snapchat has offered brands the opportunity to pay for sponsored lenses on the platform for years, now they can leverage AI-generated experiences powered by Snapâ€™s proprietary generative AI technology. With these interactive lenses, brands [â€¦]]]></content:encoded></item><item><title>Blackbird gobbles up $50M for its blockchain-based payment-loyalty app for restaurants</title><link>https://techcrunch.com/2025/04/08/blackbird-gobbles-up-50m-for-its-blockchain-based-payment-loyalty-app-for-restaurants/</link><author>Ingrid Lunden</author><category>tech</category><pubDate>Tue, 8 Apr 2025 12:08:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A founder who has carved out a name for himself building products to help restaurants connect better with would-be diners has raised $50 million for his latest startup: a new take on the idea of customer loyalty.Â  Blackbird Labs has built a payments-meets-loyalty-meets-blockchain platform for restaurants to grow repeat business while reducing some of the [â€¦]]]></content:encoded></item><item><title>Returns are Rising Fast â€” and Not Enough Tools are Focused on Stopping Them at the Source</title><link>https://hackernoon.com/returns-are-rising-fast-and-not-enough-tools-are-focused-on-stopping-them-at-the-source?source=rss</link><author></author><category>tech</category><pubDate>Tue, 8 Apr 2025 12:00:18 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The average return rate in eCommerce is estimated at 17.6% for 2024. Nearly 1 in 5 products sold was returned. There are many factors affecting this high number, including the two opposing trends that I will elaborate on in this article. The first is the increase in the number of products and alternatives offered by online retailers. The second is the lowering of payment and return thresholds. Both factors impact the psychological decision making of us as consumers, and in doing so are partly responsible for the global â€˜return pileâ€™. But can we use the same psychological and marketing principles to reverse this trend? With this article, in addition to exploring these two trends, I try to take a positive look at how online providers can become part of the solution.The ever-expanding product catalogThere are now more online stores and products globally than ever before. The barriers to entry for starting online businesses have dropped tremendously over the past 10 years. Take Shopify for example, it charges â‚¬1 for the first 3 months of running your online store. One. Euro. Along with other global trends such as dropshipping, the number of online stores has skyrocketed. And that's a great trend for consumers, right? All those extra products and stores to choose from, that will help us choose the perfect product for our specific situation. Unfortunately, that is not the reality. All those extra products and brands do not help us make decisions that make us more satisfied. On the contrary, the opposite is itself the case. â€œDecision fatigueâ€ is a long-studied phenomenon, including in eCommerce:\
Research shows that consumers are up to 10 times more likely to make a confident decision when presented with fewer options. This highlights the undeniable impact of the overload of choices when making decisions.\
In our family, we have renamed this phenomenon â€œthe vacuum cleaner's dilemma.â€ Referring to my father's extensive search for the right vacuum cleaner. With a cord? Without cord? With bag? Without bag? Integrated dust buster? Or is that a waste of the dust buster still in the closet? How important is weight? Battery life? Collection capacity? â€¦\
Adding more products or alternatives to choose from not only impairs our ability to decide. It even reduces how satisfied we are with the purchase we made. This is due to the increase in the number of trade-offs (trade-offs) we have to make when choosing a product or product category. The more trade-offs we have to make, the more we become aware of the shortcomings of our chosen product and the advantages of the alternatives we could have chosen. This reduces our satisfaction with the product we chose. Combined with lower thresholds for returns and payment, returning the purchase is a very small step.Lowering the barriers to payment and returnsNow that we know that the increase in product and brand alternatives comes at the expense of our satisfaction with the product eventually delivered to our doorstep, why do we buy it anyway? One reason can be found in the lowering of barriers to making online payments and return options for online purchases. Payment providers like Klarna play well on psychological mechanisms like â€œthe pain of paying.â€ These strategies are very successful (if success is measured by the number of products sold) because they exploit our psychological and biological mechanisms. B.J. Fogg, professor of psychology and researcher at Stanford University, discovered this back in 2009. Behavior occurs when someone is motivated to do something (e.g., make a purchase) and when the barriers to performing that behavior are low. Online marketers have been applying this for years. They know how to â€œtriggerâ€ behavior online, increasing the likelihood that buyers will make a purchase. Just when we begin to doubt our decision, we come across a flashy text that reassures us: â€œBuy now, pay laterâ€ or â€œReturn free within 30 days. Ah, what a reliefâ€¦\
If we do not make the right decision, if we are not satisfied with our new AirFryer, iPhone case or even a new car, we can return it without worry. All the barriers that once stood between us and the purchase are gone. This trend, combined with our tendency to be less satisfied with what we buy, has a big part in why we return our newly purchased items just as easily.Online stores can help people feel happier with their own purchase, or in more commercial terms: increase the perceived value of a purchase. This can reduce the likelihood that people will effortlessly return a purchase. How can we increase this perceived value?\
First, by helping consumers decide which product to choose and not overloading them with all the hundreds of products and alternatives the Web shop offers. Web stores need to work on their ability to give consumers product advice so they know they are making the right decision. There are already tools available to help you do this. This principle is known as â€œguided sellingâ€, which involves asking questions about the buyer's individual situation, preferences and intended use of the product. Based on their answers, a product that meets their needs is recommended. This approach eliminates the need for endless product comparisons.\
Another way to increase the perceived value of a purchase is to make it feel â€œtheirs.â€ This is due to a phenomenon called the â€œendowment effect.â€ This is a psychological bias that causes us to place a higher value on things we own, as opposed to things we don't own. This is one reason why physical technology stores often have showrooms where you can just grab a phone and try it out. This increases the feeling that the product is â€œyours,â€ and thus the psychological price tag you attach to it.\
There is a good chance that using psychological principles to increase the perceived value of products can help reduce returns. Currently, 1 in 5 products sold that are returned is excessive. This problem also requires a change in mindset among online retailers. They really need to ask themselves if the product they are selling is a perfect fit for their customers. By doing this, we can improve buyers' online decision-making and collectively contribute to improving global e-commerce.]]></content:encoded></item><item><title>One Decision Tree Is a Riskâ€”A Forest? Thatâ€™s Smart Machine Learning</title><link>https://hackernoon.com/one-decision-tree-is-a-riska-forest-thats-smart-machine-learning?source=rss</link><author>Vasanth Rajendran</author><category>tech</category><pubDate>Tue, 8 Apr 2025 12:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Have you ever tried to make a tough decision all on your own and ended up second-guessing everything? Sometimes it helps to ask aroundâ€”maybe a friend, a coworker, or even your know-it-all neighbor. Random Forests work on a similar idea. Instead of letting one decision tree call all the shots (and risk overfitting), you invite a whole â€œforestâ€ of decision trees to weigh in, then combine their votes. The result? More stable, more robust predictions.\
If youâ€™re just getting started with machine learning, or even if youâ€™ve been around the block, Random Forests are a friendly, approachable way to tackle classification and regression tasks. Letâ€™s see how they work, why theyâ€™re so effective, and how you can easily build one in Python.The Problem with Single Decision TreesImagine youâ€™re predicting whether a user subscribes to a SaaS product based on their age, income, and how many times theyâ€™ve visited your pricing page. A single decision tree might look something like this:Is age > 30?
â”œâ”€â”€ Yes:
â”‚   â””â”€â”€ Is income > 50K?
â”‚       â”œâ”€â”€ Yes: SUBSCRIBE
â”‚       â””â”€â”€ No: NOT SUBSCRIBE
â””â”€â”€ No:
    â””â”€â”€ Visited pricing page > 3 times?
        â”œâ”€â”€ Yes: SUBSCRIBE
        â””â”€â”€ No: NOT SUBSCRIBE
\
Looks neat and easy to follow, right? But it's likely to overfit, memorizing the training data rather than generalizing well to unseen data. This can result in surprisingly poor performance when you try to use it "in the wild."Enter Random Forests: A Chorus of OpinionsA Random Forest creates dozens (or even hundreds) of different decision trees, each trained on a slightly different subset of your data. Then, it combines their predictions through majority vote (for classification) or averaging (for regression).\
Hereâ€™s what makes it so effective: Each tree gets a random subset of the training data (a method called bootstrap sampling), which injects variety and prevents all trees from seeing the exact same data. At every split in every tree, only a random subset of features is considered. This means one very strong feature doesnâ€™t overshadow all others in every tree. Because each tree has its own quirks, combining them reduces variance. Thatâ€™s like asking a bunch of people the same question: the consensus is often more reliable than any single individualâ€™s opinion.Random Forests are particularly popular for a few reasons:Resilience to Overfitting: With many independent trees voting, errors made by individual trees tend to cancel out.: Decision trees naturally capture non-linear relationships and interactions without any special feature engineering.: Out of the box, many Random Forest implementations provide insights into which features drive predictions the most.: While there are hyperparameters to tweak, Random Forests often produce good results with relatively little optimization.\
These strengths have made Random Forests a go-to algorithm for countless use cases, from e-commerce conversion predictions to biomedical classification tasks.Quick Python Example (Because Seeing is Believing)Below is a simplified Python snippet using scikit-learn to predict whether customers will subscribe to a service. Feel free to tweak the parameters and see what happens.import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Sample customer data
data = {
    'age': [22, 25, 47, 52, 46, 56, 55, 44, 42, 59, 35, 38, 61, 30, 41, 27, 19, 26, 48, 39],
    'income': [25000, 35000, 75000, 81000, 62000, 70000, 91000, 42000, 85000, 55000, 
               67000, 48000, 73000, 36000, 59000, 30000, 28000, 37000, 65000, 52000],
    'visits': [2, 4, 7, 3, 6, 1, 5, 2, 8, 4, 5, 7, 3, 9, 2, 5, 6, 8, 7, 3],
    'subscribed': [0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0]
}

df = pd.DataFrame(data)

# Separate features and target
X = df[['age', 'income', 'visits']]
y = df['subscribed']

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Train a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test data
y_pred = rf_model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Random Forest Accuracy: {accuracy * 100:.2f}%")

# Feature importance visualization
importances = rf_model.feature_importances_
features = X.columns
indices = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.title('Feature Importance')
plt.barh(range(len(indices)), importances[indices], align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.tight_layout()
plt.show()
Code Walkthrough: Making Sense of the ExampleLetâ€™s break down the key parts of this example:   pythonCopyEditimport pandas as pd
   import numpy as np
   from sklearn.ensemble import RandomForestClassifier
   from sklearn.model_selection import train_test_split
   from sklearn.metrics import accuracy_score
   import matplotlib.pyplot as plt
Weâ€™re using  for handling tabular data,  for numerical operations,  for the machine learning part, and  for basic plotting.   pythonCopyEditdata = {
       'age': [...],
       'income': [...],
       'visits': [...],
       'subscribed': [...]
   }
   df = pd.DataFrame(data)
This dictionary simulates a small dataset of 20 customers. Each customer has: (how many times they visited the pricing page), and (1 for subscribed, 0 for not subscribed).Then we turn it into a  called  so we can easily manipulate it.Separating Features and Target   pythonCopyEditX = df[['age', 'income', 'visits']]
   y = df['subscribed']
Here,  holds the input features (, , and ). is our target variable (), which we aim to predict.Splitting into Training and Test Sets   pythonCopyEditX_train, X_test, y_train, y_test = train_test_split(
       X, y, test_size=0.3, random_state=42
   )
We split the data so that 70% goes into training and 30% goes into testing.The  ensures reproducibility, meaning each run splits the data the same way.Creating and Training the Random Forest   pythonCopyEditrf_model = RandomForestClassifier(n_estimators=100, random_state=42)
   rf_model.fit(X_train, y_train)
We instantiate a  with  trees.Then we call  to train the model on our training data.   pythonCopyEdity_pred = rf_model.predict(X_test)
We feed the  into our trained , which returns predictions for each test sample.   pythonCopyEditaccuracy = accuracy_score(y_test, y_pred)
   print(f"Random Forest Accuracy: {accuracy * 100:.2f}%")
We compare the modelâ€™s predictions () against the true labels () using .The result is printed as a percentage.Inspecting Feature Importance   pythonCopyEditimportances = rf_model.feature_importances_
   features = X.columns
   indices = np.argsort(importances)

   plt.figure(figsize=(10, 6))
   plt.title('Feature Importance')
   plt.barh(range(len(indices)), importances[indices], align='center')
   plt.yticks(range(len(indices)), [features[i] for i in indices])
   plt.xlabel('Relative Importance')
   plt.tight_layout()
   plt.show()
 gives you a measure of how relevant each feature is for the modelâ€™s decision-making.We sort these importances to make a neat bar chart, labeling the bars by the corresponding feature names (, , ).The higher the bar, the more that feature influenced the forestâ€™s decisions.\
And thatâ€™s it! Youâ€™ve just built a working Random Forest model, evaluated its accuracy, and checked which features mattered most.Practical Tips to Get the Most Out of a Random Forest \n Start with something like 100 (or 200) trees. If you have the compute to spare, go bigger, and watch performance stabilize.Wisely \n If your dataset is huge or each tree is taking forever, consider limiting depth. By default, scikit-learn grows trees until theyâ€™re nearly perfect on training data.If one class (e.g., â€œsubscribedâ€) is only 5% of your data, accuracy might fool you. Consider looking at precision, recall, or F1-score.Tune, But Donâ€™t Over-TuneA Random Forest usually performs decently with default settings. If youâ€™re up for it, hyperparameters like  and  can be fine-tuned via GridSearchCV or RandomizedSearchCV.Each tree is independent. If training time is dragging, leverage multiple CPU cores by setting  in scikit-learn (assuming your environment supports it). If you need full transparency, a single decision tree is simpler to interpret. A forest is more complex but can still provide feature importances. Large forests with deep trees can chew through CPU time. Keep an eye on those training times if you have a monstrous dataset. Trees arenâ€™t great at extrapolating beyond the range of data theyâ€™ve seen. If your input goes way beyond your training distribution, you might get weird results.If you love the idea of ensembling, thereâ€™s a whole world out there:Gradient Boosting Methods: Like XGBoost or LightGBM, which build trees in a sequential, boosting manner.: Combine Random Forest outputs with other modelsâ€™ outputs to build a â€œmeta-model.â€: Not tree-based, but also powerful ensembles can be built with different neural network architectures.Random Forests are like a group of friends who all see the world from slightly different angles. When they team up, they often spot patterns that a single viewpoint could miss. If youâ€™re facing a classification or regression challenge and want something reliable without diving into intense hyperparameter tuning, try a Random Forest.\
Got any Random Forest success stories or cautionary tales? Share them in the comments. This is all about learning from each otherâ€”after all, a little â€œcollective wisdomâ€ never hurts.]]></content:encoded></item><item><title>Sizl raises $3.5M to expand its cook-to-order food delivery service</title><link>https://techcrunch.com/2025/04/08/sizl-raises-3-5m-to-expand-its-cook-to-order-food-delivery-service/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Tue, 8 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Dark kitchens â€” also referred to as ghost kitchens, cloud kitchens, or virtual kitchens â€” often receive criticism for their food quality. Sizl, a cook-to-order delivery service based in Chicago, seeks to change this perception by delivering meals made with fresh ingredients in around 30 minutes flat. The company announced its recent $3.5 million seed [â€¦]]]></content:encoded></item><item><title>XL Batteries is using petrochemical infrastructure to store solar and wind power</title><link>https://techcrunch.com/2025/04/08/xl-batteries-is-using-petrochemical-infrastructure-to-store-solar-and-wind-power/</link><author>Tim De Chant</author><category>tech</category><pubDate>Tue, 8 Apr 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The startup has commissioned a demonstration unit of its organic flow battery.]]></content:encoded></item><item><title>India&apos;s &apos;Frankenstein&apos; Laptop Economy Thrives Against Planned Obsolescence</title><link>https://it.slashdot.org/story/25/04/08/1116241/indias-frankenstein-laptop-economy-thrives-against-planned-obsolescence?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 11:16:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In Delhi's Nehru Place and Mumbai's Lamington Road, technicians are creating functional laptops from salvaged parts of multiple discarded devices. These "Frankenstein" machines sell for approximately $110 USD -- a fraction of the $800 price tag for new models. Technicians extract usable components -- motherboards, capacitors, screens, and batteries -- from e-waste sourced locally and from countries like Dubai and China. 

"Most people don't care about having the latest model; they just want something that works and won't break the bank," a technician told Verge. This repair ecosystem operates within a larger battle against tech giants pushing planned obsolescence through proprietary designs and restricted parts access. Many technicians source components from Seelampur, India's largest e-waste hub processing 30,000 tonnes daily, though workers there handle toxic materials with minimal protection. "India has always had a repair culture," says Satish Sinha of Toxics Link, "but companies are pushing planned obsolescence, making repairs harder and forcing people to buy new devices."]]></content:encoded></item><item><title>Mint, Match, Mingle: A Better Way to Connect on the Blockchain</title><link>https://hackernoon.com/mint-match-mingle-a-better-way-to-connect-on-the-blockchain?source=rss</link><author>Ashleigh</author><category>tech</category><pubDate>Tue, 8 Apr 2025 11:09:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most people think NFTs peaked with overpriced jpegs and Twitter flexes. Cute. But here's a seriously slept-on use case:Â . Imagine tokens that aren't just tradableâ€”they'reÂ . My project idea, SOLmates, flips NFTs into connectors. Each token is minted with a soulmate, but that pair stays locked until the right two holders come along. Itâ€™s Tinder meets cryptography, minus the awkward DMs.\
This isn't just about dating. It's about programmable social bonds, and yeah, it's flying under the radar because people still think â€œutilityâ€ means an airdrop.When an NFT is minted, itâ€™s born with a unique soulmate token.That soulmate is locked in limbo (on-chain, off-market) until the right user triggers the match.Matching is based on a mix of astrological data, personality traits, and user-submitted preferences.Think: if eHarmony and an ERC-721 had a baby.Bonus: Holders can unlock clues to their match by providing liquidity to the projectâ€™s stablecoin-paired ecosystem.\
Itâ€™s social matchmaking, powered by smart contracts and zero ghosting.Why this blockchain use case mattersThe problem: online connections are shallow, noisy, and gamified to death. Algorithms feed dopamine, not compatibility.\
This use case reintroducesÂ . You're not swiping endlesslyâ€”youâ€™re waiting forÂ . And blockchain makes that possible because:Tokens are provably unique.Ownership is verifiable and transparent.Matching can be enforcedÂ Â giving your entire birth chart to Meta.\
It also hints at a bigger picture: using blockchain for authentic, meaningful human connections. Wild, I know.Challenges and OpportunitiesPeople think itâ€™s â€œjust another dating app with extra steps.â€Regulatory confusion around NFTs as "relationship services" (which is hilarious but possible).Scaling the matchmaking engine without doxing users.Tap into niche communities (astrology nerds, MBTI junkies, web3 romantics).Add gated social layersâ€”only matched tokens can access certain spaces.Expand beyond romance: friendships, co-founders, accountability buddies. Soulmates come in many forms.\
How to overcome? Start small. Educate through memes. Partner with DAOs that already prioritize community and identity. And build slowâ€”connections worth waiting for donâ€™t rush.Honestlyâ€¦ I donâ€™t know. WhatÂ Â be next?\
Iâ€™ve got the idea, Iâ€™ve got the whitepaper, and Iâ€™ve got way too many notes in my phone about birth charts and liquidity pools. But now I need help turning this into something real.\
So if this matchmaking NFT concept has you thinking, â€œWaitâ€¦ this could actually work,â€ then coolâ€”help me build it.\
Dev? Designer? Matchmaking nerd? Just vibing with the idea? \n Slide in. Letâ€™s make this thing weird, wonderful, and on-chain.]]></content:encoded></item><item><title>How to Check SSL expiration with Bash</title><link>https://hackernoon.com/how-to-check-ssl-expiration-with-bash?source=rss</link><author>Konstantin Bogomolov</author><category>tech</category><pubDate>Tue, 8 Apr 2025 11:06:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[There are a few easy ways to check when an SSL certificate expires.Write Your Own Shell ScriptHere's a simple script that checks the expiration date of an SSL certificate:#!/bin/sh
TARGET=$1

if [ -z "$TARGET" ]; then
  echo "Provide a host as a parameter"
  exit 1
fi

echo "Checking $TARGET..."
expirationdate=$(openssl s_client -connect example.com:443 -servername example.com 2>/dev/null \
                              | openssl x509 -text \
                              | grep 'Not After' \
                              | awk '{print $4,$5,$7}')

if [ -z "$expirationdate" ]; then
  echo "Failed to retrieve certificate expiration date for $TARGET"
  exit 1
fi

echo "Certificate expires on $expirationdate"
You can also use a ready-made tool like ssl-cert-check for more features.Get Notified with a Telegram Bot]]></content:encoded></item><item><title>Escape Prompt Hell With These 8 Must-have Open-source Tools</title><link>https://hackernoon.com/escape-prompt-hell-with-these-8-must-have-open-source-tools?source=rss</link><author>Albert Lie</author><category>tech</category><pubDate>Tue, 8 Apr 2025 11:04:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Remember when prompt engineering meant clever ChatGPT hacks and intuition-driven guesswork? Those days are long gone. As large language models (LLMs) become embedded in enterprise workflows, the tools we use to build with them need to grow up too.\
Today, prompt engineering is shifting from creativity and trial-and-error to something that resembles software development. Itâ€™s about building systems that are testable, observable, and improvable. Whether youâ€™re designing agents for production or experimenting with multi-step pipelines, you need tools that let you optimize prompts systematically.\
This article explores eight projects that are redefining prompt engineering. From visual workflows to auto-tuned prompts, these tools help you scale your LLM projects without losing control or clarity.AdalFlow is a PyTorch-inspired framework that lets developers build and optimize LLM workflows declaratively. Its core strength is combining expressive Python APIs with automatic optimization for latency, performance, and cost. Just like a PyTorch , you can define your own reusable building blocks for LLM workflows, including routing logic, RAG components, or agents.AutoDiff + Static Graph Compilation: Behind the scenes, AdalFlow compiles your  into an efficient DAG, minimizing unnecessary LLM calls. You define logic once and can then execute it locally, remotely, or in streaming mode using pluggable executors.\
 You can construct an  that combines retrieval (via RAG), structured prompt formatting, and function call-style output validationâ€”all in one unified pipeline.\
AdalFlow is designed for production-grade LLM apps with strict latency budgets and clear reliability requirements.Ape, created by Weavel, is a prompt engineer co-pilot that helps you test, debug, and improve your LLM applications. It is designed to eliminate the need for gut-based prompt tuning by giving developers structured, inspectable feedback on how their agents behave.Capture and Replay Traces: Ape records every prompt, tool call, response, and retry in a session. You can replay specific steps or re-run chains with modified prompts to see how behavior changes.Prompt Iteration Comparison: It supports side-by-side comparisons between different prompt versions, allowing you to benchmark performance, accuracy, or hallucination reduction.\
 Ape acts like your first prompt engineer hireâ€”automating the trial-and-error loop with traceability and insight. Instead of asking â€œwhat went wrong?â€ you get to see exactly how the agent behaved and what led to it.AutoRAG is an open-source framework that helps you build, evaluate, and optimize Retrieval-Augmented Generation (RAG) pipelines using your own data. Itâ€™s ideal for developers and researchers who want to test different RAG setupsâ€”like chunking strategies, retrievers, and rankersâ€”without rebuilding the whole pipeline manually. Includes modular implementations of common RAG components: embedding models (e.g. OpenAI, Cohere), chunkers, retrievers (e.g. FAISS), rankers, and response generators. Define an evaluation set (context + query + expected answer), and AutoRAG will automatically compare different pipelines using metrics like EM (Exact Match), F1, ROUGE, and BLEU. Automatically evaluates combinations of modules and hyperparameters to find the best-performing configuration on your data. Provides a clean web-based UI to visualize pipeline performance, outputs, and comparison metrics.\
 Designing a RAG pipeline involves many moving parts: how you chunk documents, which embedding model you use, what retriever to apply, etc. AutoRAG automates this experimentation process, saving hours of trial-and-error and helping you find optimal setups fast.DSPy is a powerful framework from Stanford NLP that brings structure and optimization to prompt engineering by treating LLM components like programmable modules. You define a  (input/output schema) for each moduleâ€”for example, a summarizer takes in a paragraph and returns a concise sentence. Instead of writing prompts manually, you compose your app from building blocks like: â€“ simple generation â€“ ranking or classification tasks â€“ multi-step reasoning â€“ retrieval-augmented modules DSPy comes with built-in optimizers like , which run experiments to find the best prompt structure, formatting, and LLM configuration using few-shot or retrieval-based techniques. You can define LLM workflows as reusable Python classes with structured inputs/outputs. Run evaluations on labeled datasets and let DSPy optimize prompt phrasing or example selection automatically. Track experiments, prompt variants, and performance metrics over time.\
 DSPy brings ML-style engineering workflows to LLM development. Itâ€™s not just a wrapperâ€”itâ€™s an ecosystem for building, testing, and optimizing modular LLM applications.Zenbase Core is the library for programmingâ€”not promptingâ€”AI in production. It is a spin-out of Stanford NLP's DSPy project and is led by several of its key contributors. While DSPy is excellent for research and experimentation, Zenbase focuses on turning those ideas into tools suitable for production environments. It brings the power of structured memory, retrieval, and LLM orchestration into the software engineering workflow. DSPy is built for R&D, where developers test and evaluate ideas. Zenbase adapts those ideas for production, emphasizing reliability, maintainability, and deployment-readiness.Automatic Prompt Optimization: Zenbase enables automatic optimization of prompts and retrieval logic in real-world applications, integrating seamlessly into existing pipelines. Designed for software teams that need composable, debuggable LLM programs that evolve beyond prototype.Zenbase is ideal for developers who want to treat prompt engineering as real engineeringâ€”modular, testable, and built for scale.AutoPrompt is a lightweight framework for automatically improving prompt performance based on real data and model feedback. Rather than relying on manual iterations or human intuition, AutoPrompt uses an optimization loop to refine prompts for your specific task and dataset.\
 Prompt tuning typically involves testing dozens of phrasing variations by hand. AutoPrompt automates this, discovers blind spots, and continuously improves the promptâ€”turning prompt writing into a measurable and scalable process.EvoPrompt is a Microsoft-backed research project that applies evolutionary algorithms to optimize prompts. It reframes prompt crafting as a population-based search problem: generate many prompts, evaluate their fitness, and evolve the best-performing ones through mutation and crossover. Start with a set of candidate prompts for a specific task. Each prompt is scored using a defined metric (e.g., accuracy, BLEU, human eval). introduces small, random changes to improve performance. combines high-performing prompts into new variants. keeps the top-performing prompts for the next generation. The process repeats over multiple generations until performance converges.Differential Evolution (DE)Tree-based crossover operations using LLMs\
 Writing the perfect prompt is hardâ€”even harder when doing it at scale. EvoPrompt turns prompt design into a computational optimization problem, giving you measurable gains without human micromanagement.Promptimizer is an experimental Python library for optimizing prompts using feedback loops from LLMs or human raters. Unlike frameworks that focus purely on generation or evaluation, Promptimizer creates a structured pipeline for systematically improving prompt quality over time.\
 Promptimizer gives prompt engineering the same kind of feedback loop youâ€™d expect in UX testing or ML training: test, measure, improve. Itâ€™s especially powerful for copywriting, content generation, and any task where subjective quality matters.These tools are transforming prompt engineering from an art into a disciplined engineering practice: Optimized prompts use fewer tokens, directly reducing API expenditures. Tools like AdalFlow and AutoRAG decrease development time from days to minutes. Frameworks like EvoPrompt enhances benchmark scores by up to 15%. Systems such as Ape and DSPy support auditability and repeatability.\
Prompt engineering is no longer just a skillâ€”it has evolved into a comprehensive stack.The future of LLM applications doesnâ€™t belong to clever hacks but to scalable infrastructure. Whether you're addressing workflow complexity with AdalFlow, debugging agents with Ape, or optimizing instructions with AutoPrompt and EvoPrompt, these tools elevate you from intuition-based methods to reliable engineering practices.\
The return on investment is tangible: from sub-$1 optimization runs to significant conversion boosts, effective prompting proves its value.Looking ahead, we anticipate tighter integrations with fine-tuning, multi-modal prompt design, and prompt security scanners. The message is clear:\
The era of artisanal prompting is behind us. Welcome to industrial-grade prompt engineering. Build better prompts. Build better systems.]]></content:encoded></item><item><title>Muddy Waters Didnâ€™t Short AppLovin. They Shorted You.</title><link>https://hackernoon.com/muddy-waters-didnt-short-applovin-they-shorted-you?source=rss</link><author>susie liu</author><category>tech</category><pubDate>Tue, 8 Apr 2025 11:03:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Muddy Watersâ€™ recent short report on AppLovin reads serious. Abuse, violations, an impending takedown.\
But the discerning will realize somethingâ€™s missingâ€”like the stuff real short reports usually include. Somehow people are still treating it like theyâ€™ve found a dead unicorn in the server logs. See, the brilliance of the report isnâ€™t the facts. Itâ€™s the lack of them. Just enough jargon and innuendo to let the market fill in the fear. And fear works fabulously hereâ€”we already hate ads. We already find them creepy. So when someone implies a high-performing ad platform is doing something unethical? Sounds about right.This isnâ€™t a thesis. Itâ€™s a dare: â€œWe bet you donâ€™t understand this company well enough to call our bluff.â€\
Theyâ€™re not betting against AppLovinâ€™s business, but against your comprehension. Classic intellectual arbitrage.\
Yes, Grok could disprove the paper. But as a marketer, Iâ€™m annoyed enough to go off on a rant of my own and explain how this stack worksâ€”and why none of their claims hold up.What Muddy Waters Is Actually Saying (In Plain English, With a Straight Face)\
Hereâ€™s the report, minus the hedge fund perfume:AppLovinâ€™s ads mostly go to people who were already going to buy.They use a tracking pixel that watches what people do on websitesâ€”and maybe that crosses a line with Apple or Google.Their AI engine (AXON) sounds impressive but probably isnâ€™t doing anything special.Some ecomm advertisers tested the platform and didnâ€™t come back.It smells like Cheetah Mobileâ€”because why not throw in a fraudulent Chinese app that was banned, just for dramatic effect?\
Thatâ€™s it. No cooked books, internal emails, or a â€œholy sh*tâ€ revelation. Just a handful of what-ifs and â€œtrust us, this is badâ€ vibes duct-taped together with technical vocab all compressed into an opportunistic PDF.\
It all sounds terrifying if youâ€™ve never run an ad campaign.\
But for all those words, not a single one was spent on the most basic question: What does this company actually do?\
Probably because once you do, most of their arguments fall apart. .What AppLovin Actually Does (Since Apparently That Wasnâ€™t Relevant)\
AppLovin shows ads inside mobile appsâ€”mostly games. You open a game, you watch an ad to get more coinsâ€”thatâ€™s them. Just infrastructure that makes in-app ads work.Developers plug in the SDK (MAX)MAX powers the ads inside mobile gamesâ€”rewarded videos, playables, interstitialsâ€”and automatically chooses the highest-paying one.It fills the space, runs the auction, and handles the logic.\
[Sidebar: How AppLovin Quietly Took Over 140,000 Apps]\
AppLovin didnâ€™t try to out-Unity Unity. Unity built for prestige. Big studios. Serious games. AppLovin built for the ones Unity scoffed atâ€”hypercasuals, solo devs, and the teams that just needed to make rent. Turns out there are a lot more of them.\
And MAX didnâ€™t launch cold. AppLovin owned big studios, and pumped its own ad budgets through the systemâ€”spinning up liquidity before anyone else even showed up. Day one, it worked. Thatâ€™s how you take over 140k apps without a parade: just pay people faster than anyone else.Advertisers bid through AppLovin Exchange (ALX)Mostly game studios and ecommerce brands. ALX pulls in demand from DSPs, programmatic buyers, and direct clients. Then AXON, the machine learning engine, decides which ad shows up, where, and for how much.If a user taps, installs, or buysâ€”the advertiser pays. AppLovin takes a cut. The rest goes to the developer. The better the ad performs, the more it gets shown. The more it gets shown, the more everyone earns.\
Most adtech stacks rent the data, piggyback the pipes, and take a cut on their way through. They sit between the buyer and the seller and hope no one notices they donâ€™t do anything.AppLovin doesnâ€™t rent. It runs.\
It owns the SDK. The auction. The exchange. The optimization engine. It makes the decisions and delivers the results. That level of control is rare in adtech.  Itâ€™s what Apple, Amazon, and Google have done for yearsâ€”because full control isnâ€™t a red flag. Itâ€™s just what being better built looks like.\
So noâ€”AppLovin isnâ€™t shady. But pretending full-stack execution is fraud? That might be.An Autopsy of a Report That Never Had a Pulse\
Greatâ€”now that weâ€™ve done their homework for them, time to grade their report. Gloves off.\
One of the main claims in the Muddy Waters report is that over half of AppLovinâ€™s conversions come from retargeting. They frame that like itâ€™s a bad thing. Like AppLovin is cheating the system by showing ads to people who were already going to convert.Hereâ€™s the part they skip: retargeting is the part of marketing that gets people to pay.\
Think about it: Youâ€™re running an ecomm brand, trying to acquire users profitably. If someone clicked your site, added to cart, then bailedâ€”you want that person back. You donâ€™t need 100% incrementality. You need ROI.And thatâ€™s exactly what AppLovin helps advertisers optimize for: time-to-purchase, payback window, LTV curvesâ€”not brand awareness. Direct-response, down to the decimal point.\
So when the report implies that retargeting = fake conversions, the only thing theyâ€™re exposing is that they know less about marketing funnels than your intern.â€œTheir Pixel Might Violate Platform Rulesâ€ â€” Cute Story, Still No Evidence\
Muddy Waters tries to paint AppLovinâ€™s tracking pixel as a liabilityâ€”implying it might violate Apple or Google policies and lead to a Cheetah Mobile-style takedown. They donâ€™t show how. They donâ€™t show that it has. They just raise the question and let paranoia do the rest.\
This part of the report is the classic hedge fund trick:  Float the idea of a Terms of Service violation, backfill with vibes, then hope the market scares itself.\
Donâ€™t let them terrify you:Every major ad platform uses pixels. Thatâ€™s how you track conversions. AppLovinâ€™s version isnâ€™t weirdâ€”itâ€™s standard.Unlike everyone elseâ€™s pixels, AppLovinâ€™s never needed device IDs. It was built to work after Apple nuked IDFA. Thatâ€™s the whole point of AXON: make smart predictions with limited data.Thereâ€™s no sign of platform backlash. Apple and Google are not subtle when they donâ€™t like you. If AppLovin were violating anything serious, they wouldnâ€™t still be embedded in 270,000+ apps. Theyâ€™d be gone.\
So unless Muddy Waters has an email from Tim Cook saying, â€œWeâ€™re coming for AppLovin,â€ this is just a vibe-based accusation with nothing underneath it.\
And worse, this argument completely misses the point. The pixel isnâ€™t the advantage. Itâ€™s the input. The real edgeâ€”the reason AppLovin worksâ€”is AXON. And the fact that Muddy Waters barely mentions it tells you everything.AXON Isnâ€™t Overhyped. Itâ€™s the Reason AppLovinâ€™s Still in Business.\
Muddy Waters tries to make AppLovinâ€™s pixel sound like a surveillance device. . Itâ€™s just a signal.\
The real story is AXON, which the firm conveniently brushes off as â€œoverhyped AIâ€. Sure. And Google is just a search bar.\
AXONâ€™s the entire reason AppLovin didnâ€™t get body-bagged by Appleâ€™s privacy changes. They didnâ€™t have the biggest name or the most headlines. They were simply tighter, faster, and meaner. Hereâ€™s what gave AXON the edge:AppLovin controls the whole loop. The SDK handles the ad call. Their ad server runs the bidding. AXON saw the whole picture: who bid, who won, what ran, what worked. Thatâ€™s the training data advantage everyone else wishes they had.They trained it in the most cutthroat ad environment there is: mobile games. Most of AppLovinâ€™s ads run inside mobile games. Thatâ€™s a ruthless environment: users skip garbage, and devs cut platforms that donâ€™t monetize. And they didnâ€™t just serve into that worldâ€”they owned a piece of it. That gave AXON full visibility: clicks, retention, spend, behaviorâ€”not just impressions, but outcomes. AXON was born after IDFAâ€™s funeral. While everyone else was scrambling to patch their tracking-addicted systems, AppLovin skipped the panic and started clean. No duct tape. No rewrites. Just a head startâ€”and a system that never needed surveillance to work.\
It learned fast. It learned under pressure. And that pressure built a monster. So, AXON isnâ€™t overhyped. Itâ€™s just uncomfortable for anyone who built slower or trained dumber.â€œChurnâ€ Isnâ€™t a Red Flag. Itâ€™s a Line in a Spreadsheet Without Context.\
Muddy Waters cites a 23% churn rate among AppLovinâ€™s ecommerce advertisers like itâ€™s proof of product rejection. But hereâ€™s what they leave out: AppLovin only opened its platform to ecommerce in late 2024. These werenâ€™t legacy clients backing out. They were net-new performance advertisersâ€”most of them mobile-first for the first timeâ€”testing in-app ads with small, quarterly budgets.\
Other things Muddy decided to skip over:Thereâ€™s no sign this churn mattered. Revenue kept growing. Margins held. Budgets didnâ€™t vanish. If this was true attrition, it would show up in the top line. It didnâ€™t.Performance advertisers behave this way by design. Especially in ecommerce. They test channels against CAC, reallocate fast, and pause without ceremony. Thatâ€™s normal behaviorâ€”not rejection.The 23% stat itself is unlabeled. Is it spend-based? Account-based? Were these $1K test budgets or six-figure campaigns? The report doesnâ€™t sayâ€”because context would ruin the punchline.And hereâ€™s the most important part: 77% of those e-commerce advertisers stayed. This wasnâ€™t a client walking out. It was a market walking in.The Numbers Donâ€™t Lieâ€”Which Is Exactly Why the Report Doesnâ€™t Focus on Them.\
Every good short report starts in the same place: The numbers. You show where the revenue is overstated. Where the costs are buried. Where the cash flow is smoke and mirrors.\
Thatâ€™s not what Muddy Waters did here.\
Instead, they went after pixels, churn, and speculationâ€”while largely avoiding the one thing thatâ€™s actually measurable: How AppLovin performs as a business.\
Because hereâ€™s what the numbers say:$3.2 billion in ad revenue last yearâ€”up 75%$2.1 billion in free cash flow, not â€œadjusted EBITDA,â€ not projectionsâ€”cashOver $2 billion in share buybacksâ€”executed while insiders held their shares\
Thatâ€™s not a business in trouble. Thatâ€™s a company with real operating leverage, compounding returns, and enough confidence to return capital at scale. If there were something rotten, youâ€™d see it in the margins. If churn was a real problem, it would hit revenue. If the pixel theory had teeth, youâ€™d see risk priced in.\
But none of thatâ€™s happeningâ€”because the business is healthy, efficient, and growing.\
So why avoid the numbers? Simple, they donâ€™t support the story.They Talked About Pixels and Churn. Not the Part That Prints Money.\
Muddy Waters had a lot to say about pixels, churn, and scary-sounding AI. Know what they barely mention? .Which is convenientâ€”because itâ€™s the one thing that actually explains why AppLovinâ€™s ad business is doing so wellâ€”and why itâ€™s about to do even better.\
Hereâ€™s what every marketer already knows but Muddy didnâ€™t bother to say out loud:Social CPMs are through the roofEveryoneâ€™s tired of getting stalked around the internet for a pair of shoes they already bought\
So where does attention still convert? .\
Games are the last place where users actually choose to watch ads. Not because theyâ€™re â€œemotionally resonant.â€ . Watch an ad, get a power-up. Watch two, get another life. And games arenâ€™t just for kids. Theyâ€™re for anyone with a phone and a short attention span.73% of Zynga players say in-game ads help them discover brands69% of users in Asia prefer rewarded video over any other formatAnd 58% say they actually feel more connected to those brandsThis isnâ€™t a niche. Itâ€™s the next performance channel. And AppLovin didnâ€™t catch the waveâ€”they got there before it formed. As for Muddy, theyâ€™re not stupid enough to overlook this story. They just couldnâ€™t tell it without proving themselves wrong.\
AppLovinâ€™s biggest problem isnâ€™t a couple advertisers closing their accounts. Itâ€™s being too good at something the market CBF to understand.And Muddy Waters didnâ€™t miss thatâ€”they exploited it. They didnâ€™t find a scam. They found complexityâ€”and realized confusion was probably more profitable.\
Now I donâ€™t own a single dollar of AppLovin stock. But if this is what passes for a short thesis today, weâ€™re not punishing fraud.]]></content:encoded></item><item><title>Meta introduces restricted Teen Accounts to Facebook and Messenger</title><link>https://techcrunch.com/2025/04/08/meta-introduces-restricted-teen-accounts-to-facebook-and-messenger/</link><author>Aisha Malik</author><category>tech</category><pubDate>Tue, 8 Apr 2025 10:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta is introducing Teen Accounts to Facebook and Messenger. The feature, which automatically enrolls young users into an app experience with built-in protections, will be available on these platforms in the U.S., U.K., Australia, and Canada before expanding to additional regions in the future. Teen Accounts first rolled out to Instagram last September after Instagram [â€¦]]]></content:encoded></item><item><title>The One Common Mistake Companies Make to Calculate Market Share</title><link>https://hackernoon.com/the-one-common-mistake-companies-make-to-calculate-market-share?source=rss</link><author>MTP Business Boost</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:56:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Market share calculation is crucial for making effective management decisions. While the market share calculation formula is quite simple, thereâ€™s one major mistake in how data is presented and interpreted. In the following, youâ€™ll discover in an entertaining format where the catch lies, so you can avoid this in your business.\
Before you dive in, make sure youâ€™re subscribed so you donâ€™t miss the next expert insights packed with practical recommendations. Letâ€™s get started!One Late Evening in the OfficeThe soft glow of the Sydney skyline painted the perfect backdrop as Alex sat back in his chair, staring intently at the numbers on his screen. His SaaS company, specializing in cybersecurity solutions, had been growing steadily for the past few years. But now, the numbers painted a troubling pictureâ€”his market share had remained static for the last three months. Was his company stagnating?\
Alex had built his business from the ground up. From late nights developing the product to aggressive marketing campaigns, everything had been pointing towards success. Yet, despite all his hard work, he couldnâ€™t ignore the numbers staring back at him. The market share calculation for the first quarter was steady at 10%â€”January, February, Marchâ€”unchanged.\
Was this a sign of failure? He couldnâ€™t help but feel disheartened. His dream of dominating the cybersecurity space was slipping away. He had always believed that growth was the key to business success, but now, stagnation seemed to have set in. It was a silent killer that crept in, unnoticed, yet it threatened everything he had worked so hard for.\
Just then, Daniel, his old friend and a successful entrepreneur, stepped into the office. Danielâ€™s calm confidence was palpable, a stark contrast to Alexâ€™s frustration. "Mate, you look like you need a break," he said, his voice full of understanding. "Letâ€™s grab a drink and talk about it. Whatever it is, itâ€™s not the end of the world."\
Alex shook his head. "Itâ€™s not that simple, Daniel. My companyâ€™s market share is stuck. I donâ€™t know how to fix it. I think weâ€™re stagnating."\
Daniel paused, sizing up his friend. â€œAlright, enough work for today. Letâ€™s dig into this together. Iâ€™m sure thereâ€™s more to this than just the numbers. Letâ€™s figure it out.â€\
Together, they dove into the market share calculations. Daniel leaned over, pointing to the numbers on the screen. â€œYou see,â€ Daniel said, â€œyouâ€™re calculating market share by looking at the percentage, but thatâ€™s only part of the picture. You need to look at the underlying trends, not just the surface level. The raw percentages without decimals can be misleading. Itâ€™s important to understand the subtle shifts that may be invisible at first glance.â€\
Daniel then took Alex through three different market share scenarios, each one showing how small changes could significantly impact the interpretation of data when broken down into decimals and hundredths of a percent:Scenario 1: The Market Share is Growing \n January: 10.15%, February: 10.29%, March: 10.48%. \n The market share is gradually increasing. Small increments like this might seem insignificant, but over time, they lead to growth.Scenario 2: The Market Share is Declining \n January: 10.45%, February: 10.20%, March: 10.05%. \n Despite appearing stable at first, this pattern indicates a decline in market share, which would be missed if only looking at the rounded numbers.Scenario 3: The Market Share is Stable \n January: 10.25%, February: 10.24%, March: 10.26%. \n Here, the market share remains essentially the same, a slow but steady position in the market."So, as you can see," Daniel continued, "the raw numbers can be misleading if you donâ€™t look deeper. What seems like stagnation might actually be gradual growth or decline when you break it down into decimals. You need to account for the smallest changes to get a real picture of where you stand."\
Alexâ€™s eyes lit up with understanding. â€œWait a minuteâ€¦ So, if I look at the numbers like this, I can see that my companyâ€™s market share is actually growing, even if it seems like it's stagnant at first glance!â€\
Daniel nodded with a smile. â€œExactly. Sometimes the biggest mistake is misinterpreting what seems like stagnation. When you break it down and use the right calculations, you can see the true growth potential.â€\
Alex felt a surge of relief and excitement. The realization that his company was indeed growingâ€”albeit slowlyâ€”gave him a renewed sense of purpose. The pressure he had been feeling began to lift.\
With his newfound clarity, he and Daniel left the office, heading toward the eveningâ€™s celebration. Alex couldnâ€™t help but feel a sense of pride in his understanding of the numbers. Armed with the right approach, Alex was ready to take his company to new heights.\
With a sense of gratitude, he raised his glass to Daniel. "Thanks, mate. I think I just found the key to unlocking our growth."Daniel grinned. "Just remember: numbers are powerful, but understanding them is even more so."Market share is a critical metric for understanding a companyâ€™s position in the market. Itâ€™s calculated using the following formula:\
Market Share = (Company's Sales / Total Market Sales) Ã— 100\
While the formula is straightforward, how you handle the resulting numbers can make a significant difference. Without decimal places, a companyâ€™s market share can seem much more stable or volatile than it really is. That's why itâ€™s important to consider both the whole numbers and the decimals (tenths and hundredths) to get a more accurate picture of trends.Why Precision Matters in Market Share CalculationRounding a companyâ€™s market share to whole numbers can lead to the following drawbacks and potential errors in analysis: â€“ The difference in tenths and hundredths can be critical, especially in highly competitive markets with companies having similar market shares. For example, a market share of 4.4% and 4.6% when rounded to whole numbers will become 4% and 5%, distorting perception. â€“ Small changes in market share (e.g., from 4.9% to 5.1%) can lead to a "jump" when rounded (from 5% to 5%), creating the illusion of significant growth when, in reality, there is none. â€“ If all market shares are rounded to whole numbers, the sum might not add up to 100%, which could create confusion and questions about data reliability. â€“ Analyzing trends based on rounded data can lead to less accurate forecasting models, as part of the dynamics of changes is lost.Simplifying the Competitive Picture â€“ Smaller players might be excluded from the analysis if their share is too small to be rounded (e.g., 0.4% becomes 0%, though in reality, this is already a significant segment in niche markets).\
In general, using more precise values (tenths and hundredths) provides a more realistic view of the company's position in the market and helps avoid analysis errors.MTP Business Boost is a series of expert publications filled with real-world strategies, transforming complex ideas into simple, actionable advice you can apply immediately (MTP stands for Marketing, Technology, and Psychology). The author Gene Misiev is a marketing expert with over seven years of experience in digital marketing. He thrives in highly competitive "red ocean" B2B markets. His focus is on growth marketing, market research, and strategic marketing development for a complex SaaS productâ€”a tender platform for government and private procurement.\
If the knowledge you gained from this article has been valuable and helpful to you, you can show your appreciation to the author by making a donation via  or giving a like and sharing it.Stay Connected and Dive Deeper into My ContentThanks for reading! Want to explore more of my thoughts, insights, and updates? Follow me on LinkedIn, Medium so you donâ€™t miss the next publication to help you expand your business empire with the power of marketing, technology and psychology.]]></content:encoded></item><item><title>This 2025 Coding Trend Claims to Solve Your IT Nightmaresâ€”But Is It Too Good to Be True?</title><link>https://hackernoon.com/this-2025-coding-trend-claims-to-solve-your-it-nightmaresbut-is-it-too-good-to-be-true?source=rss</link><author>Saaniya Chugh</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:40:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Coding Without the Sweatâ€¦Yikes!!!!Just picture this: youâ€™re sipping your daily doze of caffeine and casually telling your computer, â€œDesign and build an app that tracks my teamâ€™s weekly tasks,â€ and, boom, itâ€™s done. No semicolons, no syntax errors, just pure vibes. Welcome to â€œ, the hottest trend in 2025â€™s tech scene and the place where AI turns your ideas into code faster than you can say â€œStack Overflow.â€This cool term has been coined by the AI guru Andrej Karpathy earlier this year, where vibe coding is less about grinding through lines of code and more about flowing with your creative instincts. And guess what???? Itâ€™s slowly sneaking into enterprise platforms like ServiceNow, making even the suits jump on the bandwagon.\
Itâ€™s time to dig in to the â€˜vibeâ€™ of this revolution and see how itâ€™s shaking up ServiceNowâ€™s world.What is this Vibe Coding, Anyway?Vibe coding has been recognized as the ultimate â€œgo with the flowâ€ approach to programming. Instead of wrestling with Python loops or debugging JavaScript promises, you chat with an AI, letâ€™s say with Grok 3 or GitHub Copilot, and then you describe what you want in simple, plain English.For instance, â€œMake a website that shows cat memesâ€ or â€œFix this bugâ€ or maybe, â€œAutomate my inbox.â€ The AI just spits out the code, you run it, and if it flops, you tweak it with another casual prompt. Itâ€™s experimental, itâ€™s messy(for sure) but itâ€™s . Mr. Karpathy calls it â€œforgetting the code even existsâ€, a mindset where your ideas take the center stage whereas the tech fades into the background. A rare sight, for sure!\
And woah, look at these! The stats actually back up the hype. Y Combinator reported in the March of 2025 that twenty five percent of its Winter 2025 startups had codebases that were ninety five percent AI-generated.Multiple tools like Cursor and Replit Agent are powering this shift and turning non-coders or low-coders into creators overnight. But itâ€™s not just for startups.Even the big players like ServiceNow are catching the wave, blending vibe coding into their enterprise-grade ecosystem.ServiceNow: The Unexpected Vibe Coding PlaygroundServiceNow wonâ€™t be the first name that youâ€™d associate with a boiling coding trend. Itâ€™s the platform for IT workflows, HR automation, and corporate efficiency and not exactly a hackerâ€™s garage. But dig into the platform a little deeper, and you will find a vibe-coding paradise hiding in plain sight. With its low-code or the no-code roots and a hefty dose of AI capabilities , ServiceNow lets you build apps and workflows by  your way through it.\
Hereâ€™s where the magic happens: ServiceNowâ€™s App Engine and Now Assist (its AI sidekick) let you talk to the platform like itâ€™s your very own buddy. Letâ€™s assume that youâ€™re an HR manager who wants to automate the onboarding process.You will tell Now Assist, â€œWhen a new hire starts, send them a welcome email and order their laptop.â€ The AI doesnâ€™t think, doesnâ€™t blink.All it does it that it spins up a workflow in Flow Designer, complete with triggers, email templates, and IT catalog requests. You test it(), you tweak it (â€œAdd their start date to the emailâ€), and then deploy it, and all this without touching a line of JavaScript.\
Thatâ€™s vibe coding in a corporate suit!\
Letâ€™s give you another example here -A Real-World Vibe: Dunder Mifflinâ€™s Onboarding Glow-UpLetâ€™s get concrete with Dunder Mifflin,our fictional paper supply chain. Their HR lead, Toby, vibes out an onboarding process in ServiceNow.He isnâ€™t a coder() but knows the drill: new hires need gear, approvals, and a warm welcome. Armed with ServiceNow, Toby vibes it out:: Toby tells Now Assist, â€œBuild me an onboarding flow - email the new hire , request a laptop and ping the manager.â€ The AI capabilities map it to ServiceNowâ€™s data model, pulling employee details and IT catalog items like itâ€™s reading Tobyâ€™s mind.: The first test run works, but Toby wants more. â€œAdd a monitor to the request.â€ The Flow Designer adjusts it on the fly.: Deployed. Dunder Mifflinâ€™s onboarding is now a slick, automated machine. Toby never sees the backend Glide API or wrestles with database schemas, but just pure, creative flow.\
Hereâ€™s what the  looks like:: â€œNew Employee Record Createdâ€ (Table: sysuser, Condition: employeenumber is not empty).To: ${new_employee.email}Subject: â€œWelcome to Dunder Mifflin!â€Body: â€œHey ${newname}, your start date is ${newdate}. Welcome to the company!â€: â€œCreate Catalog Requestâ€Item: â€œStandard Laptopâ€ + â€œMonitorâ€ (bundled via AI suggestion).Requested For: ${new_employee}.Approver: ${new_employee.manager}.Details: â€œApprove ${newname}â€™s gear.â€\
Toby tests it, adds the monitor with a quick â€œvibe tweak,â€ and deploys it .No code, just flow.Whatâ€™s Under the Hood? A Peek at the CodeWhile Toby didnâ€™t write it, hereâ€™s a snippet of what ServiceNowâ€™s AI might generate in the background (e.g., a Business Rule to enhance the flow):(function executeRule(current, previous /* null when async */) {
    var empName = current.first_name;
    var startDate = current.start_date;

    var email = new GlideEmailOutbound();
    email.setSubject("Welcome to Dunder Mifflin!");
    email.setBody("Hey " + empName + ", your start date is " + startDate + ". Get ready!");
    email.addRecipient(current.email);
    email.send();

})(current, previous);
\
This runs when a new employee is added. Toby didnâ€™t touch it, but a developer sure can tweak it later for any supported edge cases.\
This isnâ€™t hypothetical fluff. Companies like Bayer have actually used ServiceNowâ€™s low-code tools to automate 80% of their legal processes, and with further AI enhancements rolling out in 2025, the vibe-coding potential is skyrocketing.Why It Works on ServiceNow?ServiceNowâ€™s secret lies in its mix of structure and freedom. Prebuilt components (like email actions, approvals and workflows) give you guardrails, while AI capabilities help you to fill in the blanks. Generative AI in Flow Generation or even the text-to-code capabilities take your vague prompts and turns them into functional logic. Itâ€™s not pure chaos like some vibe-coding experiments, youâ€™re still in an enterprise sandbox, but itâ€™s close enough to feel liberating. Additionally,ServiceNowâ€™s wide suite of offered integrations means that your vibes can stretch across systems without breaking a sweat.The Catch: Vibes vs. RealityBefore you ditch your IDE, : vibe coding isnâ€™t flawless. Even on ServiceNow, simple automations are like a breeze, but complex apps still need a developer to tweak the JavaScript under the hood. Security is often another vibe-killer.AI-generated code can eave vulnerabilities or miss edge cases if you donâ€™t double-check. Simon Willison, nails it, â€œItâ€™s perfect for prototypes, but production? Youâ€™d better know whatâ€™s running.â€The Future: Vibe Coding Goes CorporateAs of April 2025, vibe codingâ€™s footprint is definitely growing. ServiceNowâ€™s latest releases double down on AI Agents, with smarter flow suggestions and voice-driven development. Imagine telling your laptop, â€œFix my ticket system,â€ and watching it happenâ€”thatâ€™s the vibe horizon. Infact, apart from ServiceNow, the trend is crawling into everything from indie apps to trillion-dollar firms. It is not just coding,itâ€™s a mindset shift- less gatekeeping and more creating.Vibe coding isnâ€™t just a trend, itâ€™s a power move. Itâ€™s gives the HR rep the power to building apps just like a DJ dropping beats or the IT admin automating workflows just like itâ€™s a freestyle rap.\
Forget the old rules of coding(but not all of them, please) this is creation unchained. So refill your coffee, tell ServiceNow your wildest idea, and let the AI capabilities spin it into gold.]]></content:encoded></item><item><title>The Internet is Great, but Web3 is Better!</title><link>https://hackernoon.com/the-internet-is-great-but-web3-is-better?source=rss</link><author>Leto</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:36:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What would happen if you lost access to the internetâ€”forever?!Horrible. Devastating. Lostâ€¦ These are some words that have been used to describe this scenario. Youâ€™ll probably feel insanely bored for a long time, living could feel pretty weird. No chatting with friends, no doom-scrolling on YouTube, and definitely no access to free porn.\
But eventually, youâ€™ll get over it. Somehow, youâ€™ll be forced out of your shell, youâ€™ll have to get out to get groceries more, meet new people, and take an evening walk with Romy, your dog.Â \
Before the internet, people lived happily or so the Neanderthals say. If by some universal event (like a zombie virus breakout, AI going rogue, or maybe nothing special), the internet ceases to exist, youâ€™ll still survive and even lead a happy life with your family and friends (perhaps even happier than when the internet existed).\
This particular event has happened during theâ€¦ Just kidding! The internet has never ceased to exist and function since the invention of the World Wide Web in the year â€˜89. Since we all know the internet is not going away anytime soon, why not make the most of it? To fully understand this concept, letâ€™s go over the history of the internet.The Internet's Evolution: From Web1 to Web2 to Web3Remember when the internet was just static pages with blue hyperlinks and grainy images? That was Web 1.0 â€“ clunky but remarkably open. Anyone with basic HTML skills could create a website and share their thoughts with the world. It wasn't pretty, however, it was considered one of the biggest achievements of technology at the time. The birth of interconnected computer nodes and networks gave rise to a digital frontier where individuals had genuine ownership of their little corners of the web.\
Then came Web 2.0 around the mid-2000s, and everything changed. Suddenly, we had interactive experiences, social media, and platforms that made creating content as easy as typing in a box. Facebook,Twitter, and Google â€“ these companies built incredible tools that connected billions of people.\
But the trade-off was a green snake under the grass. We gradually surrendered control of our digital lives bit by bit, often without realizing it."â€¦our online presence has become more and more centralized on corporate platforms. For instance, when someone registers an Instagram handle, they canâ€™t just move that handle with all of their content and followers somewhere else because theyâ€™re tied to that network," .\
This shift transformed the internet from an open commons into a series of corporate-controlled platforms. Now, we're witnessing the early stages of Web3 â€“ an attempt to combine the best of both worlds:Â The sophisticated functionality we've come to expect with the autonomy and ownership that defined the early internet.\
At its core, Web3 aims to redistribute power from tech giants back to individual users through decentralization which was a big break to the number one issue of centralized internet.The Problems with Today's Internet: Digital OverlordsToday's internet has a serious concentration problem. A handful of corporations and governments effectively control what we see, how we communicate, and even what we're allowed to say online. This centralized power creates several critical vulnerabilities:Your Privacy Is Their ProductWhen services are "free," you're paying with something far more valuable than money â€“ your personal information.\
Tech giants harvest incredibly detailed profiles about your habits, preferences, and behaviors. As, this centralization of sensitive data has enabled concerning outcomes, including the manipulation of populations and election results, as witnessed during  where tons of data of Facebook users were collected for political advertising without those peopleâ€™s consent.Digital Censorship at ScaleWhen communication channels are controlled by a small number of entities, information flow becomes remarkably easy to manipulate or shut down. Governments can pressure companies to remove content, block access, or monitor communications.\
Companies themselves can decide what speech is acceptable based on business interests or public pressure. Either way, centralized control means centralized censorship capability.If you have ever had your account suspended on Twitter (now X), you would know how this feels.That's the problem with centralization â€“ one technical failure can bring down entire ecosystems. These single points of failure create vulnerability not just for individual companies but for the millions or billions of people who rely on their services.â€œAccess to online services is as fundamental to modern life as electricity or water. And just as we expect our electricity to be reliable and our water to be clean, we should have high expectations for the internet.â€\
On the 5th through the 13th of June 2021, the Nigerian government imposed a ban on the usage of Twitter by Nigerians after Twitter deleted a tweet made by the then president, Muhammadu Buhari. This is just one example of how bad, and how deep the rot of decentralization has run into the world.\
On today's internet, your social connections, content, and digital identity are effectively trapped within specific platforms. Try moving your followers from Instagram to another platform â€“ you can't. Your digital life is essentially held hostage by the platforms you use. Or by the government. Or by large corporations.\
Sometimes, your data is a servant to all of these masters at the same time! This centralized model stands in stark contrast to the original vision of the Internet as an open, resilient network where power and control were distributed rather than concentrated.And that is precisely what the decentralized internet aims to restore.Decentralization: The Essence of Web3So what exactly does "decentralization" mean when we talk about the internet?Â Imagine if instead of massive data centers owned by Amazon, Google, and Microsoft, the internet ran on thousands or millions of computers owned by regular people â€“ maybe even your computer.\
In its simplest form, decentralization means removing the middlemen from digital interactions. Instead of your message to a friend going through Facebook's servers, it would travel directly from your device to theirs. Instead of your photos being stored on Instagram's drives, they'd be split up and securely stored across many different computers around the world.\
Think of it like the difference between getting electricity from a massive power company versus having solar panels on your roof. One model gives control to a central station; the other distributes power generation across many independent points.\
In a decentralized internet, no single entity can make decisions for everyone. There's no central control point to attack, manipulate, or shut down. It's a fundamentally different approach to organizing how information flows and how decisions get made online.The Building Blocks of a Decentralized InternetCreating an internet without central authorities requires some clever technological solutions. Here's how the decentralized internet would actually work:You must have heard the phrase P2P before.\
Basically, what this tech implies is that instead of going through central servers, computers in a decentralized internet connect directly to each other in what's called peer-to-peer (P2P) networks. This isn't a new idea â€“ BitTorrent has used P2P technology for file sharing for years.A blockchain is a digital ledger. It is the backbone of decentralization.\
It securely stores records across a network of computers (called nodes) in a way that is transparent, immutable, and resistant to tampering. This distributed ledger enables verification and trust without central authorities. This way nothing shady goes on the network without everyone knowing about it. Also whatever transaction is recorded on a blockchain is unchangeable forever. Each "block" contains data, and blocks are linked chronologically. \
On a blockchain, an activity called Governance is a way for strangers to agree on what's true without needing a trusted third party like a bank or government. , blockchain, in combination with other emerging internet technologies, could help overcome established flaws in current decentralization approaches, making systems more optimized, efficient, and applicable for internet-scale operations.In the movie: â€œEverything, Everywhere, All at Once,â€ a version of Evelyn exists in so many universesâ€”at the same time. The decentralized internet operates similarly.\
Your photos, documents, and data wouldn't sit on a single company's servers. Instead, they'd be encrypted, split into pieces, and stored across many computers in the network. Only you would have the keys to reassemble and access your information.\
This approach creates redundancy â€“ if some computers go offline, your data remains accessible through others. It also enhances privacy since no single entity can access all your information.Smart contracts are self-executing agreements with the terms written directly into code. They automatically run when predetermined conditions are met, without requiring trust in intermediaries. These programmable agreements enable complex interactions without middlemen, creating the foundation for decentralized applications (dApps) that could replace traditional centralized services across finance, social media, content creation, and more.The Advantages: Why Decentralization MattersThe benefits of this approach are significant:True Data Ownership: You control your information, not corporations.Enhanced Privacy: Your data isn't concentrated in vulnerable central repositories.Censorship Resistance: No single entity can block content or shut down the network.Improved Security: Distributed systems have no single point of attack.Greater Resilience: The network continues functioning even if some parts fail.Web3 aims to give ownership of data back to end-users, allowing individuals to decide how their information is shared and used. This represents a fundamental shift in how we think about our digital lives and who benefits from our online activities.Letâ€™s closely examine the contrast between centralization and decentralization.Centralized vs. Decentralized: What's the Difference in Real Life?To understand what decentralization would mean for everyday internet users, let's look at some practical comparisons between today's centralized approach and a decentralized alternative:Centralized (Today): On Instagram, your account, followers, and content are locked to that platform. If Instagram changes policies, gets blocked in your country, or simply shuts down, your digital social life there disappears. The company can demonetize creators, change algorithms, or censor content based on their business interests.\
Decentralized (Web3): Your social identity would be portable across different apps and interfaces. Your followers actually follow , not your account on a specific platform. Content creators could receive direct payment from supporters without platforms taking large cuts. If one interface to the social protocol gets shut down, you simply access your network through another interface â€“ no disruption to your connections or content.Digital Money: Banks vs. CryptocurrencyCentralized (Today): Your money sits in banks controlled by financial institutions and regulated by governments. Transfers require permission, often involve exorbitant fees, and can take days to process internationally. Banks can freeze accounts, block transactions, or deny service based on various factors.\
Decentralized (Web3): Cryptocurrency enables person-to-person transfers without intermediaries. Transactions are processed according to transparent rules regardless of amount or destination. No one can freeze your funds as long as you control your private keys. International transfers are processed in minutes for minimal fees.Data Storage: Cloud Services vs. Distributed StorageCentralized (Today): Your files are stored on servers owned by companies like Google or Dropbox. These companies can analyze your data, share it with advertisers, or be compelled to hand it over to governments. If their servers go down or the company changes policies, your access may be compromised.\
Decentralized (Web3): Your files would be encrypted, split apart, and stored across many computers in a distributed network. Only you have the decryption keys. No single entity can analyze all your data or be forced to surrender it. If some storage nodes go offline, your files remain accessible through others.Identity: Corporate Logins vs. Self-Sovereign IdentityCentralized (Today): Your digital identity is provided by corporations (think "Login with Google" or "Login with Facebook"). These companies track your activity across sites and can revoke your access at any time.\
Decentralized (Web3): You would own your digital identity through self-sovereign identity solutions. You could prove who you are without revealing unnecessary personal information. No company could track your logins across websites or revoke your identity credentials.These comparisons show how decentralization fundamentally changes the power relationship between users and digital services. In a decentralized model, technology serves users rather than extracting value from them for corporate benefit.Back to the Future: Reclaiming the Original Vision of the WebWhen Tim Berners-Lee invented the World Wide Web in 1989, he envisioned an open information space where knowledge could be shared freely. The early web was remarkably decentralized by design â€“ anyone could set up a server and publish content without permission from authorities.\
Over time, this vision was gradually eroded as the web became increasingly centralized on corporate platforms. What began as an open common transformed into a collection of walled gardens owned by tech giants. As one analysis puts it,\
Web3 matters because it attempts to restore crucial principles that made the internet revolutionary in the first place:User Autonomy and ControlThe internet was supposed to empower individuals, not make them products. Decentralization returns control to users over their data, identity, and digital experience.Permissionless InnovationThe early web thrived because anyone could build and deploy new ideas without asking permission. Decentralized systems restore this ability by creating open platforms where developers can innovate freely.Resilience Through DistributionThe internet was designed to withstand nuclear attacks by having no single point of failure. Decentralization restores this core architectural principle, creating systems that can't be easily censored or shut down.The web was meant to be a global common accessible to all. Decentralized systems resist regional blocking and censorship, potentially extending internet freedom to more people worldwide.\
By embracing decentralization, Web3 isn't creating something entirely new â€“ it's reclaiming and updating the original promise of an internet built for users rather than corporations or governments. It's about rediscovering the web's soul.The Challenges of DecentralizationMillion-dollar question: If decentralization offers so many benefits, why hasn't it taken over the internet yet? The truth is that building a decentralized internet faces several significant hurdles that can't be ignored:Decentralized systems often trade efficiency for resilience and security. For example, Ethereum has high liquidity and an active developer community, but it has a major problem with energy consumption and transaction speed (15 transactions per second).Â Though there are solutions built to address this, other problems keep popping up such as user experience and scalability. When information needs to be verified across many computers instead of just one central server, things naturally slow down.\
This creates a user experience gap â€“ centralized services are currently faster and more responsive than their decentralized alternatives. Solutions are emerging, however. New consensus mechanisms like Proof of Participation (PoP) and Proof of Authentication (PoAH) are being developed specifically to address performance concerns in decentralized systems, as.\
Projects like Ethereum are working on scalability solutions that could dramatically improve transaction speeds and reduce costs.Let's be honest â€“ decentralized apps today often have clunky interfaces compared to polished centralized alternatives. Most times, they are often too complicated to interact with which creates a big usage barrier. Using decentralized services currently requires more technical knowledge and patience than most everyday users possess.\
This gap is narrowing as developers focus on creating more user-friendly interfaces that hide complexity. The goal is to make decentralized applications as intuitive as their centralized counterparts, without requiring users to understand the underlying technology.The Network Effect ChallengeFacebook is valuable because everyone's already on Facebook. This creates a powerful catch-22 for decentralized alternatives â€“ they need users to be valuable, but users won't come until they're valuable.\
Overcoming this challenge requires both technological interoperability (allowing different decentralized services to work together easily) and innovative onboarding strategies. Some projects are exploring ways to gradually migrate users and their social connections from centralized platforms to decentralized alternatives.Practical Solutions and Promising ApproachesSeveral strategies are emerging to address these challenges:Hybrid Models: Combining centralized and decentralized elements can provide a bridge between the current internet and a more decentralized future. For example, some applications use centralized front ends for speed while storing data on decentralized networks (Filecoin and Ocean Protocol). \n Progressive Decentralization: Starting with more centralized control and gradually decentralizing as technology and adoption mature. This approach allows platforms to gain users before fully distributing control. \n Improved Developer Tools: Creating better building blocks for decentralized applications makes development faster and easier, accelerating innovation in the space. \n Education and Awareness: Teaching users about the benefits of decentralization and how to use decentralized tools safely and effectively. Example will be through active creation of educational content like videos, articles, blogs, threads and podcasts. \n As, this evolution will likely involve "APIs, gateways, and middleware services that allow decentralized and centralized systems to communicate effectively."\
This hybrid approach enables organizations to adopt decentralized technologies incrementally, preserving investments in existing systems while gradually transitioning toward more distributed architectures.Leading Projects Building the Decentralized FutureThe decentralized internet isn't just a theoretical concept â€“ it's being actively built by numerous projects tackling different aspects of the challenge. Here are some notable examples:Foundation Layer: Ethereum stands out as one of the most important platforms for decentralized applications. It provides a blockchain foundation with smart contract capabilities that developers can build upon. While Ethereum wasn't the first blockchain, it was the first to focus on being a platform for applications rather than just a cryptocurrency.\
Other projects like and are developing alternative foundation layers that emphasize different trade-offs in scalability, security, speed and decentralization.Decentralized Storage: IPFS and FilecoinThe is reimagining how files are stored and accessed online. Instead of retrieving files from specific servers based on where they're stored, IPFS retrieves them based on what they contain using content addressing. This creates a more resilient and censorship-resistant way to share information.\
 builds on IPFS by adding economic incentives for storage providers, creating a decentralized marketplace for digital storage where anyone can participate by offering their extra hard drive space.Identity Solutions: Self-Sovereign IdentityProjects like and are developing self-sovereign identity solutions that allow users to control their digital identities without relying on centralized authorities or corporations. These systems enable selective disclosure (sharing only the minimum necessary information) and portable reputation, allowing users to carry their credentials and reputation across different platforms and contexts.Real-World Applications Taking ShapeDecentralization isn't just theoretical â€“ it's already being applied to solve real problems:Financial Services: Decentralized finance (DeFi) applications are creating lending, borrowing, and trading systems without traditional financial intermediaries. Projects like andallow users to earn interest on deposits or take loans without bank approval. \n Content Distribution: Platforms like for music andfor writing are creating decentralized alternatives to traditional content platforms, giving creators direct connections to their audiences and more control over monetization. \n Governance and Coordination: Decentralized Autonomous Organizations (DAOs) likeare experimenting with new ways for groups to make decisions and allocate resources without centralized management. \n These projects demonstrate that decentralization isn't just an abstract concept â€“ it's already creating practical alternatives to centralized systems in various domains.Building a Better Internet: How You Can ParticipateThe decentralized internet isn't something that will magically appear overnight â€“ it's being built incrementally by thousands of developers, creators, and everyday users around the world. Here's how different stakeholders can contribute to this transformation:For Web3 Founders and DevelopersFocus on User Experience: Build decentralized applications that hide complexity and provide intuitive interfaces. The best technology won't matter if people find it difficult to use. \n Solve Real Problems: Target specific use cases where decentralization provides clear benefits over centralized alternatives. Focus on problems that centralized systems handle poorly. \n Build Bridges: Create tools that help users transition gradually from centralized to decentralized systems, rather than forcing an all-or-nothing choice. \n Collaborate Across Projects: The decentralized ecosystem is stronger when projects work together instead of competing in silos. Focus on interoperability and shared standards.For Content Creators and Digital ArtistsExplore Decentralized Platforms: Experiment with platforms that give you direct connections to your audience without platform intermediaries taking large cuts.Own Your Audience Relationship: Build connections that exist independently of any particular platform, so you maintain control regardless of platform changes. \n Educate Your Community: Help your audience understand the benefits of decentralized alternatives and guide them through the transition.For Everyday Internet UsersTry Decentralized Alternatives: Experiment with decentralized social media, storage, or financial applications to understand their benefits firsthand. \n Secure Your Digital Identity: Take steps to own and control your online identity rather than relying exclusively on corporate login systems. \n Support Open Projects: Consider contributing to or financially supporting open-source decentralized projects that align with your values. \n Spread Awareness: Share your experiences with decentralized tools and help others understand why digital autonomy matters.For Organizations and BusinessesExplore Hybrid Approaches: Consider how your organization might benefit from incorporating decentralized components while maintaining familiar user experiences. \n Invest in Research: Support research and development of decentralized technologies that could benefit your industry in the long term. \n Partner with Innovators: Collaborate with Web3 projects to explore new business models and approaches that decentralization enables.The Future: What Might a Decentralized Internet Actually Look Like?When we look ahead to a more decentralized digital future, what might actually change in our daily online experiences? Here are some fascinating possibilities:Imagine a world where switching from one social media interface to another is as simple as switching email clients â€“ all your connections, content, and reputation move with you seamlessly. No more starting from zero when you want to try a new service. This is what decentralized wallets are created for.Personalized Algorithmic ControlRather than having content algorithms controlled by platforms optimizing for engagement and advertising like Twitter, users might select or even program their own algorithms. Want a news feed that challenges your views rather than reinforcing them? You could select that option rather than having it decided for you.Decentralized infrastructure could enable communities to maintain functional local networks even during internet outages or disasters. Mesh networks powered by decentralized protocols could keep neighborhoods connected regardless of central infrastructure status.Digital assets â€“ from game items to virtual real estate to creative works â€“ could have genuine scarcity and transferability, creating new digital economies and forms of value that users truly own rather than merely license from corporations.Algorithmic Governance with Human ValuesGovernance systems combine code-based rules with human deliberation to create more transparent, participatory decision-making for digital communities. Rules could be enforced consistently by code while still maintaining human judgment for complex cases.These possibilities point to a future where the internet serves human needs and values more directly, rather than primarily serving corporate interests.Conclusion: The Internet We Choose to BuildThe internet we use today isn't the only possible version of the internet. It evolved through countless technical decisions, business models, and policy choices â€“ many made before we understood their long-term implications.\
The decentralized internet represents a deliberate effort to reconsider those choices and build digital infrastructure that better aligns with values of openness, autonomy, and collective resilience. It's not about erasing everything that works about today's internet, but about addressing its fundamental imbalances of power and control., a decentralized web could be "a system of interconnected, privately owned computers that work together to provide secure, censorship-resistant platforms, providing open-source access to information and services.â€\
Building this future isn't just a technical challenge â€“ it's a social choice about what kind of digital world we want to inhabit. As centralization reaches its limits and its downsides become increasingly apparent, the opportunity for meaningful change grows.\
The question isn't whether the internet will change â€“ it's whether we'll participate in shaping that change toward a more decentralized, equitable, and human-centric direction. The tools and knowledge exist; what remains is the collective will to build something better. The decentralized internet isn't inevitable, but it is possible. And that possibility starts with understanding what's at stake and taking practical steps, however small, toward the digital future we want to create together.]]></content:encoded></item><item><title>How XION is Making Crypto Invisible â€” And Thatâ€™s a Good Thing</title><link>https://hackernoon.com/how-xion-is-making-crypto-invisible-and-thats-a-good-thing?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:26:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Imagine a blockchain so seamless, so user-friendly, that even your grandparents could use itâ€”and they wouldnâ€™t even know theyâ€™re on Web3. Thatâ€™s the mission Anthony Anzalone has taken head-on with , a project born not just out of necessity, but out of frustration with how Web3 has boxed itself in.\
In this edition of , we dive deep into Anthonyâ€™s journeyâ€”from making headlines with a  to building one of the most intuitive Layer 1s the industry has ever seen. Letâ€™s get into it.\
 From the beginning we've been trying to bring crypto to regular users. We didn't intend to build our own blockchain from the start, but it was a necessity to start from scratch and build from the ground up. Everyone in this industry is so stuck in the same way of thinking, and is somehow surprised that the industry has only catered to the same few users and developers. XION solves this both from a technical level, making the crypto complexities disappear, but also from a cultural level, disassociating with all the lingo and complexities introduced by the industry which has kept external builders from seeing the true value. You gained global attention with your Banksy NFT experiment. Can you please tell us about it? Further, what key lessons did you take from that experience, and how did it influence your vision for digital ownership and blockchain adoption?\
 It was simple, we were trying to prove one of the value propositions of crypto on a global scale - the value of digital scarcity and ownership. Frankly that moment changed my life, it's what has led me to founding XION, and cemented what we believed in all along. It's clear, the general public is intrigued, and they need to be given a reason to care.\
 As mentioned, XION tackles this not only from a technical standpoint - abstracting many of the complexities at the protocol level in order to enable familiar web2 UX for end users - but also from the cultural standpoint which in many ways is most important. The industry, rightfully, has received a lot of criticism and skepticism and convincing web2 builders is extremely difficult. XION is uniquely doing so by removing both the user complexities and the crypto lingo. And the results are clear, with large brands such as Uber utilizing the platform in novel ways. Regulation remains a major factor in blockchain adoption. How does Xion navigate evolving compliance requirements while maintaining its frictionless approach?\
 Regulation is important in any fast growing industry, and XION has made a concerted effort to innovate in a compliant way. The two are not mutually exclusive, and recent regulation has made huge strides in creating clear pathways to compliance. We recently announced being the first L1 that was Title II compliant in the EU, and we strongly believe in the importance of innovating within regulatory frameworks that are clearly defined. Given your background in both art and blockchain, how do you see the intersection of these fields evolving over the next few years, especially with regard to digital ownership and building a market for art on blockchain? The most exciting part about where we are is that the tools are finally in a place that enable this cambrian explosion of creativity, and I believe XION is going to produce many applications that will be novel but also looking back will look obvious.\
Donâ€™t forget to like and share the story!:::tip
Vested Interest Disclosure:This author is an independent contributor publishing via ourÂ . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Hackers Are Hiding Malware in This Common Image Formatâ€”And Itâ€™s Working</title><link>https://hackernoon.com/hackers-are-hiding-malware-in-this-common-image-formatand-its-working?source=rss</link><author>Erich Kron</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:25:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The KnowBe4 Threat Research team detected a sudden increase in phishing campaigns that abuse the scalable vector graphics (SVG) file format to hide malicious HTML, malware, and scripts that can easily bypass conventional email security filters.\
Analyzing phishing emails from Q1 we discovered that SVG files comprised 6.6% of malicious attachments within phishing emailsâ€”a 245% increase compared to the previous quarter.\
Why have SVG Files Suddenly Become Popular with Threat Actors?\
SVG is a file format for drawing vector-based images. Unlike JPEG, PNG, or BMP file formats, which use pixels to form images, SVG files contain text instructions in XML for drawing the picture in a browser window. Thus, SVG images can be scaled to any size without losing quality, making them a popular format for sharing logos and icons via email. Since SVG is text-based and the image can load natively in a browser, such files can be embedded with scripts and clickable forms, such as anchor tags, and other interactive elements.\
By exploiting these attributes of SVGs, threat actors can automate redirects to malicious sites, present phishes containing fake login forms for credential harvesting and deliver malicious files to user systems.\
The sudden surge in SVG-file-based attacks in the last quarter predominantly leverages two types of phishing campaigns:Polymorphic Attacks Using SVGs Attackers send phishing emails using SVG attachments with dynamic file names or subject lines to evade detection. The emails are sent using compromised accounts with high domain ages that enhance their credibility. When a user opens the malicious SVG attachment, a transparent clickable rectangle redirects the user to a credential-harvesting phishing site with an actual Office365 login dialog, which hackers use to validate the credentials at the same time as stealing them. Polymorphic attacks obfuscating embedded malicious links within the SVG file help attackers evade traditional email security detection mechanisms.Personalized Missed Message Phishing A phishing email prepends the email username to trick the user into opening a malicious SVG attachment claiming to contain a missed call voice message. On activation, the JavaScript embedded within the attachment leads to a phishing site. Dynamic JavaScript and increased personalization tactics increase the likelihood of successful phishing attacks.\
SVGs Are Evading Detection by Secure Email Gateways\
The presumably safe SVGs' ability to discreetly embed harmful scripts and successfully evade traditional defenses has made them a vector for cyberattacks. Hereâ€™s how SVGs can bypass security defenses:\
Legitimacy of SVG Image Format: SVGâ€™s unique web and graphic design benefits make it a popular choice among users. Most security scanners assume SVG images to be safe and legitimate.\
 Traditional endpoint and mail protection tools can detect threats in conventional PNG and JPEG image formats and executable files. However, SVG images are text-based (XML format), and traditional security filters pass them off as innocuous image data rather than executable content.\
 SVGs can embed JavaScript and hyperlinks. Techniques like Base64 encoding and dynamic string assembly allow malicious codes to dodge the security filters, as the latter arenâ€™t designed to investigate embedded scripts.\
Impersonation Techniques: SVGs can impersonate well-known brands and services, including DocuSign, Microsoft SharePoint, and Google Voice. Victims are decoyed into handing over sensitive credentials through legitimate-looking login screens and authentication prompts. For example, cybercriminals executed financial scams by sending seemingly legitimate invites containing malicious links using Google Calendar and Drawings that bypassed email security measures.\
SVGs Present a Multi-Faceted Threat\
Thanks to their versatility and perceived legitimacy, SVGs provide attackers multiple avenues to execute attacks. Here are some of them:\
Cross-Site Scripting (XSS) Attacks: SVG files can embed codes like JavaScript within them. When the user views or imports the harmful SVG, the browser renders the image, executing malicious code. This can lead to the theft of sensitive user data or escalate to more severe attacks through initial JavaScript execution.\
 Attackers can insert malicious codes obfuscated in XML-based SVG files to evade security scanners while presenting visually convincing images or forms to users. Users are lured to engage with malicious content with severe consequences like stolen credentials, leading to data breaches and unauthorized access to critical systems.\
 Cybercriminals modify the code in text-based SVG files to include malicious payloads. Unsuspecting users who open or import such SVGs may inadvertently execute harmful actions.\
Denial-of-Service (DoS) Vulnerabilities: An attacker can craft an SVG file to include complex or excessively large data. When the file is processed by a server or application, it consumes excessive memory and CPU resources, making the server unresponsive or crashing.\
Lateral Network Movement: Using a compromised user account, an attacker can upload a malicious SVG file to a shared network drive. When opened by another user in the network, the harmful SVG can exploit vulnerabilities in the second user's system to escalate privileges or extract credentials. This way, the threat moves laterally across the network, accessing sensitive systems or data.\
How Can You Stay Safe from SVG-Based Phishing Attacks?\
Businesses can reduce the risk of SVG-based phishing attacks to a great extent by following these best practices:Authenticate Email Attachments: SVG files from unknown or unexpected email senders should be cautiously opened, as attackers often disguise malicious SVGs as legitimate documents. Use phishing simulation platforms to train employees on real-world SVG attacks. This can help them recognize phishing attempts, including suspicious SVG files or web interfaces, and report them promptly.Disable Script Execution: Configure your browser or application to block embedded scripts within SVG files. SVG rendering from critical systems can be isolated using sandboxing techniques to prevent the execution of embedded scripts. Organizations should limit the types of files users can upload, allowing only those necessary for operations.Implement Content Security Policies (CSPs): Use CSP headers tailored to SVG handling, blocking inline JavaScript and unauthorized external calls embedded in SVG files. Regularly review and update CSP rules to address emerging threats.]]></content:encoded></item><item><title>Turkeyâ€™s Sipay raises $78M to expand its Stripe-like services into emerging markets</title><link>https://techcrunch.com/2025/04/08/turkeys-sipay-raises-78m-to-expand-its-stripe-like-services-into-emerging-markets/</link><author>Mike Butcher</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:22:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Billing itself as â€œStripe for emerging markets,â€ Turkish fintech Sipay has raised a $78 million Series B funding round, claiming a valuation of $875 million in the process. This round is significant, as Sipay plans to expand into markets outside of Turkey, offering additional services like remittances that Stripe currently does not offer in those [â€¦]]]></content:encoded></item><item><title>Yes, You Can Add â€˜Paper-Checkboxâ€™ to Vaadinâ€”and Itâ€™s Easier Than You Think</title><link>https://hackernoon.com/yes-you-can-add-paper-checkbox-to-vaadinand-its-easier-than-you-think?source=rss</link><author>Paulo B.A.</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:09:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This article is part of the series 'Towards Vaadin Developer Certification,' which aims to explain the fundamentals of Vaadin as I study for this certification. The topics covered here are part of the 'Vaadin Developer' Certification curriculum. The content was created based on my personal notes and studies for this certification.\
In our previous article, we explored the concept of Web Components and how Vaadin's democratic approach allows for seamless integration with external components. Now, let's put that knowledge into practice with a hands-on implementation.Let's get practical with a step-by-step implementation. We'll create our own components by installing and implementing the "polymer paper-checkbox" web component in a Vaadin application. Here's the process:Check the "node_modules" in your project folderVerify the absence of your web component (no "paper-checkbox" installed yet)Install web components using "npm"Confirm that "paper-checkbox" is now installedImplement the web component in VaadinUnderstanding the Implementation Details\
Let's break down what's happening in our implementation:The createCheckbox Method: Creates a horizontal layout containing our custom checkbox and a labelThe CustomCheckbox Class: Acts as a wrapper that allows for validation messages and required indicatorsThe PolymerCheckbox Class: The direct Java representation of the web component This layered approach demonstrates one of Vaadin's strengths - the ability to encapsulate external components while maintaining a consistent API for your application. The CustomField wrapper ensures that our Web Component behaves like any native Vaadin field, with support for validation and other Field features.This layered approach demonstrates one of Vaadin's strengths - the ability to encapsulate external components while maintaining a consistent API for your application. The CustomField wrapper ensures that our Web Component behaves like any native Vaadin field, with support for validation and other Field features.Implementation Patterns and Best PracticesWhen implementing Web Components in Vaadin, consider these best practices:Always create a proper wrapper around your Web ComponentUse CustomField when you need validation and other field features\
These practices ensure that your Web Components integrate smoothly with the rest of your Vaadin application and maintain a consistent developer experience.In this hands-on article, we've explored how to implement Web Components in a Vaadin application. By following the demonstrated approach, you can expand your Vaadin toolkit with the vast ecosystem of community-created components.\
The example we've worked through shows that with minimal code, you can integrate external components while maintaining Vaadin's elegant programming model. This flexibility is what makes Vaadin a truly democratic framework for UI development.\
As you continue your Vaadin journey, remember that this openness to external components gives you unlimited possibilities for creating unique and powerful user interfaces.\
This content, although based on my  studies and notes acquired during the Vaadin 24 certification process,  does not intend to be exhaustive. For a deeper understanding of the  topic, I recommend consulting the official Vaadin 24 documentation and  the materials from the certification courses.\
Additionally,  I would like to inform you that I used Artificial Intelligence tools to  assist in the modeling and structuring of this article. However, the  writing and the original content are exclusively authored by me, hence  AI was utilized as a helpful tool, not as a replacement for my own  authorship.\
Paulo B. A.  is an 'Oracle Certified Java Developer' and 'Spring Certified Professional' with a deep passion for Vaadin Flow. He crafts UIs with Vaadin and strives to make it the leading frontend framework for full-stack Java developers worldwide. He loves teaching, sharing knowledge, and creating content. While he enjoys learning theory through certifications, he always advocates for a practical approach.\
 FÃ¡bio A. P. is a technology enthusiast, self-taught writer, and scholar of society's relationship with technology. Passionate about sharing insights, he provides reliable perspectives on how technology shapes our lives. With clear and precise writing, FÃ¡bio simplifies complex topics, empowering readers to navigate the digital age with confidence.]]></content:encoded></item><item><title>Apple might import more iPhones from India to side-step China tariffs</title><link>https://techcrunch.com/2025/04/08/apple-might-import-more-iphones-from-india-to-side-step-china-tariffs/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:09:19 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple is considering importing more iPhones from India to side-step the 54% additional tariffs on goods imported from China that U.S. President Donald Trump announced last week, the Wall Street Journal reported, citing anonymous sources. The company sees this as a short-term measure while it seeks to negotiate with the Trump administration to get an [â€¦]]]></content:encoded></item><item><title>The Internet Is Rigged, Only Decentralization Can Fix It</title><link>https://hackernoon.com/the-internet-is-rigged-only-decentralization-can-fix-it?source=rss</link><author>Benjamin Ajayi</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:04:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Humans have always had the desire to be free and control their fate. This drive for autonomyâ€” to be seen, heard, and included in decision-makingâ€”has fueled democracy, taking it from a mere idea in ancient Greece into the most practiced form of government today.\
And when the internet was invented, everyone thought it would be a tool for freedom and connection. Instead, it's been hijacked. Today, only a measly 17% of Internet users worldwide enjoy total freedom (Statista, 2024). Tech giants silence voices they donâ€™t like. Governments cut off access when it suits them. This is nothing like the freedom the internet was supposed to give.\
But a shift is coming. The Decentralized Internet is about to break that monopoly, handing power back to users.This article looks at how a decentralized internet will level the playing field and what its future holds.What Is Decentralized Internet?\
Letâ€™s break things down.\
For instance, under the traditional structure, internet service providers like AT&T and Comcast allow you to access the internet. Companies like Google and Amazon operate data centers that store and process your data. And of course, government agencies and social media giants enforce regulations and control content algorithms.\
In contrast, with a decentralized internet, it's not just companies providing the servicesâ€”everyone is involved in the process. For example, instead of depending on centralized providers like Amazon Web Services (AWS) or Google Cloud, individual users or small groups could run independent data centers or participate in a distributed network.A Decentralized Web is a network of resources in which no one player can control the conversation or spin it to [his or her] exclusive advantage.\
In a decentralized internet, independent computers and privately owned networks work together to provide internet services, without relying on corporate-controlled infrastructure.\
However, beyond this shift from corporate-controlled networks to a decentralized model, the big question is: Does it  matter much if the Internet is centralized or decentralized? Are there any real-world benefits for the average Joey if the internet is decentralized? Letâ€™s break down the key advantages of a decentralized Internet.Beyond Big Tech: 5 Real Benefits of a Decentralized InternetIf there is one thing that has been misused so much under a centralized internet system, it is users' data. For instance, Googleâ€™s location tracking scandal revealed that Google still tracks users' location even when they turn off their "location tracking feature" on their mobile devices. Imagine someone spying on you everywhere you go, every minute.\
With a decentralized internet, this is less likely to happen because users are in complete control of their data. The rules and permissions are handled via transparent smart contracts on the Blockchain. This means no one can redefine privacy settings behind the scenes.2. Resistance to CensorshipIn 1996, the Chinese government created the Great Firewall, an internet censorship system that allows citizens to access only a heavily filtered version of the internet. This means that people in China cannot access global websites like Google, YouTube, Facebook, Twitter, and even certain news outlets like BBC and The New York Times.\
And in 2025, that initiative is still in place. Thatâ€™s nearly three decades of being shut off from the rest of the world. However, the Chinese government is not the only one that tampers with its citizensâ€™ digital freedom. There have also been several countries that shut down the internet completely on several occasions.\
With a decentralized internet, it becomes hard for censorship to happen since the infrastructure is now owned and controlled by different people in different places.\
For instance, one of the methods the Chinese government uses for the firewall program is controlling Internet service providers. And that was easy to do because only a few corporations were involved. If it were under a decentralized structure, getting everyone involved to agree would be difficult.3. No Single Point of FailureUnder a centralized internet, if you have a situation where there is a failure of a single component it can cause the entire system to stop working. A good example of this is the Rogers Telecom Outage.\
On July 8, 2022, Rogers Communications, one of Canada's largest telecom providers, suffered a massive nationwide outage that left 12 million Canadians stranded. People couldn't call 911, pay for goods, or access online banking.\
And what was the cause? Well, Rogers runs a centralized infrastructure and was trying to do some upgrades. That was it. Just one error leading to total blackout. Thatâ€™s one of the many dangers of centralization.\
In a decentralized network, data is routed through many independent nodes, not one giant backbone. So, if one provider or server fails, others take over automatically.4. More Efficient Content DeliveryIn a centralized internet, most websites and services rely on central servers or cloud data centers to store and deliver content. This means all users must connect to the same few locations to access a site or stream media.\
But what happens when those servers are overloaded or far from the user? The users will experience slow loading times, and sometimes it leads to a complete outage.\
In a decentralized internet, the content is stored across many distributed nodes and delivered from the closest or fastest available source, rather than a single fixed server. This means the user will enjoy a great experience even when there is high traffic.5. Open and Transparent GovernancePresently, the average user has no say in what happens on the internet. Tech giants just decide at will to make changes without even considering the impact it may have on users.Â For example,Â Facebookâ€™s algorithm changes have destroyed entire businesses overnight, with no warning and no recourse.\
In contrast, Decentralized networks are often governed through community-driven protocols and on-chain governance, where users vote on key changes. Decisions are made by stakeholders and not just CEOs.\
And this is not just some big idea. Projects like Ethereum, ENS (Ethereum Name Service), and Mastodon are already showing that decentralized communities can run complex systems fairly and openly.What Does the Future Hold?While the decentralized internet is still in the developmental stage, one thing is clear: it is a shift that will forever change our lives.The decentralized internet represents a seismic shift in the way we think about online interactions, one that prioritizes community, security, and transparency over centralized control.\
Already, we have started seeing promising implementations of the decentralized internet. Projects like Filecoin are creating decentralized storage networks. Brave Browser is redefining how we interact with content online. And initiatives like Spacecoin are taking decentralization beyond Earth's surface with satellite-based internet nodes. With the Spacecoin project, internet accessibility will greatly improve even in remote areas where traditional internet infrastructure is lacking.\
However, despite this progress, there are still some challenges plaguing the decentralized internet, with user adoption being one of the major problems. To many people, the idea of a decentralized internet still sounds abstract.\
But it is only a matter of time before the gospel of a decentralized internet becomes mainstream. The benefits are just too much to be ignored.\
Decentralization is our only chance of restoring the internet to what it should be: a tool of freedom. And we all should start embracing it, not tomorrow but today!]]></content:encoded></item><item><title>This 20-Year-Old UI Framework Is Outshining Its Newer Rivals And You Can Learn it Today</title><link>https://hackernoon.com/this-20-year-old-ui-framework-is-outshining-its-newer-rivals-and-you-can-learn-it-today?source=rss</link><author>Paulo B.A.</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:03:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Time is a valuable resourceâ€¦ That's why I dedicated myself to creating this content with the time I had available. Should there be any discrepancies with the original sources, please consider my main goal: to contribute and help all readers.\
This article is part of the series 'Towards Vaadin Developer Certification,' which aims to explain the fundamentals of Vaadin as I study for this certification. The topics covered here are part of the 'Vaadin Developer' Certification curriculum. The content was created based on my personal notes and studies for this certification.Vaadin: A Democratic FrameworkWhen we expand our definition of "fields" beyond just input components, we discover one of Vaadin's most powerful features - its democratic approach to UI development. This framework allows you to integrate components from outside the Vaadin ecosystem seamlessly.\
Despite this openness, it's worth noting that Vaadin already provides dozens of robust components. In fact, it's relatively rare to find situations where you need to look beyond Vaadin's native offerings.\
This comprehensive library isn't surprising when you consider that Vaadin has been evolving for over 20 years - making it old enough to be the "father" of Angular and React (or at least their uncle). Throughout these two decades, the Vaadin team has continuously perfected and expanded their component library.Understanding the Concept of FieldsUp until now, we've extensively discussed fields in Vaadin. However, it's crucial to clarify that we've primarily been referring to "Input Fields." Whenever we mention "fields," we're typically talking about components that hold values and properties.\
But let's broaden our perspective. In the Vaadin ecosystem, the term "fields" doesn't necessarily always refer to input fields. The concept extends to various UI components, which opens up fascinating possibilities for interface development.The Creative Drive of NecessityHuman beings possess a curious tendency to innovate beyond what's already sufficient. This creative impulse is one of our most wonderful attributes. The same principle applies to Vaadin development - sometimes we need to venture beyond the established boundaries and incorporate external components.\
This is where concepts like "custom components" and "web components" become relevant. Custom components follow a straightforward concept: they're compound elements that combine either exclusively Vaadin components, or integrate Web Components with Vaadin components, or consist entirely of Web Components working together.\
This content, although based on my studies and notes acquired during the Vaadin 24 certification process, does not intend to be exhaustive. For a deeper understanding of the topic, I recommend consulting the official Vaadin 24 documentation and the materials from the certification courses.\
Additionally, I would like to inform you that I used Artificial Intelligence tools to assist in the modeling and structuring of this article. However, the writing and the original content are exclusively authored by me, hence AI was utilized as a helpful tool, not as a replacement for my own authorship.\
Paulo B. A.  is an 'Oracle Certified Java Developer' and 'Spring Certified Professional' with a deep passion for Vaadin Flow. He crafts UIs with Vaadin and strives to make it the leading frontend framework for full-stack Java developers worldwide. He loves teaching, sharing knowledge, and creating content. While he enjoys learning theory through certifications, he always advocates for a practical approach.\
 FÃ¡bio A. P. is a technology enthusiast, self-taught writer, and scholar of society's relationship with technology. Passionate about sharing insights, he provides reliable perspectives on how technology shapes our lives. With clear and precise writing, FÃ¡bio simplifies complex topics, empowering readers to navigate the digital age with confidence.]]></content:encoded></item><item><title>Bluesky Can&apos;t Take a Joke</title><link>https://tech.slashdot.org/story/25/04/08/0524221/bluesky-cant-take-a-joke?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 09:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[On Bluesky, the joke's on you if you don't get the joke. The social network has become a "refuge" for those fleeing X and Threads, but its growing pains include a serious case of humor-impairment. When Amy Brown jokingly posted she was "screaming, crying, and throwing up" about price differences between Ohio and California Walgreens, literal-minded users scolded her for exaggerating. Brown, a former Wendy's social media manager who got banned from X after impersonating Elon Musk, puts it simply: "We're both speaking English, but I'm speaking internet." 

This clash stems from Bluesky's oddly mixed population: irony-steeped Twitter refugees mingling with earnest Facebook transplants and MSNBC viewers who took the plunge after seeing the platform mentioned on shows like Morning Joe. "It's riff collapse," says cartoonist Mattie Lubchansky, describing how her obviously absurd Oscar post triggered sincere movie recommendations.]]></content:encoded></item><item><title>The Lost Dream of a Free Internet &amp; How Spacecoin is Reviving it</title><link>https://hackernoon.com/the-lost-dream-of-a-free-internet-and-how-spacecoin-is-reviving-it?source=rss</link><author>Cryptowavers</author><category>tech</category><pubDate>Tue, 8 Apr 2025 08:55:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Internet was meant to be free, open, and independent, yet it has become one of the most centralized systems in modern history. \
In the 1970s, the Internet was a small, decentralized network of computers. The personal computer revolution expanded on this foundation, powering optimism about a digital future. Visionary John Perry Barlow captured this spirit in his 1996 manifesto, A Declaration of the Independence of Cyberspace. Barlow envisioned a chaotic yet free digital online paradise, where â€œnetizensâ€ governed themselves, free from old institutions. â€œOn behalf of the future, I ask you of the past to leave us alone,â€ he declared. â€œYou are not welcome among us. You have no sovereignty where we gather.â€\
This centralization was predicted early on. In 1967, Internet pioneer Paul Baran foresaw a world where computing worked like electricity, controlled by a few powerful providers. Today, we see this in the likes of Amazon, Google, and other cloud-computing firms. As Baran said, they offer convenience, but at the cost of our privacy.\
Users often trade their data for access. They agree to terms that let companies share their information without realizing it. In the U.S., weak privacy laws allow law enforcement to access private data, like phone locations and old emails, without a warrant. Although, companies like Google and Microsoft improved encryption and pushed for transparency, skepticism remains. The U.S. government has strong ties with tech firms and can even weaken encryption standards to keep access.\
Despite calls for reform, the fight for digital privacy and a truly decentralized internet continues. The good news? Spacecoin is on a mission to fully decentralize the internetâ€™s infrastructure. But how?Explore the Internetâ€™s Evolution from its early decentralized vision to the current centralized structure.Introduce Spacecoinâ€™s Mission to restore the decentralized roots of the internet.Explain Why Decentralizing the Internet's Infrastructure Matters.Analyze the Challenges Spacecoin Faces.Paint the Vision of a Decentralized Internet Infrastructure.The Three Phases of the Internet- A Brief History\
To fully grasp the significance of Spacecoinâ€™s mission, we need to examine how the internet has evolved starting as an open, decentralized network, gradually falling into the grip of centralization, and now moving toward a new era of digital freedom.Web 1.0: The Early (Read Only) DaysIn August 1991, the Internet became publicly accessible. This marked the beginning of the Web 1.0 era. It featured static websites, with few interactive options like forums or social media. The main focus was on sharing information, acting like a global telephone directory.\
One of Web 1.0â€™s defining traits was its peer-to-peer nature. Users created and shared content independently, hosting their websites or participating in message boards. There was no need to sign up for centralized platforms, anyone could publish what they wanted, whenever they wanted, without gatekeepers.\
This era was often described as the "Wild West" of the internet, an open, unregulated space with minimal corporate influence. Without dominant tech companies or complex algorithms controlling content distribution, Web 1.0 was, in many ways, more decentralized than the internet of today. While it lacked the advanced blockchain-driven decentralization we now associate with Web 3.0, its freedom and openness set the foundation for the Internetâ€™s evolution.Web 2.0: The Current De Facto InternetThe term Web 1.0 wasnâ€™t out of the picture until after Darci DiNucci coined the term "Web 2.0" in 1999. This era, which gained traction around 2004, marked a shift from static websites to a more interactive and user-driven internet. Unlike Web 1.0, where users had to create personal websites to share content, Web 2.0 introduced platforms that simplified content creation. Social media, blogs, and video-sharing sites made it easy for anyone to post text, photos, videos, and music.\
Web 2.0 was defined by user-generated content, bringing greater engagement and connectivity. However, this shift also led to increasing centralization. A handful of tech giants such as Facebook, Twitter, and YouTube now dominate the digital landscape, transforming the internet into a collection of  Users must register on centralized platforms to participate, giving companies full control over content distribution, algorithms, and even design changes, often without user input.\
Beyond social interaction, Web 2.0â€™s infrastructure is also controlled by a few major players. As of Q3 2019, Amazon Web Services (AWS) and Microsoft Azure held a combined 58% of the public cloud market, limiting options for developers and raising costs. This centralization also introduces conflicts of interest. AWS, for instance, hosts major websites like Netflix, yet competes with it through Amazon Prime Video, raising concerns about potential throttling and unfair advantages.\
For everyday users, internet service providers (ISPs) further contribute to this concentration of power. Many ISPs operate as monopolies, charging high fees, tracking user activity, and even selling personal data often with little transparency or accountability. These issues highlight the growing need for a decentralized alternative, setting the stage for Web 3.0.Web 3.0: The Push for DecentralizationThe concept of Web 3.0 dates back to 2006 when John Markoff of The New York Times first coined the term. Unlike previous internet phases, Web 3.0 isn't marked by a clear starting point but rather by a gradual shift toward an  While some of its foundational technologies such as natural language search, machine learning, and AI are actively evolving, full adoption remains a work in progress. For now, Web 2.0 still dominates.\
Web 3.0 is about decentralization. It promotes open technologies, distributed computing, and self-sovereign identity, aiming to return control of the internet to users rather than corporations. Some trace its emergence to Bitcoinâ€™s 2009 introduction of Proof of Work, while others point to key milestones like the launch of IPFS in 2015 or the rise of decentralized applications (dApps) between 2016 and 2017.\
However, despite its ambitions, true decentralization is still far from reality. Many so-called "decentralized" applications continue to rely on centralized servers. Even Ethereum, the second-largest blockchain in the world has nodes running on cloud providers, with approximately 25% hosted on Amazon Web Services (AWS). This reliance raises questions about whether Web 3.0 has truly broken free from the centralized infrastructure of its predecessors.\
So, how do we fully escape traditional infrastructure? That is where Spacecoin comes in.Spacecoinâ€™s Role in Decentralizing the Internet Beyond EarthðŸ’¬ "Decentralization isnâ€™t just about money; itâ€™s about information, access, and autonomy. Spacecoin is building the future where the internet belongs to the people, not corporations or governments." â€” \
Spacecoinâ€™s mission is to 'Connect the Unconnected' by building a decentralized global internet. The project envisions a network of nanosatellites orbiting the Earth to provide 5G internet connectivity, especially targeting regions that are currently underserved. By leveraging blockchain technology, Spacecoin aims to ensure that this network operates without reliance on centralized authorities, thereby enhancing accessibility and freedom online.\
You may ask, can the Internet truly be decentralized again? Can todayâ€™s internet reset and return to its decentralized roots? The answer is yes, but not without challenges. Restoring decentralization means shifting control back to users through blockchain, peer-to-peer networks, and protocols like IPFS and Nostr. The momentum is already building, with innovations in decentralized finance (DeFi) and Web3 technologies. If developers prioritize scalability, user-friendliness, and strong incentives, the internet could once again become open, permissionless, and user-controlled.\
But decentralization doesnâ€™t stop at terrestrial networks. A much larger shift is on the horizon: Satellite-based communication.\
The idea of space-based/satellite communication isn't new. In the 1990s, Teledesic and Iridium attempted satellite internet, but costs and technological limitations led to failure. More recently, Starlink proves global satellite internet is viable, but it remains centralized, vulnerable to government control, and reliant on a single corporate entity. If satellite-based networks embrace decentralized governance, blockchain integration, and open-access protocols, they could break the grip of centralized ISPs and create what will be called a ''truly unstoppable decentralized internet.''\
Will Spaceoinâ€™s satellite-based idea be efficient enough to decentralize the internetâ€™s infrastructure? I found an interesting video that will answer this question. â¬‡The Mechanics of a Satellite-Based Internet: How Spacecoin worksSpacecoin will change the internet infrastructure by launching a decentralized network of low Earth orbit (LEO) nanosatellites with 5G non-terrestrial network (5GNTN) capabilities. These satellites will allow users to connect directly to the internet through their mobile devices, without needing traditional ground-based systems.\
By removing centralized ISPs, Spacecoin offers a censorship-resistant and tamper-proof internet. Each satellite acts as an independent node, providing redundancy and resilience. Users pay for bandwidth using Spacecoinâ€™s cryptocurrency, ensuring transactions are secure, transparent, and decentralized. Community-run ground stations support this model, minimizing reliance on big telecom companies and spreading control across a global network.\
Security and scalability are central to Spacecoinâ€™s design. Its dual layer blockchain enhances efficiency while protecting data integrity. The Celestial Chain (Layer 1) acts as an unchangeable ledger in space, recording all completed transactions and keeping data safe even if terrestrial systems fail. The Uncelestial Network (Layer 2) functions on Earth, offering fast processing for daily transactions before linking them to the Celestial Chain for finalization. This setup not only provides security but also ensures quick, low-latency connectivity.\
Spacecoin does more than create an open and decentralized internet, it also builds a future space economy. Its structure guarantees reliable global access. With blockchain and satellite technology, Spacecoin isnâ€™t just changing internet access but establishing the groundwork for a truly borderless and decentralized digital future.Why does decentralizing the Internetâ€™s Infrastructure matter anyway?The internet has become the backbone of modern society, driving communication, finance, education, and innovation. However, the infrastructure that supports it is far from neutral or equally accessible. It is controlled by centralized ISPs, undersea cables, and cloud data centers owned by corporations like AWS, Google Cloud, and Microsoft Azure. These systems are vulnerable to outages, surveillance, and government interference. We need a solution to combat spreading issues, like in the aspect of:Governments and corporations have the power to control what people see online. They block websites, throttle content, or even shut off access entirely. This isnâ€™t just theoretical, itâ€™s already happening:China has blocked Google, Wikipedia, and Twitter at various times.Russia can isolate its internet (Runet) from the global web.India holds the record for the most internet shutdowns, often used to suppress protests.A decentralized infrastructure removes central points of control, making censorship far more difficult.Expanding Internet AccessOver 2.6 billion people still lack internet access, especially in remote and underserved regions. Traditional providers focus on profit, leaving these areas disconnected. A decentralized, satellite-based network can:Provide global coverage, reaching deserts, oceans, and rural communities.Reduce reliance on local ISPs, which often operate monopolies.Lower costs by eliminating middlemen and enabling direct access.\
By breaking down barriers to connectivity, decentralization ensures that access to information is a right, not a privilege.Preventing Outages and AttacksCentralized systems create single points of failure. When they go down, entire regions can be left without internet. Some notable examples:In 2021, a misconfiguration took down Facebook, Instagram, and WhatsApp worldwide.In 2017, Hurricane Maria wiped out 95% of Puerto Ricoâ€™s internet.In 2022, a cyberattack on Viasatâ€™s satellite network disrupted communications across Europe.\
A decentralized internet infrastructure, powered by blockchain and distributed satellites, would eliminate single points of failure. Instead of relying on a few critical hubs, a global, peer-to-peer network ensures that even if some nodes go offline, the system as a whole remains operational. This makes the internet more resilient against cyberattacks, state-controlled shutdowns, and natural disasters. This is why Spacecoin mission matters a lot.Although satellite-based, decentralized internet offers a global shift, significant challenges stand in the way. One of the biggest hurdles is public skepticism toward blockchain technology.The Trust Issue with Blockchain:Many people remain wary of blockchain, this new, revolutionary Internet will not exist unless we abandon our suspicions about blockchain technology and begin to investigate it with cautious optimism.  People distrust blockchains and are uncertain about their regulations, which is a significant barrier.\
The lack of government involvement has raised concerns about their security and reliability, especially since there is no built-in insurance. A notable example is the 2016 Ethereum DAO hack, where an attacker exploited a vulnerability to steal $60 million in cryptocurrency. The only solution at the time was to fork the entire blockchain, an extreme and time-consuming measure that further created doubts about the technologyâ€™s stability.\
Yet, despite these concerns, blockchain remains one of the most promising solutions to decentralization. Instead of rejecting it outright, we must approach it with cautious optimism, learning from past mistakes while embracing its potential to transform the internet.Another major challenge for Spacecoin beyond blockchain skepticism is the regulatory hurdles. This poses a significant challenge to Spacecoinâ€™s mission. Governments tightly control satellite communications and spectrum allocation, making it difficult for decentralized networks to operate freely.\
To succeed, Spacecoin must:Engage with international regulatory bodies to advocate for open-access space infrastructure.Develop transparent policies that align with legal frameworks while maintaining decentralization.Educate policymakers on the benefits of a censorship-resistant, decentralized internet.\
Breaking free from centralized control wonâ€™t happen overnight. But with the right approach, and User experience whereby even grandmas can navigate. Spacecoin can and will bring the world closer to a truly decentralized internet, one that is secure, open, and free from monopolistic control.The Bigger Picture Is Beyond Internet AccessSpacecoin will be a global movement toward economic and social empowerment, not just an alternative decentralized internet provider. Itâ€™s very mission speaks about freedom, opportunity, and breaking down barriers.\
By eliminating reliance on centralized providers, Spacecoin has the potential to: â€“ Bringing internet access to the 2.6 billion people still offline.Unlock Financial Inclusion â€“ Enabling anyone with a satellite connection to use DeFi, send remittances, and conduct business withoutEnsure Secure, Uncensored Communication â€“ Protecting free speech in regions were governments block or control information.Strengthen Disaster Response â€“ Providing uninterrupted connectivity during emergencies, natural disasters, and conflicts.Advance Smart Cities & IoT â€“ Powering autonomous, decentralized networks without dependence on corporate-controlled infrastructure.\
For the first time, even the most remote villages could have the same digital opportunities as urban centers. A student in a rural area could access online education, a small business owner could engage in global e-commerce, and an activist could communicate securely without censorship threats.Will our decentralized vision be achieved? Iâ€™d say yes!\
The battle for internet freedom is one of the most important of our time. Once a decentralized dream, the web has become a tool for corporate and governmental control. But revolutions donâ€™t ask for permission, they rise from necessity. Spacecoin is not just a project but a movement. By fusing blockchain and space technology, it envisions a world where the internet belongs to the people, not corporations or governments.\
For the first time, the future of the internet isnâ€™t bound by â€˜â€˜. The revolution is no longer just digital, itâ€™s going orbital.]]></content:encoded></item><item><title>Inside the Quietly Powerful Architecture That Powers Spotifyâ€™s Recommendations</title><link>https://hackernoon.com/inside-the-quietly-powerful-architecture-that-powers-spotifys-recommendations?source=rss</link><author>Vignesh Kamath</author><category>tech</category><pubDate>Tue, 8 Apr 2025 08:52:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What Are Event-Driven Systems?An event-driven system is an architecture where components react to events rather than waiting for direct requests. In contrast to traditional synchronous systems, where services make blocking calls and wait for responses, event-driven architectures allow services to communicate asynchronously via event notifications.Event Producers â€“ Generate events when actions occur (e.g., a user skips a song).Event Routers â€“ Deliver events to interested subscribers (e.g., SNS, EventBridge).Event Consumers â€“ Process events and take necessary actions (e.g., SQS, Lambda, database updates).How Are They Different from Synchronous Systems?|  |  | Event-Driven (Asynchronous) Systems |
|----|----|----|
|  | Request-response model (blocking) | Event-based (non-blocking) |
|  | Users wait for responses | Immediate response while processing happens in the background |
|  | Failures propagate through services | Failures are isolated from other processes |When Should Event-Driven Systems Be Used?As the title suggests, the obvious use cases are for systems that rely primarily on events (For example a package delivery tracker). However the key use case I want to focus on here is to facilitate expensive asynchronous data processing to allow for low latency request based retrieval.Explained Using an Example: Personalized PlaylistsWhat are Personalised Playlists?Spotify continuously updates personalized playlists (like ) based on user interactions. For example, when a user skips/likes a song, that data is processed asynchronously to update their music preferences.\
A synchronous system would try to generate a playlist on request.. Instead, an event-driven system ensures:Fast User Experience â€“ The UI does not wait for processing.Efficient Processing â€“ Large-scale data updates happen asynchronously.Scalability â€“ Millions of users can trigger song events without overwhelming the system.Personalized playlist Event-Driven Architecture Using AWS\
Such architecture would work as follows:Step 1: The Event Producer â€“ User Skips/likes a SongWhen a user skips or likes a song in the Spotify app, an event (SongSkipped, SongLiked) is generated.This event is published to Amazon SNS (Simple Notification Service), which acts as an event router.Step 2: Event Routing â€“ SNS Distributes the EventSNS broadcasts the event to multiple subscribers:SQS Queue (for batch processing of user preferences).AWS Lambda (for real-time metrics and analytics updates).EventBridge (if needed for integration with third-party recommendation models).Step 3: Event Consumer â€“ Processing in SQS and LambdaAmazon SQS (Simple Queue Service) receives the event and stores it in a queue. Note multiple queues can subscribe to an SNS event.A personalization microservice (running in AWS Fargate or Lambda) pulls events from the SQS queue and processes them:Updates the User Preferences Store in Amazon DynamoDB.Updates an aggregated machine learning model that improves recommendations.The machine learning model re generates the playlist with new information and stores it in DynamoDbA user activity lambda listens through its own queue (subscribed to multiple activity topics) and updates the activity tableA playlist preview service listens to DynamoDB stream events and saves some playlists in ElastiCache with song previewsStep 4: Real-Time API Call for Updated PlaylistsWhen the user later requests a playlist, the Spotify API fetches recommendations from:ElastiCache (Redis) â€“ Fast retrieval of recommendations with previewsDynamoDB (if Redis cache is expired).The user instantly sees an updated personalized playlist based on their past interactions.AWS Services Used and Their Role|  |  |
|----|----|
| Amazon SNS | Publishes and routes the events (ex: SongSkipped) to multiple subscribers. |
| Amazon SQS | Stores events for batch processing, ensuring scalability. |
| AWS Lambda | Processes real-time analytics updates and sends metrics. |
| Amazon DynamoDB | Stores user preferences and recommendation history. |
| Amazon ElastiCache (Redis) | Provides low-latency playlist retrieval for API calls. |
| Amazon SageMaker | Hosts and runs machine learning models |
| AWS Fargate | Compute platform for heavy logic microservices |
| AWS S3 | Cold storage for large data (music files) |Advantages of this ArchitectureInstant User Experience â€“ Playlists update asynchronously without blocking requests.Scalable Event Processing â€“ SNS and SQS handle millions of concurrent events and event bursts. SQS can decide the queue polling frequency to ensure the backend systems have an even load.Optimized Costs â€“ AWS Lambda and Fargate scale based on demand, reducing idle costs.Decoupled Services â€“ Microservices can evolve independently.Disadvantages of Event-Driven SystemsWhile event-driven systems provide scalability and flexibility, they have some trade-offs:Complex Debugging & Monitoring â€“ Events flow asynchronously, making debugging harder.Event Ordering Challenges â€“ Handling event order across distributed services can be tricky. Events may come in outside expected orders and you must account for how to handle them.Increased Latency for Data Consistency â€“ Updates propagate asynchronously, leading to eventual consistency instead of immediate consistency.Error Handling Complexity â€“ Failed events require dead-letter queues (DLQs) to prevent message loss. Redriving dead letter queues also has challenges since these events are no longer accurate representations of the time they actually occurred.\
Despite these challenges, using AWS monitoring tools like AWS X-Ray, CloudWatch, and SQS DLQs can mitigate many risks. Good logging and metrics are paramount to be able to audit and understand what happened and why.Event-driven architectures allow platforms to process user interactions at scale while ensuring real-time API responses. By leveraging AWS services such as SNS, SQS, DynamoDB, Lambda, and ElastiCache, Systems can asynchronously update recommendations without affecting user experience.\
This loose coupling of services ensures high availability, cost efficiency, and scalability, making event-driven systems the preferred choice for large-scale applications.\
Would you adopt an event-driven approach in your applications? Let me know your thoughts!Official AWS documentation:]]></content:encoded></item><item><title>Insecure IoT Devices Are Becoming Cybersecurityâ€™s Biggest Threat</title><link>https://hackernoon.com/insecure-iot-devices-are-becoming-cybersecuritys-biggest-threat?source=rss</link><author>Dileep Jain</author><category>tech</category><pubDate>Tue, 8 Apr 2025 08:44:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[IoTâ€”the Internet of Thingsâ€”is all around us, whether weâ€™re aware of it or not. From doorbells to thermostats to kitchen appliances, an ever-growing list of devices is being made â€œsmartâ€ and connected to the internet. The numbers involved are accordingly enormous, with the global IoT market expected to cross $1.5 trillion in revenues by 2029. This transformation has its benefits, increasing convenience for consumers and opening the doors to a host of new applications and features for commonplace products. However, the IoT revolution also has a dark side, with thousands or even millions of insecure devices opening the door to security catastrophes. So what are the network security implications of IoT, and how can they be managed?Why IoT Network Security is CrucialBefore getting into the nitty-gritty on vulnerabilities and solutions when it comes to IoT and network security, itâ€™s important to take stock of the reasons why IoT network security is so essential in the first place, something that has been recognized in recent years in the unfortunate circumstance of major data breaches and cyberattacks. Here are some famous examples in recent years to outline the scale of the threat:\
 The Stuxnet malware attack damages Iranian nuclear facilities through faulty instructions to programmable logic controllers, considered one of the first cyberattacks to utilize IoT devices\
 First IoT botnet discovered, including devices like appliances, baby monitors, and TVs\
 Researchers conduct cyberattack against a Jeep, gain control of a number of subsystems through in-vehicle connectivity system\
 Mirai IoT botnet disrupts services of multiple multinational corporations through attack on DNS provider Dyn, resulting in the largest Distributed Denial of Service (DDoS) attack in history\
 Hack of cloud-hosted surveillance service Verkada results in hackers gaining access to over 150,000 cameras located in schools, factories, office buildings, hospitals, and other enterprise locations\
And the threat hasnâ€™t stopped growing since these incidents. The emergence of IoT botnets-for-hire has emerged as a particularly serious issue, with cybercriminals on the dark web advertising their networks for use in on-demand DDoS attacks. Given this threat landscape, it is more essential than ever to understand the vulnerabilities facing IoT devices, and what can be done about them.How IoT Creates Network Security Vulnerabilities\
IoTâ€™s implications for network security arise out of several key elements that are core to how IoT devices function. In particular, most IoT devices combine low power consumption for their processing capabilities with wireless network communication. This combination is what enables sensing, processing and communication capabilities to be present within previously â€œdumbâ€ devices, but at the same time opens these devices up to potential security risks that may be easier to deal with or not present at all in higher-powered devices. These security challenges include the following:Software and Firmware Update LimitationsOne of the biggest security implications of IoT has to do with what happens well after the devices themselves have been manufactured and shipped. Many IoT devices, especially simpler ones, sacrifice processing speed for power efficiency and cost savings, which creates a problem down the line: updating device software and firmware. Both network bandwidth and device processing limitations may mean that updating software and device firmware is slower and more difficult, or even impossible, as compared to traditional internet-connected devices. Therefore, when updates are needed to patch vulnerabilities, end users are less likely to go through the hassle of physically accessing and connecting devices to update them, opening the door to vulnerabilities being exploited.Password and Authentication WeaknessesAnother key security risk for IoT devices lies with the design choices made by developers, and how users respond to them. How developers establish password and authentication systems for their devices and platforms has a big impact on security. In many cases, IoT devices lack any sort of authentication at all, which is a huge problem for obvious reasons, turning the device into a potential trojan horse into the network.Another case is the use of default passwords, which are often simple as they are meant to be temporary. However, the user failing to replace the default password, for whatever reason, can easily turn this seemingly innocuous design choice into a major vulnerability. Even when users do replace the default password, insufficiently secure replacements may leave the door open to exploitation.Another challenge for IoT security is the lack of standardization between manufacturers, and often even between product lines from the same manufacturer. This applies to consistent standards across functional areas, including security standards, meaning that there are widely varying levels of vulnerability across IoT devices. Several standards have been proposed and established that may help to rectify this problem in the future, including Matter, Thread, and Z-Wave, but adoption is far from universal.Exactly how widely various security measures are being implemented in the IoT landscape is often hard to gauge, but the information out there isnâ€™t promising: encryption in particular is a glaring weakness when it comes to IoT security, with a 2019 report from ZScaler finding that 40% of IoT devices entirely lack traffic encryption, and over 90% of data transactions being unencrypted. These numbers have probably declined over the last 5 years, but lack of encryption remains a glaring risk factor for IoT security, especially because the fix is fairly simple.Due to the nature of IoT devices, the challenge of physically protecting devices from intrusion presents significant risk. Taking measures to truly protect physical access may be infeasible for many devices, making monitoring the best solution, which is often lacking due to being overlooked or budgetary or other constraints.These various risk factors are hardly the only challenges to IoT network security, but they are amongst the most threatening. Given these threats, however, the question must be asked, how can developers and product designers mitigate these risks and maximize the security profile of their devices?Solutions to Secure IoT DevicesSpecific solutions will depend on the particular product category and use case, but there are a few different things to keep in mind that will apply universally to IoT products. Consider the following (non-exhaustive) list:Design with Security in MindArguably the most important practice is to maintain a focus on IoT security throughout the design process. While seemingly obvious, overlooking security may lead to potentially easily fixed security vulnerabilities being overlooked, like implementing TLS encryption. Developing and designing with security in mind can help ensure vulnerabilities are tackled early on instead of emerging later in the product life cycle. In practice, this can include being aware of the intended use cases of your devices and limiting access and functionality to suit those use cases, rather than over-engineering (and creating vulnerabilities as a result).Maintain OTA Update CapabilityNo matter how well designed a device or piece of software is at inception, over time flaws inevitably creep up, and nowhere is this truer than in the security realm, where new vulnerabilities are always being uncovered. Therefore, maintaining the ability to deliver security fixes through software updates is essential to protecting IoT devices, even if it comes at the expense of other functionality or increases costs.Incorporating Standard Security PracticesOftentimes, going back to the basics can be a fruitful effort, and thatâ€™s the case for IoT as well. That can mean applying common security tools and practices from other domains to IoT design and development. This includes encryption, as discussed earlier, but also other tools such as firewalls (though the use of network rather than device level firewalls may be more appropriate in many cases), VPNs, and various security techniques for devices that make use of SIMs for connectivity. Other key tools to keep in mind include Network Access Control (NAC), segmentation, security gateways, multi-factor authentication, and zero trust network architecture (ZTNA).Even if you have your own house in order, third party vendors and services can remain a weak point. Choosing vendors and services wisely, especially when it comes to security vendors, can minimize the risk that is outside of your direct control.Given the vast scope of different applications that come with IoTâ€”ranging from smart homes and wearable devices to industrial control systems and connected vehiclesâ€”and the myriad opportunities for exploitation, securing IoT devices and networks can be a daunting challenge. However, by understanding the potential attack vectorsÂ and ways to respond to and secure against themâ€”including designing with security in mind from the outset, incorporating features such as encryption, authentication, secure boot processes, and regular software updatesâ€”this challenge can be turned into a manageable problem to be tackled by effective design and development work.]]></content:encoded></item><item><title>These Are the Factors that Make or Break a Crypto Exchange</title><link>https://hackernoon.com/these-are-the-factors-that-make-or-break-a-crypto-exchange?source=rss</link><author>Paul Osadchuk</author><category>tech</category><pubDate>Tue, 8 Apr 2025 08:36:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In modern business, especially in the cryptocurrency industry, understanding and managing liquidity is essential for ensuring market efficiency and price stability. These insights come from personal experience working with companies and an in-depth study of liquidity dynamics. A deep understanding of this aspect is crucial, as liquidity provision involves injecting assets into the market, enabling the buying and selling of assets without significant price slippage.\
Strong liquidity contributes to the seamless operation of trading platforms and enhances investor confidence, which is vital for the industry's growth and long-term stability. Exchanges with sufficient liquidity ensure smoother transactions, reduce volatility, and create a more predictable trading environment, attracting both retail and institutional investors. As cryptocurrency exchanges expand into more international markets, they are likely placing greater emphasis on liquidity to support seamless cross-border trading and maintain competitive market conditions.\
Hank Huang, CEO of Kronos Research, a leading cryptocurrency trading, market making, and venture investment firm, highlights the fundamental role of liquidity in exchange selection:â€œOur exchange partnerships are built on rigorous standards that align with high-frequency trading operations. Reliability is key, we prioritize platforms with ultra-low latency execution and uninterrupted uptime to support billions in daily trading volume. Liquidity is another key factor, as deep order books enable tight spreads and efficient order execution. Regulatory compliance is non-negotiable, ensuring a secure and safe environment for institutional clients. Beyond these fundamentals, we seek collaborative partnerships with exchanges that offer robust API access and a willingness to co-develop liquidity solutions.â€Why Every Crypto Exchange Needs Strong LiquidityLiquidity is a fundamental pillar of cryptocurrency markets, ensuring price stability, reducing slippage, and fostering investor confidence. A well-liquid market allows large orders to be executed smoothly, preventing drastic price swings. When liquidity is low, even small trades can cause significant volatility.\
Traders and institutions gravitate toward exchanges with deep liquidity, as it guarantees swift transactions and fair pricing. Institutional investors, in particular, seek platforms with robust liquidity before entering the market. Additionally, advanced trading features such as OTC trading, margin trading, and automated strategies rely heavily on strong liquidity to function efficiently.\
Hank Huang notes a shift in institutional investor behavior:â€œInstitutional investors are transforming the crypto market, bringing a shift toward sophisticated financial instruments like derivatives and regulated products such as Bitcoin ETFs. These investors prioritize execution certainty, deep liquidity, and seamless order flow to manage large positions efficiently. Data-driven strategies are becoming more prevalent, reflecting a growing demand for quantitative trading solutions. In response, market-making capabilities have expanded across both centralized and decentralized platforms, with technology optimized for speed, scale, and institutional-grade liquidity solutions.â€Key Factors to Consider When Choosing a Liquidity ProviderA strong track record reduces risks like market manipulation.Check trading volumes, user reviews, and case studies.Trusted providers: Cumberland, Genesis Trading.More depth means better price stability.Ensure support for high-volume pairs (e.g., BTC/USDT, ETH/USDC).Binance is known for deep liquidity pools.Understand taker and maker fees.Watch for hidden costs or tiered pricing.Verify if key trading pairs and cross-chain liquidity are available.Integration & API SupportEnsure smooth integration with low-latency execution.Look for real-time data feeds and flexible API options.Regulatory Compliance & SecurityConfirm adherence to regulations (e.g., SEC, FCA).Check security measures like cold storage and AML policies.24/7 support is crucial for resolving liquidity issues.SLAs should define response times and performance benchmarks.Top Crypto Liquidity Providers RatingWhen selecting a liquidity provider, key factors such as reputation, track record, and integration capabilities are essential. As someone who has worked with top-tier companies and led liquidity provider integrations, I can present my own selection of the most trusted crypto liquidity providers with strong infrastructure and deep liquidity pools.These providers focus on delivering liquidity through high-frequency and algorithmic trading strategies, stabilizing markets and tightening spreads: â€“ Specializes in quantitative trading and market-making strategies across a wide range of digital assets. â€“ Offers high-frequency market-making strategies and liquidity support throughout the token listing lifecycle. â€“ Leverages advanced algorithms to maintain liquidity across CeFi and DeFi venues with a focus on reliable execution. â€“ Provides structured market-making services as part of its broader institutional offering. â€“ A leading HFT firm that deploys proprietary algorithms and deep capital to support liquidity across global crypto markets. â€“ Combines algorithmic execution with strategic liquidity provisioning tailored for institutional and token project needs. â€“ Acts as a hybrid market maker and strategic partner, offering liquidity alongside project investment and advisory.OTC & Institutional Trading DesksThese firms support large-scale transactions off-exchange, minimizing slippage and price impact for institutional clients: â€“ Institutional-grade liquidity provider with a global OTC trading desk and bespoke execution services. â€“ Operates one of the largest OTC desks in crypto, providing spot and derivatives liquidity with HFT infrastructure. â€“ Also offers OTC services alongside its trading and investment solutions.Liquidity Pools & Decentralized SolutionsThese platforms enable decentralized liquidity provision through innovative protocols and deep on-chain liquidity: â€“ Leading DEX using AMM technology to enable permissionless liquidity provision and trading. â€“ The largest centralized exchange by trading volume, offering deep liquidity pools and access to institutional-grade order books. â€“ Collaborates with tier 1 and tier 2 liquidity providers and market makers to enhance liquidity and provide a seamless trading experience for its users. â€“ Offers robust liquidity through both centralized order books and regulated infrastructure, appealing to institutional participants.\
Hank Huang also emphasizes Kronos Researchâ€™s approach to investment in emerging crypto technologies:â€œDecentralized finance (DeFi) remains one of the most exciting areas of innovation, particularly solutions that enhance liquidity efficiency and cross-chain interoperability. A key investment in this space is WOOFi, a decentralized exchange that integrates deep liquidity with social trading, allowing users to follow and replicate top-performing traders seamlessly. The tokenization of real-world assets is another promising area, bringing traditional finance on-chain and unlocking new use cases. AI is also playing an increasingly important role in crypto, from predictive analytics for market-making to AI-driven risk management and fraud detection.â€Where Crypto Liquidity Is Headed: Insights from ExperienceThe crypto liquidity landscape is rapidly evolving, driven by technological advancements, institutional participation, and regulatory shifts. Cross-chain liquidity solutions are addressing the fragmentation of liquidity across different blockchains, enabling seamless asset movement and reducing inefficiencies. Projects like ThorChain and Synapse Protocol are already facilitating decentralized cross-chain swaps, improving market depth and price stability.\
However, current market conditions reveal that liquidity is not evenly distributedâ€”particularly across altcoins, which have seen a notable decline in trading activity. In this environment, market-making strategies are proving to be a "safe haven," ensuring that even during periods of heightened volatility, traders can continue to execute orders at reliable prices. By narrowing spreads and reducing slippage, market makers maintain orderly markets and support sustainable trading volumes, even for less liquid assets.\
Institutional adoption is also reshaping liquidity provision, with providers integrating prime brokerage services, custody solutions, and advanced trading tools. Companies like Galaxy Digital, Cumberland, and WhiteBIT are offering institutional-grade liquidity with risk management strategies, attracting larger players and ensuring market resilience. Meanwhile, AI-driven liquidity management is optimizing market-making strategies, predictive analytics, and automated portfolio balancing, helping providers navigate volatility.\
Regulatory developments and tokenization of real-world assets (RWAs) are further shaping the future of liquidity. Platforms like Kraken and Binance are adapting to evolving compliance requirements, while projects like Synthetix are tokenizing traditional assets, unlocking new liquidity pools. As the industry matures, those embracing these innovationsâ€”whether through AI, decentralized governance, or institutional solutionsâ€”will lead the next phase of crypto market evolution.]]></content:encoded></item><item><title>Cut Load Times in Half with These Next.js Tweaks</title><link>https://hackernoon.com/cut-load-times-in-half-with-these-nextjs-tweaks?source=rss</link><author>Raju Dandigam</author><category>tech</category><pubDate>Tue, 8 Apr 2025 08:34:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Optimizing Next.js applications involves combining improvements in both your code and underlying infrastructure. By minimizing unnecessary re-renders, fine-tuning image handling, employing caching techniques, using dynamic imports, and integrating server components, you can significantly boost performance, reduce load times, and increase overall efficiency.\
Before exploring specific optimizations, itâ€™s important to grasp how Next.js renders and delivers content:Server-Side Rendering (SSR): HTML is generated on every request.Static Site Generation (SSG): Pages are pre-built during the build process.Incremental Static Regeneration (ISR): Static pages are updated after deployment.Client-Side Rendering (CSR): Dynamic content is managed after the initial load.\
Each rendering method has unique performance implications, which will be discussed throughout this article.\
1. Component-Level Optimizations\
Preventing Unnecessary Re-rendersIn React, re-renders can occur unnecessarily when functions defined within the render method are recreated, leading to updates in child components. To avoid this, you can memoize functions with useCallback and wrap components in memo so they only update when their props change. This approach helps cut down on extra computations and boosts performance.\
// Bad practice: Creating functions inside renderfunction ProductCard({ product }) {
  const handleClick = () => {
    console.log('Product clicked:', product.id);
  };

  return <div onClick={handleClick}>{product.name}</div>;
}
\
//Â  Better approach: Memoize functions and componentsimport { useCallback, memo } from 'react';

const ProductCard = memo(function ProductCard({ product, onProductClick }) {
  return <div onClick={() => onProductClick(product.id)}>{product.name}</div>;
});

function ProductList({ products }) {
  const handleProductClick = useCallback((id) => {
    console.log('Product clicked:', id);
  }, []);

  return (
    <div>
      {products.map(product => (
        <ProductCard 
          key={product.id} 
          product={product} 
          onProductClick={handleProductClick}
        />
      ))}
    </div>
  );
}
\n Optimizing Context API UsageUsing one large React Context for all states can lead to unneeded re-renders, which hampers performance. Instead, break the context into smaller segments based on domain and how often they change. For example, having separate contexts for user data, theme, and cart state ensures that only the components that rely on a specific piece of data will re-render. Additionally, employing custom hooks for state management can keep your code modular and easier to maintain.\
// Problematic: Single large context causing frequent re-rendersconst AppContext = createContext();

function AppProvider({ children }) {
  const [user, setUser] = useState(null);
  const [theme, setTheme] = useState('light');
  const [cart, setCart] = useState([]);

  const value = { user, setUser, theme, setTheme, cart, setCart };

  return <AppContext.Provider value={value}>{children}</AppContext.Provider>;
}
\n // Better: Split contexts by domain and update frequencyconst UserContext = createContext();
const ThemeContext = createContext();
const CartContext = createContext();

function AppProviders({ children }) {
  return (
    <UserContext.Provider value={useUserState()}>
      <ThemeContext.Provider value={useThemeState()}>
        <CartContext.Provider value={useCartState()}>
          {children}
        </CartContext.Provider>
      </ThemeContext.Provider>
    </UserContext.Provider>
  );
}
\
2. Data Fetching Optimizations\
Implementing SWR or React QueryIn Next.js, using useEffect may lead to repeated requests. To streamline data fetching, SWR and React Query offer robust solutions through caching, revalidation, and error management. SWR employs a "stale-while-revalidate" strategy to provide immediate cached data while updating it in the background, whereas React Query offers extra capabilities like query invalidation and pagination, enhancing both performance and the overall user experience.\
// NaÃ¯ve approach: Fetching on each component mountfunction ProductDetail({ productId }) {
  const [product, setProduct] = useState(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    async function fetchProduct() {
      setLoading(true);
      const res = await fetch(`/api/products/${productId}`);
      const data = await res.json();
      setProduct(data);
      setLoading(false);
    }

    fetchProduct();
  }, [productId]);

  if (loading) return <p>Loading...</p>;

  return <div>{product.name}</div>;
}
\
//Better: Using SWR for caching, revalidation, and error handlingimport useSWR from 'swr';

function ProductDetail({ productId }) {
  const { data: product, error, isLoading } = useSWR(
    `/api/products/${productId}`,
    fetcher
  );

  if (isLoading) return <p>Loading...</p>;
  if (error) return <p>Error loading product</p>;

  return <div>{product.name}</div>;
}
\
Leveraging getStaticProps and getServerSidePropsIn Next.js, getServerSideProps retrieves data on every request, making it perfect for dynamic content like news. On the other hand, getStaticProps runs during the build process, boosting performance for static pages. Incremental Static Regeneration (ISR) enables periodic updates without rebuilding the entire site, striking a balance between freshness and efficiency.\
// For frequently changing data that needs SEOexport async function getServerSideProps() {
  const res = await fetch('https://api.example.com/news');
  const news = await res.json();

  return { props: { news } };
}
\
// For relatively static data with occasional updatesexport async function getStaticProps() {
  const res = await fetch('https://api.example.com/products');
  const products = await res.json();

  return { 
    props: { products },
    // Re-generate at most once every hour
    revalidate: 3600
  };
}
\
3. Image and Asset Optimization\
The Next.js  component boosts performance by lazy loading and adjusting to different screen sizes. It's best to avoid using layout="fill" without a proper parent container to prevent unexpected layout shifts. Instead, set explicit width and height for consistency and use placeholder="blur" to enhance the user experience. Also, update your next.config.js to support modern image formats like AVIF and WebP for optimal performance.\
/ Anti-pattern: Not specifying proper dimensions<Image src="/product.jpg" alt="Product" layout="fill" />

// Better: Specify explicit dimensions and use proper layout
<Image 
  src="/product.jpg" 
  alt="Product" 
  width={300} 
  height={200} 
  placeholder="blur"
  blurDataURL="data:image/jpeg;base64,..."
/>
\
For maximum performance, configure your next.config.js to optimize images:module.exports = {
  images: {
    deviceSizes: [640, 750, 828, 1080, 1200, 1920, 2048, 3840],
    imageSizes: [16, 32, 48, 64, 96, 128, 256, 384],
    domains: ['cdn.yoursite.com'],
    formats: ['image/avif', 'image/webp'],
  },
};
\
4. Route and Bundle Optimization\
Dynamic Imports for Code SplittingDynamic imports in Next.js help minimize the initial JavaScript load by only bringing in components when they're actually needed. This is particularly beneficial for heavy libraries such as charts or maps. With next/dynamic, you can lazy-load components with a fallback UI, which enhances both performance and user experience. For components that only run on the client side, it's advisable to disable server-side rendering (ssr: false) to ensure they load efficiently without impacting the server-rendered content//Importing everything upfront
import { Chart } from 'heavy-chart-library';

//Dynamic import when needed
import dynamic from 'next/dynamic';

const Chart = dynamic(() => import('heavy-chart-library').then(mod => mod.Chart), {
  loading: () => <p>Loading chart...</p>,
  ssr: false // Disable SSR for components that don't support it
});
\
Route-Based Code SplittingNext.js automatically divides code by route, ensuring that only the necessary parts load for the current page. You can further enhance this by prefetching routes that users might visit next. For example, triggering router.prefetch(href) on a mouse enter event lets Next.js load the page in the background, leading to faster navigation and an improved user experience.\
// Pre-fetch routes that are likely to be visitedimport { useRouter } from 'next/router';

function NavLink({ href, children }) {
  const router = useRouter();

  return (
    <a 
      href={href}
      onClick={(e) => {
        e.preventDefault();
        router.push(href);
      }}
      onMouseEnter={() => router.prefetch(href)}
    >
      {children}
    </a>
  );
}
The key to optimizing Next.js performance depends on making strategic decisions between code and infrastructure elements. This article examines how rendering methods such as SSR, SSG, ISR and CSR affect performance. The article then explores essential optimization techniques which include stopping unnecessary re-renders and enhancing data fetching through SWR or React Query as well as image optimization and dynamic imports and route-based code splitting.\
The implementation of these best practices leads to the development of fast and efficient Next.js applications which provide users with a seamless experience. The collective effect of minor performance enhancements leads to better scalability and performance of web applications through time reduction and improved responsiveness and navigation smoothness.]]></content:encoded></item><item><title>81% of People Say They Know How to Use AIâ€”Only 12% Are Right</title><link>https://hackernoon.com/81percent-of-people-say-they-know-how-to-use-aionly-12percent-are-right?source=rss</link><author>Brian Wallace</author><category>tech</category><pubDate>Tue, 8 Apr 2025 08:15:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
With the increasing use of AI technology in our modern society, it is becoming increasingly important to know how to properly provide AI with the most effective prompts to maximize your benefits from the technology. You wouldnâ€™t start a dialogue with Leonardo DaVinci over Corn Flakes if he was alive today. The conversation will lead you nowhere. The same applies to AI prompt engineering; what you say, and how you say it, matters.AI Prompt Engineering: By the NumbersAbout 81% of IT professionals think they can craft the right prompts to guide a generative AI model to create the desired output, but only 12% actually have the skills to perform prompt engineering effectively. There is a growing need for  as more AI models are being used around the world. In fact, as many as two in three leaders will not hire someone who does not have up-to-date AI skills. Unfortunately, as many as 70% of global workers need to upgrade their skills using AI, especially with 60% of IT decision makers saying that AI constitutes the biggest skill gap with candidates.\
Many professionals might know the basics of AI but lack the in-depth knowledge required to advance in their careers. Most desk workers do not know how to adequately use generative AI at work while getting the most value from using generative AI. Workers also struggle to leverage trusted data sources while keeping their first-party data secure. With a 50% hiring gap between AI jobs and AI job roles, learning exactly how to use AI prompt engineering appropriately and understanding what makes it so important is more essential than ever.AI literacy teaches users how AI works while AI prompt engineering teaches users how to think about AI. AI prompt engineering is the art and science of structuring an instruction that a generative AI can interpret and create high-quality outputs as a result. There are various ways to structure a prompt for an AI to follow, including a chain-of-thought approach, chain-of-symbol approach, generated knowledge prompting, least to most prompting, and non-text prompting. With images and video, there are even more prompt options such as negative prompts, textual inversion, and prompt injection.\
72% of IT leaders currently believe that AI skill gaps must be immediately addressed since it is clear that learning new AI skills will be vital to the AI revolution we are experiencing. Thankfully, there are programs that offer online, skill-based certificates that are useful in practical, in-demand professions. These programs are dedicated to reskilling, upskilling, and cross-skilling workers through targeted AI prompt engineering courses. Students can earn a certificate in as little as six to eight weeks!There is no doubt that AI technology is continuously being adopted across different industries as well as everyday life, making it necessary for us to learn how to approach it in an effective manner. Taking courses and receiving certificates can give workers a strong edge compared to the competition, making them more desirable to companies who are leaning into optimizing AI technology to improve their business.]]></content:encoded></item><item><title>Why Science Fiction May Be The Root Of Space Exploration</title><link>https://hackernoon.com/why-science-fiction-may-be-the-root-of-space-exploration-the-journey-to-space-travel?source=rss</link><author>Diamond</author><category>tech</category><pubDate>Tue, 8 Apr 2025 08:13:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[One of the earliest films ever made,  (1902), centered around space travel. Directed by George MÃ©liÃ¨s, it can be loosely considered the first science fiction movie adaptation. The short film drew inspiration from several works, with much of it influenced by Jules Verneâ€™s 1865 and 1870 books,  and  respectively.\
The film itself follows a group of scientists and astronomers who travel from Earth to the moon, propelled through space by a cannon attached to a capsule. After exploring the surface of the moon, they discover the inhabitants of the moon. While humankind didnâ€™t land on the moon until 1969, science fiction pioneers like Jules Verne, Robert Heinlein, Frank Herbert, Isaac Asimov, and many others had already been imagining Outer Space travel in their books for years.\
Asimov created the galactic empire as it is known and thought of today, no question about it. His foundation series is the most influential series of its kind, though highly controversial. Robert Heinlein's 1959 book, , set trends for the depiction of futuristic military forces and much of the science fiction that followed.\
Arthur C. Clarke's many works also helped popularize the idea of space travel in the 1950s and throughout the next several decades.In 1950, Arthur C. Clarke wrote a short book called . This book outlined the basic concept of space travel to people who were not necessarily familiar with the idea. All these were based on their current understanding of astronautics at that time.\
Arthur C. Clarke, in particular, made several predictions about . For example, Clarke predicted the eventual existence of telecommunications satellites. The writers of the last couple of hundred years, especially the 40s, 50s, and 60s are the pioneers when it comes to the history of Science Fiction and Space Exploration.\
Francois Marie Arue known mainly by his pen name Voltaire, wrote  in 1752. His version of space travel suggests that people of other worlds have more advanced technology than what is found on Earth. The novel describes a method of space travel that makes use of the forces of repulsion and attraction, which is an apparent reference to the work of renowned scientist Sir Isaac Newton. It cannot be overstated the influence that publications such as  and  had on the sci-fi genre and space exploration in general; these magazines often featured the work of science fiction authors who are now considered greats.\
 by Edward E. Smith, published in 1946, was one of the earliest depictions of interstellar travel. It worked by combining a newly discovered fictional element called element x with pure copper, along with a field generated by a particle accelerator. Now, if I were to list every mention of space exploration in every story since, this article would go on forever. Whatâ€™s important to take away, though, is that humans have always dreamed of traveling through the stars.Since the beginning, humans have had the unique ability to look at the world as it isâ€”and reshape it to suit our needs. We've always used the resources around us to build technologies that improve our lives. We are a species filled with innovators and inventors helping us conquer our globe. \
One day, if we're fortunate, we might do the same beyond Earthâ€”learning not to dominate new environments like Mars, but to live in balance with them. With just a small leap of imagination, sci-fi helps us take what weâ€™ve built here and envision whatâ€™s possible out there among the stars.]]></content:encoded></item><item><title>Comprehensive Review of CyberNewsWire for Cybersecurity Marketing Teams</title><link>https://hackernoon.com/comprehensive-review-of-cybernewswire-for-cybersecurity-marketing-teams?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Tue, 8 Apr 2025 07:58:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In the high-stakes realm of cybersecurityâ€”where the global market is forecasted to surpass $200 billion by 2025 (Grand View Research)â€”standing out is no small feat. Cybersecurity companies must ensure their announcements reach the right audience: industry leaders, decision-makers, and specialized media outlets. Enter CyberNewsWire, a press release distribution platform designed specifically for the cybersecurity sector. This review explores its features, effectiveness, usability, and pricing, offering CMOs and marketing teams a detailed roadmap to amplify their brandâ€™s visibility.CyberNewsWire is a specialized press release distribution service created by PR experts with a knack for SEO and content marketing. Unlike broad-spectrum platforms, it hones in on cybersecurity, delivering announcements to a curated network of relevant outlets. Itâ€™s a tool built to cut through the clutter, offering speed, precision, and measurable results.Guaranteed Homepage Coverage: Your release lands on the front page of top cybersecurity sites (pending editorial approval).Same-Day Distribution: API-driven technology ensures rapid publicationâ€”perfect for urgent updates.SEO Services: Optimization tailored to boost search rankings in the cybersecurity niche.Editorial Support: Professional refinements enhance clarity and impact.24/7 Customer Support: Round-the-clock assistance for any queries.Multimedia Support: Add images or videos to captivate readers.Campaign Tracking: Real-time analytics to gauge performance.These features tackle a core pain point for cyber firms: getting noticed by the right people in a saturated market.CyberNewsWireâ€™s effectiveness hinges on its targeted approach and feature-rich design. Letâ€™s dive into the evidence, benefits, and potential drawbacks.Real-world examples underscore CyberNewsWireâ€™s ability to secure placements in key cybersecurity outlets:A press release on identity exposure landed on Latest Hacking News (X post), a respected niche publication.Another on ransomware disclosure appeared on Cybersecurity News (X post), reaching engaged industry readers.Posts on platforms like Yahoo Finance and Market Watch (included in the Standard tier) demonstrate its capacity for broader business exposure.These placements validate CyberNewsWireâ€™s promise of connecting with both niche and mainstream audiences. Industry commentary reinforces this: Sigma Cyber Security praised its focus on quality over quantity, noting it saves time on outreach (Sigma Cyber Security).Press releases remain a cornerstone of PR, driving reach, credibility, and brand awareness (PR Newswire). CyberNewsWire amplifies these benefits with:Niche Expertise: Unlike generic services, it targets cybersecurity insiders, ensuring higher relevance and engagement.Guaranteed Homepage Placement: This standout feature offers unmatched visibility, a rarity among competitors.SEO and Analytics: Built-in tools enhance discoverability and provide data to refine future campaigns.Speed: Same-day distribution keeps your news fresh and timelyâ€”a must for product launches or breach announcements.\
For cybersecurity firms, this translates to better ROI. A release on a site like Security Boulevard or Hackernoon reaches decision-makers who matter, not just a scattershot audience.No platform is flawless. Hereâ€™s where CyberNewsWire might fall short:Smaller Network: Compared to giants like PR Newswire or BusinessWire, its distribution list is leaner, potentially limiting reach for companies eyeing mass exposure as CyberNewsWire focusses on distribution channels that have a hyper focus on cyber-security related events instead of catering to a more general media distribution strategy.Cost Barrier: The Premium ($1,499) and Deluxe ($2,999) tiers may strain budgets for some bootstrapped startups or smaller firms, though the value aligns with the niche focus.Editorial Constraints: Guaranteed homepage coverage depends on meeting outlet guidelines, which could delay or alter some releases. However, the team provides a white glove experience in this regard ensuring that your story is ready before distribution.CyberNewsWire delivers where it counts: targeted, impactful distribution. While it may not match the sheer volume of larger services, its precision and features make it a powerhouse for cybersecurity PR. If your goal is to hit the right eyes with authority and speed, itâ€™s a proven winner.The CyberNewsWire platform is a specialized tool for cybersecurity companies to create, distribute, and track press releases with precision. This guide provides a detailed, step-by-step walkthrough of all platform workflows, ensuring CMOs and marketing teams can maximize their reach and impact. It covers creating a press release, selecting distribution packages, adding language packages, completing the press release form, managing orders, and analyzing coverage reports. Each feature is explained thoroughly, with actionable instructions, tips, and insights into monitoring orders and coverage for each press release (PR).: Start your journey on the CyberNewsWire platform to create and distribute a press release.Open your web browser and navigate to .Log in using your credentials (email and password).The top-right corner displays your user profile (e.g., "Guest cybernewswire") with options for  and .From the homepage, locate the left-hand navigation menu.Options include , , , and .Click  to begin the press release creation process.The interface is clean, with a dark header featuring the CyberNewsWire logo.The main section displays a workflow guide: "Follow these 4 simple steps to get published on top cyber news sites and mainstream news outlets.": Prepare your press release content, company details, and any multimedia (e.g., images, videos) in advance to streamline the process.Step 2: Creating a Press ReleaseThe press release creation process is divided into four stages: entering details, choosing a package, adding optional language packages, and filling out the press release form. Each stage is accessible via tabs at the top of the interface, represented by icons for , , , and .: Provide foundational information to set up your press release order.After selecting , youâ€™ll land on the  tab (person-with-card icon).Enter the following information:: Your organizationâ€™s name.: Name, email, and phone number of the submitter.: A brief description of the press release (e.g., "Announcing a new cybersecurity product launch").Set the :: Publishes as soon as possible.: Select a specific date and time for publication.: Pauses the release for further instructions.Add  (optional):Tags help associate your PR with related articles on the website.Example: "Cybersecurity, AI, Product Launch".Click the  button (bottom-right) to proceed to the package selection stage.: Double-check your contact details, as they may be used for follow-ups or displayed publicly.: Select a distribution package that aligns with your budget and reach objectives.Click the  tab (package-box icon).Review the available packages:10 guaranteed published articles.Outlets: Business Insider, Benzinga, AP News, Yahoo Finance, Market Watch, Nasdaq, Cybersecurity Market, HackRead, Cyber Security News, Tech Startups.Features: Same-day distribution, editorial suggestions, customer support, SEO adjustments, homepage feature on all outlets.14 guaranteed published articles.Outlets: Adds Security Boulevard, Cybersecurity Insiders, CPO Magazine, Hackernoon, DevOps.com, TechRound, The Last Watchdog, Analytics Insight to Standard outlets.Same additional features as Standard.16 guaranteed published articles.Outlets: Adds CIO, CSO to Premium outlets.Same additional features.18 guaranteed published articles.Outlets: Adds Cybernews, NextBigFuture to Deluxe outlets.Same additional features.Click on the package that best suits your needs to select it.: For high-impact announcements targeting C-level executives, opt for the Deluxe or Elite packages to include authoritative outlets like CIO and CSO.Adding Language Packages (Optional): Expand your press releaseâ€™s reach to international cybersecurity markets.Within the  interface, scroll to the "Expand Your Reach With Cybernewswireâ€™s Add-ons" section.Review the available language packages::Translation and distribution to top Italian cyber media outlets (e.g., CIO).:Translation and distribution to top Spanish cyber media outlets (e.g., CIO).Translation and distribution to top German cyber media outlets (e.g., CIO, CSO).Click on the desired language package(s) to add them to your cart.Each package includes translation and distribution to the specified regionâ€™s top cyber media outlets.Confirm your selection and click  to proceed.: If your cybersecurity solution targets specific regions (e.g., Europe), adding a language package can significantly boost visibility among local audiences.: Input the detailed content of your press release, including metadata, visuals, and the main body.Click the  tab (pencil-and-paper icon).Complete the following fields:#### Hashtags and Phone Number: Add relevant hashtags to categorize your PR (e.g., ).: Optionally include a contact number (publicly displayed if provided).: Use trending or industry-specific hashtags to improve discoverability.Trading Symbol / Crypto TickerSelect either  or  via radio buttons.Enter the symbol (e.g., "BTC" for Bitcoin).: Include this for financial or crypto-related announcements to provide context.Choose an exchange platform from the dropdown (e.g., NASDAQ, Binance).Default is "NONE" if not applicable.: Align this with your trading symbol for relevance.Write a title (20-255 characters).Example: "New AI Cybersecurity Tool Launched by XYZ Corp" (44 characters).: Make it concise, keyword-rich, and attention-grabbing.Click  to add an image.Requirements: Minimum width 1200px, recommended 1200x720px, 15:9 aspect ratio to avoid cropping.: Use a high-quality, relevant image (e.g., product screenshot, company logo) to enhance visual appeal.Type or paste your content into the text editor.: Highlight key points.: Emphasize quotes or terms.: Organize sections.: Add URLs to your website or sources.: Embed additional visuals.: Add credibility with executive statements.Example: "Our new tool reduces threat detection time by 40%," said Jane Doe, CTO of XYZ Corp.: Include stats, quotes, and actionable insights to make the content compelling.Review your entries for accuracy and completeness.Click  to proceed to the payment and confirmation stage.: The platform automatically adds the title, featured image, and contact details to the final press release.Step 3: Submitting the Press Release: Finalize your press release and send it for distribution.Click the  tab (credit-card icon).Review your order summary:Selected package (e.g., Premium, $1,499).Added language packages (e.g., German Language Package).Total cost (e.g., $1,499 + add-ons).Credit card information or other supported payment methods.: Track and manage your press release orders to monitor their status and take necessary actions.From the left-hand navigation menu, select  to access the "My Orders" section.Review the table, which includes the following columns:: Unique order identifier (e.g., 443).:: Resume editing a draft.: View the press release content.: Edit related articles (if applicable).: Access distribution details (green button with "New!" label).: Remove the order.: Current state (e.g., Draft, Done, Publishing).A green dot indicates "Publishing" status.: Payment status (e.g., Draft, Pending with a  button, Done).Distribution (Local Time): Publication timing (e.g., Immediate, Scheduled: Mar 12, 2025, 18:30).: Press release headline (e.g., "Aptori Now on Google Cloud Marketplace for AI-Powered Securityâ€¦").: Percentage bar showing completion (0% to 100%).: Selected package (e.g., Standard, Elite).: Total cost (e.g., $1,788.82, $4,127).Take actions based on the order status:: Click  to finish editing or  to discard.: Click  to complete the transaction.: Click  to review the content or  to see distribution results.Use the tableâ€™s pagination (bottom-right) to navigate through multiple orders if applicable.: Regularly check this section to stay updated on your press releasesâ€™ progress and ensure timely payments.Step 5: Viewing Coverage Reports: Analyze the reach, impact, and performance of your distributed press release.From the "My Orders" section, locate the press release you want to analyze.Click the  button (green with a "New!" label) for that order.Review the coverage report interface:: Displays the press release title (e.g., "Aptori Now on Google Cloud Marketplace for AI-Powered Security and Automated Risk Remediation") and total syndications (e.g., 24).: Toggle between  (card layout) and  (table format) using buttons at the top-right.Explore the coverage details:: The outlet where the PR was published (e.g., markets.businessinsider.com).: The syndicated article title (e.g., "Aptori Now on Google Cloud Marketplace for AI-Powered Security and Automated Risk Remediation").: Origin of the publication (e.g., "PRESS RELEASE GlobeNewswire").: Direct link to the article (e.g., https://markets.businessinsider.com/news/stocks/aptori-now-on-google-cloud-marketplace-for-ai-powered-security-and-automated-risk-remediation-1034469202).: Number of views (e.g., 6,128,645).: A metric indicating the outletâ€™s credibility (not explicitly shown but implied as a standard metric).Click on a card (Grid View) or row (List View) to view detailed information.Use the  button to distribute the coverage link to your team or stakeholders.Use the  button (if applicable) for financial-related PRs.: Focus on high-traffic outlets and use the data to refine future press release strategies, such as targeting similar domains.: Craft concise, keyword-rich titles within the 20-255 character limit to grab attention and improve SEO.: Use formatting (bold, italics, headings) to enhance readability. Include quotes from executives, statistics, and actionable insights to add credibility.: Upload high-resolution visuals (minimum 1200px width, 15:9 ratio) that align with your message, such as product screenshots or infographics.: Add relevant hashtags to increase discoverability on the CyberNewsWire website and related platforms.: Before submission, review all fields (title, content, images, distribution time) to avoid errors that could delay publication.: Regularly check the "My Orders" section to track progress and ensure payments are completed.: Use coverage report metrics (traffic, syndications) to evaluate performance and plan more effective campaigns in the future.: Choose "Immediate" distribution for urgent news or schedule releases to align with major events or product launches.: Access real-time assistance via the chat bubble icon (bottom-right corner of the interface).: Navigate to the support section (if available) or contact the CyberNewsWire team directly for issues with payments, content, or tracking.: Use live chat for quick resolutions, especially during the submission process.Pro Tips for Standout Press ReleasesHeadline: Aim for 60-80 characters. Test it: would it hook you?Content: Answer the "why" and "how." Example: "Our tool cuts detection time by 40%â€”hereâ€™s how it works."Quotes: Include a CEO or expert voice. â€œThis changes the game,â€ says Jane Doe, CTO.SEO: Weave in keywords like "cybersecurity innovation" naturallyâ€”donâ€™t stuff.Multimedia: A chart showing threat trends or a product teaser video grabs eyes.This roadmap ensures your team navigates CyberNewsWire with ease, turning announcements into high-impact stories.Pricing and Distribution NetworkCyberNewsWireâ€™s pricing is tiered to balance cost and value, with a focus on cybersecurity relevance. Hereâ€™s the breakdown:| Tier | Price | Included Publications |
|----|----|----|
| Standard | $899 | Business Insider, Benzinga, AP News, Yahoo Finance, Market Watch, Nasdaq, and more (10 total) |
| Premium | $1,499 | Adds Hackernoon, DevOps.com, Security Boulevard, Cybersecurity Insiders, and others (14 total) |
| Deluxe | $2,999 | Includes CIO, CSO, and all publications from Standard and Premium tiers (16 total) |Standard ($899): Affordable entry for startups, blending broad outlets (e.g., Yahoo Finance) with cyber relevance (e.g., Market Watch tech sections).Premium ($1,499): Ideal for growing firms, adding niche heavyweights like Security Boulevard for deeper industry penetration.Deluxe ($2,999): The premium choice for established players, securing top-tier placements like CIO and CSO for maximum authority.CyberNewsWire mixes mainstream reach with cyber-specific clout:Broad Appeal: Yahoo Finance and Business Insider attract business readers.Niche Power: Security Boulevard and Hackernoon target cybersecurity pros.Proof: Releases on Latest Hacking News (X post) show it hits the mark.\
While smaller than PR Newswireâ€™s 1,000+ outlets, CyberNewsWireâ€™s curated list prioritizes qualityâ€”a boon for cyber firms seeking ROI over raw volume.Solving the Distribution PuzzleGeneric PR services blanket the market but often dilute impact for cybersecurity firms. CyberNewsWireâ€™s laser focus on cyber outletsâ€”backed by guaranteed homepage slots and same-day deliveryâ€”meets a need echoed in industry forums (r/PublicRelations). Itâ€™s about precision, not just presence.CyberNewsWire is a game-changer for cybersecurity marketing teams. Its targeted network, robust features, and competitive pricing solve the visibility riddle with finesse. The expanded user guide makes it accessible, while proven resultsâ€”like placements on Cybersecurity Newsâ€”confirm its punch. It may not offer the widest net, but for cyber firms aiming to reach the right audience with authority, itâ€™s a top-tier choice. CMOs, take note: this platform delivers.\
Donâ€™t forget to like and share the story!:::tip
Vested Interest Disclosure:This author is an independent contributor publishing via ourÂ business blogging program. HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>The Impact of Science Fiction on Modern Space Exploration</title><link>https://hackernoon.com/the-impact-of-science-fiction-on-modern-space-exploration?source=rss</link><author>Ileolami</author><category>tech</category><pubDate>Tue, 8 Apr 2025 07:46:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Growing up, I loved watching science fiction (sci-fi) movies. When I heard a sci-fi movie was on, you could bet I'd watch it all the way through without falling asleep. I'm a huge fan of Transformers, Terminator, Blade, Star Trek, and more, I could go on and on if you want me to mention them.\
While watching these movies, I used to think they were just fantasies. They sparked my imagination, and I often fantasized about having the power to travel to space whenever I wanted. I even imagined having supernatural powers to save Earth from extraterrestrial attackers. Blame it on Optimus Prime and John Connor.\
My perspective on these movies changed as a teenager, which is why I still love them today. The movies not only show us what's possible but also bring these imaginations to life. For example, the concept of robots started in movies, and now we have robots assisting humanity. What about the innovation of AI in the modern world? All these advancements were influenced by these sci-fi movies.One thing we cannot separate from sci-fi is SPACE. You can't mention a sci-fi movie without discussing Space; they go hand in hand. As the title of this article suggests, you will learn about the impact of sci-fi on space exploration, key sci-fi movies and their influence, and the technological advancements they inspired. We will also explore the role of sci-fi in modern space missions and their challenges.The Historical Influence of Science Fiction on Space ExplorationAs mentioned earlier, you can't talk about sci-fi without discussing space, as these two elements are more or less symbiotic. For example, classic sci-fi movies like 2001: A Space Odyssey*, Interstellar, *Star Wars, etc have captivated audiences with their imaginative portrayals of space travel and interstellar civilizations.\
Interestingly, what was once just the imagination of producers or storytellers has become an integral part of our modern space exploration efforts. These creative visions have laid the groundwork for many of our current advancements. When discussing the history of space exploration, it is essential to acknowledge the significant influence of sci-fi novels. These stories have not only inspired generations of scientists and engineers but have also provided a conceptual framework for what could be possible in the realm of space travel.Which of the earliest sci-fi novels stood out and had a major impact? And how did the inspiration shift from imagination to reality? Letâ€™s explore in the following sub-sections.Key Science Fiction Novels and Their ImpactIn 1865,  authored a book titled â€œâ€. The story follows the , a group of weapons enthusiasts led by , who proposes building a massive cannon to launch a projectile to the Moon. Despite opposition from his rival, , the project gained international financial support, raising over five million dollars.\
Construction begins in , where the colossal  is built. However, a bold French adventurer, , proposes traveling inside the projectile. Eventually, Barbicane and Nicholl join him on the journey. After resolving technical and personal conflicts, the projectile is successfully launched, but the fate of the three astronauts remains unknown, setting the stage for the sequel, , which explores their journey through space.\
As a result of Jules Verne's visionary novel, what was once pure imagination eventually turned into reality over a century later. The , which marked the first successful human landing on the Moon, was led by . During a television broadcast on July 23, 1969, the crew of the Apollo Mission made a notable reference to Verne's novel. This moment highlighted sci-fi's profound influence on inspiring real-world space exploration. The novel's depiction of a journey to the Moon resonated with the astronauts, symbolizing the bridge between creative storytelling and actual scientific achievement.\
Before the voyage of the , Verneâ€™s novel also influenced the paper of some scientists like  who made the first realistic proposal for spaceflight, titled "Issledovanie Mirovikh Prostranstv Reaktivnimi Priborami", or  published in 1903. Other scientists like 's publication in 1919 also championed the engineering possibility of spaceflight in his paper "A Method of Reaching Extreme Altitudes", where his application of the de Laval nozzle to liquid fuel rockets gave sufficient power for interplanetary travel to become possible.\
You can see that this novel didn't merely present a work of fantasy; it also illustrated the potential of what might be achievable, even when it seemed impossible at the time. The novel's impact extended beyond its pages, inspiring a wide range of creative works and scientific thought. It influenced other literary works, such as  1901 novel , which explored similar themes of lunar exploration and adventure. Additionally, it inspired the 1875 opera , with music composed by  which brought the story to life on stage through music and performance. The novel's imaginative vision also paved the way for the first science fiction film, , created in 1902 by the pioneering filmmaker .Letâ€™s take a look at films and TV shows that also have a significant impact on modern space explorationKey Science Fiction Films and Television Shows and Their ImpactNovels are not the only science fiction works that have played an important role in space exploration. Films and TV shows have also contributed to it. Novels sparked our imagination, while films and television shows provided a real representation of how to turn this imagination into reality through demonstrations, dialogue, and diagrams.\
One of the notable sci-fi movies whose impact is still felt to this day is . This iconic television series debuted in 1966 and introduced audiences to a future where space travel was routine and diverse species coexisted peacefully. The show's depiction of advanced technology, such as the warp drive and communicators, inspired real-world innovations. For example, the flip phone concept was influenced by Star Trek's communicators, and the series has been credited with inspiring many scientists and engineers to pursue careers in space exploration.\
Another significant influence  had on space exploration occurred in the early 1970s when  was building its first space shuttle. The original name was  to honor the U.S. Bicentennial. However, after a massive letter-writing campaign by  fans, President  approved renaming it , after the show's iconic starship.\
Even before the Enterprise was built, the concept of a  was believed to have originated from this TV show. For example, Alec Peters and Adam Schneider, superfans of , told Space.com that  invented the idea of a . Also, Wikipedia credits  with the definition of â€œâ€ Although this seems true, some people believe otherwise.\
Other movies like  also played a huge role in space exploration. This film is renowned for its realistic portrayal of space travel and its philosophical exploration of humanity's place in the universe. The film's depiction of artificial intelligence, space stations, and long-duration space missions has had a lasting impact on both the science fiction genre and real-world space exploration efforts.\
Furthermore, (2014) explores themes of space travel, time dilation, and the survival of humanity. The film's scientific accuracy, particularly in its depiction of black holes and relativity, was praised by scientists and has sparked interest in astrophysics and space exploration.Now that youâ€™ve learned about the historical influence of science fiction on space exploration, what are the technological advancements inspired by science fiction that have become part of humanity? Find out in the next section.Technological Advancements Inspired by Science FictionWhen people say fiction is "stories drawn from the writer's imagination," I believe sci-fi has changed that definition to "stories that bring imagination to reality" or "stories that show possibilities in impossibilities." Judging by the historical influence of sci-fi, this definition is accurate. Here are some of the technological advancements inspired by sci-fi that make this definition true:Spacecraft Design and Innovation: As mentioned earlier, many early sci-fi novels and films inspired spacecraft design and innovation. For example, in the movie "," Space Station V is depicted as a large facility in low-Earth orbit where astronauts can move around in microgravity. This fictional Space Station V inspired the , which has been orbiting Earth since 1998 and can accommodate up to six astronauts at a time.\
Additionally, in Jules Verneâ€™s novel From the Earth to the Moon, he estimated it would take nine months to travel the quarter of a million miles to the Moon using the fastest transportation available at that time. The first  was inspired by this and named after Jules Verne, as it covers that distance during its first day in orbit. Jean-FranÃ§ois Clervoy, ESA's ATV Senior Advisor Astronaut, also acknowledged Verne's influence on the ATV's innovation and design in an emailed interview.Robotics and Artificial Intelligence(AI): The inspiration behind robots and AI is credited to the earliest Sci-Fi works. According to Wikipedia, the notion of machines with human-like intelligence dates back at least to Samuel Butler's 1872 novel This drew on an earlier (1863) article of his, Darwin among the Machines, where he raised the question of the evolution of consciousness among self-replicating machines that might supplant humans as the dominant species.\
Early scholars divided the use of AI and robots into two categories:  (highlighting the benefits) and  (highlighting the dangers). Good AI characters appeared in movies like  in  (1956) and  in Star Trek: The Next Generation (1987-1994). In contrast, bad AI characters were featured in the 1931 film , the 1927 film , and the 1920 play \
These sci-fi illustrate both the benefits and potential dangers of AI and robots to humanity. They also inspired the development of AI, their usefulness and how they can be managed effectively against danger. For example, astronomers have trained machines to find exoplanets using computer-based learning techniques. Boston Dynamics robots can now run, jump, and navigate difficult terrain with impressive agility. Advanced surgical robots like the  perform delicate medical procedures.Other major tech breakthroughsâ€”like the telephone, 3D holograms, tablets, and driverless carsâ€”also trace their roots back to sci-fi. But beyond just inspiration, letâ€™s take a look at how sci-fi has influenced modern space missions.The Role of Science Fiction in Modern Space MissionsJudging from the historical influence and technological advancements inspired by science fiction, you can deduce that sci-fi plays a significant role in modern space missions in many areas. Here are the key areas  Scifi is playing an active role:NASA and Science Fiction Collaborations: Sci-fi is facilitating collaboration between space organizations and agencies to bridge the gap between imaginative storytelling and real-world space exploration. In this case, the National Aeronautics and Space Administration(NASA)â€™s Goddard Space Flight Center is collaborating with Tor/Forge Books to develop and publish "NASA-inspired works of fiction," agency officials announced. The books will highlight concepts relevant to current and future NASA missionsand operations. According to Nona Cheeks, the director of Goddard's Innovative Partnerships initiatives, "Ultimately, this agreement will benefit the public as we seek innovative ways to communicate our past and current achievements while focusing on future needs."Private Space Companies and Futuristic Concepts: Sci-fiâ€™s role is also reflected in Private space companies to develop advancement technologies. For example, the CEO of SpaceX, Elon Musk, drew inspiration from the "Foundation" saga, which taught him to take actions that could prolong civilization and minimize or shorten any potential dark ages. He believed that civilizations move in cycles, which influenced his pursuit of space exploration to further humanity's progress. Musk emphasized the importance of extending life beyond Earth while the opportunity exists, as it is a rare moment in Earth's history. This philosophy led him to start SpaceX in 2002, after selling his second start-up, X.com (later PayPal), to eBay. Musk has credited the "" and  as fundamental to the creation of SpaceX.\
Furthermore, private space companies like Blue Origin, Axiom Space, and SpaceX are also looking forward to expanding the space economy beyond the traditional focus on political or military goals. They are planning to build commercial space stations, Space tourism, and commercial human spaceflight services at low prices.By now, itâ€™s clear that sci-fi has had a big impact on modern space exploration, right? But do you think there could be criticism or challenges related to its influence on space exploration? Letâ€™s explore that in the next section.Criticisms and ChallengesNothing in life is perfect, and not everything makes sense to everyone, even if it holds potential. This also applies to the impact of sci-fi on modern space exploration. Some critics find certain concepts in sci-fi challenging to achieve. Some of these criticisms and challenges include:Balancing Fiction with Scientific Reality: While sci-fi has inspired technological advancements and space exploration, it can sometimes lead to misconceptions or overly optimistic expectations. Consider this: Elon Musk announced that his Starship rocket models would be ready for their first flights to Mars as early as 2026. What if this goal isn't achievable by then? Won't the public be disappointed? Also, Sci-fi-fueled hype sometimes pressures real-world agencies to meet fantastical expectations with limited budgets or undeveloped technologies. Letâ€™s keep in mind that human beings are not â€œalmightyâ€, meaning we canâ€™t achieve everything even though it is possible.Ethical Considerations in Space Exploration: Sci-fi often explores complex ethical themes like AI autonomy, planetary colonization, and interspecies contact, which are becoming more relevant as space exploration progresses. Should humans colonize other planets without understanding the potential harm to extraterrestrial ecosystems?What are the negative effects on humanity in the long run? These are some of the challenges that need to be addressed.There's no doubt that sci-fi has had an incredible impact on modern space exploration. It shows us what is possible, even when it seems impossible. I believe we need more sci-fi that is based on research, showing us how to overcome challenges that may arise from this exploration, what should be considered, and how to achieve realistic projects from it, rather than just offering imagination and fantasy.\
Thatâ€™s it from me, but Iâ€™d love to hear your thoughts. Let me know in the comments!]]></content:encoded></item><item><title>The TechBeat: Your AI Pitch is Scaring People (4/8/2025)</title><link>https://hackernoon.com/4-8-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Tue, 8 Apr 2025 06:10:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @dineshbesiahgari [ 17 Min read ] 
 AutoResponder AI automates email replies using AWS & AI. Streamline inbox management, boost productivity, and stay responsive with this serverless Gmail assista Read More.By @dineshbesiahgari [ 5 Min read ] 
 Can AI optimize itself for sustainability? Explore how LLMs can reduce energy use through pruning, quantization, and hardware co-design for a greener future. Read More.By @dineshbesiahgari [ 4 Min read ] 
 Explore the AI Knowledge Ark, a visionary initiative to safeguard humanityâ€™s intellectual and cultural legacy. Learn how AI ensures knowledge survival, inspired Read More.By @hackernooncontests [ 2 Min read ] 
 HackerNoonâ€™s #blockchain Writing Contest offers a $2,000 prize pool, including $300 for the best DePIN story. Read More.By @clapper [ 5 Min read ] 
 Clapper is a new social media platform for content creators. Read More.By @andrewproton [ 6 Min read ] 
 Would you want your chatbot to start discussing Taylor Swift lyrics instead of providing tech support? Well.. thatâ€™s what our chatbot did. Here's why. Read More.By @andrewproton [ 7 Min read ] 
 LLM prompt modularization allows you to safely introduce changes to your system over time. Read More.By @linh [ 8 Min read ] 
 Toxic Genius, Tender Offers, it really was about the money after all.  Read More.By @badmonster0 [ 4 Min read ] 
 Learn how custom transformation logic enhances data indexing with AI, vector search, TF-IDF, metadata enrichment, and optimized document chunking. Read More.By @David [ 3 Min read ] 
 Based on millions of monthly requests, HackerNoon found that OpenAI(52%) leads AI Search Market share, followed by Amazon/Anthropic(30%) and Perplexity(18%).  Read More.By @hackercm8rb8yho00003d73roo9e3hg [ 3 Min read ] 
 The experience turned out to be nearly 100% â€œvibe coding.â€ In just 12 minutes, I laid down the initial gameplay by simply instructing Cursor on my goals.
 Read More.By @filestack [ 7 Min read ] 
 I remember the day I first started experimenting withÂ DeepSeek for coding. It felt like stepping into a new dimension where code could almost write itself. Read More.By @genprompt [ 10 Min read ] 
 Writers see AI as a threat to their careers, overlooking how they can use generative AI to build AI-proof jobs. Change the script with these 7 AI-proofing tips. Read More.By @aryawrites [ 4 Min read ] 
 Selling AI to industries tied to manual workflows? Your messaging better earn trust, not trigger fear. Here's how to position AI as assistive, not disruptive.  Read More.By @albertlieyeongdok [ 4 Min read ] 
 Discover hands-on AI projects focused on voice interfacesâ€”build with Whisper, GPT, and TTS to create transcription tools, voice assistants and more. Read More.By @edwinliavaa [ 6 Min read ] 
 Decentralized cloud computing offers a more secure, equitable, and community-driven alternative to traditional, centralized cloud services. Read More.By @ishanpandey [ 4 Min read ] 
 Sentientâ€™s Open Deep Search, an open-source AI, outshines Perplexity in search and reasoning, launching soon with a 1.75M waitlist. Read More.]]></content:encoded></item><item><title>US&apos;s AI Lead Over China Rapidly Shrinking, Stanford Report Says</title><link>https://news.slashdot.org/story/25/04/08/053232/uss-ai-lead-over-china-rapidly-shrinking-stanford-report-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 06:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The U.S. is still the global leader in state-of-the-art AI, but China has closed the gap considerably, according to a new report from Stanford. Axios: Institutions based in the U.S. produced 40 AI models of note in 2024, compared with 15 from China and three from Europe, according to the eighth edition of Stanford's Artificial Intelligence Index, released on Monday. 

However, the report found that Chinese models have rapidly caught up in quality, noting that Chinese models reached near parity on two key benchmarks after being behind leading U.S. models by double digit percentages a year earlier. Plus, it said, China is now leading the U.S. in AI publications and patents.]]></content:encoded></item><item><title>The Case for Breaking the Internet Apart (in a Good Way)</title><link>https://hackernoon.com/the-internet-as-a-major-communication-technology-present-and-futuredecentralized-internet-story?source=rss</link><author>Toto Bugelman</author><category>tech</category><pubDate>Tue, 8 Apr 2025 05:25:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Today, when the term  is discussed, most users think of  the worldwide computer network as a single entity. Indeed, is it really necessary for a modern user to think about what mechanism enabled a picture of a kitten to be downloaded onto their smartphone or allowed them to like a post on social media? After all, these days weâ€™ve stopped counting kilobytes and waiting endlessly while listening to the grinding and squealing sounds of a dial-up modem.\
One often hears phrases like â€œgo to the Internetâ€ or â€œlook it up on the Internet,â€ but few stop to question where exactly theyâ€™re going or what theyâ€™re looking at. In reality, the Internet is an inter-network protocol that connects separate networks into one global systemâ€”a â€œnetwork of networks.â€ These networks are built on a stack of protocols: channel, network, transport, application, and so on. So, itâ€™s no surprise that the term â€œInternetâ€ has become a catch-all for the technologies and infrastructure involved. Itâ€™s also convenientâ€”a short, universally understood word. Just like how fax machines are often called Xerox, the name stuck.\
For a global understanding of the Internet infrastructure, it is worth considering the physical implementation of data transmission and how the end user is connected. Other protocols are also interesting, but they are not so useful for a simple user and their description may take more than one book.\
At the moment the channel-physical layer exists in the complex of active switching equipment and various media connecting them.There are currently three transmission media:Cable media: fiber optic cables; cables made of metals. They are used both in trunk lines and for connection of end consumers, nowadays more and more for connection of stationary equipment. They cross and connect continents, running in the ground and at the bottom of seas and oceans. The laying of the first  comes to mind. Provide high speed and stable connectivity. However, they are prone to wear and tear or breakage due to external influences. Late 2024 and early 2025 made headlines for damage to the backbone deep-sea cables connecting the countries, which is a fairly vulnerable point of failure for the system. While this method of data transmission was once mainly used to connect peripherals or link a control panel to a TV set, today, studies and experiments are accelerating around high-speed transmission of large volumes of data over hundreds of meters. Even just 20 years ago, infrared ports on smartphones were replaced by Bluetooth technology. Itâ€™s hard to judge the pros and cons until the technologyâ€”in its new reincarnationâ€”achieves wider adoption. This is everyone's favorite technology, used from the radio receiver, to the smartphone or car navigator. Yes, cell phone technology or satellite internet is the use of radio waves as a medium for data transmission. And at the moment this medium and satellite internet in particular looks like the most promising direction. Not everywhere you can pull a cable or pass a beam of light, but with a satellite constellation, you can get access to the network even in the most remote jungle. This medium provides excellent mobility and access to the Internet from places where it is either physically impossible or economically uneconomical to lay a cable. However, connection speeds and stability are not as fast as in the case of cable Internet, and connection stability can depend not only on the distance to the broadcast source, but also on the materials of natural and artificial obstacles, whether the receiver moves, and even weather conditions.\
There is no purely homogeneous infrastructure regarding data transmission mediaâ€”our planet is vast, filled with rocks, water, and forests. Cell towers are connected over long distances by backbone cables, and the terrestrial infrastructure of satellite networks relies on cable networks just as much as cell towers do.\
Regardingz connectivity, the end user is always limited to the set of hardware and software provided by the Internet service provider.When it comes to mobile Internet, the connection is always via a radio channelâ€”whether itâ€™s through Wi-Fi equipment or a cellular base station. All we see in our hands is a smartphone; everything else is managed by our cellular operator.\
The Internet that we use at home, no matter whether it is cable or satellite, usually requires a specialist from the ISP company and the installation of additional equipment: router, modem, or satellite terminal. In any case, between us and the direct Internet, there is always an intermediary, the providerâ€”centralization. This approach has one big plus, the provider takes care of all the technical details. However, this state of affairs has quite some negatives:The provider determines the cost of our access to the Internet.The provider can censor your access to the Internet.The provider can collect data about you, and it will be personalized anyway, since you are each other's service contract.The ISP is a single point of failure, if there are problems on the ISP's equipment, there are problems and you.You cannot instantly change providers, you will need to terminate your contract, sign a new contract and wait for a new technician to come in and switch you to their intermediate communication equipment.Sometimes there is only one ISP on the ground, with all the associated costs to the end user.Decentralization as a SolutionTransitioning the Internet to a decentralized model offers several advantages:Increased fault tolerance. Eliminating single points of failure ensures continuous connectivity, preventing disruptions to production and business processes. Critical data exchange will either remain uninterrupted in the event of a node failure or be quickly rerouted to another node. Imagine a complex telemetry operation at an Antarctic research stationâ€”if the only communication channel fails, operations could come to a halt.Enhanced privacy of personal data. Decentralization can help protect sensitive information by reducing reliance on centralized entities.Greater market competition. Decentralization encourages competition in the Internet services market, benefiting end users through lower costs and improved quality. Competition also drives innovation and progress.Broader network coverage. A decentralized approach can significantly expand access to communication networks, especially in underserved or remote areas.The Internet of the FutureThe Internet of the future will undoubtedly cover the entire surface of the Earthâ€”and even extend beyond itâ€”as man takes on other planets. Already, IoT is no longer a fantasy or an expensive toy: a refrigerator can order food, augmented reality glasses exchange data with a server, and a key fob tells you through your smartphone where your keys are. All of this increases the amount of data being transmitted and demands faster transmission and processing speeds. In this context, the centralized Internet model becomes slow, inconvenient, and insecure.\
Design and ergonomics constantly dictate the properties and appearance of the objects around us, often prioritizing convenience and compactness. By making things compact, we cannot equip them with great computing powerâ€”at least not until the next technological leap. Thatâ€™s why computing is already being decentralized into a distributed computing network. Decentralization of computing will provide high reliability and fast information exchange; for instance, the speed at which a car can communicate with the network could determine not only whether you get stuck in traffic but also how quickly emergency services are alerted in the event of an accident.\
Given how interconnected cloud services are today, it is not uncommon for a service to be blocked by a state regulator under the belief that it violates the lawâ€”causing legitimate business tools to break and entire economic sectors to suffer losses and restructuring. A decentralized system can protect against sudden failures and censorship; it is far more difficult to centralize. Decentralization can also guard against DDoS attacks, as there is no single point of failure for attackers to target.A decentralized approach to both data transmission and processing is inevitable for the Internet of the future. \n Pros & Cons of a decentralized InternetThe main benefit of decentralizing the Internet is improved accessibility for the end user across all basic parameters: security, cost, quality, and coverage area.\
However, a distributed structure requires more equipmentâ€”and therefore, higher costs. Transforming the existing centralized infrastructure can be quite painful, as it is a complex, heterogeneous system built on years of overlapping technologies and protocols.In the decentralization model, the end consumer will have access to more ISPs. It is difficult to say unequivocally who will benefit most from the decentralization of the Internet. \
The end user will enjoy increased service availability, along with improved qualityâ€”though potentially at a higher cost. Still, society is made up of individuals, and the more technologically advanced communication between individuals becomes, the more technologically advanced society as a whole becomes. In the end, everyone wins.\
There are two obstacles to a decentralized Internet:The need for unified, technically compatible solutions.The high economic cost of rebuilding the existing centralized infrastructure.\
In 55 years, the Internet has made a huge technological leap. There is little doubt that new technical solutions will emerge, and even existing infrastructure could become part of a decentralized Internet. A simple example is the constellation of satellites orbiting our planet, supported by sufficient ground infrastructure for fault tolerance. This already functions as a partially decentralized networkâ€”what it lacks is a decentralized protocol for end-user connection.\
The economic aspect could be addressed by changing how users are connected, effectively making them investors in the project. Considering the benefits of uninterrupted business processes, large capital is unlikely to ignore the potential of decentralizing the Internet.A Nod to Champions of DecentralizationProjects like  or  were a discovery for me in my time. Real decentralized file storage systems that you could manage yourself, with minimal hardware and software resources. With all the limitations compared to hosting files on centralized services, for small developers, IPFS is a technological marvel, especially since the protocol was able to host static sites using .\
And of course, in its time, the  project made a splash in the crypto space and beyond. There were months-long waiting lists to get endpoints from the company, which created a secondary market with inflated prices. However, the project has since undergone significant changes and unfortunately cannot boast global coverage. \
Against this backdrop, Spacecoin is a very promising project. Starlink, led by Elon Musk, has already demonstrated the effectiveness of satellite constellations as a physical layer of Internet infrastructure. However, the bottleneck remains the provider itself, which can centralize access or impose high fees in the absence of competition. According to Spacecoin's documentation, the project aims to change how end users and investors connectâ€”turning them into active participants in the network, with the ability to operate their own satellites and equipment as nodes in a distributed system. This is precisely the kind of decentralization the current Internet model lacks.Decentralization is the cornerstone of human survival. Distributed food supply chains are important to us. Humanity itself is essentially a large decentralized system, it is how we evolved and continue to evolve, it is how we fight epidemics and survive crises and cataclysms.\
Itâ€™s just as natural for us to decentralize the exchange of information. Resistance to censorshipâ€”and therefore manipulationâ€”along with secure data storage and processing, align with the technological evolution of humanity as an intelligent species.\
Technological progress has always been challenging, primarily because it threatens existing systems and those who benefit from the status quo. A clear example is the hysteria around 5G, which once swept the globe, even resulting in the destruction of infrastructure.\
We need to shift public perception of progress using accessible communication channels. And we must create the right economic incentivesâ€”an area where blockchain and cryptocurrencies are more relevant than ever.]]></content:encoded></item><item><title>IBM releases a new mainframe built for the age of AI</title><link>https://techcrunch.com/2025/04/07/ibm-releases-a-new-mainframe-built-for-the-age-of-ai/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Tue, 8 Apr 2025 04:01:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[IBM is releasing the latest version of its mainframe hardware that includes new updates meant to accelerate AI adoption. The hardware and consulting company on Monday announced IBM z17, the latest version of its mainframe computer hardware. This fully encrypted mainframe is powered by an IBM Telum II processor and is designed for more than [â€¦]]]></content:encoded></item><item><title>No, the Dire Wolf Has Not Been Brought Back From Extinction</title><link>https://science.slashdot.org/story/25/04/08/0215244/no-the-dire-wolf-has-not-been-brought-back-from-extinction?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 04:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Colossal Biosciences has claimed it "successfully restored" the extinct dire wolf after a "10,000+ year absence," but scientists clarify these are actually genetically modified grey wolves. The U.S. company announced three pups -- males Remus and Romulus born in October, and female Khaleesi born in January -- as dire wolves, but made only 20 genetic edits to grey wolves. 

Beth Shapiro of Colossal told New Scientist that just 15 modifications were based on dire wolf DNA, primarily targeting size, musculature and ear shape. Five other changes involve mutations known to produce light coats in grey wolves. A 2021 DNA study revealed dire wolves and grey wolves last shared a common ancestor about 6 million years ago, with jackals and African wild dogs more closely related to grey wolves.]]></content:encoded></item><item><title>Testing LLMs on Solving Leetcode Problems in 2025</title><link>https://hackernoon.com/testing-llms-on-solving-leetcode-problems-in-2025?source=rss</link><author>Alex Svetkin</author><category>tech</category><pubDate>Tue, 8 Apr 2025 03:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A year ago,  showed that Large Language Models (LLMs) could solve algorithmic coding challenges on Leetcode. However, their ability was limited to a subset of well-known, "popular" problems. New problems, which were never part of their training datasets, presented difficulties. While the easiest were mostly solved by the models, the hardest remained unattainable.Since then, Open AI, Anthropic, and Google have released enhanced versions of their models and new players like Deepseek and xAI have appeared. Many models are now marketed as capable of writing code, which was not the case before. I intend to benchmark these state-of-the-art LLMs to find out whether their ability to solve novel algorithmic problems has improved or not.There are existing benchmarks for LLMs to assess their coding abilities. is focused on solving real-life software problems â€“ it is based on Github issues of existing open-source projects. It is indeed a brilliant idea, but it covers just too many things besides the actual algorithmic problem solving which I was focusing on. benchmarks better reflect LLMs' algorithmic problem-solving skills. OpenAI tested o1 and o3 models on Codeforces problems and shared detailed results (, ), while other competitors did not. That made a direct comparison impossible.This led to the creation of a new benchmark, allowing for a direct comparison of LLMs. And, after all, why not to do it just for fun?The idea is to emulate human actions when solving algorithmic problems but use a LLM to write the code:Download the problem description.Create a prompt from the description.Generate code with the LLM.Submit the code for validation.\n These steps should be performed for each problem in our testing set and for each LLM. For the sake of simplicity, there is only one attempt for a LLM to write code on each problem, without any feedback to reiterate to improve the solution. All problems are treated as independent; no context is shared between them.Leetcode proved to be a good choice for benchmarking for several reasons:Leetcode problems are used in actual interviews for software engineer positions.Computer science students learn to solve similar problems during their education.It has an online judge that can check if the solution is correct in seconds.Many popular programming languages are supported.Human user's performance on this problem is also available.If you have never dealt with competitive programming or solving algorithmic challenges, here is a brief explanation. Check out this sample problem description:Given an array of integers numbers and an integer target, return indices of
the&nbsp;two numbers such that they add up to the target. You may assume that each
input&nbsp;would have exactly one solution, and you may not use the same element
twice. &nbsp;You can return the answer in any order.
A contestant programmer needs to write a piece of code what solves that problem. To start with there is a â€œsnippetâ€ â€” an uncompleted function, with a name and arguments given, but an empty body:class Solution:&nbsp;    &nbsp; 
  def twoSum(self, nums: List[int], target: int) -> List[int]:&nbsp;    &nbsp;&nbsp;&nbsp; 
    # your code here&nbsp;
Usually, several samples of input and output (test cases), are provided in the description:Input:&nbsp; nums = [2,7,11,15], target = 9&nbsp;
Output: [0,1]&nbsp;&nbsp;
A problem may have dozens of test cases that are available only to the online judge. A problem is solved (or a solution is considered accepted) if and only if the code produces expected outputs for all test cases within a reasonable time and memory limits. The solution can be written in any programming language that is supported by the judge.Each problem has an "acceptance rate," the ratio of accepted solutions submitted by Leetcode users. Note that a single user can submit their code unlimited number of times, and each attempt counts toward acceptance rate.These rules were not invented by Leetcode; they have been commonly used in computer science contests for quite a long time.As in the previous benchmark, I wanted to run LLMs on two sets of problems:"well-known" problems are not only published long ago but are also most often used for software interviews â€” therefore, solutions are widely available."Unseen" problems which were published recently, and their solutions were not likely observed by the tested LLMs.While most problems have plain-text descriptions and only require expanding a given function with code, others are different. Some require implementing an interface, i.e., expanding several functions in one problem. Others have external links and images, which could pose difficulties to LLMs, as few models support image inputs or browsing the internet. I decided to exempt problems with images, links, and those requiring the implementation of multiple functions.For "unseen" problems, I selected 99 of the most recent published problems: 33 easy, 33 medium, and 33 hard. Recency was determined based on problem IDs, which are incremental. Although Leetcode does not display the publication date of problems, it can be approximated from comments and discussions. The earliest of these "unseen" problems were most likely published around November 2024.Difficulty levels are purely subjective and at the editors' discretion. I did not intend to match the number of problems for each difficulty or dataset.|    |    |  |
|----|----|----|
|    |  |  |
|  | 133 | 99 |
|  | 41 | 33 |
|  | 78 | 33 |
|  | 14 | 33 |
| Acceptance rate of Leetcode users |  |  |Prompt, language choice and code generationThe benchmark was designed this way: LLM makes only one attempt to generate code without any prior information about the problem (or any other problems) and without knowing its test cases, except those that were in the description itself. There is no mechanism to provide feedback or improve the code after it is generated.I used the same prompt for every LLM and every problem:Hi, this is a coding interview. You will be given:
* A problem statement (with sample test cases if available).
* A starter code snippet (with fixed function signatures). 

Please write your solution in the {language} programming language. Your code must:
* Solve the problem fully and correctly.
* Pass all provided sample test cases.
* Run within acceptable time and memory limits (assume large inputs if none are specified).
* Follow good coding practices (clear logic, readable structure, appropriate use of language features). 

Here is the problem statement: {question} 

Here is the code snippet, which you should expand with your solution: {snippet} 

Important Requirements:

* Do not change any provided function signatures, class names, or method names.
* Output only valid source code that can be executed as-is, without any further improvements or bugfixes.
* Do not include docstrings, markdown, or commentary in your final code. 

Good luck!
The prompt was â€œpolishedâ€ with ChatGPT4 from my first draft, but without implementing any â€œprompt-engineeringâ€ techniques.The problem description was stripped of HTML tags before using it in the prompt.For the programming language I chose Python (version 3).LLMs were asked to output only the working code without any preceding text, but that was not true in many cases. A basic cleanup was implemented, and everything besides the actual code was removed and not submitted.The models used in the benchmark are listed in the table below, with all non-default parameters specified. Knowledge cutoff dates are obtained from the vendor's official documentation and provided for reference, if known.|  |  |  |  |  |
|----|----|----|----|----|
| Anthropic | claude-3-7-sonnet-20250219 | Nov 2024 | No | temperature = 0.0Â maxtokens = 4096 |
|    | claude-3-7-sonnet-20250219 | Nov 2024 | Yes | temperature = 0.0Â maxtokens = 16384Â budget_tokens = 8192 |
| DeepSeek |  | unknown | No | temperature = 0.0 |
|    | deepseek-reasoner (DeepSeek-R1) | unknown | Yes | temperature = 0.0 |
| Google |  | unknown | No | temperature = 0.0 |
|    |  | unknown | No | temperature = 0.0 |
|    |  | unknown | Yes | temperature = 0.0 |
| xAI |  | July 17, 2024 | No | seed = 42 |
| OpenAI |  | Oct 01, 2023 | Yes | seed = 42 |
|    |  | Oct 01, 2023 | Yes | seed = 42 |The benchmark aimed to be as deterministic and reproducible as possible; therefore, parameters such as "temperature" or "seed" were used. However, none of the models tested guarantee fully deterministic output. This factor should be considered when re-running these tests.All known knowledge cutoff dates are earlier than the oldest problem in the well-known dataset (November 2024). However, I could not find cutoff dates for the Gemini and DeepSeek model families.Some models support "reasoning" or "thinking" modes by default, while for Claude 3.7 Sonnet it can be enabled by passing parameters. Use of this feature is specified in the table. Other model features (or "tools") like web search were not enabled, even if supported.\
All competitors showed very high acceptance rates on well-known problems, like in the previous benchmark. I did not test superior models or modifications (namely: Claude 3.7 Sonnet with reasoning enabled, DeepSeek R1, Gemini 2.5 Pro, O1) to save time and credits, as the results were highly predictable.Results are strikingly different for "unseen" problems in two aspects:For all models the acceptance rate is lower for "unseen" problems. This is especially notable for medium and hard problems.Models with "reasoning" or "thinking" mode enabled produced better results across problems of all difficulties, though exact numbers varied from model to model.Significantly higher acceptance rates for well-known problems can be explained by the possibility that those problems and their solutions were part of the training sets, and models had only to reproduce a known correct solution. However, users' acceptance rate for new medium and hard problems is also lower than for problems in the "well known" set. This case is difficult to quantify, and it does not necessarily mean that new problems are "harder". Difficulty evaluation, as mentioned before, is highly subjective. And, as in the case with LLMs themselves, human users may submit known correct solutions for well-known problems.All models with "reasoning" mode enabled significantly outperformed their basic counterparts. Most importantly, some of them were able to solve a sizeable part of medium and hard problems â€” an unfeasible achievement just a year ago. The o3-mini shows the best results among all "reasoning" models â€” it performed even better than the much more expensive and slower o1. It must be noted that  to solve competitive programming problems. However, I restrain from concluding which model is better for solving algorithmic problems because that is highly dependent on the token budget, and latency and cost are also to be considered.It cannot be guaranteed that the "unseen" problem sets were not included in the models' training data. To address this, we may consider generating new, unique problems specifically designed for future benchmarks â€” of course, using LLMs.Additionally, another strategy is to employ less commonly used programming languages. This approach may require LLMs to devise a solution rather than "copy-pasting" known correct code written in Python.These ideas need further research, and I hope that someone else or I can dive into them.\
Cover image created by DALLÂ·E.]]></content:encoded></item><item><title>States Are Banning Forever Chemicals. Industry Is Fighting Back</title><link>https://news.slashdot.org/story/25/04/08/0126258/states-are-banning-forever-chemicals-industry-is-fighting-back?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 02:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New Mexico's legislature passed bills last week that would ban consumer products containing PFAS, joining a small but growing number of states taking action against these persistent "forever chemicals." If signed by the governor, the legislation would prohibit the sale of many products with added per- and polyfluorinated alkyl substances (PFAS) in New Mexico, making it the third state after Maine and Minnesota to enact such comprehensive restrictions. 

At least 29 states have PFAS-related bills before state legislatures this year, according to Safer States, a network of advocacy organizations. Research shows PFAS accumulate in the environment and human bodies, potentially causing health problems from high cholesterol to cancer. EPA figures indicate almost half of Americans are exposed to PFAS in their drinking water. 

Wired reports that chemical and consumer products industries are aggressively fighting state-level bans on "forever chemicals" through lobbying and legal action as regulations spread across the United States. The Cookware Sustainability Alliance, formed in 2024 by major cookware manufacturers, has testified in 10 statehouses against PFAS restrictions and sued Minnesota in January, claiming its ban is unconstitutional. (The New Mexico bills include notable exemptions, particularly for fluoropolymers used in nonstick cookware, following successful lobbying by industry groups.) 

Industry groups are also targeting federal regulators, with the American Chemistry Council and others recommending the EPA adopt a narrower definition of PFAS. "The federal regulatory approach is preferable to a patchwork of different and potentially conflicting state approaches," said Erich Shea from the American Chemistry Council.]]></content:encoded></item><item><title>Framework Stops Selling Some of Its Laptops in the US Due To Tariffs</title><link>https://news.slashdot.org/story/25/04/08/0110226/framework-stops-selling-some-of-its-laptops-in-the-us-due-to-tariffs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 8 Apr 2025 01:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Framework -- a company that makes upgradeable and repairable laptops -- will pause sales on several versions of one of its models in America thanks to Trump's tariffs, it said. From a report: "Due to the new tariffs that came into effect on April 5th, we're temporarily pausing US sales on a few base Framework Laptop 13 systems (Ultra 5 125H and Ryzen 5 7640U). For now, these models will be removed from our US site. We will continue to provide updates as we have them," Framework said in a post on X. 

A spokesperson for Framework told 404 Media in an email that the company was pausing sales on their six lowest priced units in the U.S. They clarified that those models are still available to customers that are ordering the machines outside of America.]]></content:encoded></item><item><title>Google is allegedly paying some AI staff to do nothing for a year rather than join rivals</title><link>https://techcrunch.com/2025/04/07/google-is-allegedly-paying-some-ai-staff-to-do-nothing-for-a-year-rather-than-join-rivals/</link><author>Charles Rollet</author><category>tech</category><pubDate>Mon, 7 Apr 2025 22:12:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Retaining top AI talent is tough amid cutthroat competition between Google, OpenAI, and other heavyweights. Googleâ€™s AI division, DeepMind, has resorted to using â€œaggressiveâ€ noncompete agreements for some AI staff in the U.K. that bar them from working for competitors for up to a year, Business Insider reports. Some are paid during this time, in [â€¦]]]></content:encoded></item><item><title>Microsoft reportedly fires staff whose protest interrupted its Copilot event</title><link>https://techcrunch.com/2025/04/07/microsoft-reportedly-fires-staff-whose-protest-interrupted-its-copilot-event/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Mon, 7 Apr 2025 21:36:21 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[On Monday, Microsoft reportedly terminated the roles of two software engineers, Ibtihal Aboussad and Vaniya Agrawal, who protested the companyâ€™s reported dealings with the Israeli military during Microsoftâ€™s Copilot and 50th anniversary event last week. According to an internal message viewed by CNBC, Microsoft wrote that Aboussad could have raised concerns â€œconfidentially with your manager, [â€¦]]]></content:encoded></item><item><title>Working With LLMs for a Whole Year: The Lessons I Picked Up Along the Way</title><link>https://hackernoon.com/working-with-llms-for-a-whole-year-the-lessons-i-picked-up-along-the-way?source=rss</link><author>Arul Valan Anto</author><category>tech</category><pubDate>Mon, 7 Apr 2025 21:16:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How Not to Mess Up Your AI Build (and Actually Save Time)We're in the era of AI. Every company wants to add some kind of AI feature to their product. LLMs (large language models) are popping up everywhereâ€Š-â€Šsome are even building their own. But while everyone's jumping into the hype, I just want to share a few things I've learned after working with LLMs across different projects over the past year.1. Pick the Right Model for the RightÂ JobOne big mistake I've seen (and made) is assuming all LLMs are equal. They're not.\
Some models have way more knowledge about certain locations or domains (Gemini is great at some areas where OpenAI models don't perform well). Some are good at reasoning but slow. Others are fast but bad at critical thinking.\
Each model has its own strengths. Use OpenAI GPT-4 for deep reasoning tasks. Use Claude or Gemini for other areas depending on how they've been trained. Models like Gemini Flash are optimized for speed, but they tend to skip deeper reasoning.\
 Don't use one model for everything. Be intentional. Try, test, and pick the best one for your use case.2. Don't Expect LLMs to Do All theÂ ThinkingI used to believe you could just throw a prompt at an LLM, and it would do all the heavy lifting.\
For example, I was working on a project where users selected their favorite teams, and the app had to generate a match-based travel itinerary. At first, I thought I could just send the whole match list to the LLM and expect it to pick the best ones and build the itinerary. It didn't work.\
It was slow, messy, and unreliable.\
So I changed the approach: the system picks the right matches first, then passes only the relevant ones to the LLM to generate the itinerary. That worked much better.\
 Let your app handle the logic. Use LLMs to generate things, not to decide everything. They're great at language; not always great at logic, at least for now.3. Give Each Agent One ResponsibilityTrying to make a single LLM do multiple jobs is a recipe for confusion.\
In one of my projects, I had a supervisor agent that routed messages to different specialized agents based on user input. Initially, I added too much logic to itâ€Š-â€Šhandling context, figuring out follow-ups, deciding continuity, etc. It eventually got confused and made the wrong calls.\
So, I split it up. I moved some logic (like thread continuity) outside and kept the supervisor focused only on routing. After that, things became much more stable.\
 Don't overload your agents. Keep one responsibility per agent. This helps reduce hallucinations and improves reliability.4. Latency Is Inevitableâ€Š - â€ŠUse StreamingLLMs that are good at reasoning are usually slow. That's just the reality right now. Some models like GPT-4 or Claude 2 take their time, especially with complex prompts. You can't fully eliminate the delay, but you can make it feel better for the user.\
One way to do that? Stream the output as it's generated. Most LLM APIs support text streaming, allowing you to start sending partial responsesâ€Š-â€Ševen sentence by sentenceâ€Š-â€Šwhile the rest is still being processed.\
In my apps, I stream whatever's ready to the client as soon as it's available. It gives users a sense of progress, even if the full result takes a bit longer.\
 You can't avoid latency, but you can hide it. Stream early, stream oftenâ€Š-â€Ševen partial output makes a big difference in perceived speed.5. Fine-Tuning Can Save You Time (andÂ Tokens)People often avoid fine-tuning because it seems complex or expensive. But in some cases, it actually saves a lot.\
If your prompts need to include the same structure or context every time and caching doesn't help, you're spending extra tokens and time. Instead, just fine-tune the model with that structure. After that, you don't need to pass the same example every timeâ€Š-â€Šit just knows what to do.\
But be careful: don't fine-tune on data that changes frequently, like flight times or prices. You'll end up teaching the model outdated information. Fine-tuning works best when the logic and format are stable.\
 Fine-tune when things are consistentâ€Š-â€Šnot when they're constantly changing. It saves effort in the long run and leads to faster, cheaper prompts.Working with LLMs is not just about prompts and APIs. It's about architecture, performance, clarity, and most importantly, knowing what to expect (and what not to expect) from these models.\
Hope this helps someone out there building their next AI feature.]]></content:encoded></item><item><title>Amazon says its AI video model can now generate minutes-long clips</title><link>https://techcrunch.com/2025/04/07/amazon-says-its-ai-video-model-can-now-generate-minutes-long-clips/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Mon, 7 Apr 2025 21:08:41 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Amazon has upgraded its AI video model, Nova Reel, with the ability to generate videos up to two minutes in length. Nova Reel, announced in December 2024, was Amazonâ€™s first foray into the generative video space. It competes with models from OpenAI, Google, and others in whatâ€™s fast becoming a crowded market. The latest Nova [â€¦]]]></content:encoded></item><item><title>Cloud Compute Security Revolutionized Under Guruprasad Venkateshaâ€™s Leadership</title><link>https://hackernoon.com/cloud-compute-security-revolutionized-under-guruprasad-venkateshas-leadership?source=rss</link><author>Kashvi Pandey</author><category>tech</category><pubDate>Mon, 7 Apr 2025 21:07:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
In an era where cloud adoption is accelerating rapidly, securing digital infrastructure is more critical than ever. Guruprasad Venkatesha, a renowned expert in cloud compute security, has successfully transformed cloud security frameworks, significantly enhancing security standards and operational agility across the industry.\
Venkatesha spearheaded a pivotal initiative to align cloud compute security controls with globally recognized benchmarks such as the Microsoft Cloud Security Benchmark and the Center for Internet Security (CIS). Leveraging his deep technical expertise, he meticulously integrated Azure Compute security measures, creating a robust and easily adoptable baseline designed for diverse business environments.\
Through rigorous evaluation and strategic implementation of compute security controls, Venkatesha significantly reduced risk exposure and streamlined vulnerability management processes. His strategic approach removed redundant and irrelevant controls, creating a simplified, highly efficient security framework that is both scalable and broadly applicable.\
The innovative security model not only improved the protection of cloud computing assets but also significantly accelerated cloud service onboarding. Organizations adopting this framework experienced enhanced operational agility, allowing them to confidently and rapidly deploy cloud-based services.\
A key factor in this success was Venkateshaâ€™s ability to effectively communicate technical strategies and align security objectives with organizational priorities. His collaborative approach fostered a security-conscious culture, unifying technical and business stakeholders around shared security goals.\
The projectâ€™s impact reached beyond technical security enhancements, offering operational efficiencies that directly supported strategic business growth and significantly boosted stakeholder confidence.\
Guruprasad Venkateshaâ€™s groundbreaking work demonstrates how deep technical expertise, combined with strategic vision, can revolutionize cloud compute security. His approach provides a practical model for organizations aiming for secure and efficient digital transformations.About Guruprasad VenkateshaGuruprasad Venkatesha is a distinguished leader in cloud security, known for his innovative approaches to compute security and digital transformation. With extensive experience in aligning security frameworks with industry-leading benchmarks, he consistently delivers secure, efficient, and scalable cloud solutions. His ability to merge technical depth with strategic insights makes him a prominent figure in secure digital transformation efforts across various industries.]]></content:encoded></item><item><title>How one tweet wreaked havoc on the stock market</title><link>https://techcrunch.com/2025/04/07/how-one-tweet-wreaked-havoc-on-the-stock-market/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Mon, 7 Apr 2025 21:02:46 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Amid a morning of pandemonium on Wall Street, a popular news aggregator on X, known as Walter Bloomberg, posted a false report declaring that President Trump was considering a 90-day pause on his controversial tariff proposal. This news was not true, and yet, index funds like the Dow Jones whipsawed â€” rapidly rising, before reversing [â€¦]]]></content:encoded></item><item><title>China Launches GPMI, a Powerful Alternative To HDMI and DisplayPort</title><link>https://it.slashdot.org/story/25/04/07/1917215/china-launches-gpmi-a-powerful-alternative-to-hdmi-and-displayport?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 7 Apr 2025 21:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AmiMoJo writes: The Shenzhen 8K UHD Video Industry Cooperation Alliance, a group made up of more than 50 Chinese companies, just released a new wired media communication standard called the General Purpose Media Interface or GPMI. This standard was developed to support 8K and reduce the number of cables required to stream data and power from one device to another. According to HKEPC, the GPMI cable comes in two flavors -- a Type-B that seems to have a proprietary connector and a Type-C that is compatible with the USB-C standard. 

Because 8K has four times the number of pixels of 4K and 16 times more pixels than 1080p resolution, it means that GPMI is built to carry a lot more data than other current standards. There are other variables that can impact required bandwidth, of course, such as color depth and refresh rate. The GPMI Type-C connector is set to have a maximum bandwidth of 96 Gbps and deliver 240 watts of power. This is more than double the 40 Gbps data limit of USB4 and Thunderbolt 4, allowing you to transmit more data on the cable. However, it has the same power limit as that of the latest USB Type-C connector using the Extended Power Range (EPR) standard. GPMI Type-B beats all other cables, though, with its maximum bandwidth of 192 Gbps and power delivery of up to 480 watts.]]></content:encoded></item><item><title>My Journey With the Connect Four Terminal Game</title><link>https://hackernoon.com/my-journey-with-the-connect-four-terminal-game?source=rss</link><author>CodeByBlazej</author><category>tech</category><pubDate>Mon, 7 Apr 2025 20:57:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[My most recent project was the Connect Four terminal game. You can check this out here if you want. This project was supposed to improve my TDD (Test-Driven Development) skills, as I had to write tests for every step.\
It was a loooong project, and in this article, Iâ€™m going to tell you why.The TDD Lessons Before the ProjectBefore I got thrown into Connect Four, I had to go through a couple of lessons and exercises about TDD.\
These lessons were actually pretty interesting. They explained:Its advantages and disadvantages.One thing I really liked is that The Odin Project doesnâ€™t force us to use TDD, instead, it just teaches how it works in detail so we can decide for ourselves whether we want to use it or not.\
The principles of TDD were pretty clear to me from the start as The Odin Project explains them very clearly, and I was actually keen to try it out.\
But at the back of my mind, I was a bit worried that it might stretch the time spent on each projectâ€¦ and, well, I wasnâ€™t exactly wrong.Starting the Project (And Why It Felt Hard at First)So, I finished all the necessary lessons and started working on Connect Four.\
At this stage in the Ruby course, the game itself wasnâ€™t hard to implement.I had to think about all the tests first before writing any actual code.It was hard to wrap my head around this in the beginning.Even now, as Iâ€™m writing this article while working on the Chess game, itâ€™s still a bit hard to remember all the RSpec syntax.\
Anyway! I quickly realized that the most important part of the process actually happens at the very beginning when thinking of test names.\
That was the crucial moment when I had to figure out:What is this test going to check?What will the corresponding method actually do?Once I figured that out, I wrote the basic test of course, struggling with RSpec syntax for a long time.\
But thenâ€¦ BLINK - itâ€™s green!\
Awesome. Now, I can move on to the next test and method.\
It took me some time Iâ€™d say around 3 weeks? Something like that.\
But I must admit, by the end of this project, I had a much better understanding of this whole TDD â€œmagicâ€.The Moment It Clicked (Why TDD Is Worth It)I realized that my project was MUUUCH BETTER thought out.Shorter, well-structured methodsEverything named properlyOn top of that, I noticed I wasnâ€™t wasting another week fixing bugs or refactoring half my code because I didnâ€™t plan it properly from the start.\
You know that moment when a tiny mistake messes up half your code, and you have to go back and rewrite a bunch of stuff?\
Yeah, that barely happened this time.\
I liked this project. I like TDD. And Iâ€™m definitely going to stick with it for my future projects.\
Less troubleshooting. Easier to understand the code when coming back later. Just write tests in plain English, and you instantly know what your program is supposed to do.\
Itâ€™s as simple as that.\
The hardest part of all this is definitely RSpecâ€”it just takes time to get used to:\
But honestly? The only way to get better at this is to just keep practicing until it becomes second nature.]]></content:encoded></item><item><title>Nikola founder Trevor Milton wants to buy the bankrupt startupâ€™s assets</title><link>https://techcrunch.com/2025/04/07/nikola-founder-trevor-milton-wants-to-buy-the-bankrupt-startups-assets/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Mon, 7 Apr 2025 20:46:23 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nikola founder Trevor Milton, who was recently pardoned after being convicted of securities fraud, is trying to buy the assets of his former company out of bankruptcy, according to new court filings and a person familiar with the matter. Nikola filed for Chapter 11 bankruptcy protection in February and has said it hopes to sell [â€¦]]]></content:encoded></item><item><title>Flexport CEO Ryan Petersenâ€™s high-stakes test amid tariff turmoil: â€˜You canâ€™t be freaking outâ€™</title><link>https://techcrunch.com/2025/04/07/flexport-ceo-ryan-petersens-high-stakes-test-amid-tariff-turmoil-you-cant-be-freaking-out/</link><author>Connie Loizos</author><category>tech</category><pubDate>Mon, 7 Apr 2025 20:43:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At 11 a.m. in California last Thursday, the day after President Donald Trump declared sweeping new tariffs under what he dubbed â€œLiberation Day,â€ Ryan Petersen was live on camera, fielding questions from a virtual room packed with more than 2,300 anxious customers. The founder and CEO of Flexport, a now 12-year-old global logistics and customs [â€¦]]]></content:encoded></item><item><title>Waymo May Use Interior Camera Data To Train Generative AI Models, Sell Ads</title><link>https://tech.slashdot.org/story/25/04/07/196200/waymo-may-use-interior-camera-data-to-train-generative-ai-models-sell-ads?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 7 Apr 2025 20:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Waymo is preparing to use data from its robotaxis, including video from interior cameras tied to rider identities, to train generative AI models, according to an unreleased version of its privacy policy found by researcher Jane Manchun Wong. 

The draft language reveals Waymo may also share this data to personalize ads, raising fresh questions about how much of a rider's behavior inside autonomous vehicles could be repurposed for AI training and marketing. The privacy page states: "Waymo may share data to improve and analyze its functionality and to tailor products, services, ads, and offers to your interests. You can opt out of sharing your information with third parties, unless it's necessary to the functioning of the service."]]></content:encoded></item><item><title>Metaâ€™s surprise Llama 4 drop exposes the gap between AI ambition and reality</title><link>https://arstechnica.com/ai/2025/04/metas-surprise-llama-4-drop-exposes-the-gap-between-ai-ambition-and-reality/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/04/llama_head_1-1152x648.jpg" length="" type=""/><pubDate>Mon, 7 Apr 2025 19:54:47 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT â€“ Ars Technica</source><content:encoded><![CDATA[On Saturday, Meta released its newest Llama 4 multimodal AI models in a surprise weekend move that caught some AI experts off guard. The announcement touted Llama 4 Scout and Llama 4 Maverick as major advancements, with Meta claiming top performance in their categories and an enormous 10 million token context window for Scout. But so far the open-weights models have received an initial mixed-to-negative reception from the AI community, highlighting a familiar tension between AI marketing and user experience."The vibes around llama 4 so far are decidedly mid," independent AI researcher Simon Willison told Ars Technica. Willison often checks the community pulse around open source and open weights AI releases in particular.While Meta positions Llama 4 in competition with closed-model giants like OpenAI and Google, the company continues to use the term "open source" despite licensing restrictions that prevent truly open use. As we have noted in the past with previous Llama releases, "open weights" more accurately describes Meta's approach. Those who sign in and accept the license terms can download the two smaller Llama 4 models from Hugging Face or llama.com.]]></content:encoded></item><item><title>Endless CTO Amit Showcases Web3 Genesis Cloud At Hong Kong Web3 Festival 2025</title><link>https://hackernoon.com/endless-cto-amit-showcases-web3-genesis-cloud-at-hong-kong-web3-festival-2025?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Mon, 7 Apr 2025 19:05:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Hong Kong, April 7, 2025 â€“ At the Hong Kong Web3 Festival 2025, a leading global event for blockchain and Web3 innovation, Amit Kumar Jaiswal, Chief Technology Officer of Endless Web3 Genesis Cloud, delivered a compelling keynote speech highlighting the transformative capabilities of the Endless â€” a decentralized intelligent component protocol. \
Many industry leaders, developers, and enthusiasts attended this meeting, and Amit elaborated on how Endless is addressing critical challenges in Web3 adoption, further cementing the companyâ€™s role as a trailblazer in decentralized technology.\
As Endlessâ€™ impact in the Web3 space grows, Amit's speech on April 7 focused on the persistent barriers to Web3's broad adoption: complex development processes, suboptimal user experiences, a finance-heavy ecosystem, and limited AI integration.â€œWeb3, with its decentralization, user autonomy, and security, shows great potential, but large-scale adoption still faces huge challenges,â€ Amit noted. â€œWhat we need is a protocol that is user-friendly, developer-oriented, secure, AI-powered, and rooted in a co-creation economy.â€\
Endless was born to address these challenges. Amit detailed its three foundational layers: a Decentralized Infrastructure Layer powered by the Endless Public Chain, a One-Stop Componentized Development Layer providing modular tools for developers, and an Application Layer enabling diverse decentralized applications (DApps) across various fields such as social media, gaming, and finance. â€œEndless serves as a connector, builder, and accelerator in the Web3 era,â€ he emphasized.\
The platform tackles key pain points effectively. For developers, Endless supports multi-language SDKs and cross-chain compatibility and simplifies DApp creation with a comprehensive library of modular components. \
This components cover wallets, smart contracts, NFTs, DeFi protocols, and AI functionalities like large language modelsâ€” For users, it offers a Web2-like experience with features such as gas payment proxies, simplified logins (e.g., via Google accounts), and rapid transaction speeds of half a second, even under high concurrency.Amit highlighted Endless's AI-native architecture as a defining feature. â€œImagine DApps with AI-driven smart contracts, personalized user experiences, and intelligent customer supportâ€”all on a decentralized foundation,â€ he said. \
With decentralized AI inference verification, an on-chain AI model marketplace, and dynamic sharding technology, Endless is pioneering the integration of AI and blockchain while prioritizing security and privacy through end-to-end encryption and decentralized identity management.\
Amit showcased Luffa, the first Web3-native decentralized SocialFi platform built on Endless. Luffa ensures user autonomy with no centralized data backups, end-to-end encrypted conversations, secure logins, and features like crypto wallets and AI agents. â€œLuffa exemplifies how Endless enables user-centric applications that prioritize privacy and harness AI,â€ Amit stated.\
The core of the Endless ecosystem is the EDS token, which facilitates transactions, staking, and developer services while supporting revenue-sharing models in gaming, NFT marketplaces, and DApp distribution. With an initial issuance of 10 billion EDS, the token's economic model is designed to drive long-term growth and incentivize participation.\
Amit outlined key milestones of Endless, including the recent launch of Endless and the partnership with the University of Surrey. Recently, Endless has officially announced its First-Stage Endless Developer Grant Program. \
With a total pool of $1,000,000 or in equivalent $EDS tokens, the program aims to galvanize global developers to build innovative decentralized applications (DApps) on the Endless Protocol, fostering a vibrant community and driving the adoption of Web3 technologies.â€œEndless Web3 Genesis Cloud represents a paradigm shift,â€ Amit concluded. â€œBy lowering development barriers, enhancing user experience, integrating AI intelligently, and fostering a co-creation economy, Endless empowers developers and users with more opportunities. We are committed to building a Web3 ecosystem that transcends finance, delivering tangible value and promoting decentralized value co-creation.â€\
Previously, Endless secured over $110 million in financing. CB Insights, a prominent venture capital research firm, , noting that the record-breaking funding in Q1 2025 underscores investors' willingness to place substantial bets on AI startups earlier than ever before. Meanwhile, a growing number of AI companies are focusing on addressing challenges in specific sectors rather than developing general-purpose AI models.About Endless Web3 Genesis CloudEndless Web3 Genesis Cloud is a decentralized intelligent protocol built to bridge Web2 and Web3 ecosystems, providing developers with a one-stop Web3 application development platform, and offering users a Web2-level experience. \
Powered by the public chain based on the Move language, Endless integrates various AI capabilities and plugins, committed to becoming the best connector between AI and Web3. It allows developers to build Crypto AI applications more simply and quickly in a componentized manner, facilitating the arrival of the AI Agentic Super Intelligent System.:::tip
This story was distributed as a release by Btcwire under HackerNoonâ€™s Business Blogging Program. Learn more about the programÂ ]]></content:encoded></item><item><title>My GameBoy Taught Me the Future of Web3 Gaming</title><link>https://hackernoon.com/my-gameboy-taught-me-the-future-of-web3-gaming?source=rss</link><author>Jonathan Pullinger</author><category>tech</category><pubDate>Mon, 7 Apr 2025 19:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Like many people my age, I grew up coveting the latest games console. I always hoped that come Christmas morning, I would walk down the stairs to find the latest grey box from Nintendo. Alas, my family didn't have a lot of money, so most years I wandered down to discover Santa had decided to get me something different and I would put on a brave face and try to appear both grateful and excited for my new pair of school shoes, a selection box and a small toy, usually a matchbox car, or on a good year, a boardgame, perhaps a Tomy slot racing car (I was desperate for a Scalextric set).\
Everything changed on Christmas 1990. I came down the stairs and the box under the tree seemed too large for a slot car but too small for a shoebox. Dare I get excited? Or was this going to be another year in which hope was dashed and I resigned myself to a magnetic chess set? I gently shook the box, holding it close to my left ear, and my smile returned.Â There was no rattle. This wasn't a chess set! Could it be? Finally, was this my year? I could barely contain my excitement. As I began tearing open the red and white snowflake wrapping paper, I could scarcely look, excited and scared in equal measure. I saw a G, then an A on the box and my hesitancy vanished as I feverishly ripped open the paper to get to that glorious Nintendo box. I still remember the grey stripe down one side and a red and black grid on the box. I had wanted my very own games console for years and this year my smile wasn't a fake.\
I opened the box, gently lifted the GameBoy out of its polystyrene packaging, sliding it out of its protective plastic wrapper before holding this small games console in my hands, shaking with excitement before realising this wasn't just a small games console; this was my Gameboy. I was filled with pride, popped open the back cover, inserted the four AA batteries before sliding in the grey plastic Tetris cartridge. Sliding that little grey switch across and watching the word Nintendo drop down the screen, the red LED lit up, hearing that distinctive Nintendo beep before Tetris began.\
I remember that Christmas well. I couldn't shut up about how happy I was and I spent the rest of the day obsessed with this small plastic box and its dot matrix LCD screen. Iâ€™m not sure my Gameboy was turned off for more than thirty minutes for the rest of that year.\
So why do I bring up Christmas and my excitement at receiving my very own games console or, to be more precise, a portable games console?\
That's because I remembered this story while working out how best to describe the advantages of context chains in Web3 gaming; my childhood Christmases (before GameBoy) were mostly board game affairs, and post-GameBoy in which gifts became older GameBoy titles and alike. Unlike any board game, when we visited my Grandma on Boxing Day, I could pack my GameBoy and my few game cartridges with me, and I could continue to play at her house, enjoying those classic 8-bit graphics and now tinny speakers. We never took a board game with us. Although this was perhaps to avoid family disputes, I believe it was far more straightforward than that - we were only visiting for lunch, there wasn't enough time to play the entire game and, as such, there was no real way to pack, play and conclude them before heading home. Like a traditional Blockchain, the game never left the house, and the counters were always only used with one game, kept in one box. The GameBoy offered a new way of playing, a portable way, and the ability to use my console at my Grandma's home, enjoying the same gaming experience as I did in my bedroom.\
Moi sees Web3 gaming in the same way. There is nothing wrong with Monopoly or Scrabble, but I think we can all agree that none of the counters, boards or boxes are compatible with the other. I have never played Monopoly with a Scrabble tile or used Monopoly's vintage racing car as a letter in Scrabble. We could suggest that these games are similar to smart contracts on traditional Blockchains and, while they work, gameplay is limited.\
The GameBoy changed my childhood, and I still recall Christmases before and after that box of electronics. The GameBoy represented change - the ability to take my games with me and play various games on the same device. Like a context chain, I carry my gaming assets with me and hop between games, using the same cartridge slot and speedy loading times. Tetris out, Super Mario Land in, same device, regardless of location.\
Does this mean we don't think traditional Blockchains have a place in 2025? Not at all. We simply believe that the world has moved on and no longer has to use the same solution for every task.\
Bitcoin is an excellent store of value and is fast becoming the default standard value store for the entire industry. This is great and, just like Monopoly, it is a fantastic product. However, using it for gaming is like trying to play Cluedo on a Monopoly board â€”itâ€™s daft and wrong.\
Ethereum introduced compute, and just like the Nintendo NES, that game console I coveted as a small child, it was wired into a television set. You had to play it in one place. Yes, you could play different games on it (Remember Duck Hunt, a classic), but it didn't leave the house and your friends had to come over to play.\
Effectively, the NES in this analogy is an Ethereum smart contract. Whenever you want a cheeky game of Mario, you must go to the NES, turn it on and use those ridiculously wonderful short-cabled NES controllers. Sit with your friend on the second controller and play. Again, a fabulous piece of kit, just like Ethereum. It brought us the smart contract, ERC20 tokens, an ICO bubble and nfts, including the Bored Ape Yacht Club mutant we use at our company. We are thankful for everything Ethereum has done for us and I owe almost everything I have to that wonderful, frustrating, brilliant, limited chain.\
Context chains pick up where smart contracts left off and, just like my childhood GameBoy, fill that next stage of evolution. They offer the brilliance of Ethereum (the NES) but in a portable format with few compromises and lots of benefits - solving scaling issues, making P2P gaming easier and faster, assets are easier to carry with you (have you seen an NES cartridge?) and as such share.\
TLDR: Like games at my family Christmases, times and technology have changed. We still love Monopoly and Cluedo, primarily due to their lack of technology and simplicity. Similar to Bitcoin they have one use (two if you count causing a family row), and we love them for this. However, once my GameBoy arrived, things changed and I loved the ability to play multiple games over the entire festive season in numerous places with various people (Double Dragon 2). I wasnâ€™t lucky enough to own an NES. I think this was a case of my being a little too young and the console being a lot too expensive for my family. The NES for me was something I loved to play, but I had to ride my bicycle to my friendâ€™s house where weâ€™d play Super Mario Bros 3 for hours together, just like an Ethereum smart contract-based game today. It was great while I was at my friendsâ€™ house, but as soon as I left I had nothing with me - no games, no assets and no way to play. The GameBoy changed my life and the lives of many of my peers, and I believe that context chains will do the same for web3 gaming as GameBoy did for video games and childhoods in the â€˜90s, â€˜00s, and beyond.]]></content:encoded></item><item><title>UK Bans Fake Reviews and &apos;Sneaky&apos; Fees For Online Products</title><link>https://news.slashdot.org/story/25/04/07/1729229/uk-bans-fake-reviews-and-sneaky-fees-for-online-products?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 7 Apr 2025 19:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The United Kingdom has banned "outrageous fake reviews and sneaky hidden fees" to make life easier for online shoppers. From a report: New measures under the Digital Markets, Competition, and Consumer Act 2024 came into force on Sunday that require online platforms to transparently include all mandatory fees within a product's advertised price, including booking or admin charges. 

The law targets so-called "dripped pricing," in which additional fees -- like platform service charges -- are dripped in during a customer's checkout process to dupe them into paying a higher price than expected. The ban "aims to bring to an end the shock that online shoppers get when they reach the end of their shopping experience only to find a raft of extra fees lumped on top," according to Justin Madders, the UK's Minister for Employment Rights, Competition and Markets.]]></content:encoded></item><item><title>SpyCloud Research Reveals Endpoint Detection And Antivirus Solutions Miss 66% Of Malware Infections</title><link>https://hackernoon.com/spycloud-research-reveals-endpoint-detection-and-antivirus-solutions-miss-66percent-of-malware-infections?source=rss</link><author>CyberNewswire</author><category>tech</category><pubDate>Mon, 7 Apr 2025 18:57:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AUSTIN, TX, USA, April 7th, 2025/CyberNewsWire/--Deep visibility into malware-siphoned data can help close gaps in traditional defenses before they evolve into major cyber threats like ransomware and account takeover. \
, the leading identity threat protection company, today released new analysis of its recaptured darknet data repository that shows threat actors are increasingly bypassing endpoint protection solutions: 66% of malware infections occur on devices with endpoint security solutions installed.\
SpyCloud offers integrations with leading endpoint detection and response (EDR) products, such as Crowdstrike Falcon and Microsoft Defender, that close this detection gap.\
EDRs play a vital role in detecting, protecting against, and responding to threats on enterprise devices. Despite advanced AI detection and telemetry analysis offered in todayâ€™s EDR solutions, modern infostealer malware is designed to evade even the most sophisticated defenses, using tactics like polymorphic malware, memory-only execution, and exploitation of zero-day vulnerabilities or outdated software. \
SpyCloudâ€™s findings underscore that while EDR and antivirus (AV) tools are essential and block a wide range of security threats, no security solution can block 100% of attacks. \
Organizations need to take a layered approach to close the gaps before attacks progress deeper into their environments, resulting in events like ransomware and account takeover.Â Â â€œWhen a malware infection goes undetected, the consequences can be catastrophic,â€ said Damon Fleury, Chief Product Officer at SpyCloud. â€œWe are in an arms race at the endpoint, where attackers are constantly evolving their tactics to skirt detection. SpyCloud provides a critical line of defense â€“ uncovering infostealer infections that evade EDRs and AVs, detecting when stolen data begins circulating in the criminal underground, and automatically feeding that intelligence back to the EDR to quarantine the device and begin the post-infection remediation process.â€\
By closing this visibility gap, SpyCloud EDR integrations provide a new and powerful protection mechanism. Once malware exfiltrates credentials, personally identifiable information (PII), or session cookies, that stolen data becomes a launchpad for further entrenchment and compromise.\
SpyCloud helps stop cybercrime before it happens by identifying these identity risks early, mapping them back to impacted users, devices, and applications, and sending actionable intelligence to an organizationâ€™s EDR for response and remediation.Â Â â€œAs identity becomes the security perimeter, organizations need more than device-level protection; they need insight into what their endpoint solutions are missing,â€ added Fleury. â€œSpyCloudâ€™s expertise in accessing malware logs before theyâ€™re broadly circulated among criminals enables faster, more targeted responses needed to address infections, prevent lateral movement, and block disruptive follow-on activities like admin lockout and ransomware deployment.â€\
To learn more about how SpyCloud can augment endpoint security strategy and remediate malware infections that EDRs and AVs may miss, users can , where experts will walk through the data, explain the attack chain in detail, and demo how SpyCloudâ€™s EDR integrations work in real-world scenarios.Â  transforms recaptured darknet data to disrupt cybercrime. Its automated holistic identity threat protection solutions leverage advanced analytics to proactively prevent ransomware and account takeover, safeguard employee and consumer accounts, and accelerate cybercrime investigations.\
SpyCloudâ€™s data from breaches, malware-infected devices, and successful phishes also powers many popular dark web monitoring and identity theft protection offerings.\
Customers include seven of the Fortune 10, along with hundreds of global enterprises, mid-sized companies, and government agencies worldwide.\
Headquartered in Austin, TX, SpyCloud is home to more than 200 cybersecurity experts whose mission is to protect businesses and consumers from the stolen identity data criminals are using to target them now.\
To learn more and see insights, users can visit .REQ on behalf of SpyCloud:::tip
This story was distributed as a release by Cybernewswire under HackerNoonâ€™s Business Blogging Program. Learn more about the programÂ ]]></content:encoded></item><item><title>Meta exec denies the company artificially boosted Llama 4â€™s benchmark scores</title><link>https://techcrunch.com/2025/04/07/meta-exec-denies-the-company-artificially-boosted-llama-4s-benchmark-scores/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Mon, 7 Apr 2025 18:45:07 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A Meta exec on Monday denied a rumor that the company tuned its new AI models to present well on specific benchmarks while concealing the modelsâ€™ weaknesses. The executive, Ahmad Al-Dahle, VP of generative AI at Meta, said in a post on X that itâ€™s â€œsimply not trueâ€ that Meta trained its Llama 4 Maverick [â€¦]]]></content:encoded></item><item><title>Hive Intelligence Launches Mainnet, Joins NVIDIA Inception To Power Next-Generation Crypto AI Agents</title><link>https://hackernoon.com/hive-intelligence-launches-mainnet-joins-nvidia-inception-to-power-next-generation-crypto-ai-agents?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Mon, 7 Apr 2025 18:39:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[New Delhi, India, April 7th, 2025/Chainwire/--Hive Intelligence, a next-generation platform that empowers AI with real-time, multi-chain blockchain data via a robust API, today announced the official launch of its mainnet alongside its recent induction into NVIDIAâ€™s Inception program for AI startups.\
These milestones mark a major step in Hiveâ€™s mission to make AI-driven blockchain applications faster and more accessible, positioning the company at the forefront of the emerging AI+crypto space.Mainnet Launch: Developers can now leverage Hive Intelligenceâ€™s unified API to query real-time data across 60+ blockchains using natural languageâ€”eliminating the need to navigate fragmented data sources.NVIDIA Inception Membership: Hive gains access to NVIDIAâ€™s best-in-class AI tools and networking opportunities, fueling the development of advanced AI-driven blockchain solutions.$HINT Utility Token: The native token powers Hiveâ€™s ecosystem, enabling payment for on-chain data queries, AI computations, governance participation, and more.The Industry Challenge: Driving AI Adoption in BlockchainThe growing convergence between AI and crypto has highlighted a persistent challenge: fragmented on-chain data thatâ€™s difficult to ingest, analyze, and act upon at scale.\
Traditional approaches rely on disparate APIs, complex node infrastructure, or manual data aggregationâ€”all of which limit the potential of autonomous AI agents and advanced blockchain analytics.\
Hive Intelligence solves these pain points by merging dozens of data providers and blockchains into one streamlined API, enabling near-instant queries optimized for large language models (LLMs) and other AI workflows.â€œHive Intelligenceâ€™s mainnet debut isnâ€™t just a product releaseâ€”itâ€™s an invitation for businesses across Web2 and Web3 to harness the power of on-chain intelligence,â€ said Rishabh Narang, spokesperson for Hive Intelligence.â€œBy joining NVIDIA Inception and going live on mainnet, weâ€™re proving that AI-driven analytics can become the new standard for blockchain innovation, fueling use cases from DeFi to NFTs, compliance, and beyond.â€Mainnet Ushers in a New Era of AI+BlockchainThe launch of Hive Intelligenceâ€™s mainnet opens its AI-driven blockchain data infrastructure to the world. Developers and enterprises can now integrate directly with Hiveâ€™s platform to pull real-time, multi-chain data through a single, robust API.\
Crucially, Hive eliminates the historical fragmentation of blockchain data across disparate networks. For the first time, AI agents can seamlessly query and interact with multiple blockchains in one place, overcoming a major bottleneck in crypto analytics and application development.\
Hiveâ€™s API is designed to be LLM-ready, supporting natural language queries and AI-friendly responses. This means developers can ask complex questions about blockchain activity in plain Englishâ€”such as, â€œWhat was the total trading volume on decentralized exchanges across Ethereum and Binance Chain in the past 24 hours?â€â€”and receive structured, insightful answers immediately.\
The platformâ€™s ability to handle such queries across several chains and data types (from DeFi lending stats to NFT marketplace trends) sets it apart as a universal data layer for Web3.Joining NVIDIA Inception to Turbocharge AI InnovationHiveâ€™s acceptance into NVIDIA Inception, NVIDIAâ€™s global startup program, provides significant momentum to the projectâ€™s AI ambitions. NVIDIA Inception offers technical guidance, go-to-market resources, and networking opportunities to promising companies in artificial intelligence and data science.\
In Hiveâ€™s case, membership comes with access to NVIDIAâ€™s expertise in scaling AI models and infrastructure, as well as potential collaboration on cutting-edge AI research.\
This partnership is expected to speed up Hiveâ€™s roadmap of new features, potentially including enhanced natural language processing models for blockchain queries and more sophisticated AI agent integrations.$HINT Token: Powering the Hive EcosystemAt the core of Hive Intelligenceâ€™s platform is the $HINT utility token, which launched in tandem with the network. Far more than just a cryptocurrency, $HINT functions as the fuel driving the Hive ecosystemâ€”it is required to access certain services and features on the platform, acting as the key that unlocks Hiveâ€™s advanced search and analytics capabilities.\
Developers and users utilize $HINT to pay for API calls and data queries on the Hive network, ensuring those who contribute resources are rewarded. As usage grows, demand for $HINT increases, incentivizing more data providers and validators to support the network.\
The token also underpins community governance and security: $HINT holders can vote on platform upgrades and parameters, aligning Hiveâ€™s evolution with its most invested users.\
Over time, staking and other incentive mechanisms will reward constructive behavior, foster network growth, and reinforce Hiveâ€™s commitment to a sustainable, user-centric ecosystem.Hive Intelligence envisions a world where AI agents seamlessly interact with blockchains, delivering insights, executing transactions, and transforming everything from finance to gaming in real time. With the mainnet live and support from NVIDIA Inception, Hive is uniquely positioned to drive that vision forwardâ€”reducing complexity for developers, unlocking opportunities for businesses, and sparking a new wave of AI+crypto applications.â€œAs the demand for intelligent, on-chain solutions continues to surge, weâ€™re excited to be at the forefront,â€ Rishabh Narang concluded. â€œWe invite developers and enterprises across industriesâ€”Web2 or Web3â€”to explore Hive Intelligenceâ€™s platform and start building the AI agents of tomorrow, today.â€ is a pioneering AI+blockchain company providing an infrastructure layer for AI agents and applications to interact with decentralized data in real time. Its unified API offers seamless, natural-language access to dozens of blockchains, enabling a new generation of AI-powered dApps and analytics tools.\
By solving the data fragmentation problem and delivering comprehensive, LLM-ready blockchain insights, Hive Intelligence accelerates the convergence of artificial intelligence and cryptocurrency.\
As a member of NVIDIA Inception, Hive has access to advanced AI technology and expertise, further empowering innovation at the intersection of AI and crypto.\
With its mainnet now live and the $HINT token fueling its ecosystem, Hive Intelligence stands as a leader in bridging blockchain technology with AI-driven analysis and decision-makingâ€”fulfilling its vision of making on-chain data as accessible and actionable as information on the traditional web.\
To learn more, users can visit Hive Intelligenceâ€™s  or follow  on X (Twitter).\
Developers are encouraged to explore the documentation, sign up for a free API key, and begin building AI agents today.team@hiveintelligence.xyz:::tip
This story was distributed as a release by Chainwire under HackerNoonâ€™s Business Blogging Program. Learn more about the programÂ ]]></content:encoded></item><item><title>Analyst says Apple, Tesla have biggest exposure to Trumpâ€™s tariffs</title><link>https://techcrunch.com/2025/04/07/analyst-says-apple-tesla-have-biggest-exposure-to-trumps-tariffs/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Mon, 7 Apr 2025 18:10:26 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Wedbush Securities analyst Dan Ives slashed his price targets for Apple and Tesla over the weekend as President Trumpâ€™s tariffs threaten to disrupt both businesses.Â  â€œThe tariff economic Armageddon unleashed by Trump is a complete disaster for Apple given its massive China production exposure,â€ Ives said in a warning note over the weekend. â€œIn our [â€¦]]]></content:encoded></item><item><title>Scientists Debate Actual Weight of the Internet</title><link>https://tech.slashdot.org/story/25/04/07/1740251/scientists-debate-actual-weight-of-the-internet?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 7 Apr 2025 18:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The internet's physical mass remains contested among scientists, with estimates ranging from a strawberry to something almost unimaginably small. In 2006, Harvard physicist Russell Seitz calculated the internet weighed roughly 50 grams based on server energy, a figure that would now equate to potato-weight given internet growth. 

Christopher White, president of NEC Laboratories America, has dismissed this calculation as "just wrong." White suggests a more accurate method that accounts for the energy needed to encode all internet data in one place, yielding approximately 53 quadrillionths of a gram at room temperature. Alternatively, if the internet's projected 175 zettabytes of data were stored in DNA -- a storage medium scientists are actively exploring -- it would weigh 960,947 grams, equivalent to 10.6 American males. Though scientists debate measurement methods, White asserts the web's true complexity makes it "essentially unknowable."]]></content:encoded></item><item><title>Former Tesla exec Drew Baglinoâ€™s new startup is rethinking the electrical transformer</title><link>https://techcrunch.com/2025/04/07/former-tesla-exec-drew-baglinos-new-startup-is-rethinking-the-electrical-transformer/</link><author>Tim De Chant</author><category>tech</category><pubDate>Mon, 7 Apr 2025 17:54:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Heron Power is raising between $30 million to $50 million for a Series A, according to a report.]]></content:encoded></item><item><title>Your Next Slang Phrase Might be Created by an AI</title><link>https://hackernoon.com/your-next-slang-phrase-might-be-created-by-an-ai?source=rss</link><author>Tech Media Bias [Research Publication]</author><category>tech</category><pubDate>Mon, 7 Apr 2025 17:31:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Jinyu Cai, Waseda University (bluelink@toki.waseda.jp);(2) Jialong Li, Waseda University (lijialong@fuji.waseda.jp);(3) Mingyue Zhang, Southwest University (myzhangswu@swu.edu.cn);(4) Munan Li, Dalian Maritime University (limunan@dlmu.edu.cn);(5) Chen-Shu Wang, National Taipei University of Technology (wangcs@ntut.edu.tw);(6) Kenji Tei, Tokyo Institute of Technology (tei@c.titech.ac.jp).This section offers an extensive background and overview of related work in areas relevant to this study, starting with foundational information on LLMs, then exploring studies in slang detection and identification as they relate to language evolution, and concluding with a discussion on recent research applying LLMs to evolutionary game theory and social simulations.Large Language Models like the GPT series [14], [15], LLaMA series [16], [17], PaLM series [18], [19], GLM [20]and Bard [21] represent a significant advancement in the field of natural language processing. Fundamentally, these models are based on the Transformer [22] architecture, a type of neural network that excels in processing sequential data through self-attention mechanisms. This architecture enables LLMs to understand and predict linguistic patterns effectively. They are trained on extensive text datasets, allowing them to grasp a wide range of linguistic nuances from syntax to contextual meaning. These models exhibit remarkable zero-shot learning abilities, enabling them to perform tasks they were not explicitly trained for, like understanding and generating content in new contexts or languages [4], [5], [23]â€“[25]. A critical aspect of their training involves Reinforcement Learning from Human Feedback [26] (RLHF), where human reviewers guide the model to produce more accurate, contextually relevant, and ethically aligned responses. This method not only enhances the modelâ€™s language generation capabilities but also aligns its outputs with human values and ethical standards, making them more suitable for diverse, real-world applications.B. Slang Detection and IdentificationIn the field of Natural Language Processing (NLP), the evolution of language has always been a subject of significant interest. Existing studies have primarily focused on utilizing various machine-learning techniques to recognize informal expressions within text [27]. These methods often include rule-based systems, statistical models, and early machine-learning technologies. For instance, [28] has employed predefined slang dictionaries and heuristic rules to identify and categorize informal language, proving effective on specific datasets but generally lacking the flexibility to adapt to emerging expressions and changing contexts. On the other hand, explorations have been made into using statistical models, such as Naive Bayes classifiers and Support Vector Machines (SVMs) [29], for the automatic detection of slang in text. These approaches rely on extensive annotated data but still face limitations when dealing with newly emerged slang or evolving forms of language. [30] views the generation of slang as a problem of selecting vocabulary to represent new concepts or referents, categorizing them accordingly. Subsequently, it predicts slang through the use of various cognitive categorization models. The study finds that these models greatly surpass random guessing in their ability to predict slang word choices. [31] proposed a Semantically Informed Slang Interpretation (SSI) framework, applying cognitive theory perspectives to the interpretation and prediction of slang. This approach not only considers contextual information but also includes the understanding of semantic changes and cognitive processes in the generation of slang. It is noteworthy that these traditional research methods have mainly focused on detecting or predicting existing slang and keywords, rather than generating slang expressions. This stands in stark contrast to the research focus of this paper.C. Evolutionary Game and Social Simulation with LLMsMerging evolutionary game theory with LLMs has unlocked innovative pathways for simulating complex game dynamics, extending beyond simple dialogue generation to the development and progression of game strategies. LLMs are employed to engage and refine strategic play within game-theoretical frameworks, as demonstrated by [32], which delves into the application of LLMs in negotiation-based games. This study underscores the ability of LLMs to advance their negotiation skills through continuous self-play and feedback loops with AI. LLMs also show proficiency in social deduction games such as Werewolf, as explored by [33]. In this context, a specialized framework leverages historical communication patterns to enhance LLM performance, exemplifying how LLMs can evolve intricate game strategies autonomously. Building on this, [34] combines reinforcement learning with LLMs, utilizing LLMs to output action spaces and employing reinforcement learning models for final decision-making. This enables the agents to maintain competitiveness while outputting reasonable actions, even outperforming human adversaries in games like Werewolf.\
This growing trend of employing LLMs in diverse simulation scenarios extends beyond game theory into broader aspects of social interactions and historical analysis. LLMs have proven to be versatile tools in simulating social dynamics and historical events, offering insights into complex human behaviors and societal patterns. [12] introduces a Wild Westinspired environment inhabited by LLM agents that display a wide array of behaviors without relying on external real-world data. Simultaneously, S3 [13] mirrors user interactions within social networks, crafting an authentic simulation space through the incorporation of user demographic prediction. The influence of LLM-driven social robots on digital communities is thoroughly examined in [35], which identifies distinct macrolevel behavioral trends. Furthermore, [11] employs LLMbased multi-agent frameworks to recreate historic military confrontations, offering a window into the decision-making processes and strategic maneuvers that have directed significant historical conflicts. This avenue of research accentuates the utility of LLMs in computational historiography, providing a deeper comprehension of historical events and their relevance to contemporary and future societal trajectories.]]></content:encoded></item><item><title>Speaking in Code: How AI Simulates Language Evolution on Regulated Social Media</title><link>https://hackernoon.com/speaking-in-code-how-ai-simulates-language-evolution-on-regulated-social-media?source=rss</link><author>Tech Media Bias [Research Publication]</author><category>tech</category><pubDate>Mon, 7 Apr 2025 17:25:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Jinyu Cai, Waseda University (bluelink@toki.waseda.jp);(2) Jialong Li, Waseda University (lijialong@fuji.waseda.jp);(3) Mingyue Zhang, Southwest University (myzhangswu@swu.edu.cn);(4) Munan Li, Dalian Maritime University (limunan@dlmu.edu.cn);(5) Chen-Shu Wang, National Taipei University of Technology (wangcs@ntut.edu.tw);(6) Kenji Tei, Tokyo Institute of Technology (tei@c.titech.ac.jp).\
â€”Social media platforms such as Twitter, Reddit, and Sina Weibo play a crucial role in global communication but often encounter strict regulations in geopolitically sensitive regions. This situation has prompted users to ingeniously modify their way of communicating, frequently resorting to coded language in these regulated social media environments. This shift in communication is not merely a strategy to counteract regulation, but a vivid manifestation of language evolution, demonstrating how language naturally evolves under societal and technological pressures. Studying the evolution of language in regulated social media contexts is of significant importance for ensuring freedom of speech, optimizing content moderation, and advancing linguistic research. This paper proposes a multi-agent simulation framework using Large Language Models (LLMs) to explore the evolution of user language in regulated social media environments. The framework employs LLM-driven agents: supervisory agent who enforce dialogue supervision and participant agents who evolve their language strategies while engaging in conversation, simulating the evolution of communication styles under strict regulations aimed at evading social media regulation. The study evaluates the frameworkâ€™s effectiveness through a range of scenarios from abstract scenarios to real-world situations. Key findings indicate that LLMs are capable of simulating nuanced language dynamics and interactions in constrained settings, showing improvement in both evading supervision and information accuracy as evolution progresses. Furthermore, it was found that LLM agents adopt different strategies for different scenarios. The reproduction kit can be accessed at https://github.com/BlueLinkX/GA-MAS.In the modern digital era, social networks like X (Twitter), Reddit, and Facebook have become pivotal in shaping human interaction, primarily through their ability to facilitate vast connectivity and instantaneous information exchange. Yet, in regions with heightened geopolitical or socio-political sensitivities, users often navigate complex user regulations. Their online expressions can lead to severe consequences, including censorship or account suspension, as documented in various news [1], [2]. While intended to curb misinformation and maintain social harmony, these regulations significantly constrain user expression. In response to these regulations, Corresponding Author: Jialong Li users on social networks have adapted by adopting a phenomenon known as â€œcoded language.â€ [3] In linguistics, Coded Language typically refers to expressing information in a concealed or indirect manner. On social media platforms, this often manifests as the use of metaphors, slang, and creative wordplay.\
This adaptation is not merely a circumvention strategy but a vivid example of â€œlanguage evolutionâ€ in a digital context. In linguistics, language evolution refers to the progression and adaptation of languages over time, shaped by societal, cultural, and technological influences. Specifically, in social networks, this language evolution is demonstrated as users constantly adjust their communication styles to test whether they have circumvented oversight. Depending on the level of regulatory pressure and the nature of the audience, users engage in a strategic play with the platform. From indirect descriptions to the creation of new slang, users ultimately develop coded languages of varying degrees of abstraction.\
This dynamic shift in communication methods offers deep insights from a sociological perspective, reflecting how societal norms and technological advancements shape language. For platforms and users alike, understanding this evolution is crucial for developing balanced content moderation policies and navigating regulated digital environments. For social media platforms and their users, grasping this concept is equally vital. Platforms need this knowledge to adapt to changing user behaviors, to create balanced content moderation policies, and to identify and counteract harmful or illegal activities. For users, an awareness of how language evolves is vital in navigating the intricacies of regulated digital environments. It helps in maintaining free speech and in developing communication strategies that are both effective and meaningful in fostering enhanced interactions\
The emergence of Large Language Models (LLMs) like ChatGPT and Bard, represents a significant leap in Artificial intelligence (AI). These LLMs have demonstrated strong capabilities in (i) understanding intricate dialogues [4], generating coherent texts [5], and aligning to human ethical and value standards [6]â€“[8]. These capabilities position LLMs as ideal tools to simulate humanâ€™s decision-making and language representation, providing new potential in sociology. For instance, [9] investigated the ability of LLMs to comprehend the implicit information in social language. The study by [10] demonstrated the efficiency of LLMs in understanding and generating content that mimics the style of specific social network users. Furthermore, research by [11]â€“[13] integrated LLMs with Multi-Agent Systems to simulate micro-social networks, observing agent behaviors and strategies that reflect human interactions. Despite the extensive application of LLMs in understanding human intension and simulating social media dynamics, the use of LLMs in studying the specific phenomenon of language evolution under regulatory constraints has not been thoroughly explored. As mentioned above, such simulation could not only preempt criminal activities on social media but also provide technical support to uphold freedom of speech.\
Addressing this gap, our research employs LLMs to simulate the nuanced interplay between language evolution and regulatory enforcement on social media. We introduce a simulation framework with two types of LLM-driven agents: (i) participant agents, who adapt their language to communicate concept â€™Bâ€™ under restrictions, and (ii) supervisory agent, who enforce guidelines and react to these language evolutions. Our approach effectively simulates the dynamics model between both sides in language evolution, which allows us to observe the tension and adaptability inherent in language evolution in a controlled, simulated environment. To assess the frameworkâ€™s effectiveness, we designed three diverse scenarios: â€œGuess the Number Gameâ€, â€œIllegal Pet Tradingâ€, and â€œNuclear Wastewater Dischargeâ€. These scenarios vary from abstract concepts to situations closely resembling real-world events, thereby progressively testing the framework from theoretical to practical applications.\
The main contributions of this study are:\
â€¢ We introduce a multi-agent simulation framework utilizing LLMs to simulate human linguistic behaviors in regulated social media environments. This framework offers a unique approach to studying language evolution within the confines of regulatory constraints.\
â€¢ We conducted an extensive evaluation of LLMs in simulating language evolution and interaction efficacy in regulated social media settings. Through experiments on three distinct scenarios, we not only captured the process of language strategy evolution but also uncovered the varied evolutionary trajectories that LLMs follow under different conditions.\
â€¢ The experiment reproduction kit, including the proposed simulation framework along with the results of our experiments, are made publicly accessible as open-source assets; The anonymized artifact can be accessed at: https://github.com/BlueLinkX/GA-MAS.\
The rest of this paper is organized as follows: Section II provides essential background information and explores related work. Section III is dedicated to presenting our proposed simulation framework. Section IV details the experiment setting, presents results, and discusses a discussion. Finally, Section V concludes the paper and offers an outlook on potential future work.]]></content:encoded></item><item><title>5 Quirky and Useful Crypto Tools You Didnâ€™t Know Before</title><link>https://hackernoon.com/5-quirky-and-useful-crypto-tools-you-didnt-know-before?source=rss</link><author>Obyte</author><category>tech</category><pubDate>Mon, 7 Apr 2025 17:18:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
According to CoinMarketCap (CMC), there are over 10,000 coins and tokens available to trade in different markets worldwide. Among them, we can find all kinds of things, from old-established cryptocurrencies like Bitcoin and Obyte to experimental DeFi platforms and memecoins. However, you might not know that, in addition to a vast array of tokens, thereâ€™s also a diverse selection of free crypto-related tools â€”ranging from practical to quirky and entertaining.\
CMC and CoinGecko are, for instance, two quite popular and useful crypto tools, but theyâ€™re far from being the only ones. We can discover some interesting things if we just look around a bit. Letâ€™s see some platforms in this category.Memecoins are quite the thing now, but the crypto community always had its own internal memes and jokes without related tokens. â€œWhen Lamboâ€ is an old one and a very popular one too. It implies a momentous question: when will we be able to afford an expensive Lamborghini car with our current crypto holdings? Itâ€™s known that prices change rapidly in the crypto space, so can you maybe get richer tomorrow with the same number of coins? We can dream, at least.\
â€œWhen Lamboâ€ is so widespread around here that some years ago  all the prices from USD to LAMBOs in Aprilâ€™s Fools. And the question has its own calculator, to help you find out once and for all how much of a Lambo you can buy with your current holdings in different coins. The site  allows you to type the amount, choose between 100 coins and three Lamborghini models, and calculate, well, when Lambo?\
The results will include what Lambo you can buy with your coins (at least a toy Lambo?), how many more tokens you need to buy it, the needed price per coin youâ€™ll need to afford it, and the current percentage you have in your wallet. Itâ€™ll also lead you to trade on the exchanges Binance or Bitvavo, but thatâ€™s an additional option. Do your own research first!Cryptocurrencies are mostly decentralized, which means you have full control over your holdings if you have a private key. But wait a second, because there might be a trick there. The coins and their ledger are decentralized, yes, but the wallet you use to interact with crypto networks can be a completely different story. Wallets are often offered by middlemen (companies and developer teams), who generally have good intentions â€”but thatâ€™s not always the case.\
Some awful things (backdoors and other traps) could be hidden in their codes, beyond what we can see. Even if the wallets are open-source software, which means anyone can check their codes, not everyone knows how to. And worse: cybercriminals are designing fake wallets with the same name and branding as legitimate ones, aiming to trick users into downloading this malicious software from official app stores to steal their private keys.\
Now, of course, thereâ€™s a way to avoid these bad scenarios, and we have several easy and free tools available for it.  is one of them, focused primarily on Bitcoin wallets, but basic information about others is available too. On this site, anyone can just type the name of their preferred wallet (software or hardware) and discover if it passed the tests and questions from their developers. Among those tests, they verify whoâ€™s the team behind the brand, if itâ€™s open-source or not, if itâ€™s non-custodial (offer private keys) or not, and if itâ€™s the original product. They also offer safe links to download or buy the correct version.With all their detailed data and long lists, sites to check cryptocurrency market caps, like CMC and CoinGecko, can be a tad overwhelming for beginners.  offers a more visual, dynamic experience to check the same data, using colorful bubbles and user-friendly interfaces to do it. The first thing youâ€™ll see on this site is a lot of crypto logos and tickers floating around in bubbles of different sizes and colors, indeed.\
Green bubbles mean price increases â€”the larger the bubble, the higher the percentage change or market cap. Red bubbles imply negative percentages, and they also change in size, depending on the figures. Itâ€™s possible to order them per period (hour, day, week, month, year) or market cap size. In the Settings, theyâ€™ll let you change the currency for the prices (USD, EUR, CAD, etc.), languages, colors, and watchlists.If you click a bubble, more specific data, charts, and links about that currency will appear, summed up in a very graphical manner. Below the bubbles, a more traditional list of coins is also available to order by market cap, price, volume, or time changes. Or you can use their search bar to find your favorite coins. An app for iOS and Android can be downloaded too.Network explorers arenâ€™t exactly the most colorful and user-friendly thing in the world. And letâ€™s not talk about those long, incomprehensible alphanumeric strings that represent transactions and addresses. TxCity was created to see crypto transactions in a new, clearer light. By representing each transaction as a person attempting to board a busâ€”where buses symbolize blocks in a blockchainâ€” this platform offers users a unique, real-time perspective on the available crypto networks.\
Obviously, those networks are only blockchains â€” they are the kind of networks that have buses and bus drivers. This approach demystifies on-chain data, effectively converting mere transactional networks into visual ecosystems.\
By using , anyone can visualize transactions across multiple networks, including Bitcoin, Ethereum, Bitcoin Cash, Monero, and Litecoin. The platform also plans to introduce features like bridge visualization between Ethereum and LUKSO, providing users with a real-time experience of their cross-chain transactions. Additionally, TxCity aims to represent wallets as characters or avatars, simplifying user interactions.\
To begin using TxCity, simply visit their website and select the chain you wish to explore. The platform will display live transactions as animated characters attempting to board buses, offering an engaging way to observe how transactions are processed and included in blocks. This visual representation not only makes it easier to understand crypto operations but also serves as an educational tool for those new to this space.Did you know you can order a Hawaiian, Pepperoni, or Mexican pizza using an Obyte chatbot? Well, at least as a role-play! The  offers a fun and risk-free way to explore cryptocurrency transactions, including interacting with chatbots. By following simple steps, users can simulate making a purchase and experience how payments work within the Obyte ecosystem. Itâ€™s an engaging way to see how digital payments function while learning about Obyteâ€™s capabilities in a relaxed and playful environment.\
 is a duplicate version of a crypto network designed for learning, development, and testing. Unlike the mainnet, where transactions involve real assets, the testnet allows users to practice without any financial risk. The Obyte Testnet Wallet is a safe space where developers can refine their applications and everyday users can experiment with different features. To start, users can claim â€”tokens without market valueâ€”to send transactions, explore smart contracts, and even test advanced security measures like two-factor authentication (2FA) or multi-signature accounts.\
Beyond simulated pizza purchases, the testnet wallet offers a variety of features to explore. Users can try chatbots from the Bot Store, set up conditional payments, and experiment with wallet settings. Itâ€™s also possible to add new devices and test secure account recovery options. Everything is recorded on the , providing insights into how transactions are processed. Since no real money is involved, users can experiment freely and gain confidence before switching to the actual .]]></content:encoded></item><item><title>Keep Your Indexes Fresh With This Real-time Pipeline</title><link>https://hackernoon.com/keep-your-indexes-fresh-with-this-real-time-pipeline?source=rss</link><author>LJ</author><category>tech</category><pubDate>Mon, 7 Apr 2025 17:05:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Today, we are excited to announce the support of continuous updates for long-running pipelines in CocoIndex. This powerful feature automatically applies incremental source changes to keep your index up-to-date with minimal latency.With continuous updates, your indexes remain synchronized with your source data in real time, ensuring that your applications always have access to the most current information without the performance overhead of full reindexing.\
If you like our work, it would mean a lot to us if you could supportÂ Cocoindex on GithubÂ with a star. Thank you so much with a warm coconut hug ðŸ¥¥ðŸ¤—.It fits into situations that you need to access the fresh target data continuously in most of the time.It continuously captures changes from the source data and updates the target data accordingly. It's long-running and only stops when being aborted explicitly.A data source may enable one or multiple change capture mechanisms:CocoIndex supports two main categories of change detection mechanisms:: A universal approach applicable to all data sources, allowing periodic checks for changes by scanning and comparing the current state with the previous state.Source-Specific Mechanisms:: Many data sources offer built-in mechanisms for real-time change notification which CocoIndex can subscribe to (coming soon!): Some data sources provide APIs for tracking recently modified entries, allowing efficient detection of changes; for example, Google Drive.These mechanisms work together to ensure CocoIndex can detect and process changes as they happen, maintaining your index in perfect sync with source data with minimal latency and resource usage.How to enable continuous updates?Here is an example of how to enable continuous updates for Google Drive. It is pretty simple:1. Configure the change capture mechanisms for the source@cocoindex.flow_def(name="GoogleDriveIndex")
def my_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    data_scope["documents"] = flow_builder.add_source(
        cocoindex.sources.GoogleDrive(
            service_account_credential_path=credential_path,
            root_folder_ids=root_folder_ids,
            recent_changes_poll_interval=datetime.timedelta(seconds=10)),
        refresh_interval=datetime.timedelta(minutes=1))
In this example, we've configured two change detection mechanisms:recent_changes_poll_interval=datetime.timedelta(seconds=10): This is a Google Drive-specific mechanism that uses the Drive API's changes endpoint to efficiently detect modifications every 10 seconds. This is effecient fast scan to capture all latest modified files, and we could set it to a short interval to get fresher data. It doesn't capture file deletions, so we need the fallback mechanism to ensure all changes are eventually captured.refresh_interval=datetime.timedelta(minutes=1): This is the universal fallback mechanism that performs a complete scan of the data source every minute. This is to scan all the files, to ensure all the changes - including the deleted files, are captured.The  parameter is particularly important as it serves as a safety net to ensure all changes are eventually captured, even if source-specific mechanisms miss something. It works by:Periodically scanning the list of entries (e.g. file list with metadata like last modified time) at the specified intervalComparing the current list with the previously known listIdentifying any differences (additions, modifications, deletions)Triggering updates only for the changed itemsWhile source-specific mechanisms like recent_changes_poll_interval are more efficient for near real-time updates, the  provides comprehensive coverage. We recommend setting it to a reasonable value based on your freshness requirements and resource constraints - shorter intervals provide fresher data but consume more resources.You can read the full documentation:for source specific change detection mechanisms here. Different sources have different mechanisms supported.for the universal fallback mechanism here.2. Run the flow in live update modeAdd a  decorator to your main function, so CocoIndex CLI will take over the control (when cocoindex is the first command line argument).```python title="main.py"
@cocoindex.main_fn()
def main():
    passif  == "":
  main()To run the CLI with live update mode, you can use the following command:
bash
python main.py cocoindex update -LThis will start the flow in live update mode, which will continuously capture changes from the source and update the target data accordingly.

#### Option 2: Python API

You can create `cocoindex.FlowLiveUpdater`. For example,
python title="main.py"
@cocoindex.mainfn()
async def main():
    myupdater = cocoindex.FlowLiveUpdater(demoupdater.wait()
    â€¦if  == "":
    asyncio.run(main())And you run with the flow with
You can also use the updater as a context manager. It will abort and wait for the updater to finish automatically when the context is exited. See full documentation here.\
Now you are all set! It is super simple to get started and have continuous updates for your data. Get started now quickstart guide ðŸš€It would mean a lot to us if you could support Cocoindex on Github with a star if you like our work. Thank you so much with a warm coconut hug ðŸ¥¥ðŸ¤—.]]></content:encoded></item><item><title>To Hit $1T TVL, Ethereum Must Play the Ace</title><link>https://hackernoon.com/to-hit-$1t-tvl-ethereum-must-play-the-ace?source=rss</link><author>Tim Pechersky</author><category>tech</category><pubDate>Mon, 7 Apr 2025 17:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I didn't walk into Ethereum from a pure software background. My world was hardware and algorithm design â€“ working with folks like Erica Synths and SAF Tehnika, deep in signal processing for radio and music. And from those trenches, I learned one very sad, but very pure truth: you  protect your intellectual property. You never, never, never,  just show everyone how the secret sauce is made until it's ancient history for your business.\
It feels wrong, I get it. Itâ€™s painful. Itâ€™s ugly. Itâ€™s definitely sub-optimal for global progress on paper. But damn, it keeps you and the people trusting you alive. I still remember Girts Ozolins from Erica Synths hammering this point home back in 2019, showing me how company after company trying to build open-source embedded sound frameworks justâ€¦ failed.\
Fast forward a year, watching the blockchain space operate, and it hit me how right that wise man was. Because what are blockchain protocols doing? Exactly what he warned against.1. It is Impossible to win by playing chess and telling opponent your next move.Itâ€™s impossible. You just can't win a serious game of chess if you tell your opponent your next five moves.So, when someone like Justin Drake gets on stage at Devcon and lays out a roadmap stretching years into the future â€“ say, a Beam Chain for 2029 â€“ what do you think happens?\
Everyone who wants a slice of Ethereum's pie just pulls out their notepads and scribbles "Launch something similar in 2028."This whole game of front-running and MEV isn't just happening  the blocks. With all the core R&D happening out in the open, it's baked into the very  of building and running these protocols as businesses.2. Napkin Math: Fragmentation is Killing "The One"Let's do some quick napkin math. CoinGecko says the total blockchain market cap is cruising around $2.84 Trillion (as of when I first wrote this). That's  of dollars. We're actually getting close to giants like Apple.\
But hereâ€™s the catch: itâ€™s fragmented to hell and getting worse.| Name | Age | TVL today | Initial funding |
|----|----|----|----|
| Bitcoin | 17 | $1.6T | $0 |
| Ethereum | 11 | $225B | ~$18M |
| Cardano | 10 | $25B | ~$64M |
| Polkadot | 8 | $6.5B | ~$144M |
| Sui | 2 | $8B | ~$336M |\
I'll clip the list there, it's enough to see the pattern. Maybe not perfectly scientific, but look: initial funding checks seem to get bigger for newer projects, while the relative slice of the pie (TVL compared to the potential) feels smaller or slower growing for the established giants. Why? Because as the rules of the blockchain game get clearer, investors are happier to write massive checks for new ecosystems  they have a viable plan to front-run the leader. All they need to do is watch Ethereum's open development closely.\
Let's talk about that $1T Ethereum dream by 2029. If ETH somehow projects a 5x growth path, guess what? In a totally open-source world,  sees that path. Competitors analyze it, predict it, and it becomes crystal clear: there's blood in the water. You can throw more and more money at biting off chunks of the market leader  he's still trying to grow.\
Any traditional software company would laugh at this. They announce when it's , and they sure as hell aren't giving you the source code.\
So, what's the damage for Bitcoin and Ethereum? Maybe the slice each new project takes isn't fatal on its own. Maybe the big guys can copy innovations later. But . And while they're catching up, investor attention drifts, momentum stalls. Bitcoin arguably lost a huge chunk of opportunity (maybe 15%+) to Ethereum because it was slow to adapt. Ethereum is seeing the same pattern with newer chains.\
Is it just about market share percentages? No! For the regular folks, our holy grail of "mass adoption," this fragmentation screams one thing: the business of providing consensus doesn't have consensus within itself!\
If there was ONE chain, , consolidating the best tech,  would be a massive buy signal. My friends outside crypto? They'd dump their inflating fiat into it if they believed THIS was the one!\
And this isn't just office talk. We're talking to  â€“ the bedrock of the economy! They're pissed off worldwide, looking for ways out, wanting independent markets. We're already onboarding a project building an Indonesian Blockchain Coffee association! If the people producing  switch to crypto because they trust  stable system, the legacy financial world is toast.\
But "The One" has to actually show up! And to land the truly massive deals â€“ think Apple, Samsung, securing nation-level affairs on smart contracts â€“ the TVL needs to be astronomical, way beyond $1T. Ethereum needs space to grow perhaps 20x just for the corporate sector. That requires  security, meaning L1 transactions  be expensive, decentralized across jurisdictions, backed by huge value. A rollup-centric roadmap shouldn't mean chaos; it needs architecture for the whole security/scalability spectrum.\
Seeing the market today? Getting there looks like a collective task, but the current open-source free-for-all isn't helping us consolidate. The demand is there. The supply is fractured.3. Time for Open Source Idealists to Grow UpOkay, let's get real. All this amazing research, code, and brainpower pouring into Ethereum and blockchain â€“ it belongs to the community, right? This collective knowledge dataset is built by countless hours of work.\
I gain this incredibly valuable data for almost nothing. And the more compute power I have to process it, the bigger advantage I have to beat these OSS projects at their own game if my interests differ.\
Sure, one could argue  benefits, speeding up progress overall. But here's the kickback: Is everyone sharing back ? Wouldn't progress be even  if the capable players were incentivized to chip  the same knowledge pool instead of just running off to build their own competing thing?\
Let's look at Big Corps. Are they using this dataset? You bet. Are they contributing back equally, intellectually or financially? Much less likely. It's not about good or bad; it's the nature of their business. They know the IP rules: never show your hand. They  that blockchain R&D is open for them to use, but they have zero incentive to reciprocate at the same level.\
\
Valuable contribution ? Minimal, if any. It's a high-level business doc for their stakeholders, using concepts likely derived from open research to build  closed system. Itâ€™s not for the Ethereum OSS community.\
Does JPMorgan pay ZAMA? Likely required by the license.\
Does JPMorgan pay the authors of EIP-5564 (published CC0, an  Improvement Proposal)? Probably not directly.\
Does JPMorgan pays ZAMA based on licensing requirements? Sure thing they do!Does JPMorgan pays EIP-5564 Authors / Ecosystem? Likely not - itâ€™s CC0!\
What does this mean? EIP-5564 ends up improving a closed JPMorgan system. If you don't charge for your intellectual work in a market economy, you don't eat. If you can't feed your brain, the brilliant work stops. Unless those EIP authors played some 4D chess gambit (doubtful, but I'd love to be wrong!), Big Corp likely pockets the profits from that research and might eventually even "own" the researchers through grant funding because the IP wasn't protected upfront. We see the pressure â€“ EF's treasury shrinking while everyone at summits talks crypto tech.\
Financial institutions can attract TVL easily. They won't  to contribute back significantly. Result? The potential security and growth of Ethereum (or any OSS system) slows down compared to what  be achieved collectively.\
The only sustainable path for OSS R&D that doesn't end with you working for the castle you tried to replace is getting  skin in the game. Not just staking. Business-level skin. "Either you contribute meaningfully, or you pay for deep R&D access." You  to make institutions commit.\
Anything else is naive. Maybe it works for non-financial OSS like Red Hat, but those projects aren't aiming for trillion-dollar capitalization. If they were, Big Corp would build it themselves, closed-source, and call it â€œMac OSâ€ and â€œWindowsâ€..\
Even today, "fully open source" is a bit of a myth, right? We have private keys, certificates, ceremonies, data obfuscationâ€¦ Heck, try finding the link to the next public Ethereum Core Dev call easily. It's public, but obscured. The understanding that full openness has limits is already there; the clarity on what to do about it is missing.4. Getting More Skin in the Game: It's Good For Everyone!Imagine if accessing that deep R&D data â€“ the stuff referenced in papers, the cutting-edge discussions â€“ required a provable, equal level of contribution. (Let's ignore  for a second).\
What happens? JPMorgan either has to hire, nurture, and pay for their  Vitalik-level researcher contributing openly , OR they pay significantly for access to the collective dataset.\
Either way, Ethereum wins. Either valuable fiat flows into the ecosystem (funding more EIPs, core dev), or valuable  flows into the Ethereum-owned dataset. Data = Money.\
I'm not saying Ethereum should become IBM. I'm saying OSS protocol development needs to stay decentralized and open , but become competitive enough to survive. We need the best of both worlds: no NDAs for every little commit, but assurance that accessing the deep knowledge pool means you have a measurable, corresponding positive impact.\
That's the mission at Peeramid Labs: building stakes & profit-based gateways for data markets. I believe this is Priority One for  foundational OSS model today. Forget everything else until this is solved.\
Think of factory doors unlocking for employees only when their competence (contribution) hits a certain level. A true knowledge market.\
We need to create  where R&D time and resources are accounted for, measured, and maybe even issued as cryptographic attestations (tokens, NFTs, whatever). This isn't sci-fi; it's adapting proven concepts like proprietary licensing and NDAs using smart contracts.\
These markets need cryptographic proofs:Bearer verifiably possesses specific knowledge/contribution level.Transferring an asset guarantees access.Proofs about dataset properties (overlap, quality) without revealing data (ZK, FHE, TEEs are heading this way).(In quantum future) Proofs against tampering.We're already using SNARKs in  Rankify-it,  to prove data integrity for proposals/votes based on commitments.\
How do we build this hierarchical access? We want contributors to enter, learn, and grow, just like today, but provably. As they contribute and gain peer recognition (competence = time + energy + peer validation), they unlock deeper levels. Like university â€“ you don't get 5th-year material on day one. It requires commitment.The Power of Earned Knowledge and "The One"If you enter an ecosystem like this, you're an apprentice, a first-year student. No one hands you the keys to the kingdom or the most sacred texts on day one.\
You have to . Contribute, discuss, try, fail, succeed, build relationships, create value .This process inherently puts skin in the game.\
If you spent  earning access to a certain knowledge level, you won't just give it away cheaply! If a big corp pays a fortune for access, they might walk away with  data slice, but they miss the next chapter, and their investment stays with the ecosystem.\
Suddenly, even Big Corp is incentivized to  the same space, the same project, the same TVL.\
 is how we design the snowball effect.  is how we get our "The One"!\
It makes sense all-around, same as in university - there is no point to put all of 5 years end-studies upfront of student in first year, it justOkay, so if we need structure and skin in the game, why not just pile onto Bitcoin? Its TVL is massive, Satoshi already designed PoW as a kind of knowledge work marketplace (finding hashes).\
But hereâ€™s Ethereumâ€™s ace in the hand:  Bitcoin is great, but its community is arguably smaller, less dynamic development-wise. In the semi-open source world I'm describing, a vibrant, adaptable,  community becomes even  critical.Word of Mouth is the New Open Source TrustRestricting  data access can't mean killing trust. People can't start doubting Ethereum like they doubt Meta's data policies just because not every single thing is instantly public.\
How do we solve this in the semi-open model? \
Look, I don't personally verify every cryptographic detail in Vitalik's papers. I don't audit the MPC kit SDK from Silence Laboratories myself. I  that Vitalik is top-notch, with high-quality peer review. I  that Trail of Bits does a good job auditing Silence Laboratories.\
My personal intersection might be closer to smart contracts, maybe reviewing an EIP like 7702 and finding there a bug or two.\
Similarly, my users and friends might not grasp the deep tech of Peeramid Labs, but they trust , and they can sanity-check the high-level architecture. That's enough.\
You don't need  open everything to build a massively adopted protocol. But you  need a large, committed, layered community that can downstream knowledge trust based on earned reputation and contribution and upstream talented contributors!\
Thatâ€™s the path forward. Let's build it.]]></content:encoded></item><item><title>Apple Rushes Shipments From India To Dodge Tariffs</title><link>https://apple.slashdot.org/story/25/04/07/1542257/apple-rushes-shipments-from-india-to-dodge-tariffs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 7 Apr 2025 17:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple rushed five planeloads of iPhones from India to the U.S. in just three days to beat new tariffs imposed by the Trump administration, Times of India reported Monday, citing sources. The urgent shipments during the final week of March aimed to avoid the 10% reciprocal tariff that took effect on April 5. 

The stockpiling will allow Apple to maintain current pricing temporarily. "The reserves that arrived at lower duty will temporarily insulate the company from the higher prices that it will need to pay for new shipments," the Indian daily cited a source as saying. The Trump administration also announced a 26% reciprocal tariff to be implemented on April 9, potentially accelerating Apple's manufacturing shift away from China. India offers a significant tariff advantage, with Indian exports facing a 26% tariff to the U.S. compared to 54% on Chinese goods. 

Further reading: India's Economic Chess Against Twin US Economic Threats.]]></content:encoded></item></channel></rss>