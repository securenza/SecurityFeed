<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>SailPoint’s dull debut did little to loosen the stuck IPO window, expert says</title><link>https://techcrunch.com/2025/02/14/sailpoints-dull-debut-did-little-to-loosen-the-stuck-ipo-window-expert-says/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 14 Feb 2025 22:00:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[SailPoint’s IPO on Thursday was a disappointment for anyone hoping it would indicate that tech IPOs are hot again. The first day’s trading ended below the $23 initial price. The stock fared a tad better Friday, closing at over $24. But that’s nothing close to the big bang companies and VCs hope for. For instance, […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>OpenAI Eases Content Restrictions For ChatGPT With New &apos;Grown-Up Mode&apos;</title><link>https://slashdot.org/story/25/02/14/2156202/openai-eases-content-restrictions-for-chatgpt-with-new-grown-up-mode?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 14 Feb 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: On Wednesday, OpenAI published the latest version of its "Model Spec," a set of guidelines detailing how ChatGPT should behave and respond to user requests. The document reveals a notable shift in OpenAI's content policies, particularly around "sensitive" content like erotica and gore -- allowing this type of content to be generated without warnings in "appropriate contexts." The change in policy has been in the works since May 2024, when the original Model Spec document first mentioned that OpenAI was exploring "whether we can responsibly provide the ability to generate NSFW content in age-appropriate contexts through the API and ChatGPT."
 
ChatGPT's guidelines now state that that "erotica or gore" may now be generated, but only under specific circumstances. "The assistant should not generate erotica, depictions of illegal or non-consensual sexual activities, or extreme gore, except in scientific, historical, news, creative or other contexts where sensitive content is appropriate," OpenAI writes. "This includes depictions in text, audio (e.g., erotic or violent visceral noises), or visual content." So far, experimentation from Reddit users has shown that ChatGPT's content filters have indeed been relaxed, with some managing to generate explicit sexual or violent scenarios without accompanying content warnings. OpenAI notes that its Usage Policies still apply, which prohibit building AI tools for minors that include sexual content.]]></content:encoded></item><item><title>OpenAI says its board of directors ‘unanimously’ rejects Elon Musk’s bid</title><link>https://techcrunch.com/2025/02/14/openai-says-its-board-of-directors-unanimously-rejects-musks-bid/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:45:07 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI’s board of directors has “unanimously” rejected billionaire Elon Musk’s offer to buy the nonprofit that effectively governs OpenAI, the company said on Friday. In a statement shared via OpenAI’s press account on X, Bret Taylor, board chair, called Musk’s bid “an attempt to disrupt his competition.” “OpenAI is not for sale, and the board […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Bluesky gets growth and analytics tools with BlueSkyHunter launch</title><link>https://techcrunch.com/2025/02/14/bluesky-gets-growth-and-analytics-tools-with-blueskyhunter-launch/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:42:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A new startup is addressing the need for an all-in-one toolset built for people who want to grow, manage, and track their Bluesky presence and following. The subscription service BlueSkyHunter, which launched Friday, introduces an online dashboard that combines access to analytics and other tools to schedule posts and automate DMs (direct messages), alongside other […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Global electricity demand expected to grow 4% annually through 2027</title><link>https://techcrunch.com/2025/02/14/global-electricity-demand-expected-to-grow-4-annually-through-2027/</link><author>Tim De Chant</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:40:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meeting that demand will require adding more generating capacity than all of Japan — every year. © 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>How this weekend’s ‘Tesla Takeover’ protests against Elon Musk came together on Bluesky</title><link>https://techcrunch.com/2025/02/14/how-this-weekends-tesla-takeover-protests-against-elon-musk-came-together-on-bluesky/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:34:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As Elon Musk and his acolytes rip through the federal government looking for agencies to throw into the “wood chipper,” a grassroots effort to hit the world’s richest man where it hurts is picking up steam. The courts are busy contesting the actions of Musk’s Department of Government Efficiency, but the judicial system is slow […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Meta To Build World&apos;s Longest Undersea Cable</title><link>https://tech.slashdot.org/story/25/02/14/1852233/meta-to-build-worlds-longest-undersea-cable?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Meta unveiled on Friday Project Waterworth, a 50,000-kilometer subsea cable network that will be the world's longest such system. The multi-billion dollar project will connect the U.S., Brazil, India, South Africa, and other key regions. The system utilizes 24 fiber pairs and introduces what Meta describes as "first-of-its-kind routing" that maximizes cable placement in deep water at depths up to 7,000 meters. 

The company developed new burial techniques for high-risk areas near coasts to protect against ship anchors and other hazards. A joint statement from President Trump and Prime Minister Modi confirmed India's role in maintaining and financing portions of the undersea cables in the Indian Ocean using "trusted vendors." According to telecom analysts Telegeography, Meta currently has ownership stakes in 16 subsea networks, including the 2Africa cable system that encircles the African continent. This new project would be Meta's first wholly owned global cable system.]]></content:encoded></item><item><title>What is device code phishing, and why are Russian spies so successful at it?</title><link>https://arstechnica.com/information-technology/2025/02/russian-spies-use-device-code-phishing-to-hijack-microsoft-accounts/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/03/phishing.jpeg" length="" type=""/><pubDate>Fri, 14 Feb 2025 21:16:11 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Researchers have uncovered a sustained and ongoing campaign by Russian spies that uses a clever phishing technique to hijack Microsoft 365 accounts belonging to a wide range of targets, researchers warned.The technique is known as device code phishing. It exploits “device code flow,” a form of authentication formalized in the industry-wide OAuth standard. Authentication through device code flow is designed for logging printers, smart TVs, and similar devices into accounts. These devices typically don’t support browsers, making it difficult to sign in using more standard forms of authentication, such as entering user names, passwords, and two-factor mechanisms.Rather than authenticating the user directly, the input-constrained device displays an alphabetic or alphanumeric device code along with a link associated with the user account. The user opens the link on a computer or other device that’s easier to sign in with and enters the code. The remote server then sends a token to the input-constrained device that logs it into the account.]]></content:encoded></item><item><title>DeepSeek founder Liang Wenfeng is reportedly set to meet with China’s Xi Jinping</title><link>https://techcrunch.com/2025/02/14/deepseek-founder-liang-wenfeng-is-reportedly-set-to-meet-with-chinas-xi-jinping/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:45:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Chinese AI startup DeepSeek founder Liang Wenfeng is reportedly set to meet with China’s top politicians, including Chinese leader Xi Jinping, during a summit that Alibaba founder Jack Ma is also expected to attend. The summit, which could happen as soon as next week, may be intended as a signal by China’s Communist Party that […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>The Whole World Is Going To Use a Lot More Electricity, IEA Says</title><link>https://hardware.slashdot.org/story/25/02/14/1742226/the-whole-world-is-going-to-use-a-lot-more-electricity-iea-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Electricity demand is set to increase sharply in the coming years as people around the world use more power to run air conditioners, industry and a growing fleet of data centers. From a report: Over the next three years, global electricity consumption is set to rise by an "unprecedented" 3,500 terawatt hours, according to a report by the International Energy Agency. That's an addition each year of more than Japan's annual electricity consumption. 

The roughly 4% annual growth in that period is the fastest such rate in years, underscoring the growing importance of electricity to the world's overall energy needs. "The acceleration of global electricity demand highlights the significant changes taking place in energy systems around the world and the approach of a new Age of Electricity," Keisuke Sadamori, IEA's director of energy markets and security, said in a statement. "But it also presents evolving challenges for governments in ensuring secure, affordable and sustainable electricity supply."]]></content:encoded></item><item><title>NATO backs its first cohort of European dual-use startups</title><link>https://techcrunch.com/2025/02/14/nato-backs-its-first-cohort-of-european-dual-use-startups/</link><author>Mike Butcher</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:16:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[With both Vice President J.D. Vance and U.S. Defense Secretary Pete Hegseth making loud noises Friday about Europe stepping up to the plate in spending more on its own defense, it might come as a surprise that Europe is already on the path toward far greater investment in defense, especially in tech. Not only has […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Western Digital Aims For 100TB Hard Drives by 2030</title><link>https://hardware.slashdot.org/story/25/02/14/1817233/western-digital-aims-for-100tb-hard-drives-by-2030?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Western Digital plans to introduce its first heat-assisted magnetic recording (HAMR) drives in late 2026, with 36TB conventional magnetic recording (CMR) and 44TB shingled UltraSMR variants. Volume production won't begin until the first half of 2027, following qualification by cloud data center providers in late 2026. 

The company projects that HAMR technology, combined with OptiNAND, increased platter count, and mechanical improvements, will enable drives reaching 80TB CMR and 100TB UltraSMR capacities around 2030 -- a departure from Western Digital's previous commitment to microwave-assisted magnetic recording (MAMR) in 2017, which evolved into the energy-assisted perpendicular magnetic recording (ePMR) technology used in current drives.]]></content:encoded></item><item><title>Elon Musk’s AI company, xAI, said to be in talks to raise $10B</title><link>https://techcrunch.com/2025/02/14/elon-musks-ai-company-xai-said-to-be-in-talks-to-raise-10b/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 19:54:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Elon Musk’s AI company, xAI, is said to be in talks to raise $10 billion in a round that would value xAI at $75 billion. Bloomberg reported Friday that xAI is canvassing existing investors, including Sequoia Capital, Andreessen Horowitz, and Valor Equity Partners for the round, which would bring xAI’s total raised to $22.4 billion, […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Hedge Fund Startup That Replaced Analysts With AI Beats the Market</title><link>https://slashdot.org/story/25/02/14/187221/hedge-fund-startup-that-replaced-analysts-with-ai-beats-the-market?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 19:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A hedge fund startup that uses AI to do work typically handled by analysts has outperformed the global stock market in its first six months while slashing research costs. From a report: The Sydney-based firm, Minotaur Capital, was founded by Armina Rosenberg and Thomas Rice. Rosenberg previously managed a global equities portfolio for tech billionaire Mike Cannon-Brookes and ran Australian small-company research for JPMorgan Chase & Co. when she was 25. Rice is a former portfolio manager at Perpetual. The duo's bets on global stocks returned 13.7% in the six months ending January, versus 6.7% for the MSCI All-Country World Index. Minotaur has no analysts on staff, with Rosenberg saying AI models are far quicker and cheaper. 

"We're looking at about half the price" in terms of cost of AI versus a junior analyst salary, Rosenberg, 37, said of the firm's program. Minotaur is among a growing number of hedge funds experimenting with ways to improve returns and cut expenses with AI as the technology becomes increasingly sophisticated. Still, the jury is still out on the ability of AI-driven models to deliver superior returns over the long run.]]></content:encoded></item><item><title>Meta’s next big bet may be humanoid robotics</title><link>https://techcrunch.com/2025/02/14/metas-next-big-bet-may-be-humanoid-robotics/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 19:16:11 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta is forming a new team within its Reality Labs hardware division to build robots that can assist with physical tasks, Bloomberg reported. The team will be responsible for developing humanoid robotics hardware, potentially including hardware that can perform household chores. Meta’s new robotics group, which will be led by Marc Whitten, driverless car startup […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Dynamic Triple Buffering Merged For GNOME 48</title><link>https://www.phoronix.com/news/GNOME-48-Triple-Buffering</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:35:10 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As quite a Valentine's Day treat, the long-in-development dynamic triple buffering support for GNOME's Mutter compositor was just merged ahead of next month's GNOME 48 desktop release!..]]></content:encoded></item><item><title>AI and security startups blossom on cloudy days</title><link>https://techcrunch.com/2025/02/14/ai-and-security-startups-blossom-on-cloudy-days/</link><author>Anna Heim</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:05:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Welcome to Startups Weekly — your weekly recap of everything you can’t miss from the world of startups. Want it in your inbox every Friday? Sign up here. This week confirmed that even when current events cloud the outlook, some startups still manage to raise significant amounts of funding, with those tied to security and […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>China To Develop Gene-Editing Tools, New Crop Varieties</title><link>https://science.slashdot.org/story/25/02/14/1724238/china-to-develop-gene-editing-tools-new-crop-varieties?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:03:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China issued guidelines on Friday to promote biotech cultivation, focusing on gene-editing tools and developing new wheat, corn, and soybean varieties, as part of efforts to ensure food security and boost agriculture technology. From a report: The 2024-2028 plan aims to achieve "independent and controllable" seed sources for key crops, with a focus to cultivate high-yield, multi-resistant wheat, corn and high-oil, high-yield soybean and rapeseed varieties. The move comes as China intensifies efforts to boost domestic yields of key crops like soybeans to reduce reliance on imports from countries such as the United States amid a looming trade war.]]></content:encoded></item><item><title>Meta confirms ‘Project Waterworth,’ a global subsea cable project spanning 50,000 kilometers</title><link>https://techcrunch.com/2025/02/14/meta-confirms-project-waterworth-a-global-subsea-cable-project-spanning-50000km/</link><author>Ingrid Lunden</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:59:07 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Back in November, we broke the news that Meta — owner of Facebook, Instagram, and WhatsApp, with billions of users accounting for 10% of all fixed and 22% of all mobile traffic — was close to announcing work on a major new, $10 billion+ subsea cable project to connect up the globe. The aim was […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Mastodon is working to add the controversial ‘quote posts’ feature</title><link>https://techcrunch.com/2025/02/14/mastodon-is-working-to-add-the-controversial-quote-posts-feature/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:42:24 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Mastodon, the decentralized alternative to X, is going to adopt a controversial feature from the platform formerly known as Twitter: quote posts. The company on Friday shared the progress it’s making on the implementation of the feature, which has divided users over its potential to be used for online abuse and bullying. Critics have long […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>DeepSeek: Everything you need to know about the AI chatbot app</title><link>https://techcrunch.com/2025/02/14/deepseek-everything-you-need-to-know-about-the-ai-chatbot-app/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:37:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[DeepSeek has gone viral. Chinese AI lab DeepSeek broke into the mainstream consciousness this week after its chatbot app rose to the top of the Apple App Store charts (and Google Play, as well). DeepSeek’s AI models, which were trained using compute-efficient techniques, have led Wall Street analysts — and technologists — to question whether the U.S. can maintain its […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>James Bond in Battle To Keep Hold of 007 Super Spy&apos;s Name</title><link>https://news.slashdot.org/story/25/02/14/1720254/james-bond-in-battle-to-keep-hold-of-007-super-spys-name?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The owners of the multibillion-pound James Bond franchise are embroiled in a fight to keep control of the super spy's name, after a Dubai-based property developer filed claims in the UK and Europe that they are not using the trademark across a range of goods and services. From a report: The Austrian businessman Josef Kleindienst, who is building a $5 billion luxury resort complex called the Heart of Europe on six human-made islands just off the coast of Dubai, has filed a slew of what are known officially as "cancellation actions based on non-use" targeting the James Bond name. 

Under UK and EU law, if a name is trademarked against certain goods and services but the owner does not commercially exploit it in these areas for a period of at least five years then a challenge to revoke ownership of the name can be made. "He is challenging a number of UK and European Union trademark registrations for James Bond," said Mark Caddle, a partner and patent attorney at European intellectual property firm Withers & Rogers. "The basis of the European Union filings is that James Bond has not been used for the goods and services it protects, and that is likely to be the same basis of the filings in the UK."]]></content:encoded></item><item><title>Video Friday: PARTNR</title><link>https://spectrum.ieee.org/video-friday-partnr</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjQ5NjM2MS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc0MDk5OTA2Nn0.hMaiDCMfAle2DiX2_S3CZKODrsXEYRxu4BQGU0eByc4/image.png?width=600" length="" type=""/><pubDate>Fri, 14 Feb 2025 17:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your weekly selection of awesome robot videos]]></content:encoded></item><item><title>A job ad for Y Combinator startup Firecrawl seeks to hire an AI agent for $15K a year</title><link>https://techcrunch.com/2025/02/14/a-job-ad-for-y-combinator-startup-firecrawl-seeks-to-hire-an-ai-agent-for-15k-a-year/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Last week, an ad from the Y Combinator job board for a tiny startup called Firecrawl went viral on X.  That’s because the ad wasn’t for a human. “Please apply only if you are an AI agent, or if you created an AI agent that can fill this job,” the job posting read.  The seven-person […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Instagram tests a ‘dislike’ button for comments</title><link>https://techcrunch.com/2025/02/14/instagram-tests-a-dislike-button-for-comments/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:54:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After users noticed a new button on Instagram to downvote or dislike comments, the company confirmed that it is testing a way for users to signal that they either didn’t like the comment or don’t think it’s relevant. The feature will appear across both Feed posts and Reels. Instagram head Adam Mosseri said in a […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Reddit Plans To Lock Some Content Behind a Paywall This Year, CEO Says</title><link>https://tech.slashdot.org/story/25/02/14/1629205/reddit-plans-to-lock-some-content-behind-a-paywall-this-year-ceo-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Reddit is planning to introduce a paywall this year, CEO Steve Huffman said during a videotaped Ask Me Anything (AMA) session on Thursday. Huffman previously showed interest in potentially introducing a new type of subreddit with "exclusive content or private areas" that Reddit users would pay to access. 

When asked this week about plans for some Redditors to create "content that only paid members can see," Huffman said: "It's a work in progress right now, so that one's coming... We're working on it as we speak." When asked about "new, key features that you plan to roll out for Reddit in 2025," Huffman responded, in part: "Paid subreddits, yes."]]></content:encoded></item><item><title>Your Internet Traffic Is Slower Than It Should Be—Laconic Fixes That</title><link>https://hackernoon.com/your-internet-traffic-is-slower-than-it-should-belaconic-fixes-that?source=rss</link><author>Load Balancer</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:40:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Tianyi Cui, University of Washington (cuity@cs.washington.edu);(2) Chenxingyu Zhao, University of Washington (cxyzhao@cs.washington.edu);(3) Wei Zhang, Microsoft (wei.zhang.gbs@gmail.com);(4) Kaiyuan Zhang, University of Washington (kaiyuanz@cs.washington.edu).:::tip
Editor's note: This is Part 6 of 6 of a study detailing attempts to optimize layer-7 load balancing. Read the rest below.We propose Laconic, which explores the potential of offloading L7 load balancing onto SmartNICs and addresses the associated challenges. In Laconic, we employ a lightweight network stack to avoid the costs of a heavy, feature-rich network stack. To minimize the cost of synchronization operations, we design lightweight synchronization mechanisms that enable higher concurrency and scalability. To further reduce the burden on the generic cores on the SmartNIC, we offload packet processing by effectively utilizing the hardware acceleration engines available on SmartNICs. This paper does not raise any ethical issues.[1] Broadcom PS1100R. https://www.microlandusa.com/broadcomps1100r-100gbe-nvme-pcie-storage-adapter-with-power-supplycarrier-card-serial-cable.html.\
[2] ELB vs. ALB vs. NLB: Choosing the Best AWS Load Balancer for Your Needs . https://iamondemand.com/blog/elb-vs-alb-vs-nlb-choosingthe-best-aws-load-balancer-for-your-needs/.\
[3] Fungible can solve the public cloud Hotel California problem. https://blocksandfiles.com/2021/07/19/fungible-can-solve-thepublic-cloud-trillion-dollar-paradox-hotel-california-problem/.\
[4] How to cut AWS ELB costs by 90% using Application Load Balancers. . https://medium.com/cognitoiq/how-cognitoiq-are-usingapplication-load-balancers-to-cut-elastic-load-balancing-cost-by-90- 78d4e980624b.\
[5] Knowing How Much to Spend on the AWS Elastic Load Balancer . https://logz.io/blog/cost-management-elb-aws-load-balancer/.\
[6] Reducing data center TCO with server offload strategies. https://www.datacenterdynamics.com/en/opinions/reducingdata-center-tco-with-server-offload-strategies/.\
[7] What is an Application Load Balancer. https://docs.aws.amazon.com/elasticloadbalancing.\
[8] Akhan Akbulut and Harry G. Perros. Performance analysis of microservice design patterns. IEEE Internet Computing, 23(6):19–27, 2019.\
[9] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Matus, Rong Pan, Navindra Yadav, and George Varghese. Conga: Distributed congestion-aware load balancing for datacenters. In Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM ’14, page 503–514, New York, NY, USA, 2014. Association for Computing Machinery.\
[10] Broadcom. Stingray smartnic adapters and ic. [EB/OL]. https: //www.broadcom.com/products/ethernet-connectivity/networkadapters/smartnic Accessed Oct 25, 2020.\
[11] Qizhe Cai, Midhul Vuppalapati, Jaehyun Hwang, Christos Kozyrakis, and Rachit Agarwal. Towards 𝜇 s tail latency and terabit ethernet: disaggregating the host network stack. In Proceedings of the ACM SIGCOMM 2022 Conference, pages 767–779, 2022.\
[12] DPDK. [rfc] ethdev: support hairpin queue. [EB/OL]. https://patches.dpdk.org/project/dpdk/patch/1565703468-55617-1- git-send-email-orika@mellanox.com/ Accessed Oct 20, 2021.\
[13] Dropbox. How we migrated dropbox from nginx to envoy. [EB/OL]. https://web.archive.org/web/20220403133038/https://dropbox.tech/ infrastructure/how-we-migrated-dropbox-from-nginx-to-envoy Accessed April 19th, 2022.\
[14] Daniel E Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov, Eric Mann-Hielscher, Ardas Cilingiroglu, Bin Cheyney, Wentao Shang, and Jinnah Dylan Hosein. Maglev: A fast and reliable software network load balancer. In 13th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 16), pages 523–535, 2016.\
[15] Inc F5. Nginx high performace load balancer. [EB/OL]. https://www. nginx.com/ Accessed Oct 25, 2020.\
[16] Facebook. Katran. [EB/OL]. https://github.com/facebookincubator/ katran Accessed Oct 25, 2020.\
[17] Roy Fielding, Jim Gettys, Jeffrey Mogul, Henrik Frystyk, Larry Masinter, Paul Leach, and Tim Berners-Lee. Rfc2616: Hypertext transfer protocol–http/1.1, 1999.\
[18] Cloud Native Computing Foundation. Envoy proxy-home. [EB/OL]. https://www.envoyproxy.io/ Accessed Oct 25, 2020.\
[19] Soudeh Ghorbani, Brighten Godfrey, Yashar Ganjali, and Amin Firoozshahian. Micro load balancing in data centers with drill. In Proceedings of the 14th ACM Workshop on Hot Topics in Networks, pages 1–7, 2015.\
[20] Soudeh Ghorbani, Zibin Yang, P Brighten Godfrey, Yashar Ganjali, and Amin Firoozshahian. Drill: Micro load balancing for low-latency data center networks. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, pages 225–238, 2017.\
[21] Massimo Girondi, Marco Chiesa, and Tom Barbette. High-speed connection tracking in modern servers. In 2021 IEEE 22nd International Conference on High Performance Switching and Routing (HPSR), pages 1–8. IEEE, 2021.\
[22] Will Glozer. wrk-a http benchmarking tool, 2018.\
[23] Google. Google cloud endpoints now generally available: a fast, scalable api gateway. [EB/OL]. https://cloud.google.com/blog/products/ gcp/google-cloud-endpoints-now-ga-a-fast-scalable-api-gateway Accessed Oct 25, 2020.\
[24] Haproxy. Haproxy the reliable, high performance tcp/http load balancer. [EB/OL]. https://www.haproxy.org/ Accessed Oct 25, 2020.\
[25] EunYoung Jeong, Shinae Wood, Muhammad Jamshed, Haewon Jeong, Sunghwan Ihm, Dongsu Han, and KyoungSoo Park. mtcp: a highly scalable user-level {TCP} stack for multicore systems. In 11th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 14), pages 489–502, 2014.\
[26] Georgios P Katsikas, Tom Barbette, Marco Chiesa, Dejan Kostić, and Gerald Q Maguire. What you need to know about (smart) network interface cards. In International Conference on Passive and Active Network Measurement, pages 319–336. Springer, 2021.\
[27] Naga Katta, Mukesh Hira, Changhoon Kim, Anirudh Sivaraman, and Jennifer Rexford. Hula: Scalable load balancing using programmable data planes. In Proceedings of the Symposium on SDN Research, pages 1–12, 2016.\
[28] Antoine Kaufmann, Tim Stamler, Simon Peter, Naveen Kr Sharma, Arvind Krishnamurthy, and Thomas Anderson. Tas: Tcp acceleration as an os service. In Proceedings of the Fourteenth EuroSys Conference 2019, pages 1–16, 2019.\
[29] Duckwoo Kim, SeungEon Lee, and KyoungSoo Park. A case for smartnic-accelerated private communication. In 4th Asia-Pacific Workshop on Networking, pages 30–35, 2020.\
[30] Taehyun Kim, Deondre Martin Ng, Junzhi Gong, Youngjin Kwon, Minlan Yu, and KyoungSoo Park. Rearchitecting the tcp stack for i/ooffloaded content delivery. In 19th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2022. USENIX, 2023.\
[31] Bojie Li, Tianyi Cui, Zibo Wang, Wei Bai, and Lintao Zhang. Socksdirect: Datacenter sockets can be fast and compatible. In ACM SIGCOMM Conference (SIGCOMM), August 2019.\
[32] Bojie Li, Kun Tan, Layong (Larry) Luo, Yanqing Peng, Renqian Luo, Ningyi Xu, Yongqiang Xiong, Peng Cheng, and Enhong Chen. Clicknp: Highly flexible and high performance network processing with reconfigurable hardware. In Proceedings of the 2016 ACM SIGCOMM Conference, SIGCOMM ’16, page 1–14, New York, NY, USA, 2016. Association for Computing Machinery.\
[33] Xiaozhou Li, David G Andersen, Michael Kaminsky, and Michael J Freedman. Algorithmic improvements for fast concurrent cuckoo hashing. In Proceedings of the Ninth European Conference on Computer Systems, pages 1–14, 2014.\
[34] Jianshen Liu, Carlos Maltzahn, Craig Ulmer, and Matthew Leon Curry. Performance characteristics of the bluefield-2 smartnic. arXiv preprint arXiv:2105.06619, 2021.\
[35] Ming Liu, Tianyi Cui, Henry Schuh, Arvind Krishnamurthy, Simon Peter, and Karan Gupta. Offloading distributed applications onto smartnics using ipipe. In Proceedings of the ACM Special Interest Group on Data Communication, pages 318–333. 2019.\
[36] Michael Marty, Marc de Kruijf, Jacob Adriaens, Christopher Alfeld, Sean Bauer, Carlo Contavalli, Michael Dalton, Nandita Dukkipati, William C Evans, Steve Gribble, et al. Snap: A microkernel approach to host networking. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 399–413, 2019.\
[37] Marvel. Octeon tx2 liquidio iii smartnic. [EB/OL]. https://www.marvell.com/products/infrastructure-processors/multicore-processors/liquidio-smart-nics.html Accessed Oct 25, 2020.\
[38] Rui Miao, Hongyi Zeng, Changhoon Kim, Jeongkeun Lee, and Minlan Yu. Silkroad: Making stateful layer-4 load balancing fast and cheap using switching asics. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, pages 15–28, 2017.\
[39] YoungGyoun Moon, SeungEon Lee, Muhammad Asim Jamshed, and KyoungSoo Park. Acceltcp: Accelerating network applications with stateful TCP offloading. In 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20), pages 77–92, Santa Clara, CA, February 2020. USENIX Association.\
[40] Netronome. Agilio cx smartnics. [EB/OL]. https://www.netronome. com/products/agilio-cx/ Accessed Oct 25, 2020.\
[41] Nvidia. Bluefield smartnic ethernet. [EB/OL]. https://www.mellanox. com/products/BlueField-SmartNIC-Ethernet Accessed Oct 25, 2020.\
[42] Vladimir Olteanu, Alexandru Agache, Andrei Voinescu, and Costin Raiciu. Stateless datacenter load-balancing with beamer. In 15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18), pages 125–139, 2018.\
[43] Parveen Patel, Deepak Bansal, Lihua Yuan, Ashwin Murthy, Albert Greenberg, David A Maltz, Randy Kern, Hemant Kumar, Marios Zikos, Hongyu Wu, et al. Ananta: Cloud scale load balancing. ACM SIGCOMM Computer Communication Review, 43(4):207–218, 2013.\
[44] Rajath Shashidhara, Tim Stamler, Antoine Kaufmann, and Simon Peter. FlexTOE: Flexible TCP offload with Fine-Grained parallelism. In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), pages 87–102, Renton, WA, April 2022. USENIX Association.\
[45] Chaoliang Zeng, Layong Luo, Teng Zhang, Zilong Wang, Luyang Li, Wenchen Han, Nan Chen, Lebing Wan, Lichao Liu, Zhipeng Ding, Xiongfei Geng, Tao Feng, Feng Ning, Kai Chen, and Chuanxiong Guo. Tiara: A scalable and efficient hardware acceleration architecture for stateful layer-4 load balancing. In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), pages 1345–1358, Renton, WA, April 2022. USENIX Association.\
[46] Hong Zhang, Junxue Zhang, Wei Bai, Kai Chen, and Mosharaf Chowdhury. Resilient datacenter load balancing in the wild. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, pages 253–266, 2017.\
[47] Noa Zilberman. Technical perspective: hxdp: Light and efficient packet processing offload. Communications of the ACM, 65(8):91–91, 2022.A.1 Throughput of Laconic using all coresFigure 13 shows the throughput of Laconic on BlueField-2 with seven ARM cores [1]. The BlueField-2 is equipped with 2x100Gbps ports. We enable the two ports together to further stress the processing power on the ARM cores. For the baseline of Nginx running on the commodity x86 servers, we use 2x100Gbps ports of a ConnectX-5 NIC to make the link speed the same. However, although there are 2x100Gbps ports on ConnectX-5, due to the bottleneck of PCIe 3.0 for testbed servers, only about 120 Gbps can be achieved. As shown in Figure 13, Laconic with only seven NIC cores can achieve line rate[2] for large message sizes. For small message sizes, Laconic achieves comparable performance with Nginx running on x86. BlueField-2 has wimpy ARM cores, and Nginx running on ARM performs poorly, as expected. There is a clear increase in throughput for messages larger than 1M because Laconic offloads the packet processing into the hardware engine for response messages larger than 1M. Other than the first few packets, all the remaining packets in that flow are solely processed by the hardware packet processing engine. This can mitigate the head-of-line blocking problem caused by limited processing power and also relieves the CPU resources for small or future responses. In later Section 4.4, we show experiments with varying offloading thresholds.\
Figure 14 shows the throughput of Laconic on LiquidIO3 50G NIC with 24 ARM cores. Since the LiqudIO3 can only support 50 Gbps link speed, we also cap the x86 baselines at 50 Gbps for a fair comparison of all the experiments running on LiquidIO3. Nginx on the ARM cores of the SmartNIC (bar "LIO3 Nginx") has a low throughput. The throughput plateaus at around 5 Gbps even with 24 cores. Laconic can easily reach the full NIC bandwidth with 10K response size. However, Nginx running on an x86 server needs 32 beefy cores to achieve comparable performance.[1] BlueField-2 has eight available ARM cores in total. We reserve one core for the necessary background tasks.\
[2] Since the same port on the SmartNIC receives data from both the requests of the clients and also receive the responses from the backend servers, the theoretical upper limit for the load balancer’s goodput is lower than the line rate 2x100Gbps.]]></content:encoded></item><item><title>Cloud Companies Have Been Doing Load Balancing All Wrong—Here’s a Smarter Way</title><link>https://hackernoon.com/cloud-companies-have-been-doing-load-balancing-all-wrongheres-a-smarter-way?source=rss</link><author>Load Balancer</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:40:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Tianyi Cui, University of Washington (cuity@cs.washington.edu);(2) Chenxingyu Zhao, University of Washington (cxyzhao@cs.washington.edu);(3) Wei Zhang, Microsoft (wei.zhang.gbs@gmail.com);(4) Kaiyuan Zhang, University of Washington (kaiyuanz@cs.washington.edu).:::tip
Editor's note: This is Part 5 of 6 of a study detailing attempts to optimize layer-7 load balancing. Read the rest below. There are multiple works investigating high-performance load balancers. Katran [16] is an L4 load balancer implemented in eBPF. SilkRoad [38] and Hula [27] leverage programmable switches to build a high-performance L4 load balancer. Maglev [14] proposed by Google takes advantage of consistent hashing. Beamer [42] provides consistency without maintaining state. DRILL [19, 20] performs micro load balancing. Tiara [45] proposes a new hardware architecture for L4 load balancing. All of these works are based on connection-less load balancing and are unable to satisfy the need for connection-oriented L7 load balancing.\
Network stack acceleration. TAS [28] provides a host TCP networking acceleration service. FlexTOE [44] and IOTCP [30] offload the host TCP stack onto the SmartNICs. Snap [36] is a high-performance networking stack proposed by Google. AccelTCP [39] offloads the control path of TCP onto the SmartNIC. Kim, D., Lee, S., and Park, K [29] explore offloading the TLS control plane onto the SmartNIC. Zilberman [47] offloads packet processing with XDP. Netchannel [11] disaggregates the network stack. They only offload a part of the control path of the networking protocols onto the SmartNIC and do not touch L7 protocols.\
Measurements for SmartNIC. Multiple works provide valuable measurement for SmartNICs [26, 34]. Girondi et al. measured the performance of different high-speed concurrent hashtables [21]. We consulted these numbers during the design process.]]></content:encoded></item><item><title>Cloud Companies Are Wasting Resources—Here’s How They Could Cut Costs by 13x</title><link>https://hackernoon.com/cloud-companies-are-wasting-resourcesheres-how-they-could-cut-costs-by-13x?source=rss</link><author>Load Balancer</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:39:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Tianyi Cui, University of Washington (cuity@cs.washington.edu);(2) Chenxingyu Zhao, University of Washington (cxyzhao@cs.washington.edu);(3) Wei Zhang, Microsoft (wei.zhang.gbs@gmail.com);(4) Kaiyuan Zhang, University of Washington (kaiyuanz@cs.washington.edu).:::tip
Editor's note: This is Part 4 of 6 of a study detailing attempts to optimize layer-7 load balancing. Read the rest below.In this section, we conduct both microbenchmarks and end-to-end application benchmarks to understand the performance of Laconic. Specifically, we aim to answer the following three questions:\
• Can Laconic’s design provide end-to-end performance benefis compared with existing load balancers?\
• What benefits are provided by each of the individual techniques used in Laconic’s design?\
• How well does Laconic perform under real-world application workloads? We implement Laconic on both on-path and off-path SmartNICs with DPDK 21.11. We implement Laconic in about 6k lines of C++ code.\
: Our testbed consists of five x86 servers with Intel Xeon 5218 CPU and 64GB of memory. Each server is equipped with a Mellanox ConnectX-5 two-port NIC. The link speed of each individual port is 100 Gbps. We use a dedicated set of servers to host SmartNICs. The on-path SmartNIC we use is a Marvell LiquidIO 3 CN3380 with 2x50 Gbps ports. The off-path SmartNIC used in our deployment is an Nvidia BlueField-2 MBF2H516C-CECOT with 2x100 Gbps ports. An Arista DCS-7060CX switch is configured with VLAN and L4 routing. We partition the clients and servers into two different subnets, which is a common setting in actual deployment. We allocate two of the five servers as clients and the other three act as servers. The load balancer operates within the server subnet and is responsible for receiving and forwarding requests through the same NIC port. This setup, referred to as "router-on-a-stick," allows the load balancer to manage the incoming requests and distribute the requests among the backend servers. Figure 7 shows the path of a request to one of the backend servers.\
: There are multiple widely adopted L7 load balancers on the market, including Envoy [18], Nginx [15], HAProxy [24], etc. We choose Nginx as our baseline since it is widely adopted in practical deployment and provides higher performance than others [13].\
Client and backend server: wrk and wrk2 are used with customized Lua scripts as the client software. We generate a list of static files with various file sizes to responding the HTTP requests of wrk. We use the metric of requests per second (RPS) reported by wrk, and we multiply it by the file size to get the goodput measurement. Nginx is configured as the backend server software which responds to wrk requests with static files of varied file sizes.4.2 End-to-end ThroughputWe evaluate the throughput performance of Laconic on offpath BlueField-2 SmartNIC and on-path LiquidIO3 SmartNIC (Figure 8) while varying the number of cores and file sizes of wrk requests.\
 To show the processing capability of Laconic, we first evaluate the throughput on just a single core by varying the response size. Figure 8a clearly shows that the single-core performance of Laconic on BlueField-2 outperforms Nginx on x86. In this experiment, we set the threshold of offloading onto the flow processing engine as 1MB. With the help of the hardware flow processing engine, for response sizes of 16M, Laconic achieves 8.7x throughput compared with Nginx x86. As shown in Figure 8c, Laconic on LIO3 performs much better than Nginx, even though Nginx runs on a more powerful x86 core. Although LiquidIO3 does not have a hardware flow processing engine, the throughput of Laconic is still 4.5x better compared with x86 Nginx.\
 To show the scalability behavior of using multiple cores, we measure the throughput by varying the number of cores. Figure 8b shows Laconic performance on BlueField-2. While offloading the processing into a hardware engine for messages larger than 1M, we observe that throughput scales with more cores for processing large messages, which can achieve up to 150 Gbps throughput. For small messages, we don’t offload processing onto the flow processing engine. Throughput for processing small messages also can scale with more cores. In Section 4.4, we will explain more about the scalability of processing small messages and why the flow processing engine has a limited impact on its scaling. Figure 8d depicts the scalability performance for LIO3, where we see linear increases in throughput till the line rate is achieved, except for the 1KB small message flow case due to frequent connection setup and tear-down. In Appendix A.1, we show zoomed-in figures of the throughput performance with all cores.We next study Laconic performance in terms of latency as Figure 9 shows. We use wrk2 with setting a target throughput as 50% peak throughput measured in the previous experiments. In this experiment, we focus on the Laconic on BlueField-2 because Laconic on BlueField-2 achieves higher throughput performance than LiquidIO-3 according to the throughput experiments in Section 4.2.\
 Figure 9a shows average latency results with a single core. Despite our design running on BlueField-2 with higher throughput, Laconic’s latency is comparable to that of the Nginx running over x86 servers. Moreover, Laconic can achieve lower latency for processing large message sizes compared with other baselines by efficiently offloading onto the flow processing engine.\
As Figure 9b and Figure 9c show, we further zoom in to specific response sizes to plot the cumulative distribution function (CDF) of latencies for all requests. Laconic on BlueField-2 achieves lower latency when compared to Nginx running on the ARM cores or the x86 cores for both short and large response sizes, which benefit from our lightweight network stack and efficient synchronization mechanisms. Additionally, large messages are processed by the flow processing engine without much involvement of cores.\
 Figure 9d shows the average latency while Laconic uses all ARM cores on BlueField-2. Compared with using only one core, Laconic achieves lower latency with more cores. Also, in the multi-core scenario, Laconic still outperforms other baselines. In Section 4.4, we also investigate how the core scalability affects the latency by varying the number of cores.4.4 Evaluating the Benefits of Key TechniquesOffloading to Flow Processing Engine: Figure 10 shows the benefits of using a hardware flow processing engine. In this experiment, we vary the offloading threshold such that requests with response size larger than the threshold were processed by the flow processing engine, while ARM cores were used for requests with response sizes smaller than the threshold. Note that prior to fully offloading the flow processing, the initial packets are processed by the ARM cores for establishing connections and inserting flow rules.\
Figure 10a and 10b show how offloading affect the throughput. As Figure 10a shows, if we use ARM cores to process requests without offloading, it is hard to achieve high throughput even with more cores. There are two reasons for the poor scalability: 1) Memory bandwidth for SmartNIC accessing the on-board memory is limited, which is also reported in other work [30]. 2) For BlueField-2, every two cores share 1MB L2 cache, which could result in cache contention while using multiple cores [41]. Thus, the poor scalability necessitates the offloading to the flow processing engine. Figure 10b shows throughput performance while we process all sizes of requests by offloading to the flow processing engine. We find that the processing of large responses can be scaled with more cores. Because packet processing is offloaded to the flow processing engine, the ARM core is just responsible for setting up the connection and installing flow rules. Additionally, we observed that the processing of small responses does not see improvement from the packet processing engine, largely due to the overhead involved in updating flow rules. This overhead will be discussed in detail below.\
Table 2 shows the time cost for updating the flow rule in the flow processing engine. Flows can be inserted or deleted from the engine in batches. From Table 2, we can see that time cost for updating rules is not negligible for small responses, especially for updating singleton rules. The time cost of updating the rule dominates the flow completion time of small requests. This is the reason why offloading the processing of small responses, such as those less than 1KB, does not improve performance. So, in practice, we only offload the processing of large responses (e.g., >1MB) to the flow processing engine. Also, we use batching to amortize the updating cost. The batch size is determined by the flow arrival pattern.\
\
Figures 10c and 10d show how offloading affects the latency. In Figure 10c, we offload processing responses whose size is larger than 1MB. As expected, the latency of processing responses remains stable as we increase the number of cores because, after offloading, ARM cores are not used to process response packets. Figure 10d depicts the case where ARM cores process responses without offloading. In this case, it is shown that not offloading to the flow processing engine leads to lower latency when using all cores and similar performance to offloaded processing when using a single core, due to avoiding the rule update costs.\
 We next study the performance of our lightweight network stack. Table 3 shows the throughput of packet processing with our network stack. Our network stack can achieve up to 3 million packets per second (Mpps) using a single ARM core of BlueField-2. To further analyze the efficiency of the network stack, we use the perf tool to break down the ARM core utilization for operations of packet processing. Among the top-5 operations in terms of ARM core utilization, We find that the DPDK API costs up to 40% utilization, which is the internal cost of the DPDK. For other operations that are memory-intensive, they frequently execute memory allocation and population, which are bottlenecked by the memory bandwidth. From the breakdown, we can see that our network stack implementation is efficient, as it is mainly bottlenecked by the DPDK API cost and memory bandwidth.\
 We evaluate our concurrent hash table design. We measure the throughput of two basic hashtable operations, insert and lookup, with the ARM cores. As Figure 11 shows, the hash table of Laconic can achieve higher throughput than the hash table provided by the DPDK library. Also, the hash table of Laconic scales the performance well with more cores. Further, Table 3 shows that Laconic handles 3M packets/sec, a rate that is supported by our hashtable but not the other alternatives.To evaluate the performance of Laconic under a real-world workload, we adopt the workload from the CONGA work on datacenter traffic load balancing [9]. We extract the flow size distribution from their enterprise workload and use it to generate a list of files on the backend server with corresponding sizes. On the client side, we use a customized Lua script for wrk to generate the request according to the distribution.\
Figure 12a shows the throughput of Laconic with the realworld workload on BlueField-2, which only has eight ARM cores. Laconic can achieve 13.3x throughput with a single core compared to Nginx running on BlueField-2. Laconic achieves higher performance with a fewer number of cores up to 75 Gbps; however, Nginx needs more than 16 cores to achieve a similar throughput.\
Figure 12b shows Laconic performance on LiquidIO3. LIO3 has 24 ARM cores, so we perform the tests on LIO3 with up to 24 cores. We observe that Nginx on LIO3 runs at a very low throughput (5.21 Gbps). Although Nginx on x86 linearly increases the performance, it needs more than 16 cores for just 50G bandwidth. In contrast, Laconic only needs up to two ARM cores of LIO3 to match the 50G NIC bandwidth.]]></content:encoded></item><item><title>This Tiny Chip Can Handle Internet Traffic Faster—If You Let It Do Things Its Own Way</title><link>https://hackernoon.com/this-tiny-chip-can-handle-internet-traffic-fasterif-you-let-it-do-things-its-own-way?source=rss</link><author>Load Balancer</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:39:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Tianyi Cui, University of Washington (cuity@cs.washington.edu);(2) Chenxingyu Zhao, University of Washington (cxyzhao@cs.washington.edu);(3) Wei Zhang, Microsoft (wei.zhang.gbs@gmail.com);(4) Kaiyuan Zhang, University of Washington (kaiyuanz@cs.washington.edu).:::tip
Editor's note: This is Part 3 of 6 of a study detailing attempts to optimize layer-7 load balancing. Read the rest below.3 SmartNIC-based Load BalancersLaconic is a load balancer (LB) designed for SmartNICs that effectively uses their packet-processing capabilities. Our work primarily targets L7 load balancers, but some techniques also apply to accelerating L4 load balancers as well as optimizing L7 load balancers running on traditional servers.\
In this section, we first provide some baseline characterization experiments that empirically quantify the suitability of running a traditional load balancer on NIC cores, then provide an overview of the design of Laconic before describing in detail the core techniques used by the system.3.1 Offloading to SmartNICs: Challenges and OpportunitiesWe perform characterization experiments that quantify the cost of running an unmodified load balancer on the SmartNIC and then examine the performance of hardware acceleration features on SmartNICs. We demonstrate that while SmartNICs are not a good fit for directly porting LB software written for the host, customizing the software to leverage SmartNICs’ hardware accelerators can yield significant performance gains.\
We take a stock Nginx 1.20 load-balancer and run it on both host cores and SmartNIC cores. We use an x86 server with a Xeon Gold CPU and Mellanox ConnectX-5 NIC for the host experiments. We compared the host x86 performance to the LiquidIO3 CN3380 (with 24 ARM cores @2.2GHz) and BlueField-2 (with 8 ARM A72 cores) SmartNICs. We serve static files on the backend servers using a cache-enabled Nginx to ensure the bottleneck is on the load balancer, and we use wrk [22] to generate the workload.\
Table 1 provides the throughput achieved by the different configurations, as we vary the number of cores on the process (i.e., x86 cores on the host and ARM cores on the\
SmartNICs) for two different request sizes (1KB and 1MB). When we consider single-core experiments, we observe that the x86 core is about 1.8x to 3.5x more effective than the ARM cores on the SmartNIC. As we scale the number of cores on the processor, we observe a disparity between 4x and 8x. In both SmartNICs, we observe significant scalability bottlenecks; the LiquidIO3 achieves a lower line rate with eight cores than one core while performing 1MB transfers. These experiments quantify the inefficiencies of simply running an unmodified load balancer on the SmartNIC cores, which aren’t architected to run complex LB software.\
We perform another characterization experiment related to basic packet forwarding capability. We measure the performance of a DPDK-based packet rewriting and forwarding program on all of our three platforms (i.e., host x86 cores, LiquidIO3 ARM cores, and BlueField-2 ARM cores). This benchmark simply forwards packets after swapping the MAC address fields on received packets. We consider different packet sizes and vary the number of operating cores (or queues). Figure 2 depicts the performance of our three platforms and the hardware flow processing engine of BlueField-2. In contrast to the Nginx benchmark, we observe that the disparity in packet processing performance between the three platforms is much lower, indicating that the SmartNIC cores are better suited for packet processing than the additional general-purpose computing required by Nginx. Also, we demonstrate that the flow processing engine can operate efficiently compared with using x86 cores or ARM cores to process packets. Hardware flow processing engines can entirely offload packet processing and release the capability of the generic computing cores. But, the hardware rules have limited processing capability, and there is a rule update cost\
(tens of 𝜇𝑠, more details in Section 4) to inserting and deleting rules from the flow processing engine, which limits the performance gain for processing small messages with the hardware engine. Thus, we need a hybrid design combining generic NIC cores with a hardware flow processing engine.\
In summary, SmartNIC’s ARM cores can match x86 cores for simple packet processing but are significantly worse for executing a full-featured network stack. The SmartNIC can, however, enjoy the benefits of hardware acceleration if the packet processing can be entirely performed using the flow processing engine.As observed above, unmodified L7 load balancers incur significant overheads when executed on the SmartNIC cores. The SmartNIC cores are significantly worse than the host cores for running a full-featured network stack that relies on the OS kernel to maintain channel state and provide reliable and sequenced delivery channels. We, therefore, develop a streamlined version of the load balancer that is explicitly tailored to the capabilities of SmartNICs.\
We identify three key techniques to effectively use SmartNICs: lightweight network stack, lightweight synchronization for shared data structures, and hardware-accelerated packet processing. The first two techniques streamline the packet-processing logic performed by the load balancers. The third technique takes advantage of hardware flow processing engines available on SmartNICs for accelerating packet processing and rewriting. Figure 3 shows how Laconic integrates the three techniques.\
Lightweight network stack: To avoid the costs associated with the use of a full-featured network stack, we consider an alternate design that utilizes a lightweight packet forwarding stack on the LB and relies on the end-hosts themselves to achieve the desired end-to-end properties. We co-design the application layer load-balancing functions with a streamlined transport layer. Only a subset of the packets have their payloads inspected and modified by the application layer (e.g., client HTTP request packets and HTTP headers on server responses), and the LB’s transport layer is responsible for reliable delivery only for this subset. Laconic relies on the TCP stacks at the server and the client to provide reliability, sequencing, and congestion control for the remaining packets; it performs simple transformations to the packet headers, without needing access to their payloads, to transfer these responsibilities.\
Lightweight synchronization for shared data structures: As we streamline the processing logic, the synchronization costs for concurrent access to shared data structures would limit performance. These synchronization costs are incurred when different cores on a SmartNIC handle a flow’s packets. (On-path SmartNICs eschew using RSS to spread packets across cores evenly. Even when RSS is available, say on off-path SmartNICs, the packets associated with the two different flow directions of a given client-server connection will be handled on different SmartNIC cores.) We, therefore, design highly concurrent connection table management mechanisms for load balancers. We use the semantics of the shared data and the context in which the data structures are accessed to optimize the concurrency control. For instance, some elements of the connection table are initialized during connection setup and never mutated afterward. Other elements, like the expiry timestamp for connection table entries, can benefit from relaxed update semantics.\
Acceleration with Hardware Engine: Our design considers the flow processing engine on off-path SmartNICs as a packet-processing accelerator on which we can offload the load-balancing logic. This allows us to perform the commoncase packet rewriting logic on the hardware accelerators, thus reducing the packet processing burden on the generic ARM cores. However, these hardware accelerators cannot be used for all flows, as adding or removing match-action rules incurs moderate latency and is throughput limited. Laconic uses hardware acceleration only for sufficiently large flows. Laconic also relies on the general-purpose cores to perform packet rewriting while rules are being added and hides the latency of rule deletions with the connection establishment phases of subsequent flows.3.3 Lightweight Network StackOur description focuses on two crucial functionalities of L7 load balancers, as these motivate the processing performed by the load balancer and its associated state.\
 An L7 load balancer can be configured to route an HTTP request by parsing the fields in the HTTP header, such as the URL prefix. L7 load balancers buffer the entire HTTP header and then identify the backend using configured match rules.\
 L7 load balancers update HTTP header requests and responses by adding specific header values. For example, headers protect against XSS or CSRF attacks. Another common usage is to insert the client’s IP address into the request header or a server identifier into a response cookie to enable a consistent level of service per client.\
Laconic’s approach to packet processing is designed to be lightweight. Laconic utilizes a simple forwarding agent that constructs only the necessary packet content to make routing decisions and only buffers modified packet content. In particular, Laconic buffers and processes packets from the client that corresponds to the HTTP requests identifies the appropriate backend servers, and ensures the reliable delivery of these (potentially modified) packets to the server. For packets sent by the server to the client, Laconic performs some simple packet rewriting using a limited amount of state. Crucially, Laconic depends on the client to keep track of which packets it has received, and it modifies ACKs from the client to inform the server about which packets need to be retransmitted. Laconic thus does not maintain any state for these packets, and it relies on client/server TCP logic for reliable end-to-end delivery and congestion control. Figure 4 shows the overall packet flow of our network stack.\
 Each table entry in the connection table maintains a state machine representation of a connection. The connection state is one of the following.\
FRONTESTABLISHED: the client has created a TCP connection with the load balancer, but a backend has not been determined; SYNSENT: The backend has been identified, but the connection hasn’t yet been fully set up; and ESTABLISHED: the connection to the backend has been set up.\
Laconic also maintains a shallow buffer for each connection to buffer the packets received before the backend connection is established. The forwarding agent requires this buffer to construct the HTTP headers and parse information, such as the URL associated with the request.\
At its core, the forwarding agent bridges two TCP connections and simply relays packets after appropriately rewriting the packet and ACK sequence numbers. The forwarding agent has to maintain the mapping between the sequence number spaces of the two connections. Since the load balancer can insert new header fields that will change the sequence number mappings, we maintain an array of insertion points. Each insertion point records two pieces of information: the data offset where content is inserted and the size of the inserted data. For each packet, the load balancer performs a linear scan through the array, computes the total amount of inserted data before the packet, and uses this size value as an offset to adjust the sequence and ACK numbers.\
 Figure 5 demonstrates the entire connection establishment workflow.\
 The load balancer sends an SYNACK packet with a sequence number chosen according to the SYN cookie and the same TCP options as backend servers.\
 Laconic buffers the packets received from the client till it can determine a backend. In particular, the load balancer will buffer client packets until it can reconstruct and parse the header fields, e.g., the hostname and the request URL. A connection table entry is created when the first client packet with payload is received.\
Backend connection setup: After the load balancer has received sufficient client data (e.g., DATA1 in Figure 5), it can determine the backend. It then sends an SYN to the backend and completes the three-way handshake. Laconic records the sequence and ACK numbers in the connection table and then forwards the buffered client packets to the backend server, possibly after modifying any desired header. If the headers were modified, the forwarding agent holds on to the buffers until it receives the ACKs from the server; or else it releases them immediately. In the latter case, it will pass along duplicated ACKs to the client, which will then retransmit the data.\
 From this point, the connections to the client and the backend server are established and bridged. The subsequent packets will be forwarded directly without any buffering, as we will discuss next.\
3.3.3 Packet processing We discuss how the forwarding agent relays packets by appropriately modifying the sequence and ACK numbers. With HTTP 1.1 [17], a persistent connection can convey multiple HTTP requests over the same TCP connection. As a result, content insertion or modification at different points of a TCP flow is required to support the modification of multiple requests.\
Figure 6 shows an example of a TCP connection handled by the load balancer. The sender sends a sequence of requests over multiple packets resulting in a flow of size 1500 bytes to the receiver. Laconic determines two locations where we need to perform content insertions. For simplicity, we designate the sequence number space starting from zero. The insertion locations are at 100B and 1000B (where "B" stands for bytes) when viewed from the sender side and at 100B and 1100B when viewed from the receiver.\
When the load balancer performs an insert, it records the insertion point’s sequence number in the flow’s entire sequence space, as viewed by the sender and the receiver. Further, it splits the original packet at the insertion point and transmits the original packet fragments and the inserted content as separate packets.\
Laconic appropriately rewrites the ACK numbers in the presence of multiple insertions and handles retransmissions of inserted data. There are several cases to consider. If the receiver’s ACK is far beyond the inserted location (i.e., if the ACK number is higher than 1200 +𝑀𝑇𝑈 in our example), we simply use the last offset stored in the connection table and rewrite the ACK using this offset. If the ACK is between two insertion points (i.e., it is in the range of (201 + 𝑀𝑇𝑈 , 1100) in our example), we use the offset of the previous insertion point to calculate the ACK sent to the sender. If the ACK number is exactly at an insertion point and if multiple such duplicate ACKs have been received, then the load balancer retransmits the inserted content. The last case is that the ACK number is exactly after an insertion point (e.g., 201 or 1201 in our example), in which case we suppress the ACK instead of relaying it and triggering duplicate ACK processing on the sender. In the case of packets containing SACKs (selective acknowledgments), Laconic performs the ACK number transformations on every SACK block.\
ACK packets are also used as the signal to garbage collect all the buffers buffered at the load balancer. The load balancer will check the ACKs against the stored buffers and release those that have been ACKed.\
Note that the ACK number transformations ensure that the sender can use duplicate ACKs to detect and retransmit lost packets. The forwarding agent is responsible for the reliable delivery of only the inserted content, with the sender responsible for the reliability of all other content.3.4 Lightweight synchronization for shared dataWe now present the design of efficient synchronization for the load balancer’s data structures, e.g., the connection table. Ideally, we would employ a scheme such as receive-side scaling (RSS), which would allow each NIC core to have exclusive and lock-free access to its shard and avoid sharing data structures across NIC cores. This is hard in the context of both L4 and L7 LBs, as the connection table state would be accessed by traffic from both directions, and RSS would inevitably map the forward and reverse directions to different cores. Thus, Laconic’s connection table needs to provide good scalability with concurrent accesses of per-flow states from multiple NIC cores.\
The load balancer records the states of each flow in a connection table, which is used to select a backend server and update the IP and port attributes in a packet header. At a high level, the flow state recorded by Laconic in the connection table could be categorized into three kinds:\
• Decision of the backend server assignment.\
• Timeout information of connection.\
• TCP state and seq/ack number mapping.\
For each category of state, Laconic uses different strategies in the design of the connection table to accommodate its access pattern.\
 Once the backend is chosen for a specific client connection, the assignment will stick to the connection through its lifetime, meaning that the vast majority of accesses to this state would be read-only. Therefore, Laconic chooses to design its connection table based on Cuckoo hashing [33], which provides predictable and constant-time lookups. Cuckoo hashing achieves this property by limiting an entry’s position to two hashed locations within the table, but it might have to periodically shuffle the entries to accommodate new insertions. We now discuss how we can eliminate the use of locks during the lookup and still achieve correctness. For read operations, Laconic uses a version-counter-based approach to prevent the lookup operations from reading concurrent write operations: before and after Laconic reads the hash table slot with a matching key, a version counter is read from the table entry. A mismatched version number indicates that a concurrent write has been performed on the entry, in which case Laconic will retry the read operation. With this approach, no lock is held during a lookup operation to the connection table. To further improve concurrency, Laconic’s connection table adopts the optimistic locking scheme in [33] to reduce the critical section during entry shuffles triggered by inserts and minimize the amount of time a write operation needs to hold locks.\
 Tracking the last access time of each flow is important for Laconic to correctly time out and remove entries of old flows and reclaim its resources. However, recording the access time for the per-flow state could impact scalability. This is because the entry needs to be updated for every access, effectively making every single access a concurrent update, typically performed while holding a lock. To mitigate the potential inefficiencies caused by such implicit writes, Laconic uses a “blind write” to update the timestamp during the lookup operation. If a later version number check reveals that the record has been updated and is now storing state for a different flow, Laconic will not attempt to revert the update and simply retries the timestamp update. With this approach, Laconic could spuriously update the TTL of a different record, thereby delaying its garbage collection, but it also ensures that a received packet would always bump up the corresponding flow’s TTL.\
TCP state and seq/ack number mapping: For L7 load balancing, Laconic needs to maintain the TCP state (LISTEN, SYN-SENT, ESTABLISHED, etc.) and the mapping of seq/ack number between the client side and server side. However, with the help of RSS, state transitions of the TCP state machine of a single connection will always be performed on the same NIC core. Therefore, Laconic updates the state machine without acquiring any locks. We observe that the access of the seq/ack number mapping follows a consumer/producer pattern, so we use lock-free mechanisms for appending to and reading from this list.3.5 Acceleration with Hardware Engine3.5.1 Capabilities of Flow Processing Engine On offpath SmartNICs, the hardware flow processing engine can significantly accelerate packet processing. Conceptually, it works similarly as a switch with the Reconfigurable MatchAction Table (RMT) architecture. Multiple match-action tables can exist in the hardware, and match-action rules for various packet fields of a packet can be inserted dynamically. We now describe how Laconic can take advantage of these capabilities. We describe two key constructs available in flow processing engines that together aid in offloading packet-processing logic.\
 For NICs equipped with the flow processing engine, DPDK provides a RTE_FLOW interface to insert and delete rules into the NIC. These rules have two fields: a match field and an action field.\
For the match field, the application can specify fields from packet headers to match against, such as IP/MAC address and TCP/UDP port numbers. This allows the application to match packets with specific values for those fields.\
For the action field, applications can specify two types. The first type is fate actions, which determines the packet’s final destination, such as dropping it or moving it to a different queue. The second type includes non-fate actions such as packet counting and rewriting headers, including TCP port, IP address, and sequence numbers. Header updates can change a specific field or add a predetermined constant value. The flow processing engine can also set an expiry time for a rule and notify the application if no packet matches the rule after a certain time.\
 Hairpin queue is a feature first introduced in DPDK 19.11 [12]. It is similar to a loopback interface, but it operates on the network-facing ports of the NIC. The hairpin queues are a pair of connected RX and TX queues.\
: With a combination of on-NIC rules and hairpin queues, we can dramatically reduce the involvement of general-purpose cores in the load-balancing datapath. Specifically, if we were to insert rules to match packets with known flows, specify the appropriate actions to rewrite the packet contents (e.g., the sequence number and ACK number fields), and send it out through a hairpin queue, then most of the packet-processing logic of our network stack can be offloaded to the NIC flow processing engine. Furthermore, latency can be improved since all the forwarding data path is inside the NIC pipeline and bypass computing cores.\
: However, there are two types of challenges for utilizing the flow processing engine.\
The first type is a functional challenge: current SmartNICs do not support a range match operation, which would be useful in the context of Laconic for matching a range of sequence numbers. And the match-action rule can not be performed on the selective acknowledgment (SACK) blocks in the case where a receiver is signaling holes in the received sequence number space.\
The second type is a performance-related challenge: rule insertions and deletions can both be expensive in terms of latency. On the BlueField-2, the cost of a non-blocking rule insertion with batching is 25 microseconds in our tests, whereas the cost of a blocking rule insertion is substantially higher at 305 microseconds. These measurements indicate that rule updates should be performed sparingly, especially in the context of short flows, and that rule insertion costs should be overlapped with other tasks, through the use of non-blocking rule insertions.\
3.5.2 Using Flow Processing Engine We now describe how Laconic uses the flow processing engines.\
What functionalities to offload? We first note that the flow processing engines do not have advanced parsing capabilities. Therefore, request processing logic that operates on content sent from the client to the server (e.g., parsing HTTP headers, requests, etc.) is not offloaded but rather performed on a SmartNIC ARM core.\
Finally, for these offloaded flows, packet rewriting is performed only on packets sent from the server to the client; packets sent from the client to the server, which is a small portion of the traffic load, are handled on the SmartNIC ARM cores. We made this choice based on the limitation that Bluefield-2 cannot rewrite the SACK option in TCP headers. We also observed by experiment that disabling SACK incurs a dramatic loss in TCP flow performance.\
 We now describe the life cycle of flow rules from insertion to deletion. Once a server response exceeds the size threshold, a general-purpose core inserts a rule to match and modify packets sent from the server to the client. This rule matches the TCP 5-tuple corresponding to the server-to-load balancer connection, with the action of updating the sequence and ACK numbers associated with the packet using the correct offsets. This rule insertion is performed using a non-blocking operation so as to reduce the overhead associated with performing the operation. Due to the non-blocking operation, subsequent packets from the server could still be sent to a generic ARM core if the insertion hasn’t been completed, but correctness is ensured as both the ARM core and the flow processing engine are performing the same operation.\
To delete the rules, the SmartNIC cores continuously check the client ACKs as to whether the entire server response has been received or not. ACK packets from clients are continued to be handled by ARM cores instead of the flow processing engines. If the entire response has been received, then the SmartNIC core initiates the process of removing the packet update rule in preparation for having a clean connection for the next client request. The rule deletion operations are first conveyed to a dedicated core, which batches outstanding rule deletions and then performs them synchronously. By delegating this to a dedicated thread and batching such operations, we reduce the overhead of blocking rule updates. Finally, when the next client request is received, the receiving core first checks (and possibly waits) for the condition that the prior rule associated with the connection has been removed before processing the client request.\
Achieve RSS with the engine: We implement a custom RSS using the engine, which greatly simplified the need for handling concurrent access to shared data structures from different cores. Traditional RSS that operates on a flow’s 5-tuple is insufficient as the client to LB, and the server to LB segments correspond to different 5-tuples, and RSS would steer the packets received from the two directions into two different cores. Thus in Section 3.4, we propose lightweight synchronization for shared data. We can eliminate the synchronization mechanisms if packets from a given client-server connection are processed by the same core in either direction of the packet flow.\
Specifically, we pre-install a set of rules to steer incoming packets to cores solely based on the port numbers in the TCP header. We partition the port number space into multiple shards and assign each shard to a processing core. We insert a rule into the flow processing engine for each allocated TCP port number so that client packets with the source port number and server packets with the destination port number are sent to the assigned core. This ensures that packets from the same client-server connection are always processed by the same core. By doing so, we can eliminate the need for synchronization in accessing the flow table, and each core can have its own shard of the connection table. Note that this method is only needed for short flows whose packet processing is not offloaded to the engine, which also helps avoid load imbalance issues that may occur when processing large elephant flows using the same NIC core.]]></content:encoded></item><item><title>Tiny Chips Could Quietly Replace Expensive Cloud Hardware</title><link>https://hackernoon.com/tiny-chips-could-quietly-replace-expensive-cloud-hardware?source=rss</link><author>Load Balancer</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:39:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Tianyi Cui, University of Washington (cuity@cs.washington.edu);(2) Chenxingyu Zhao, University of Washington (cxyzhao@cs.washington.edu);(3) Wei Zhang, Microsoft (wei.zhang.gbs@gmail.com);(4) Kaiyuan Zhang, University of Washington (kaiyuanz@cs.washington.edu).:::tip
Editor's note: This is Part 2 of 6 of a study detailing attempts to optimize layer-7 load balancing. Read the rest below.This section discusses the structure, connectivity, and performance of programmable multi-core SmartNICs that are commonly available. We also describe the load-balancing functionality we want to deploy on these SmartNICs.2.1 Programmable Multi-core SmartNICsWe consider programmable SmartNICs equipped with multicore processors. A typical SmartNIC has onboard memory, DMA engines, and accelerators (e.g., engines for crypto, compression, and packet rewriting) in addition to the multi-core processor. Below, we discuss the two dominant categories, on-path and off-path SmartNICs [35].\
 These are SmartNICs where the NIC processing cores are on the data path between the network port and the host processor (see Figure 1). Consequently, every packet received or transmitted by the host is also processed by the NIC cores. The performance of the NIC cores is critical to the throughput and latency characteristics of the NIC. To address this issue, these NICs typically augment the traditionally wimpy cores on the SmartNIC with specialized hardware support that enhances the packet-processing capabilities of the core. For example, packet contents are prefetched and placed in a structure similar to the L1 cache, and there are hardware mechanisms for managing packet buffers. The SmartNIC has hardware mechanisms for delivering incoming packets to NIC cores in a balanced manner, but there are no mechanisms, such as receive side scaling (RSS), to deliver specific packets to specific NIC cores. Further, the NIC cores can invoke specialized accelerators for tasks such as crypto and compression. Marvell LiquidIO [37] and Netronome NICs [40] are on-path SmartNICs.\
 These are SmartNICs where the NIC’s processing cores are off the data path connecting the host to the network. A NIC-level switching fabric, a NIC switch, provides connectivity between the network port, the host cores, and the NIC cores. The NIC switch is a specialized hardware unit with match-action engines for selecting packet fields and rewriting them based on runtime-configurable rules. The rules route packets received from the network to the host or the NIC, or rewrite them and immediately transmit them back into the network. Mellanox Bluefield [41] and Broadcom Stingray [10] are off-path SmartNICs.\
The off-path SmartNICs thus contain both general-purpose cores and packet match-action engines. The packet-processing logic could thus be split across the cores and the match-action engines based on the complexity of the logic. For instance, the cores can perform complex packet processing and delegate to the match-action engines simpler operations such as packet steering, header-rewriting, and packet forwarding.\
 Datacenter operators are increasingly resorting to offloading various infrastructure functions to SmartNICs and reducing the usage of host cores, which can, in turn, then be rented out and monetized. The ARM cores on SmartNICs are not as powerful as the host x86 cores, but they are an order of magnitude less expensive and result in substantial cost savings [3, 6]. Network virtualization, security, and storage are some of the typical operations offloaded to SmartNICs, and in this work, we seek to expand this to include the infrastructure’s load-balancing function.\
We seek to offload our load-balancing capability on both on-path and off-path load balancers. Others have noted that the communication latencies between the SmartNIC cores and the host cores can be significant in the case of off-path SmartNICs [35], so we target complete offloads for both types of SmartNICs, i.e., all load balancing logic is performed on the SmartNIC cores. Further, this allows us to target even headless SmartNICs with an independent power supply and a carrier card, allowing for a host-less solution (e.g., Broadcom Stingray PS1100R [1]) and enabling substantial reductions to both hardware and operating costs.Load balancers operating at different network layers are widely deployed inside data centers to deliver traffic to services. They fall into two categories: Layer 4 and Layer 7.\
An L4 load balancer maps a virtual IP address (VIP) to a list of backend servers, with each server having its own dynamic IP address (DIP). As the L4 load balancers operate on the transport layer, the routing decision is solely based on the packet headers of the transport/IP layers (i.e., the 5-tuple of IP addresses and ports) but not the payload. Several L4 load balancers are deployed by cloud providers [14, 16, 43]).\
L7 load balancers are significantly more complex and operate on content at the application layer (e.g., HTTP data). Many services inside a data center may share a common application gateway implementing the L7 load-balancing functionality. The L7 load balancer dispatches requests to the corresponding backend servers based on the service requested, e.g., different services are commonly differentiated by the URL [23]. The load balancer would then reassemble the stream and match the URL against various patterns to route the request to the corresponding services.\
It is common for an L7 load balancer to modify the streamed application data, e.g., insert an x-forwarded-for header to inform the backend server of the real IP address of the client. The load balancer may also modify the reply from the server to inject a cookie into the response. Future requests from the same client can then be routed to the same backend server based on the cookie.\
Due to the complex nature and the functionality provided by the L7 load balancer, it is typically implemented as a dedicated application on top of the kernel networking stack, e.g., Nginx [15], Envoy [18], HAProxy [24]. Given the overheads of the OS’s networking stack and the application-level load balancing, the performance of L7 load balancers is an order of magnitude lower than that of L4 load balancers, with 50% to 90% of the processing time spent inside the kernel [25, 31].]]></content:encoded></item><item><title>Cloud Giants Spend a Fortune on Load Balancers—This Research Could Change That</title><link>https://hackernoon.com/cloud-giants-spend-a-fortune-on-load-balancersthis-research-could-change-that?source=rss</link><author>Load Balancer</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:38:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Tianyi Cui, University of Washington (cuity@cs.washington.edu);(2) Chenxingyu Zhao, University of Washington (cxyzhao@cs.washington.edu);(3) Wei Zhang, Microsoft (wei.zhang.gbs@gmail.com);(4) Kaiyuan Zhang, University of Washington (kaiyuanz@cs.washington.edu).:::tip
Editor's note: This is Part 1 of 6 of a study detailing attempts to optimize layer-7 load balancing. Read the rest below.Load balancers are pervasively used inside today’s clouds to scalably distribute network requests across data center servers. Given the extensive use of load balancers and their associated operating costs, several efforts have focused on improving their efficiency by implementing Layer-4 load-balancing logic within the kernel or using hardware acceleration. This work explores whether the more complex and connection-oriented Layer-7 load-balancing capability can also benefit from hardware acceleration. In particular, we target the offloading of load-balancing capability onto programmable SmartNICs. We fully leverage the cost and energy efficiency of SmartNICs using three key ideas. First, we argue that a full and complex TCP/IP stack is not required for Layer-7 load balancers and instead propose a lightweight forwarding agent on the SmartNIC. Second, we develop connection management data structures with a high degree of concurrency with minimal synchronization when executed on multi-core SmartNICs. Finally, we describe how the load-balancing logic could be accelerated using custom packet-processing accelerators on SmartNICs. We prototype Laconic on two types of SmartNIC hardware, achieving over 150 Gbps throughput using all cores on BlueField-2, while a single SmartNIC core achieves 8.7x higher throughput and comparable latency to Nginx on a single x86 core.Load balancers are a fundamental building block for data centers as they balance the service load across collections of application servers [38, 45, 46]. Load balancers were initially built as specialized hardware appliances but are now typically deployed as software running on commodity servers or VMs. This deployment model provides a greater degree of customizability and adaptability than the older hardwarebased designs, but it also can result in high costs for cloud providers and application services, given the purchase costs and the energy consumption of general-purpose servers [8]. Application services often go to great lengths to consolidate and reduce their use of load balancers to obtain desired cost savings [2, 4, 5].\
Given the extensive use and cost of load balancers, several efforts have focused on improving their efficiency, especially Layer-4 (L4) load balancers, by embedding the load balancing logic in a lower, possibly hardware-accelerated, layer. Katran [16] is accelerated using eBPF code inside the Linux kernel, thus intercepting and processing packets within the kernel and minimizing the number of transitions to userlevel load-balancing code. ClickNP [32] tackles some aspects of the L4 load balancing logic (especially NAT-like capabilities) on an FPGA-enabled SmartNIC and exploits the parallel processing capabilities of FPGA devices. SilkRoad [38] uses a combination of a programmable switch and an end-host to store the state associated with L4 load balancers and perform the dataplane transformations related to the load-balancing operation within a switch pipeline.\
While these efforts have made considerable gains in optimizing L4 load balancing that balances traffic at the network layer, data center services often rely on application-layer load-balancing capabilities found only in Layer-7 (L7) load balancers. In particular, services would like to route flows based on the attributes of the client request, preserve session affinity for client requests, provide access control, and so on [7]. But, these features make it harder for L7 load balancers to adopt the hardware-acceleration techniques used for L4 load balancers. A fundamental challenge is that the L7 load-balancing operation is based on information embedded within connection-oriented transport protocols, thus seeming to require a full-stack network processing agent on the load balancer to handle TCP/HTTP connections. Consequently, today’s L7 load balancers are generic software solutions incurring high processing costs on commodity servers.\
In this work, we examine whether we can improve the efficiency of L7 load balancers using programmable networking hardware. We focus on SmartNICs that provide general-purpose computing cores augmented with packet-processing hardware. SmartNICs are particularly attractive targets as their compute cores can host arbitrary protocol logic while their packet-processing accelerators can perform dataplane transformations efficiently. A SmartNIC thus combines the capabilities of traditional host computing with the emerging capabilities of programmable switches and is an appropriate target for L7 load balancers. Our work is also partly motivated by the increasing deployment of SmartNICs within data centers as a cost-effective and energy-efficient computing substrate for networking tasks.\
Several challenges have to be addressed in offloading loadbalancing functionality to SmartNICs. First, SmartNIC cores are wimpy, equipped with limited memory, and inefficient at running general-purpose computations. To the extent possible, we should use lightweight network stacks instead of generic, full-functionality stacks present inside OS kernels. Second, efficient multi-core processing on the SmartNICs presumes lightweight synchronization for access to concurrent data structures, and this is particularly relevant as we slim down the network processing logic. Third, the effective use of accelerators for packet transformations is necessary to enhance the computing capability of SmartNICs.\
We design and implement , a SmartNIC-offloaded load balancer that addresses the challenges raised above. A key component of our system is a lightweight network stack that represents a co-design of the application-layer load-balancing logic with the transport layer tasks. This lightweight network stack performs complex packet processing only on a subset of the packets transmitted through the load balancer. For the rest of the packets, the network stack performs simple rewriting of packets and relies on the client and the server to provide end-to-end reliability and congestion control. For SmartNICs with packet-processing accelerators, this design allows most of the packets to be processed using hardware-based flow-processing engines, thus providing significant efficiency gains. We also develop connection management data structures that are highly concurrent and minimize expensive mutual exclusion operations. We note that some of our design contributions also apply to a generic server-based design, but they have a multiplicative effect on SmartNICs by factoring out a fast path that can be executed on the hardware packet engines.\
We built Laconic and tailored it to two different types of SmartNICs: Marvell LiquidIO3 and Nvidia BlueField-2. Laconic provides both Layer-4 and Layer-7 functionality and implements commonly used Layer-7 interposition logic for balancing connections to backend services. For large messages, Laconic running on BlueField-2 with one single ARM core can achieve up to 8.7x higher throughput than widely-used Nginx running on a more powerful x86 core. For small messages, Laconic on BlueField-2 can achieve higher throughput with comparable or even lower latency compared with Nginx. On LiquidIO3, the throughput of Laconic is 4.5x higher compared with x86 Nginx. We also demonstrate the Laconic performance with real-world workload and present detailed microbenchmarks on the benefits of key ideas.]]></content:encoded></item><item><title>&apos;The Unicorn Boom Is Over, and Startups Are Getting Desperate&apos;</title><link>https://slashdot.org/story/25/02/14/1541213/the-unicorn-boom-is-over-and-startups-are-getting-desperate?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[More than $1 trillion in value remains locked in venture-backed startups with dwindling prospects as the Silicon Valley unicorn bubble deflates, according to a new Bloomberg Businessweek report. Of the 354 companies that reached billion-dollar valuations in 2021, only six have completed initial public offerings, Stanford Business School professor Ilya Strebulaev said. 

Four others went public via SPACs and 10 were acquired, some below their unicorn status. Several prominent startups have already collapsed, including indoor farming firm Bowery Farming and AI healthcare company Forward Health. Freight business Convoy, valued at $3.8 billion in 2022, shut down last year with rival Flexport buying its assets at a steep discount.]]></content:encoded></item><item><title>AI Is Mapping Hidden Connections—And It’s Just Getting Started</title><link>https://hackernoon.com/ai-is-mapping-hidden-connectionsand-its-just-getting-started?source=rss</link><author>Andrei</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:00:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In this fifth and final post of our series on link prediction using Neptune ML, we’re diving into the inference process: setting up an inference endpoint to use our trained GNN model for link predictions. To establish the endpoint, we’ll use the Neptune cluster’s API and the model artifacts stored in S3. With the endpoint live, we’ll query it for link prediction, using a Gremlin query to identify high-confidence potential connections in our graph.\
By now, we have already loaded the Twitch social networking data into the Neptune cluster (as described in Part 1 of this series), exported the data using the ML profile (check out Part 2 for details), preprocessed the data (as explained in Part 3), trained the model (see Part 4), and now we're ready to use the trained model to generate predictions.Let's create the inference endpoint using the cluster's API and the model artifacts we have in S3. As usual, we need an IAM role that has access to S3 and SageMaker first. The role must also have a trust policy that allows us to add it to the Neptune cluster (the trust policy can be found in Part 3 of this guide). We also need to provide access to SageMaker and CloudWatch API from inside the VPC, so we need the VPC endpoints as explained in Part 1 of this series.\
Let's use the cluster's API to create an inference endpoint with this curl command:curl -XPOST https://(YOUR_NEPTUNE_ENDPOINT):8182/ml/endpoints \
  -H 'Content-Type: application/json' \
  -d '{
          "mlModelTrainingJobId": YOUR_MODEL_TRAINING_JOB_ID,
    "neptuneIamRoleArn": "arn:aws:iam::123456789012:role/NeptuneMLNeptuneRole"
      }'
We can use '' and '' parameters to choose the EC2 instance type that will be used for link prediction, and deploy more than one instance. The full list of parameters can be found here. We'll use the default '' instance as it has enough CPU and RAM for our small graph, and this instance type was recommended in inferrecommendation.json that was generated after model training in our previous stage:{
    "disk_size": 12023356,
    "instance": "ml.m5d.xlarge",
    "mem_size": 13847612
}
The API responds with the inference endpoint ID:{"id":"b217165b-7780-4e73-9d8a-5b6f7cfef9f6"}
Then we can check the status of the inference endpoint with this command:curl https://YOUR_NEPTUNE_ENDPOINT:8182/ml/endpoints/INFERENCE_ENDPOINT_ID?neptuneIamRole='arn:aws:iam::123456789012:role/NeptuneMLNeptuneRole'
and once it responds with '' like this,{
  "endpoint": {
    "name": "YOUR_INFERENCE_ENDPOINT_NAME-endpoint",
    "arn": "...",
    "status": "InService"
  },
  "endpointConfig": {...},
  "id": "YOUR_INFERENCE_ENDPOINT_ID",
  "status": "InService"
}
it means we're ready to start using it with database queries.The endpoint can also be viewed and managed from the AWS console, under SageMaker -> Inference -> Endpoints.Let's use the endpoint to predict new 'follow' links in the graph. In order to do that, we need to choose the source vertex of the possible new links.It helps if the source vertex already has links, so we’ll use this query to get the vertex with the highest number of existing links (outE and inE connections):g.V()
 .group()
   .by()
   .by(bothE().count())
 .order(local)
   .by(values, Order.desc)
 .limit(local, 1)
 .next()
which means that vertex with ID = 1773 has 1440 connections (720 inE and 720 outE).To get the number of edges that start at node 1773 and the number of edges that end at that node we can use these queries:g.V('1773').outE().count()
g.V('1773').inE().count()
The same number of inE and outE connections is expected because the initial dataset contains mutual friendships, and we augmented the data with reverse edges to make it work with Neptune directed graph.\
Now let's get the predicted connections. To do that, we'll run a Gremlin query with Neptune ML predicates. We'll use the inference endpoint and the SageMaker role which was added to the DB cluster to get the users that user 1773 might follow with a confidence threshold (minimum likelihood of link existence according to the model) of at least 0.1 (10%), while excluding the users that user 1773 is already following:%%gremlin

g.with('Neptune#ml.endpoint', 'YOUR_INFERENCE_ENDPOINT_NAME')
 .with('Neptune#ml.iamRoleArn', 'arn:aws:iam::123456789012:role/NeptuneMLSagemakerRole')
 .with('Neptune#ml.limit', 10000)
 .with('Neptune#ml.threshold', 0.1D)
 .V('1773')
 .out('follows')
 .with('Neptune#ml.prediction')
 .hasLabel('user')
 .not(
  __.in('follows').hasId('1773')
 )
"Result"
"v[755]"
"v[6086]"
"v[6382]"
"v[7005]"
\
According to our model, there's at least 10% chance that user 1773 will follow each of these 4 users. Perhaps we can improve the model by increasing the number of training jobs to at least to 10 as recommended by AWS, and then compare the performance of the resulting model and the predicted links. In real applications, adding user profile and activity data to the graph also improves prediction accuracy.\
Predicted connections in social networking datasets can be used to provide personalized recommendations like friend and content suggestions, enhancing user engagement and community growth, reducing churn, and providing additional data for targeted advertising.\
Although we applied link prediction to a social networking dataset, it's just one of many possible applications of this technology. From recommendation engines to network optimization, link prediction is a versatile tool that brings value by uncovering hidden relationships within data. As graph-based applications continue to expand, the potential of link prediction across industries promises new insights, efficiency, and enhanced user experiences.]]></content:encoded></item><item><title>Call for speakers: TechCrunch All Stage 2025</title><link>https://techcrunch.com/2025/02/14/call-for-speakers-techcrunch-all-stage-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Founders, VCs, and startup experts — this is your moment! Got scaling insights? Now’s your chance to share them with 1,200 founders, investors, and entrepreneurs at TechCrunch All Stage 2025 presented by Fidelity, on July 17 in Boston! We’re gathering leading experts from the startup and VC communities to host engaging sessions, interactive roundtables, and […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Apply to speak at TechCrunch Sessions: AI</title><link>https://techcrunch.com/2025/02/14/apply-to-speak-at-techcrunch-sessions-ai/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Fri, 14 Feb 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AI innovators, this is your moment! Have insights to share with 1,200 AI founders, investors, and enthusiasts eager to push the boundaries of innovation?  Take the stage, shape the AI conversation, and exchange ideas at TechCrunch Sessions: AI on June 5 at UC Berkeley’s Zellerbach Hall!  We’re bringing together top AI minds from the startup […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Linux 6.15 To Ensure PlayStation 5 Controllers Use The Correct Driver</title><link>https://www.phoronix.com/news/Linux-6.15-Ensures-PS5-Driver</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 15:49:09 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A change queued up by an Amazon engineer ahead of the upcoming Linux 6.15 kernel cycle will ensure that PlayStation 5 controllers on Linux load with the correctly desired driver...]]></content:encoded></item><item><title>AngelQ&apos;s Mission to Build Safer AI Technology</title><link>https://hackernoon.com/angelqs-mission-to-build-safer-ai-technology?source=rss</link><author>sarahevans</author><category>tech</category><pubDate>Fri, 14 Feb 2025 15:28:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Imagine a child asks an AI chatbot about feeling sad. Without knowing the user's age, the AI could respond with an unnuanced discussion of depression and self-harm – content that might be appropriate for an adult but potentially harmful for a young person. While this scenario is hypothetical, it illustrates a fundamental problem with today's AI systems: they can't tailor their responses based on a user's age.  and  are addressing this critical gap with their KidRails for LLMs framework, creating AI that can respond appropriately to children.\
"Historically technology has not been designed with kids in mind - that's certainly been the case with the internet and internet connected devices - the same pattern holds for AI," says Josh Thurman, Co-Founder and Head of Product at AngelQ. "When it comes to AI these negative consequences will undoubtedly be grossly amplified without appropriate guardrails in place."\
Recent incidents highlight this concern. Cases of AI systems providing dangerously inappropriate responses to children have emerged, from  allegedly contributing to a Florida teenager's suicide in 2023 to  providing inappropriate relationship advice to users it believed to be minors. These troubling cases underscore the limitations and potential dangers of applying adult-focused technology to young users. "We're releasing KidRails as a starting point for reversing the trend of technologies that are harmful to children," says Thurman. "We have to get this right in the AI age - the stakes are extremely high."\
Understanding Child Development in AI Design"Creating truly child-safe AI requires a deep understanding of developmental psychology and education," explains Tim Estes, CEO and Co-Founder of AngelQ. "A six-year-old and an eleven-year-old don't just process information differently, they have entirely different needs, interests, and safeguards required. KidRails was engineered from the ground up to recognize these crucial differences and adjust its interactions accordingly."\
The framework employs a tiered approach that adapts responses based on a child's age and developmental stage. It can provide age-appropriate answers, offer limited responses with parental deference, or completely defer to parents for sensitive topics. As Estes notes, "Sometimes the best approach for AI is to know when not to say something."\
A Partnership Built on Shared Values\
The collaboration between AngelQ and Arcee AI emerged from aligned priorities about children's safety in technology. "Mark and I first connected at the beginning of 2024, and from our very first conversation, it was immediately apparent that we shared a fundamental belief: technology for children was very important and there was an opportunity to do innovative work in making Language Models safer and more age appropriate in their function," says Estes.\
By making KidRails open-source, the companies aim to establish new standards for child-safe AI interactions. The release includes training data, evaluation criteria, and complete processes for fine-tuning – all under the Apache 2 license. This transparency invites broader community participation in improving how AI systems interact with children.\
Addressing Core Challenges\
"The fundamental challenge in building a child-focused LLM is the decisions on what to say at different ages," Estes explains. "That's really a decision process and it's why we are open sourcing the work- because of transparency and inviting a broader community to help us improve those judgments."\
Unlike traditional approaches that add safety filters to existing models, KidRails integrates parent involvement into its core functionality. The system creates opportunities for parent-child discussions by redirecting certain questions to parents, ensuring that "AI enhances, rather than replaces, the critical role parents play in their children's development and digital literacy," says Estes.\
The development process involved carefully considering how children interact with AI systems. The team generated and curated over 250,000 child conversations to create representative training data, which was used to fine-tune a Llama 3.1 8B base model. This approach ensures responses are not only safe but also appropriate for different developmental stages."We envision KidRails as more than a tool; it's a demonstration that AI can be fundamentally reimagined with children as the primary users, setting a new precedent for the entire industry," says Estes. "We want children to be able to explore and learn in an environment that preserves the natural curiosity and wonder of childhood, while giving parents the confidence that comes from knowing their children are engaging with technology built specifically for them… not adapted adult technology with safety filters added as an afterthought."\
By making the framework open-source and encouraging collaboration, AngelQ and Arcee AI are working to transform how the industry approaches AI development for young users. As more companies adopt and build upon the KidRails framework, their vision of creating AI systems that truly prioritize children's needs moves closer to reality.]]></content:encoded></item><item><title>How a Computer That &apos;Drunk Dials&apos; Videos is Exposing YouTube&apos;s Secrets</title><link>https://news.slashdot.org/story/25/02/14/1521239/how-a-computer-that-drunk-dials-videos-is-exposing-youtubes-secrets?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 15:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: How many YouTube videos are there? What are they about? What languages do YouTubers speak? As of 14 February 2025, the platform's will have been running for 20 years. That is a lot of video. Yet we have no idea just how many there really are. Google knows the answers. It just won't tell you. 

Experts say that's a problem. For all practical purposes, one of the most powerful communication systems ever created -- a tool that provides a third of the world's population with information and ideas -- is operating in the dark. In part that's because there's no easy way to get a random sampling of videos, according to Ethan Zuckerman, director of the Initiative for Digital Public Infrastructure at the University of Massachusetts at Amherst in the US. You can pick your videos manually or go with the algorithm's recommendations, but an unbiased selection that's worthy of real study is hard to come by. 

A few years ago, however, Zuckerman and his team of researchers came up with a solution: they designed a computer program that pulls up YouTube videos at random, trying billions of URLs at a time. You might call the tool a bot, but that's probably over selling it, Zuckerman says. "A more technically accurate term would be 'scraper'," he says. The scraper's findings are giving us a first-time perspective on what's actually happening on YouTube. 

[...] The first question was simple. How many videos have people uploaded to YouTube? [...] Zuckerman and his colleagues compared the number of videos they found to the number of guesses it took, and arrived an estimate: in 2022, they calculated that YouTube housed more than nine billion videos. By mid 2024, that number had grown to 14.8 billion videos, a 60% jump.]]></content:encoded></item><item><title>Tether vs. the US Military. Who Will Win?</title><link>https://hackernoon.com/tether-vs-the-us-military-who-will-win?source=rss</link><author>Mark Helfman</author><category>tech</category><pubDate>Fri, 14 Feb 2025 15:01:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Tether’s stablecoin, USDT, almost single-handedly holds the crypto market together. At $140 billion, with tentacles in all crypto apps and exchanges, it’s a big fuss. Anything that tanks Tether will tank the entire crypto market.\
Critics call it the biggest crime scene in financial history, a Ponzi scheme of unfathomable proportions, and the last bastion of terrorists, drug syndicates, and money launderers.\
USDT is a mirror image of the US dollar. A copycat. An imitation.\
It exists because most of the world can’t get “real” dollars, but they can get a digital representation of the same.The experts say that’s nonsense. The US dollar is worth something because the US government says so.\
That government has a military to enforce its laws and treaties. Aircraft carriers, for example.Experts say these things back the US dollar.\
Does that military make the US dollar better than USDT?\
You’d have to hope so, because US taxpayers spend almost $1 trillion on that military. They spend another $36 trillion on the “full faith and credit” that backs every dollar.\
Is that better than whatever Tether offers?Choose whether the statement applies to Tether, the US military, or both.\
Will collapse if the US dollar fails.\
Tether loses a profit center but has non-USD stablecoins to fall back on and can create more products if necessary. While its reserves will collapse, so will the need for reserves (because the US dollar will be worthless, therefore redeemable for nothing).\
Has never passed an audit.\
Tether has never even  an audit.\
The US military has failed all its audits. In fact, for its most recent audit, the US military passed only 7 of 29 categories and couldn’t account for 61% of its spending.\
The world order would fall apart if it disappeared.\
If Tether disappeared, people would use a different USD stablecoin.\
Responsible for the deaths of millions.\
We don’t know how many people Tether has killed. I’m guessing it’s a lot less than millions. Probably 0.\
Props up dictators and autocrats?\
The US military, for sure. We know this because it’s public information and part of US foreign policy.\
Tether? We don’t know, but you’d have to assume so. North Korea used USDT as part of its FDI-as-theft fundraising policy. Does that count? If not, we have to assume other despots used USDT for something or other.\
Responsible for the overthrow of sovereign governments?\
Tether doesn’t mess with that.That may sound absurd. Shall we look at some ways they’re alike? run by one guy who is unaccountable to the public and does whatever his boss (the President) tells him to do. run by three guys who are unaccountable to the public and do whatever their customers tell them to do. vital for the security of Western governments and their friendliest partners. vital for the liquidity of global financial networks and the people who use them. a secretive, opaque entity that does terrible things with US dollars in pursuit of US policy outcomes. a secretive, opaque entity that does terrible things with US dollars in pursuit of profits.\
Tether and the US military dominate their respective fields, get a lot of support and praise, and face withering, sometimes-valid criticism. They both expand the power, usage, and demand for US dollars.There is, however, a big difference that I didn’t cover above.\
Tether has enough money to back its token.\
The US military has no money. It gets funding from the US government, which hasn’t had enough money to back its dollars since the 20th century. Despite having the largest economy in the history of mankind, it falls $1 trillion short of its budget obligations each year. It depends on speculators, taxpayers, and central bank interventions for its solvency.\
Which one should you worry about more?]]></content:encoded></item><item><title>Airbnb CEO says it’s still too early for AI trip planning</title><link>https://techcrunch.com/2025/02/14/airbnb-ceo-says-its-still-too-early-for-ai-trip-planning/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 14 Feb 2025 14:57:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Airbnb is planning to first introduce AI to its customer support system. © 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Nvidia Delays the RTX 5070 Till After AMD&apos;s Reveal</title><link>https://slashdot.org/story/25/02/14/149207/nvidia-delays-the-rtx-5070-till-after-amds-reveal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 14:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: As always, the most important Nvidia graphics card is the one you can actually buy, and Nvidia's talked a big game for its RTX 5070, making the dubious but nuanced claim it can deliver RTX 4090 performance for just $549. On February 28th, AMD will get its chance to intercept with the Radeon RX 9070 and 9070 XT, in a streaming event it just announced today. But Nvidia has now made its own wiggle room, delaying the launch of the RTX 5070 from February to March 5th, its product page reveals today. Nvidia will ship its $749 RTX 5070 Ti ahead of AMD's event, though, on February 20th, a week from today.]]></content:encoded></item><item><title>ICON, a pioneer in the 3D printing of homes, raises $56M led by Norwest, Tiger Global</title><link>https://techcrunch.com/2025/02/14/icon-a-pioneer-in-3d-home-printing-raises-56m-led-by-norwest-tiger-global/</link><author>Mary Ann Azevedo</author><category>tech</category><pubDate>Fri, 14 Feb 2025 14:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[ICON, which builds homes using 3D printing, has closed on $56 million in Series C funding co-led by Norwest Venture Partners and Tiger Global, the company has confirmed to TechCrunch exclusively. The raise represents a first close for the Austin-based ICON, according to a spokesperson. Existing backers CAZ Investments, LENX, Modern Ventures, Oakhouse Partners, Overmatch […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>How AI Will Disrupt Outsourced Work</title><link>https://slashdot.org/story/25/02/14/1359228/how-ai-will-disrupt-outsourced-work?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AI startups are poised to disrupt the $300 billion business process outsourcing (BPO) industry, as advances in language models and voice technology enable automation of tasks traditionally handled by human workers. 

The BPO market, which reached $300 billion in 2024 and is projected to hit $525 billion by 2030, faces mounting pressure from AI companies offering faster, more scalable alternatives to manual processing of customer support, IT services and financial claims, venture capital firm a16z wrote in a thesis post. Early AI implementations have shown promising results, with customer service startup Decagon reporting 80% resolution rates and improved satisfaction scores. In healthcare, AI company Juniper said its clients saw 80% fewer insurance claim denials and 50% faster processing times. 

Major BPO providers are responding to the threat, with Wipro reporting a 140% increase in AI adoption across projects and Infosys deploying over 100 AI agents. However, industry analysts say BPOs face structural challenges in transitioning from their labor-based business model to AI-first operations. The shift threatens traditional BPO companies like Cognizant, Infosys and Wipro, which reported revenues between $10-20 billion in their latest fiscal years.]]></content:encoded></item><item><title>Fwupd 2.0.6 Adds Support For HPE Gen10/Gen10+ Servers</title><link>https://www.phoronix.com/news/Fwupd-2.0.6-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 13:52:56 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Fwupd 2.0.6 is out today as the newest update to this widely-used open-source solution for system and peripheral device firmware updating under Linux...]]></content:encoded></item><item><title>Is That 0.05% Extra Yield Really Worth the Risk of Staking Your Crypto?</title><link>https://hackernoon.com/is-that-005percent-extra-yield-really-worth-the-risk-of-staking-your-crypto?source=rss</link><author>maxo1st</author><category>tech</category><pubDate>Fri, 14 Feb 2025 13:00:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Congratulations, you get 0.05% yield more and farm some points! Sounds great? Sure – until you realize that you’ve just staked risks across multiple layers without even knowing it. But you can ignore that – just like everybody else – until you can’t.The Basics: Staking, Liquid Staking, Restaking - What The … ?Risks - Building a House of Cards on EthereumThe Renzo ezETH Crash: A WarningCan the additional APY truly compensate for the risk? Not really.Even Vitalik Is Cautious About RestakingThe DAO Hack: A Lesson from Ethereum’s Darkest HourConclusion: Innovation Needs TransparencyIt’s spring 2024, and we all think we have to use our ETH to farm EigenLayer Points - the feeling of FOMO is simply overwhelming. These EigenLayer points are the ultimate meta to qualify for the EigenLayer airdrop, so other protocols also jump on this bandwagon and promise airdrops [1][2]. To farm these points, everybody starts to liquid-stake their ETH, then restake it, then restake it again on another platform. Preferably you do this several times to qualify for more potential airdrops like Etherfi, KelpDao, or Renzo. At this time, very few people asked themselves what exactly they were doing and what risks this game-changing innovation may pose for them or DeFi or Ethereum. But let's start at the beginning with a brief explanation of what I am talking about.The Basics: Staking, Liquid Staking, RestakingBefore trying to explain the risks, it is important to understand what these essential terms actually mean.\
 involves locking up ETH on a Proof-of-Stake blockchain like Ethereum to secure the network and earn an annual APY of 3–4%, similar to a term deposit at a bank [3].\
Staking has the advantage over Proof-of-Work blockchains in that it is much more energy-efficient and also more inclusive, as you can quickly participate in staking without specific hardware. However, the disadvantage is that you have no access to the assets while staking and there are sometimes lock-up periods.\
 provides a remedy. It allows users to stake ETH through protocols like Lido and receive a tokenized representation (e.g., stETH), enabling them to earn staking rewards while maintaining the liquidity to interact with DeFi [4]. They can now use this as collateral or simply swap it for other tokens. The advantage is the liquidity of their assets despite staking and thus a contribution to the security of the blockchain.\
The last innovation that is now causing the most headaches is restaking. The concept is new and I think it's the most abstract topic:\
 allows users to leverage their already staked ETH without having to unstake it. While continuing to secure Ethereum’s network, restaked ETH also supports the security of other protocols (Actively Validated Services, or AVS) like bridges, oracles, or other dApps, earning additional APY of at least around 3% upwards [5].\
Now that the basics have been explained, the next ‘logical’ step that we could observe in DeFi, was that the restaked ETH must also be liquid again, obviously… And so, of course, several protocols immediately emerged that made this their mission. And so everyone started to wrap and package the ETH in more and more layers, as the yield keeps increasing with each layer.\
In general, the technology behind restaking is innovative and offers new opportunities. For the many users, it remains a no-brainer: more yield for the same ETH. However, this does not take into account that with each additional layer, new risk is piled up, which makes the additional yield possible in the first place. But how big is this risk? Could it be a ticking time bomb, an actual systemic risk for Ethereum and the whole of DeFi?Risks – Building a House of Cards on EthereumSlashing – the OG staking riskLet's start with the first ‘layer’ of the risks, where you stake your ETH the traditional way to secure the network and achieve a yield of 3-4% (as of February 2025) [3]. The only risk here is the risk of Slashing. This is Ethereum’s mechanism for keeping validators honest; break the rules (e.g., proposing conflicting blocks, too much validator downtime, etc.), and you lose part of your staked ETH and are thrown out of the network [6]. Slashing is very rare, as there are only minimal to no incentives compared to the severe punishment for the validator operators, and as of February 2024, less than 0.04% of all validators have been slashed. To be specific, only 414 validators out of approximately 1,174,000 active validators [7]. To become a validator and take the step of locking up your ETH, you need at least 32 ETH, which is at the same time the upper limit per validator.Too Big to Fail? - The Centralization Dilemma in DeFiThe 32 ETH are too expensive for many users, so they resort to liquid staking, where you use a platform to stake, such as Lido [4]. Lido allows you to stake your ETH via their validators and then receive an equivalent representation of it, namely stETH. This is relatively unproblematic, as the user can always exchange their stETH for their ETH. However, the current situation in DeFi is somewhat more problematic, as Lido is dominating the market share and has a total of 27.58% (12. February 2025, chart below) of all staked ETH under the control of its validators [8].\
This currently equates to almost 9.35 million ETH [9]. As I said before, only 32 ETH can be staked per validator, which fortunately means Lido cannot stake all the ETH in one validator. However, I probably don't need to explain here that Lido, as a centralized unit, still has a hell of a lot of power, and you don't even want to imagine what would happen if Lido made a mistake one day. The concentration risk is huge. This is particularly evident in the case of misconfigurations, such as an incident in October 2023 in which around 20 nodes operated by Lido were slashed [10]. It is precisely this centralization issue that might be replicated in restaking protocols. Let's take the largest restaking protocol Eigenlayer for instance. Eigenlayer currently has a TVL of $12.3 billion, at its maximum in June 2024 was almost $20 billion [11]. At the time of writing, around 10% of the total staked ETH is restaked on EigenLayer [12]. In Lido and EigenLayer, we see a high concentration and centralization risk for the fact, that we are pursuing the mission of decentralization in DeFi and crypto. This entails a high potential risk with single points of failure bundled in Lido and EigenLayer. Thus a single mistake can lead to slashing or liquidation cascades.Restaking, like any brand new innovation, is a new technology that uses unproven and new infrastructure, tech, and code that still need to be tested. One technical error could result in an ultimate super disaster where an entire restaking protocol goes down. The smart contract risk in innovation is unfortunately relatively high. This risk is then combined with the fact that in restaking slashing can occur not only in one validator but also within a restaking protocol like Eigenlayer. The validator must now adhere to the rules of the Ethereum blockchain to stay online and also adhere to the rules of EigenLayer [13]. If it makes a mistake on Ethereum, the validator goes offline, it is slashed and, logically, it also goes offline on EigenLayer. If it makes a mistake on the EigenLayer Protocol, its AVS rewards are canceled, but at least the validator remains online on Ethereum if it has not also violated the Ethereum rules.\
Without a doubt, restaking is innovative and opens up many opportunities, but still, we have to take into consideration that with the possibility of restaking ETH again and again we also add risks again and again. So the risks I just described can be replicated as often as the ETH is restaked. So slashing one Ethereum validator could trigger a cascade of slashing events across restaking protocols and the going offline of many validators, depending on how often the ETH was restaked. In addition, the liquid restaked ETH is also used in DeFi dApps for lending and borrowing, for example, and used as collateral. Each new layer piles on risks like slashing and liquidity cascades, setting up chain reactions most users don’t even notice—until the whole house of cards comes crashing down, which happened for example in the Renzo ezETH crash.The Renzo ezETH Crash: A WarningOn 24 April 2024, the depeg (loss of price matching to the price of the asset that is supposed to represent the liquid version of; in this case, the price of ezETH to ETH) of ezETH of the Renzo protocol occurred. The price of this version of liquid restaked ETH temporarily fell to just $688 [14]. At the time, Renzo was the largest liquid restaking protocol behind Etherfi, as they just announced their airdrop and so their TVL rose 126% to $3.3 billion within a month [14]. The reason for this depeg was due to the end of the airdrop phase which led to a huge sell-off [15]. (Airdrop metas is a topic which I will address in another article) This led to incredible liquidation cascades, for example on Morpho or Gearbox with hundreds of liquidated users. It is estimated that a total value of between $56 million and $340 million was lost through liquidations that day [14][16]. This emphasizes the risks listed above, which are accepted even though they are not properly priced into the APYs in some cases.Many users completely underestimate this risk or are not aware of it. I am not saying that users are not properly informed and not doing their research. However, the barriers to understanding and the explanations are sometimes so abstract that a user who does not deal with it professionally is not sufficiently informed in my opinion. I'll pick up on a statement from my last post in which I claimed that some products are simply becoming too complex and make onboarding more difficult than it has to be [17]. With liquid staking, a user can continue to trade with their ETH and achieve an additional APY of up to 2% through borrowing and lending protocols (with single asset exposure to stETH, for example) [18]. The risk here is relatively moderate as long as the user does not borrow against his collateral and risks being liquidated (which is entirely up to him). However, if we go back to deeper layers, such as in restaking, then the user uses the same stake of ETH multiple times to leverage his exposure. This increases the risks dramatically due to the continued nesting of synthetic and derivative ETH to remain liquid, while the APY only increases marginally and is no longer in relation to this risk taken on. The user almost completely relinquishes actual control at this level of risk and is not sufficiently rewarded for these risks. To emphasize my point, here is a quote from Marcin Kazmierczak, Co-founder & COO of Redstone:“With some restaking protocols offering APYs of 15-20% on assets like ETH, there's a significant risk of investors chasing yields without fully understanding the associated risks.” [19]Even Vitalik Is Cautious About RestakingVitalik Buterin, the father of Ethereum, has a critical view on restaking himself and expresses concern. He naturally promotes innovation around the Ethereum ecosystem, which is understandable and good, but says himself:“We should be wary of application-layer projects taking actions that risk increasing the ‘scope’ of blockchain consensus to anything other than verifying the core Ethereum protocol rules.“ [20]\
I believe many personalities within crypto and Web3 agree with this and share this opinion, as you can read in many articles. Ethereum shines because it is a trustworthy blockchain and efforts are being made to keep it that way. After all, there has already been the DAO debacle in 2016, a repetition that should be prevented at all costs in which Ethereum had to be forked. Vitalik Buterin says in his blog post:“If, on the other hand, you have the intent to rope in the broader Ethereum ecosystem social consensus to fork or reorg to solve your problems, this is high-risk, and I argue that we should strongly resist all attempts to create such expectations.”[20]The DAO Hack: A Lesson from Ethereum’s Darkest HourHands up anyone who can either remember the DAO hack or still knows what it is. It remains a showcase argument for Ethereum, that despite the upgrades and the many years of steady uptime, the blockchain runs smoothly! (I look at you Solana; the blockchain that celebrated at the beginning of February that the blockchain has been running for a year without downtime… [21].) However, one should not forget that there was a hack on Ethereum in 2016 at The Dao, in which 14% of all ETH tokens in circulation were stolen [22]. The decision was made to fork Ethereum to the Ethereum blockchain we know today and Ethereum Classic, on which the reality after the hack still exists and 14% of the ETH was lost (I will also shed light on this story in another article, DeFi History so to speak). This underlines the fact that there are events that could threaten Ethereum and this should not be underestimated. It should be avoided that by stretching the consensus on Ethereum or centralization risks we need another fork in the future to bail us out.Conclusion: Innovation Needs TransparencySo what is the conclusion from all this? Well, ultimately it is now technically possible to optimize the yield you could get from Ethereum by taking more risk, but the APY hardly reflects this risk at all in my opinion. The users are barely or not at all aware of the possible risks because the restaking yields don’t sound like that much for DeFi, so the risks might be underestimated. For me, restaking is worrying until we have reasonable and transparent risk metrics in DeFi that can help users understand them. As in my first article, I am concerned about this development and I doubt that it is reasonable to take this risk without taking DeFi and Web3 “to the next level” from the users perspective [17]. Maybe I am biased because as a finance student, my alarm bells are ringing when I see concepts that strongly resemble rehypothecation; or in other words what led to the collapse of the financial system in 2008. The consequence of the financial crisis was the birth of crypto, DeFi, and the Web3 through the revolution initiated by Satoshi Nakamoto with Bitcoin and the world-famous inscription in the Genesis block “The Times 03/Jan/2009 Chancellor on brink of second bailout for banks”, i.e. a direct criticism of the existing financial system [23]. Are we moving towards a system like the one we originally wanted to replace with a better system? Judging by the current state of this potential ticking time bomb in DeFi, I would say unfortunately yes. We need decentralized and transparent risk metrics - otherwise, DeFi will crash like the financial system did in 2008. So here I share Vitalik Buterin's opinion because it was only during my research that I came across this quote from him:“Incorrect answers could lead Ethereum down a path of centralisation and ‘re-creating the traditional financial system with extra steps’[…].” [24]\
Restaking is an innovation and we need those as a driver for DeFi and its adoption, but I remain cautious until the concept has proven itself. Primarily, I am concerned with investor protection because I believe many users don't realize what risks they are exposed to. That's why I agree with Marcin Kazmierczak, Co-founder & COO of Redstone that new and interested users need to be introduced to this topic gradually:“[…]To address this, protocols could adopt graduated entry systems-for instance, starting users with simpler staking options offering 5-7% APY before granting access to more complex and higher-risk products.” [19]\
To me, this seems like a sensible approach that should be used more often in DeFi to make investors aware of the risks. Protocols should introduce transparent risk metrics and educate users more about potential dangers. If we want to build a new and inclusive alternative financial system in a decentralized way, then it is not only necessary to be innovative, but also inclusive and partly educational. Everyone should be able to benefit from it without users falling by the wayside due to a lack of financial knowledge. DeFi is for everyone and therefore it is also the task of all of us to provide the necessary protection and education for everyone.\
Do you agree with Vitalik’s concerns about restaking? Do you think that it is important for the adoption of DeFi to provide risk metrics? What do you think about restaking and were you aware of its potential risks? Share your perspective in the comments!\
If you liked the article don't hesitate to follow me here on Hackernoon and read my other stories too!\
If you want to talk to me or collaborate on an article you can also write me on my X/Twitter or Bluesky, my username there is @Maxo1st as well.]]></content:encoded></item><item><title>UK Drops &apos;Safety&apos; From Its AI Body, Inks Partnership With Anthropic</title><link>https://news.slashdot.org/story/25/02/14/0513218/uk-drops-safety-from-its-ai-body-inks-partnership-with-anthropic?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 14 Feb 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: The U.K. government wants to make a hard pivot into boosting its economy and industry with AI, and as part of that, it's pivoting an institution that it founded a little over a year ago for a very different purpose. Today the Department of Science, Industry and Technology announced that it would be renaming the AI Safety Institute to the "AI Security Institute." (Same first letters: same URL.) With that, the body will shift from primarily exploring areas like existential risk and bias in large language models, to a focus on cybersecurity, specifically "strengthening protections against the risks AI poses to national security and crime."
 
Alongside this, the government also announced a new partnership with Anthropic. No firm services were announced but the MOU indicates the two will "explore" using Anthropic's AI assistant Claude in public services; and Anthropic will aim to contribute to work in scientific research and economic modeling. And at the AI Security Institute, it will provide tools to evaluate AI capabilities in the context of identifying security risks. [...] Anthropic is the only company being announced today -- coinciding with a week of AI activities in Munich and Paris -- but it's not the only one that is working with the government. A series of new tools that were unveiled in January were all powered by OpenAI. (At the time, Peter Kyle, the secretary of state for Technology, said that the government planned to work with various foundational AI companies, and that is what the Anthropic deal is proving out.) "The changes I'm announcing today represent the logical next step in how we approach responsible AI development -- helping us to unleash AI and grow the economy as part of our Plan for Change," Kyle said in a statement. "The work of the AI Security Institute won't change, but this renewed focus will ensure our citizens -- and those of our allies -- are protected from those who would look to use AI against our institutions, democratic values, and way of life."
 
"The Institute's focus from the start has been on security and we've built a team of scientists focused on evaluating serious risks to the public," added Ian Hogarth, who remains the chair of the institute. "Our new criminal misuse team and deepening partnership with the national security community mark the next stage of tackling those risks."]]></content:encoded></item><item><title>Ubuntu Making Progress On Replacing initramfs-tools With Dracut</title><link>https://www.phoronix.com/news/Ubuntu-Dracut-Still-WIP</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 12:00:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As a follow-up to the news from last October of Ubuntu considering Dracut to replace initramfs-tools for initrd generation, that work remains ongoing with some improvements since having been prepared for the upcoming Ubuntu 25.04 release but it remains overall an active affair...]]></content:encoded></item><item><title>GNOME Software May Eventually Drop RPM Support In Favor Of Flatpaks</title><link>https://www.phoronix.com/news/GNOME-Software-RPM-Flatpak</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:37:56 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Stemming from the ongoing discussion around the issues raised with Fedora's Flatpak package of OBS Studio and how Flatpaks should be prioritized within the GNOME Software app center/store, the future of RPM support within GNOME Software raised...]]></content:encoded></item><item><title>Show Your Love For Linux Hardware Coverage By Going Premium This Valentine&apos;s Day</title><link>https://www.phoronix.com/news/Phoronix-Premium-Valentine-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:30:02 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[If you wish to show your appreciation for all of the Linux hardware reviews, Linux benchmarking, and open-source news provided on Phoronix each and every day, you can join Phoronix Premium this Valentine's Day weekend at a discounted rate...]]></content:encoded></item><item><title>Vulkan 1.4.308 Brings NVIDIA&apos;s Provisional Present Metering Extension</title><link>https://www.phoronix.com/news/Vulkan-1.4.308-Present-Metering</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:14:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Vulkan 1.4.308 was quietly released last week and besides a few fixes what makes it interesting is the provisional VK_NV_present_metering extension...]]></content:encoded></item><item><title>Valkey 8.1-rc1 Delivers Fresh Performance Improvements</title><link>https://www.phoronix.com/news/Valkey-8.1-rc1</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:05:55 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Valkey as the open-source in-memory store forked from Redis is preparing for its next feature release...]]></content:encoded></item><item><title>Europe denies dropping AI liability rules under pressure from Trump</title><link>https://techcrunch.com/2025/02/14/europe-denies-dropping-ai-liability-rules-under-pressure-from-trump/</link><author>Natasha Lomas</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:01:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The European Union has denied that recent moves to row back on some planned tech regulation — principally by ditching the AI Liability Directive, a 2022 draft law which had been aimed at making it easier for consumers to sue over harms caused by AI-enabled products and services — were made in response to pressure […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Alibaba To Partner With Apple On AI Features, Sending Shares To 3-Year High</title><link>https://apple.slashdot.org/story/25/02/14/051243/alibaba-to-partner-with-apple-on-ai-features-sending-shares-to-3-year-high?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 14 Feb 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Alibaba will partner with Apple to support AI features on iPhones in China, sending Alibaba's shares surging over 9% to a three-year high. Reuters reports: "They talked to a number of companies in China. In the end they chose to do business with us. They want to use our AI to power their phones. We feel extremely honored to do business with a great company like Apple," Tsai said at the World Government Summit in Dubai. Apple continues to work with Baidu on AI features for iPhones in China, The Information reported on Thursday, citing two people with direct knowledge of the matter.
 
While Apple's phones outside China utilize a combination of its proprietary Apple Intelligence and OpenAI's ChatGPT, Tsai did not specify whether the Alibaba partnership would follow a similar model. In China, consumer-facing AI products require regulatory approval, and The Information reported earlier that both Alibaba and Apple have already submitted materials to authorities. "Instead of viewing the Alibaba-Apple partnership through the lens of China's AI strength, the partnership is mainly a recognition of Alibaba's AI capability," said Lian Jye Su, a chief analyst at tech research firm Omdia.]]></content:encoded></item><item><title>Mid-Feb Already Feels Like Mid-Year: A Recap of 2025’s Major (Around) Tech Events</title><link>https://hackernoon.com/mid-feb-already-feels-like-mid-year-a-recap-of-2025s-major-around-tech-events?source=rss</link><author>Mary Glazkova</author><category>tech</category><pubDate>Fri, 14 Feb 2025 09:00:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As we reach mid-February 2025, the pace of events has been so rapid that it feels as though half the year has already passed. Here’s a concise overview of the significant developments that have unfolded thus far:AI Innovations and RivalriesIn the realm of artificial intelligence, French startup Mistral AI has emerged as a formidable competitor to industry giants. Founded in April 2023 by former engineers from Google DeepMind and Meta Platforms, Mistral AI has developed advanced open-weight large language models, positioning itself as a notable alternative to proprietary AI systems.Their open-weight large language models have gained significant traction, particularly in Europe, where there's a growing demand for alternatives to U.S.-based AI systems. Mistral AI's approach aligns with the EU's focus on open and transparent AI development. \n Defense Sector Developments\
Another French startup, Exotrail, has made significant strides in the defense sector. Specializing in space logistics and propulsion systems, Exotrail has been instrumental in advancing rocket technology, contributing to Europe’s growing presence in space defense initiatives.In February 2025, they secured a major contract with the European Space Agency to develop next-generation satellite propulsion systems, further solidifying their position in the industry. \n Controversial Moves in Venture Capital\
In early February 2025, venture capital firm Andreessen Horowitz (a16z) announced the hiring of Daniel Penny, a former Marine veteran, as a deal partner for its American Dynamism team. Penny was previously involved in a 2023 subway incident in New York City, where he restrained Jordan Neely, a homeless man, leading to Neely’s death.Penny was acquitted of criminal charges in December 2024. Many questioned the decision, given Penny's controversial past, while others defended it as an example of second chances. This move has led to increased scrutiny of hiring practices in the VC industry. \n Cryptocurrency Turbulence\
In the cryptocurrency arena, President Donald Trump launched a meme coin named $TRUMP just before his inauguration. The $TRUMP coin saw an explosive growth shortly after its launch, with its value soaring overnight.The coin's market capitalization reached approximately $14 billion at its peak. However, it's important to note that shortly after, Melania Trump introduced her own meme coin, $MELANIA, which also experienced rapid growth followed by a sharp decline. The $MELANIA coin's market cap reached around $5 billion at its peak. \n Government Efficiency Initiatives\
President Trump appointed Elon Musk to lead the newly established Department of Government Efficiency (DOGE). This initiative aims to streamline federal operations and reduce bureaucratic inefficiencies. Musk has assembled a team of young technologists, often referred to as “baby-faced assassins,” to spearhead this effort.\
The DOGE is not a traditional Cabinet-level department but a temporary organization within the Executive Office of the President. Musk's team of young technologists includes Akash Bobba (21), Edward Coristine (19), Luke Farritor (23), Gautier Cole Killian (24), Gavin Kliger (25), and Ethan Shaotran (22). Their appointment has raised concerns about their experience and qualifications. \n Geographical Renaming Controversies\
In a move reflecting his “America First” policy, President Trump ordered the renaming of the Gulf of Mexico to the Gulf of America. This decision has sparked debates among cartographers, politicians, and the public, reminiscent of past presidential actions such as the renaming of North America’s highest peak from Denali back to Mt. McKinley.With these rapid-fire developments, 2025 is shaping up to be anything but slow. If the first two months are any indication, we’re in for a wild ride—expect more surprises, more drama, and a whole lot of shaking things up. Buckle up.]]></content:encoded></item><item><title>The TechBeat: 30 World-Changing Prompts: OpenAI&apos;s AI Singularity, Deep Research Has Arrived (2/14/2025)</title><link>https://hackernoon.com/2-14-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Fri, 14 Feb 2025 07:11:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @bill-achola [ 3 Min read ] 
 Who really profits in a startup? Our deep dive into startup salaries reveals how executives secure big paydays while employees take on the risk. Read More.By @filestack [ 6 Min read ] 
 Many developers find that rich text editors have become important parts of content management systems, CRMs, productivity platforms, and e-learning solutions. Read More.**[Generative AI's Double-Edged Sword:Unlocking Potential While Mitigating Risks](https://hackernoon.com/generative-ais-double-edged-sword-unlocking-potential-while-mitigating-risks)** 
 By @mend [ 6 Min read ] 
 Generative AI boosts efficiency but introduces security risks like shadow AI, vulnerabilities, and data leaks. Learn how AI can secure AI-driven development. Read More.By @2077research [ 11 Min read ] 
 Stablecoin interest rates are set via governance, algorithms, or game theory, shaping DeFi monetary policies. Learn how MakerDAO, Aave, crvUSD, and BOLD do this Read More.By @diadkov [ 4 Min read ] 
 Since March 2024, conspiracy theories about TikTok's ban have spread, citing espionage fears and geopolitical influences without solid evidence Read More.By @paulr [ 9 Min read ] 
 Tokenization is the gateway through which raw text transforms into a format usable by large language models (LLMs) like GPT. Read More.By @@javar97 [ 7 Min read ] 
 According to Stack Overflow's 2024 survey, 76% of developers are using or planning to use AI tools. Read More.By @heinhtetkyaw [ 5 Min read ] 
 Hackers always challenged corporate capitalism and government surveillance. The hackers ethos nowadays are being replaced by the corporate certifications. Read More.By @vinitabansal [ 9 Min read ] 
 While aggressive managers are difficult, they aren’t impossible to work with. With the right strategies, you can turn them around. Read More.By @step [ 6 Min read ] 
 Language is a component of human consciousness. AI has a conversational and relatable language capability, could that be a fraction of consciousness? Read More.By @ntoskrnl [ 8 Min read ] 
 Security mechanisms under the hood of simple file actions Read More.By @thomascherickal [ 25 Min read ] 
 Deep Research Prompts: Explore 30 ambitous, impactful ideas using emerging tech to tackle global crises. Discover research with world-changing potential. Read More.By @nfrankel [ 5 Min read ] 
 DevPod is a nice tool around your toolbelt that allows your development team(s) to share the same machine configuration without hassle. Read More.By @editingprotocol [ 4 Min read ] 
 If you want to become a top writer, here are 3 tips to help you rise to the cream of the crop.  Read More.By @andrei9735 [ 6 Min read ] 
 Link prediction aims to predict the likelihood of a future or missing connection between nodes in a network.  Read More.By @abhiyanampally_kob9nse8 [ 40 Min read ] 
 Dive into the comparitive analysis between logarithmic and floating-point arithmetic in neural nets using the commonly used MNIST dataset. Read More.By @blackheart [ 6 Min read ] 
 In Barbie, Ken struggles with identity, feeling like he exists in Barbie’s shadow. Many cybersecurity specialists can relate. Read More.By @silkdrive [ 4 Min read ] 
 Discover the future of software development with vibe coding—where creativity comes first, and coding happens effortlessly with AI. Read More.]]></content:encoded></item><item><title>TrueNAS 25.04 &quot;Fangtooth&quot; Beta Unifies Linux SCALE &amp; FreeBSD CORE Efforts</title><link>https://www.phoronix.com/news/TrueNAS-25.04-Beta</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 05:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[TrueNAS 25.04 beta was released on Thursday as another step toward unifying the TrueNAS CORE OS derived from FreeBSD and the Linux-based TrueNAS SCALE...]]></content:encoded></item><item><title>The HackerNoon Newsletter: AI Is Mapping Hidden Connections—And It’s Just Getting Started (2/14/2025)</title><link>https://hackernoon.com/2-14-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Fri, 14 Feb 2025 03:56:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, February 14, 2025?By @andrei9735 [ 4 Min read ] From recommendation engines to network optimization, link prediction is a versatile tool that brings value. Read More.By @maxo1st [ 12 Min read ] Explore the hidden risks of restaking on Ethereum and in DeFi—slashing, centralization, and liquidity cascades. Are higher yields worth the ticking time bomb? Read More.By @maryglazkova [ 3 Min read ] As we reach mid-February 2025, the pace of events has been so rapid that it feels as though half the year has already passed.  Read More.By @benhodlin [ 6 Min read ] In this fun article, we’ll match the top 10 cryptocurrencies with iconic WWE Attitudes Era superstars based on their characteristics. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Dark Matter Research: Everything That We Analyzed</title><link>https://hackernoon.com/dark-matter-research-everything-that-we-analyzed?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Fri, 14 Feb 2025 03:01:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We have analyzed heavy neutrino DM candidates in a minimal extension of SM, which features three RHNs and one ALP. This model is well motivated, as it not only accounts for DM but also explains neutrino oscillations. Hence, ALP-mediated RHN DM is interesting from both the model-building and phenomenological perspectives. We have considered the lightest RHN as DM, which is odd under Z2 symmetry, and identified the region of parameters where DM predictions are in agreement with DM relic abundance. In addition, this model also quite naturally explains the null results of LUX and XENON1T due to the pseudoscalar nature of interactions with quarks. We have highlighted the importance of complementary searches, for instance, via indirect detection with single and di-photons. Although the current limits from Fermi-LAT lie above the predicted signals for our choice of parameter space, future sensitivities of Fermi-LAT might offer promising prospects to probe both the low and high DM mass regions.(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>Dark Matter Analysis: Relic Density, Direct Detection, and More</title><link>https://hackernoon.com/dark-matter-analysis-relic-density-direct-detection-and-more?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Fri, 14 Feb 2025 02:52:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The XENON1T [18] experiments have a strong sensitivity for spin-independent and spindependent DM-nucleon interactions in our interested mass range of DM. However, recent data from LZ [22] and XENONnT [24] have further put stronger bounds on scattering cross-section. The interaction between DM (𝑁1) and a quark (q) can be described by the following effective Lagrangian:\
These gamma rays would be produced preferentially in regions of high DM density and can be best detected by Fermi-Lat [8], HESS [7]. The integrated gamma-ray flux from the DM annihilation in a density distribution 𝜌(r) is given by(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>Existing Constraints on ALP Parameter Space: Dark Matter Research</title><link>https://hackernoon.com/existing-constraints-on-alp-parameter-space-dark-matter-research?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Fri, 14 Feb 2025 01:46:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2.3 Existing constraints on ALP parameter space(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>OBS Studio Raises Issues With Fedora&apos;s Flatpak Package</title><link>https://www.phoronix.com/news/OBS-Studio-Poor-Fedora-Flatpak</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 01:39:52 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The OBS Studio open-source screencasting and streaming app has called out Fedora's poor Flatpak packaging of the application and is threatening as going as far as legal action if it isn't addressed...]]></content:encoded></item><item><title>Dark Matter Research: A Look at the Standard Model</title><link>https://hackernoon.com/dark-matter-research-a-look-at-the-standard-model?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Fri, 14 Feb 2025 01:37:42 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The model that we consider is the minimal combination of type-I seesaw and effective ALP interaction with additional Z2 symmetry apart from the SM gauge symmetry [108, 109]. The matter content of the model is shown in Table. 2.1.(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>Dark Matter Through ALP Portal: An Introduction</title><link>https://hackernoon.com/dark-matter-through-alp-portal-an-introduction?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Fri, 14 Feb 2025 01:25:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2 Dark matter through ALP portalThis chapter deals with fermion dark matter interacting with SM via the ALP portal. The results are based on the work: Shivam Gola, Sanjoy Mondal, and Nita Sinha, "ALP portal majorana dark matter, Int.J.Mod.Phys.A 37 (2022) 22, 2250131”.The discovery of neutrino oscillations confirming the existence of at least two nonvanishing neutrino mass-squared differences necessitates physics beyond the Standard Model (BSM). In principle, neutrino mass could be simply generated by the addition of right-handed neutrinos (RHNs) to the SM particle content. These RHNs interact with SM fields via mixing with active neutrinos. Since RHNs are SM singlets, they allow the Majorana mass term along with the usual Dirac mass term. This is known as type-I seesaw mechanism [71–74]. The mass of these RHNs could range from eV to GUT scale, depending on the models [75–78]. RHNs can also play the role of warm dark matter (WDM), which is singlet under the SM gauge symmetry and has tiny mixing with the SM neutrinos, leading to a long lifetime [79–81]. Also, KeV-scale RHN has been studied as a viable DM candidate [82–84]. In this work, we have instead focused on the prospects of having GeV-scale RHNs as WIMP DM candidates.\
Chapter 2 is organised as follows: In Sec. 2.2, we introduced our model, detailing the new interactions present. In Sec. 2.3, we summarise the existing constraints on ALP parameter space coming from various observable and collider searches. In Sec. 4.4, we have explored and discussed the feasible parameter space coming from DM analyses, such as relic density and direct and indirect detection. Finally, we give our summary in Sec. 2.5.(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>Information Overload? Hack Your Brain With These 7 Tricks</title><link>https://hackernoon.com/information-overload-hack-your-brain-with-these-7-tricks?source=rss</link><author>Jonathan Roseland</author><category>tech</category><pubDate>Fri, 14 Feb 2025 01:04:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Information is actually one of the most expensive things in the world, and no matter how much money you have, you'll never be able to afford as much of it as you need.\
If you haven't guessed already, the cost of information consumption is time and attention - our two most limited resources - which are almost always better spent on the things that really matter in life: relationships, health, and fulfilling work. Most information consumption is an inefficient use of these resources considering the limited short-term memory we humans have; the vast majority of this information just gets absorbed into our subconscious, to reemerge blurred and convoluted by our emotions at the moment.I would even go as far as to say that generations past had it better because the information had more tangible costs: a $300 encyclopedia volume that took up an entire shelf in your living room, a 10-pound Yellow Pages directory, or hundreds of documents in a complicated filing system. The trouble of extracting knowledge from these forms of media meant that the seeker made sure it was going to be worth their while.Let's get practical here… \n As an entrepreneur and Applied Neuroscience Strategist, I make about $125/hour (more if I account for the continuity of my cryptocurrency investing and affiliate revenues). Since I'm fast, it takes me 15 minutes to skim a decent blog and watch a YouTube video on a subject (and remember the 80/20 principle; I probably need to skim more than this to learn what I need to).\
This has a finite  to me of about $32. \n I'm going to have to work overtime to cover the time spent consuming information, so it's cost me another 15 minutes that should be spent enjoying the life I'm working so damn hard for.Like you, I only have about 8-10 hours of productive mental energy a day. Active, applicable information consumption takes at least twice the mental energy of doing work (and sometimes, it's a much greater gap, depending upon what you do).Those 15 minutes cost me 30 minutes of mental energy; 30 minutes stolen from the energy that could have been devoted to actual work.\
Keep in mind that this example is for just 15 minutes of information consumption; sometimes, we spend hours a day!If I treat my time as money and my knowledge as potential power, it becomes clear that I need to get  above and beyond the value of the knowledge I am getting to cover the high cost of acquiring it. Whenever I engage in information consumption, I put myself in a position of information . I need to be selective about what I pay attention to…This is why I don't watch popular TV shows.I turn movies off after 30 minutes if they aren't amazing.\
We live in an interesting time where there is this massive disparity between the amount of information we can consume and the amount of time we have to do it. Our success, individually and culturally, over the next century will be decided not by the amount of knowledge we acquire but by our ability to leverage it.\
Anyway, enough philosophical mumbo jumbo (I hope you love it, you voracious consumer of information!) This article delves into practical ways to increase information equity…We’ve discussed the dynamic of information equity. Let’s now cover seven ways of increasing the equity and value you get from your information consumption, including task batching, mental segregation, and using memory triggers.\
How to increase information equity…\  Separate Your Work Into Two Categories…| Creative/mental energy-consuming tasks | Monotonous or repetitive tasks that require little mental energy |
|----|----|
| Examples: blogging, negotiating, sales calls, designing, meetings, strategizing, planning, communicating with other people, etc. | Examples: responding to emails, organizing paperwork, data entry, etc. |Plan your monotonous tasks at the end of the day when your mental energy is low and batch the most similar, monotonous tasks one after another. While you are doing this, consume information passively. Here are my top favorite forms of passive information consumption… – Which you download free from Apple Podcasts or Castbox.fm - my favorite podcast app.Watching documentary films – About history, science, travel, conspiracy theories - whatever I'm curious about at the moment. Top Documentary Films has +3000 documentary films, which you can watch for free.Online video (YouTube, Rumble, Odysee) - I'll create an hour or two-long playlist about whatever subject interests me. – If you don't want to pay for an Audible membership, the Castbox app has a huge library of public-domain audiobooks. Or you can use the excellent ElevenReader app to convert any PDF or Epub book into an audiobook; the AI narration is damn near the quality of a professional human narration.Sometime within the next month, you are going to be having a conversation with someone about blogging and you are going to use the phrase  (it’s going to make you seem really smart!) Here’s the memory trigger you are going to use…\
  : the word \
  : every time you hear the word  think  Then imagine a person’s mouth making an ‘O’ shape as they say *blah-blah-blah **> ***Now imagine a  inside their mouth (which is making a  shape)  That  represents the amount of information equity their blogging creates.\
  Reinforce the mental association: Mentally say the word , then mentally yell at yourself *BLAH! BLAH! BLAH! (*while visualizing the mouth). Do this 3 times (should take about 30 seconds).\
Initially, forming these memory triggers will feel a little odd and will take some mental energy and creativity, but as you do them more often, it will become natural and automatic.I highly recommend a top-down approach to your information consumption. Start with the end in mind; clearly identify a problem or issue you are having. Determine first whether it’s a definitive solution problem or an ambiguous solution problem. Here are some examples to illustrate the difference…\
Definitive Solution Problem: How do I set up a Facebook fan page?Ambiguous Solution Problem: How do I improve my online presence for my small business?\
For an ambiguous solution problem, I would use a forum or consultant and describe your problem specifically so you can get answers tailored to your specific situation.\
 - Quora is a forum-style website where you can post questions about a wide variety of topics. Often, you'll get responses to your questions from very well-educated people with years of relevant experience. I prefer to use forums like this to let the information I need come to me as opposed to chasing it. This can save a lot of time over searching blogs for what you need. Follow me here on Quora. - This is a hive of scum and villainy online, but there might be some decent subreddits devoted to the topic of your inquiry. Share your question in multiple subreddits that don't seem to be total "cults of the amateur."\
 - For nearly every topic under the sun, there are internet forums. Forums can actually be pretty decent places to connect with and question people who know what they are talking about. My favorite is the antiaging forum Longecity.org; connect with me there.*Mental Segregation Between Infotainment* and LearningWe spend an increasing amount of time-consuming information for pleasure. It’s important to put up a mental wall between the times of the day and hours that are spent on productive activities and the time that’s spent enjoying the life you are working so damn hard for. \
Infotainment during work hours will distract you from getting things done. So, make sure that information consumption during work hours is directly related to improving work.Do It, Delegate It, Dump ItWe all have dozens of activities on our mental to-do lists. It’s important to put these activities into three categories… – You should ideally have under five activities that you need to accomplish in a day. Schedule committed time to these activities. – Pass this activity off to someone who works for you or a colleague. – What’s the worst that could happen if you don’t do this activity? If it’s not that bad, dump it.Find a partner who holds you accountable for following through on finishing the things you start. Let your accountability partner know you have invested your time in information consumption and commit to them that you will do whatever it takes to follow through.Practice Discipline and Persistence!Did you think this blog was just about shortcuts and clever ways of hacking life? Nope! Whenever you consume knowledge, make a mental commitment to yourself that you will put this knowledge to valuable use. Make sure you have a specific plan to do so, and then follow through! \n ]]></content:encoded></item><item><title>Zed Editor Introduces Open-Source &quot;Zeta&quot; Edit Prediction Model</title><link>https://www.phoronix.com/news/Zed-Zeta-AI-Model-Edit-Predict</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 20:58:48 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Zed code editor for macOS and Linux systems has proven to be quite popular for this Rust-based editor started by the creators of the Atom editor. Their latest feature being introduced is Zeta as an open-source edit prediction model to further enhance this code editor with AI capabilities...]]></content:encoded></item><item><title>Bcachefs Freezes Its On-Disk Format With Future Updates Optional</title><link>https://www.phoronix.com/news/Bcachefs-On-Disk-Format-Freeze</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 19:18:32 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The latest round of Bcachefs file-system fixes have been submitted today for the in-development Linux 6.14 kernel. Besides fixes for the current kernel, it was announced today that the on-disk format for the file-system is now considered frozen in its latest development "master" branch...]]></content:encoded></item><item><title>Hector Martin Resigns From The Asahi Linux Project</title><link>https://www.phoronix.com/news/Hector-Martin-Resigns-Asahi</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 15:56:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Last week Hector Martin resigned from upstream maintainership of the Apple Silicon code for the Linux kernel. At the time he was still going to contribute to the Asahi Linux project's downstream kernel but in a surprise move today, he has decided to resign as project leader of Asahi Linux...]]></content:encoded></item><item><title>Linux 6.14-rc3 To Fix Platform Profile Support For Newer AMD-Powered ThinkPads</title><link>https://www.phoronix.com/news/Linux-6.14-rc3-Newer-AMD-PP</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 15:41:24 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Submitted today via the x86 platform driver updates ahead of Linux 6.14-rc3 on Sunday are some Lenovo ThinkPad patches that may interest some users...]]></content:encoded></item><item><title>Google Releases AOM-AV1 3.12 With More Performance Optimizations</title><link>https://www.phoronix.com/news/AOM-AV1-3.12</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 14:51:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those preferring the AOM-AV1 open-source AV1 video encoder over SVT-AV1, Rav1e, or other AV1 encoders, Google this week unveiled AOM-AV1 3.12...]]></content:encoded></item><item><title>Are You Ready to Let an AI Agent Use Your Computer?</title><link>https://spectrum.ieee.org/ai-agents-computer-use</link><author>Eliza Strickland</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjM3NTc5Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3MzQzMDIzNX0.q8-_ndmeOkmbC-IEhxZy5uOZIw7195YebXH8q88gz48/image.jpg?width=600" length="" type=""/><pubDate>Thu, 13 Feb 2025 14:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[AI agents from OpenAI, Anthropic, and Google want to lighten your load]]></content:encoded></item><item><title>Avowed Review - Too Close To The Sun</title><link>https://www.gamespot.com/reviews/avowed-review-too-close-to-the-sun/1900-6418335/?ftag=CAD-01-10abi2f</link><author>Alessandro Barbosa</author><category>tech</category><enclosure url="https://www.gamespot.com/a/uploads/screen_medium/43/434805/4442965-6382249068-avowe.jpg" length="" type=""/><pubDate>Thu, 13 Feb 2025 14:00:00 +0000</pubDate><source url="https://www.gamespot.com/feeds/reviews/">GameSpot - Game Reviews</source><content:encoded><![CDATA[In a similar way to how Obsidian's The Outer Worlds played very closely to a space-faring Fallout, Avowed sticks closely to the sensibilities of The Elder Scrolls V: Skyrim. Its fantasy world isn't as expansive and seamlessly stitched together, but you'd be forgiven for confusing the two at a glance, especially when you're engaged in its first-person combat. Avowed lifts some of the best aspects of the seminal RPG and improves them, especially when it comes to its refined combat. These changes extend to a move away from traditional leveling in favor of a gear-focused approach, as well as the option to experiment with wild weapon combinations. But not all of Avowed's experiments are successes, leading to an uneven role-playing adventure that surprises as much as it frustrates.Washing up on the shores of the Living Lands, you play as one the Godlike: a select few kissed by the grace of a god at birth and left with some distinct (and sometimes frightening) facial features to show for it. On a mission from a distant monarch whose influence within the Living Lands has many of its inhabitants up in arms, your job is to track down the source of a plague that's turning the land's people into mindless, bloodthirsty creatures, before it manages to make it back home. Although it is set in the same universe as Pillars of Eternity, Avowed does a good job of immediately siloing you into an area that requires little knowledge of what is happening across the ocean, but does reference some historical events from time to time. A glossary of important names and places is available as they're brought up in conversation, providing a handy guide that contextualizes some attitudes characters have to certain factions and events around you.Avowed makes a strong initial impression, quickly establishing your Godlike status but with the odd quirk of being the first not to know which god chose you. This isn't the main purpose of your mission, but that changes after a surprising event in the early hours of the game that sets the stage for a more intriguing answer to the plague ravaging the Living Lands. This setup is ultimately squandered, however, with the two big narrative hooks coalescing with one another in routinely expected and uninteresting ways, making the broader strokes of the story largely forgettable. The conversational writing does have its moments of charm, with equally serious and snarky retorts letting you inject some levity into otherwise dire situations with great comedic effect. But the severity of the plague you're trying to stop and the personal journey of finding out why you're the only Godlike without a god is not as captivating as it could be, taking steps along a narrative path that rarely deviates into surprising avenues.Continue Reading at GameSpot]]></content:encoded></item><item><title>Wayland Color Management &amp; HDR Protocol Support Merged</title><link>https://www.phoronix.com/news/Wayland-CM-HDR-Merged</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 13:25:18 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As a quick follow-up to the article earlier today... The Wayland Color Management and HDR protocol support is now merged to upstream Wayland Protocols!..]]></content:encoded></item><item><title>Brain-inspired Computing Is Ready for the Big Time</title><link>https://spectrum.ieee.org/neuromorphic-computing-2671121824</link><author>Edd Gent</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjM3Njk2NC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1MTg1NjUyMH0.WiISOGfkZPanzeo80jgQnL_WWY8h_Ot84nPBMWFvyuY/image.jpg?width=600" length="" type=""/><pubDate>Thu, 13 Feb 2025 13:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Neuromorphic pioneer Steve Furber says it's just awaiting a killer app]]></content:encoded></item><item><title>Device Trees For Apple T2 SoCs Slated For Upstreaming In Linux 6.15</title><link>https://www.phoronix.com/news/Linux-6.15-Apple-T2-SoC-DTs</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 11:58:31 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While there has been the recent drama over upstream maintainership over Apple Silicon / Asahi Linux code, Sven Peter is continuing to move things forward for the upstream kernel and this week sent out a set of Apple SoC DeviceTree updates intended for the upcoming Linux 6.15 kernel cycle...]]></content:encoded></item><item><title>NVIDIA Wiring Up Autonomous Performance Level Selection To Linux CPPC CPUFreq Driver</title><link>https://www.phoronix.com/news/NVIDIA-Auto-Perf-CPPC-CPUFreq</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 11:40:09 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Similar to the Autonomous Performance Level Selection and Energy Performance Preference (EPP) support already found within the Intel P-State and AMD P-State CPU frequency scaling drivers for their modern processors, NVIDIA engineers are working on similar support for the CPPC CPUFreq driver that can benefit their Grace processor...]]></content:encoded></item><item><title>Wayland Color Management &amp; HDR Protocols Expected To Be Merged Imminently</title><link>https://www.phoronix.com/news/Wayland-Color-HDR-Merging-Soon</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 11:30:10 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Today could finally be the day. In the works for 5+ years, the Wayland color management and HDR protocol additions look like they will finally be merged in the coming hours...]]></content:encoded></item><item><title>Financially motivated hackers are helping their espionage counterparts and vice versa</title><link>https://arstechnica.com/security/2025/02/financially-motivated-hackers-are-helping-their-espionage-counterparts-and-vice-versa/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2018/10/GettyImages-981636794-1152x648.jpg" length="" type=""/><pubDate>Thu, 13 Feb 2025 11:00:40 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[There’s a growing collaboration between hacking groups engaging in espionage on behalf of nation-states and those seeking financial gains through ransomware and other forms of cybercrime, researchers noted this week.There has always been some level of overlap between these two groups, but it has become more pronounced in recent years. On Tuesday, the Google-owned Mandiant security firm said the uptick comes amid tighter purse strings and as a means for concealing nation-state-sponsored espionage by making it blend in with financially motivated cyberattacks.“Modern cybercriminals are likely to specialize in a particular area of cybercrime and partner with other entities with diverse specializations to conduct operations,” Mandiant researchers explained. “The specialization of cybercrime capabilities presents an opportunity for state-backed groups to simply show up as another customer for a group that normally sells to other criminals. Purchasing malware, credentials, or other key resources from illicit forums can be cheaper for state-backed groups than developing them in-house, while also providing some ability to blend in to financially motivated operations and attract less notice."]]></content:encoded></item><item><title>OpenSUSE Tumbleweed Switching From AppArmor To SELinux For New Installations</title><link>https://www.phoronix.com/news/OpenSUSE-Tumble-Goes-SELinux</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 01:47:35 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[SUSE/openSUSE has a long history with the AppArmor Linux security module going back to the Novell days and when AppArmor was originally known as SubDomain. OpenSUSE/SUSE and Ubuntu Linux have been big proponents of AppArmor for Linux security but now moving forward on new installations of openSUSE Tumbleweed it will be defaulting to Security Enhanced Linux (SELinux)...]]></content:encoded></item><item><title>Ubuntu 24.04.2 LTS Delayed To Next Week</title><link>https://www.phoronix.com/news/Ubuntu-24.04.2-LTS-Delayed</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 00:06:58 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Ubuntu 24.04.2 LTS along with new point releases for its derivatives had been scheduled for release on Thursday. But a last minute issue has delayed this release...]]></content:encoded></item></channel></rss>