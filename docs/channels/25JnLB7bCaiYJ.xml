<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech News</title><link>https://news.securenza.be</link><description></description><item><title>Tesla asks EPA not to roll back emissions rules as Trump calls climate change a ‘con job’</title><link>https://techcrunch.com/2025/09/25/tesla-asks-epa-not-to-roll-back-emissions-rules-as-trump-calls-climate-change-a-con-job/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Thu, 25 Sep 2025 18:54:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company's regulatory policy team is trying to clean up a mess that CEO Elon Musk spent $300 million to help create.]]></content:encoded></item><item><title>Spotify denies recent accusation that it changed its terms for artists</title><link>https://techcrunch.com/2025/09/25/spotify-denies-recent-accusation-that-it-changed-its-terms-for-artists/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Thu, 25 Sep 2025 18:43:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Spotify sets the record straight about the distribution rights of artists, podcasters, creators, and authors on the platform. ]]></content:encoded></item><item><title>Cloudflare To Launch Stablecoin for AI-Driven Internet Economy</title><link>https://tech.slashdot.org/story/25/09/25/1842223/cloudflare-to-launch-stablecoin-for-ai-driven-internet-economy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 25 Sep 2025 18:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Cloudflare announced plans Thursday to launch NET Dollar, a U.S. dollar-backed stablecoin designed to enable autonomous AI agents to conduct instant financial transactions. The company says the stablecoin will support microtransactions and pay-per-use models as AI agents take over tasks like booking flights and ordering groceries. BrianFagioli comments: A U.S. dollar-backed cryptocurrency from Cloudflare feels unusual to me, and I'm still surprised by it. The decision shows just how much the Internet is shifting in response to artificial intelligence. 

CEO Matthew Prince said, "For decades, the business model of the Internet ran on ad platforms and bank transfers. The Internet's next business model will be powered by pay-per-use, fractional payments, and microtransactions -- "tools that shift incentives toward original, creative content that actually adds value." He added that by using its global network, Cloudflare aims to "help modernize the financial rails needed to move money at the speed of the Internet."]]></content:encoded></item><item><title>Experts urge caution about using ChatGPT to pick stocks</title><link>https://arstechnica.com/information-technology/2025/09/experts-urge-caution-about-using-chatgpt-to-pick-stocks/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_stocks-1152x648.jpg" length="" type=""/><pubDate>Thu, 25 Sep 2025 18:10:50 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[With AI chatbots growing in popular usage, it was only a matter of time before large numbers of people began applying them to the stock market. In fact, at least 1 in 10 retail investors now consult ChatGPT or other AI chatbots for stock-picking advice, according to a Reuters report published Thursday.Data from a survey by trading platform eToro of 11,000 retail investors worldwide suggests that 13 percent of individual investors already use AI tools like ChatGPT or Google's Gemini for stock selection, while about half say they would consider using these tools for portfolio decisions.Unlike algorithmic trading, where computers automatically execute thousands of trades per second, investors are using ChatGPT as an advisory tool in place of human experts. They type questions, read the AI model's analysis, and then manually decide whether to place trades through their brokers.]]></content:encoded></item><item><title>OpenAI Says GPT-5 Stacks Up To Humans in a Wide Range of Jobs</title><link>https://slashdot.org/story/25/09/25/176219/openai-says-gpt-5-stacks-up-to-humans-in-a-wide-range-of-jobs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 25 Sep 2025 18:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: OpenAI released a new benchmark on Thursday that tests how its AI models perform compared to human professionals across a wide range of industries and jobs. The test, GDPval, is an early attempt at understanding how close OpenAI's systems are to outperforming humans at economically valuable work -- a key part of the company's founding mission to develop artificial general intelligence or AGI. 

OpenAI says its found that its GPT-5 model and Anthropic's Claude Opus 4.1 "are already approaching the quality of work produced by industry experts." That's not to say that OpenAI's models are going to start replacing humans in their jobs immediately. Despite some CEOs' predictions that AI will take the jobs of humans in just a few years, OpenAI admits that GDPval today covers a very limited number of tasks people do in their real jobs. However, it is one of the latest ways the company is measuring AI's progress towards this milestone. GDPval is based on nine industries that contribute the most to America's gross domestic product, including domains such as healthcare, finance, manufacturing, and government. The benchmark tests an AI model's performance in 44 occupations among those industries, ranging from software engineers to nurses to journalists.]]></content:encoded></item><item><title>AI Isn&apos;t Replacing Radiologists</title><link>https://slashdot.org/story/25/09/25/1642255/ai-isnt-replacing-radiologists?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 25 Sep 2025 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Despite AI models outperforming radiologists on benchmark tests since 2017, demand for human radiologists has reached record highs. American diagnostic radiology residency programs offered 1,208 positions this year, up 4% from 2024, while average salaries hit $520,000 -- 48% higher than 2015. Over 700 FDA-cleared radiology AI models exist, yet only 48% of radiologists use AI at all. Models trained on standardized datasets lose up to 20% points accuracy when deployed in different hospitals. Radiologists spend just 36% of their time interpreting images, with the majority devoted to patient communication, teaching, and administrative tasks that current AI cannot perform.]]></content:encoded></item><item><title>OpenAI launches ChatGPT Pulse to proactively write you morning briefs</title><link>https://techcrunch.com/2025/09/25/openai-launches-chatgpt-pulse-to-proactively-write-you-morning-briefs/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Thu, 25 Sep 2025 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Pulse is part of a broader shift in OpenAI's consumer products, which are lately being designed to work for users asynchronously instead of responding to questions. ]]></content:encoded></item><item><title>Stablecoin Issuer Circle Examines &apos;Reversible&apos; Transactions in Departure For Crypto</title><link>https://news.slashdot.org/story/25/09/25/1335206/stablecoin-issuer-circle-examines-reversible-transactions-in-departure-for-crypto?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 25 Sep 2025 16:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Circle, the world's second-biggest issuer of stablecoins, is examining ways to make it possible to reverse transactions involving its tokens [non-paywalled source], in a rare admission by a major crypto firm that it needs to take lessons from the traditional financial sector. Financial Times: Circle president Heath Tarbert said a mechanism that allowed money to be refunded in cases of fraud or disputes would help the stablecoin industry's push to become part of the financial mainstream. "We are thinking through...whether or not there's the possibility of reversibility of transactions, right, but at the same time, we want settlement finality," Tarbert told the Financial Times. 

"So there's an inherent tension there between being able to transfer something immediately, but having it be irrevocable," he added. Such measures could be seen as a major departure from the crypto industry's previous emphasis on the "immutability" of the blockchain, a digital ledger that is public and records transactions that cannot be unwound.]]></content:encoded></item><item><title>Threads is developing a tool that lets you ‘tag’ its algorithm to configure your feed</title><link>https://techcrunch.com/2025/09/25/threads-is-developing-a-tool-that-lets-you-tag-its-algorithm-to-configure-your-feed/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 25 Sep 2025 16:41:37 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Threads is developing a new feature that would let users control what they see by 'tagging' the algorithm. ]]></content:encoded></item><item><title>Juicebox raises $30M from Sequoia to revolutionize hiring with LLM-powered search</title><link>https://techcrunch.com/2025/09/25/juicebox-raises-30m-from-sequoia-to-revolutionize-hiring-with-llm-powered-search/</link><author>Marina Temkin</author><category>tech</category><pubDate>Thu, 25 Sep 2025 16:31:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The recruiting startup has over 2,500 customers, including recruiters at Perplexity, Ramp, and OpenAI.]]></content:encoded></item><item><title>Steph Curry’s VC firm just backed an AI startup that wants to fix food supply chains</title><link>https://techcrunch.com/2025/09/25/steph-currys-vc-firm-just-backed-an-ai-startup-that-wants-to-fix-food-supply-chains/</link><author>Tage Kene-Okafor</author><category>tech</category><pubDate>Thu, 25 Sep 2025 16:30:04 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[While building AI for food supply chains may sound unglamorous, Burnt argues that decades of failed tech rollouts have left operators skeptical of “tech tourists” with no industry experience. ]]></content:encoded></item><item><title>Amazon to pay $2.5B in FTC settlement over ‘deceptive’ Prime tactics</title><link>https://techcrunch.com/2025/09/25/amazon-to-pay-2-5b-in-ftc-settlement-over-deceptive-prime-tactics/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 25 Sep 2025 16:29:31 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company will be required to pay a $1 billion civil penalty and provide $1.5 billion in refunds back to an estimated 35 million consumers harmed by the company's "deceptive Prime enrollment practices," the FTC says. ]]></content:encoded></item><item><title>LGBTQ+ youth have worse mental health outcomes without access to safe online spaces, studies show</title><link>https://techcrunch.com/2025/09/25/lgbtq-youth-have-worse-mental-health-outcomes-without-access-to-safe-online-spaces-studies-show/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Thu, 25 Sep 2025 16:26:45 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[These online communities have become more critical in a time LGBTQ+ rights are under attack in the U.S.]]></content:encoded></item><item><title>OpenAI says GPT-5 stacks up to humans in a wide range of jobs</title><link>https://techcrunch.com/2025/09/25/openai-says-gpt-5-stacks-up-to-humans-in-a-wide-range-of-jobs/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Thu, 25 Sep 2025 16:11:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A new test from OpenAI aims to understand how close AI is to outperforming humans at economically valuable work.]]></content:encoded></item><item><title>Elon Musk’s xAI offers Grok to federal government for 42 cents</title><link>https://techcrunch.com/2025/09/25/elon-musks-xai-offers-grok-to-federal-government-for-42-cents/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Thu, 25 Sep 2025 16:06:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[xAI has reached a deal with the U.S. GSA to sell Grok to federal agencies for 42 cents over 18 months, undercutting OpenAI and Anthropic. ]]></content:encoded></item><item><title>Amazon Reaches $2.5 Billion Settlement With FTC Over &apos;Deceptive&apos; Prime Program</title><link>https://yro.slashdot.org/story/25/09/25/1552229/amazon-reaches-25-billion-settlement-with-ftc-over-deceptive-prime-program?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 25 Sep 2025 15:52:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Amazon will pay $2.5 billion to settle Federal Trade Commission allegations that it duped users into paying for Prime memberships, the regulatory agency announced Thursday. CNBC: The surprise settlement comes as Amazon and the FTC were just three days into the trial in a Seattle federal court. Opening arguments took place on Tuesday. The lawsuit, filed by the FTC in June 2023 under the Biden administration, claimed that Amazon deceived tens of millions of customers into signing up for its Prime subscription program and sabotaged their attempts to cancel it. 

Three senior Amazon executives were at risk of being held individually liable if the jury sided with the FTC. Amazon will pay a $1 billion civil penalty to the FTC and will refund $1.5 billion to an estimated 35 million customers who were impacted by "unwanted Prime enrollment or deferred cancellation," the agency said.]]></content:encoded></item><item><title>Love, lies, and algorithms: Is AI really helping us find ‘the one’? Live at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/09/25/love-lies-and-algorithms-is-ai-really-helping-us-find-the-one/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Thu, 25 Sep 2025 15:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[From dating apps and AI-powered matchmaking to full-on digital companionship, artificial intelligence is rapidly becoming a third party in our most personal relationships. But is it truly helping us find deeper connection — or just reshaping romance into an algorithmic illusion?]]></content:encoded></item><item><title>Accenture To &apos;Exit&apos; Staff That Cannot Be Retrained For Age of AI</title><link>https://it.slashdot.org/story/25/09/25/1458207/accenture-to-exit-staff-that-cannot-be-retrained-for-age-of-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 25 Sep 2025 15:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Accenture has reduced its global workforce by more than 11,000 in the past three months and warned staff that more would be asked to leave if they cannot be retrained for the age of AI. From a report: The IT consulting group on Thursday detailed an $865 million restructuring programme and an outlook for the year ahead that reflects continuing sluggish corporate demand for consulting projects and a clampdown on spending within the US federal government. 

"We are exiting on a compressed timeline people where reskilling, based on our experience, is not a viable path for the skills we need," chief executive Julie Sweet told analysts on a conference call. The company employed 779,000 people at the end of August, it said, down from 791,000 three months earlier, after beginning a round of lay-offs that will continue until the end of November. It did not say how many jobs had gone directly as a result of the restructuring, but said severance payments and other costs totalled $615 million in the quarter just ended and would be $250 million more in the current three-month period.]]></content:encoded></item><item><title>Trust Building is Simple - Here&apos;s How</title><link>https://hackernoon.com/trust-building-is-simple-heres-how?source=rss</link><author>Startups Of The Week</author><category>tech</category><pubDate>Thu, 25 Sep 2025 15:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Starting a company. Building a brand… In  economy?\
Yeah, it is only going to get harder from now on. When content can be mass-generated, automated, and scheduled weeks or even months in advance, it is nearly impossible for new startups to keep pace with competitors. We used to think consistency is the key to building reach and customer trust, but when everyone is to be trusted, who do you actually trust?\
Trust is the shortcut to getting considered in the first place, since customers look to credible voices and reviews long before they talk to sales, it is important for startups to position themselves in a trustworthy position first.We are at the point where buyers are increasingly harder to convince. They are cautious, and they expect receipts. This requires a more creative approach, meaning the old ways with slogans, consistent social posts, and how-to articles are no longer relevant since the content and online marketing market are getting more and more saturated.\
So, how do startups tackle this problem? The answer is simple — earn attention by being helpful and transparent. Customers do most of their homework without vendors present, so trustworthy content is how a young brand gets onto the shortlist and stays there. You need to present your value to the public, and beyond what solely the product can offer. This means startups need to find a way to connect, stay genuine, and stay relevant.What Content Actually WorksStrong content opens doors by making readers more receptive to outreach and more likely to research a solution they weren’t considering before. Therefore, consider providing content that delivers new knowledge or solves a real problem. Here are some possible suggestions:Interviews and genuine storiesRun short interviews with customers and domain experts, then publish the highlights in their words with clear problems, decisions, and outcomes to let the audience “hear” peers they trust. Authenticity beats polish, so include what didn’t work and what changed after feedback, because candid follow‑through builds more confidence than perfect marketing.\
Check out  from the HackerNoon Editors on how to be the best interviewer out there!Remember, your article will only benefit from having examples and further analysis. Borrow credibility by pairing opinions with data from usage metrics or respected third‑party studies, and link those facts directly to the advice in the piece.\
When in doubt, show a chart, a measurable result, or a quote from a buyer who got the outcome. Stay grounded in the specifics because they can demonstrate the level of authenticity that no claims can. It shows that your startup is real and is doing everything with intention.\
Check out how HackerNoon editors structure their articles and provide examples to back up the claims !Your community is your free marketing army. Building one is hard, but all the efforts will be worth it. In order to find and build a community around your product or startups, you need to meet people where they already are — LinkedIn threads, review platforms, and practitioner groups — because that’s where research and recommendations actually happen. Show up consistently with AMAs, helpful replies, and quick demos, then close the loop in public when feedback ships as a fix or feature.\
It is crucial to stay consistent because people don’t like to wait, and it is pivotal for your brand to stay present for your customers.Salesy content that hides the “how” under buzzwords drives readers away and erodes credibility fast. It is important to position your company and brand as “a helpful colleague” - answer to the point, be short and sweet! Keep your customer engaged through a series of relevant content.Inconsistent posting breaks momentum; a light, steady cadence beats sporadic bursts that vanish for months. This is why having a schedule is important. Not only will you be able to keep your company up to the paces of corporate marketing, but it also helps with:::tip
Check out  on how to build a consistent content schedule!Buyers trust useful ideas that travel through their networks, not ads, and they reward brands that show up with clarity and substance long before the pitch. The shortlist is shrinking and risk is up, so consistent, evidence‑backed content and visible community presence are how emerging vendors earn a shot.It’ll help you tackle all these struggles—fast, and all in one place: Put your story in front of . Build instant credibility by publishing on a respected platform with a  (check Ahrefs 😉). Startup-friendly packages with a credit system—you only pay when your content gets accepted. Plus, your story gets extra exposure across social media, newsletters, and more—not just on our homepage.\
Plus, with Business Blogging, you get: to make your story shineMultiple permanent placements on HackerNoon and social media promotionsStories converted into  and distributed via audio RSS feedsYour brand also gains domain authority and SEOInstead of juggling three separate struggles—visibility, trust, and limited resources—you can solve them all with one move.Speaking of which… Meet some of the most trusted Startups YOU should know about!\
ShellHub is a powerful and easy-to-use tool that allows you to remotely manage and access any Linux device securely. The company has won HackerNoon Startup of The Year 2024 in Porto Alegre, Brazil.\
Didit is the advanced identity verification platform for the AI era: launch in minutes via no‑code or APIs, build custom KYC/AML/biometrics/age/PoA workflows, and stay compliant worldwide. Fully self‑service and developer‑first with complete APIs and webhooks. Modular, open, and up to 70% cheaper than legacy providers — the identity layer of the internet for fintechs, telcos, and global platforms.\
Based in Barcelona, Spain, Didit was the runner-up in its region for HackerNoon’s Startup of The Year 2024\
Founded in 2020, YOBE Ventures backs standout tech startups from Seed to Series A and advises founders on transparent, effective fundraising, bridging ambitious teams with aligned investors and LPs to drive values‑led, profitable deals while delivering high‑quality capital‑raising results.\
That’s all for this week. Until next time, hackers!About HackerNoon’s Startups of The YearStartups of The Year 2024 is HackerNoon’s flagship community-driven event celebrating startups, technology, and the spirit of innovation. Currently in its third iteration, the prestigious Internet award recognizes and celebrates tech startups of all shapes and sizes. This year, over 150,000 entities across 4200+ cities, 6 continents, and 100+ industries will participate in a bid to be crowned the best startup of the year! Millions of votes have been cast over the past few years, and many stories have been written about these daring and rising startups.\
Visit our FAQ page to learn more.\
Download our design assets here.\
Check out the Startups of the Year Merch Shop here.]]></content:encoded></item><item><title>Doorstep raises $8M seed to help find missing food deliveries</title><link>https://techcrunch.com/2025/09/25/doorstep-raises-8m-seed-to-help-find-missing-food-deliveries/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Thu, 25 Sep 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Doorstep integrates into existing delivery apps and then, using phone sensors, tracks when a driver has entered a building, gone up an elevator, and made it to the desired doorstep.  ]]></content:encoded></item><item><title>What top VCs want from AI founders: Inside the investor lens with Jon McNeill, Aileen Lee, and Steve Jang at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/09/25/what-top-vcs-want-from-ai-founders-inside-the-investor-lens-with-jon-mcneill-aileen-lee-and-steve-jang-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Thu, 25 Sep 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Jon McNeill (DVx Ventures), Aileen Lee (Cowboy Ventures), and Steve Jang (Kindred Ventures) share what AI founders need to know now: from defensibility to term sheets. TechCrunch Disrupt 2025 takes place October 27–29 in San Francisco. Register before tomorrow ends to save up to $668.]]></content:encoded></item><item><title>X-ray Scans Reveal the Hidden Risks of Cheap Batteries</title><link>https://tech.slashdot.org/story/25/09/25/126212/x-ray-scans-reveal-the-hidden-risks-of-cheap-batteries?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 25 Sep 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Lumafield's CT scan analysis of 1,054 lithium-ion 18650 batteries found 33 cells from low-cost and counterfeit brands contained a serious manufacturing defect called negative anode overhang, which increases risks of internal short-circuiting and battery fires. All defective batteries came from the 424 units sourced from budget brands on Amazon and Temu. 

The defect rate reached nearly 8% among low-cost cells, climbing to 12-15% for certain counterfeit brands claiming impossible 9,900 mAh capacities. None of the batteries from Samsung, Panasonic, and other established manufacturers exhibited the defect. The low-cost batteries also displayed significantly worse edge alignment of internal wound layers. Real-world testing revealed the counterfeit cells delivered under 1,300 mAh capacity despite their inflated specifications, compared to 3,000-3,450 mAh for legitimate 18650 batteries.]]></content:encoded></item><item><title>Meta rolls out Teen Accounts on Facebook and Messenger globally</title><link>https://techcrunch.com/2025/09/25/meta-rolls-out-teen-accounts-on-facebook-and-messenger-globally/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 25 Sep 2025 14:33:24 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Teen Accounts first rolled out to Instagram a year ago after Meta and other popular social networks were grilled by U.S. lawmakers for not doing enough to protect teens on their services.]]></content:encoded></item><item><title>Clarifai’s new reasoning engine makes AI models faster and less expensive</title><link>https://techcrunch.com/2025/09/25/clarifais-new-reasoning-engine-makes-ai-models-faster-and-less-expensive/</link><author>Russell Brandom</author><category>tech</category><pubDate>Thu, 25 Sep 2025 14:13:43 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The AI platform Clarifai has launched a new reasoning engine that it claims will make running AI models twice as fast and 40% less expensive.]]></content:encoded></item><item><title>Apple Asks EU To Scrap Landmark Digital Competition Law</title><link>https://apple.slashdot.org/story/25/09/25/0410249/apple-asks-eu-to-scrap-landmark-digital-competition-law?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 25 Sep 2025 14:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple asked the European Union to scrap its landmark digital competition law on Thursday, arguing that it poses security risks and creates a "worse experience" for consumers. From a report: The US tech giant and the EU have repeatedly locked horns over the bloc's Digital Markets Act (DMA), which Brussels says seeks to make the digital sector in the 27-nation bloc fairer and more open. "The DMA should be repealed while a more appropriate fit for purpose legislative instrument is put in place," Apple said in a formal submission to the European Commission as part of a consultation on the law. 

[...] "It's become clear that the DMA is leading to a worse experience for Apple users in the EU," the tech giant said in a blog post accompanying its submission. "It's exposing them to new risks, and disrupting the simple, seamless way their Apple products work together."]]></content:encoded></item><item><title>A Guide to Effective PR Reviews - Part 1</title><link>https://hackernoon.com/a-guide-to-effective-pr-reviews-part-1?source=rss</link><author>Jacob Landry</author><category>tech</category><pubDate>Thu, 25 Sep 2025 14:00:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How to Best Review Pull Requests (Without Governing Solutions)In tech, we’re always working toward writing better, stronger, more maintainable code. Code reviews are an essential part of that process. Yet, it’s a skill we rarely focus on improving. Too often, reviews become something we stumble through, slowing teams down and hurting overall quality.\
Becoming a quality reviewer can make you a real asset to your team, but it’s not easy. Many of us approach code reviews from the wrong angle or with misguided purposes. Learning to review the  in the  can transform your team’s dynamic and set you on a path for success.The Purpose of a PR ReviewAs engineers, our primary job is to write code. To solve problems. To keep the world turning one bit or byte at a time. But what’s our role when reviewing code? That question can be jarring, especially the first time you’re asked to review someone else’s code.\
So, let’s start with what you’re not expected to do:You’re not expected to solve the problem for the author.You’re not expected to rewrite their code to match your own vision.\
So, what are you responsible for?\
A team’s success often depends on the quality of code that makes it into production and how quickly critical issues are discovered and resolved. The main goal of a PR review is to ensure that:the problem posed has been solved,the solution is clean and testable, andthe code is efficient and complete.\
Often, the solution will look very different from how you would have solved it, and that’s fine.. That’s not relevant to the review. What matters is whether the code works, is maintainable, and can be tested effectively.\
Another major benefit of reviews is knowledge transfer. Everyone solves problems differently, and reviewing code is an opportunity for the team to learn from one another. Approach reviews as a chance to expand your perspective, not dictate solutions.\
So, let’s dive in a little more and talk about why you’re doing this.We’ve all been there: you start solving a problem, discover a cool tangential idea, and suddenly you’re building something completely different. You get to the end and you excitedly hold up your idea for all to see, only to realize that you didn’t actually solve the problem you set out to solve. You built something cool and amazing, but it doesn’t solve the intended purpose. That code may be useful someday, but it doesn’t solve the intended problem.\
The PR review can help us avoid these little misdirections from making it into production and causing customer woes. The reviewer should focus on the problem at hand. Have a small familiarity with the tickets associated with this PR, have read the README or comments on the PR, and have a solid understanding of the problem being solved. We want to be sure that the engineer understood the problem as well and tackled the right issue. Mistakes happen, and that’s ok. We just need to correct them whenever possible.Most of us will write code in an iterative fashion, meaning that we solve the problem instinctually but then rewrite the code several times to make it as clean, clear, and efficient as possible. But sometimes, we don’t make quite as many iterations as we could have to get to the true bottom line. Here’s the tricky part: your job isn’t to judge whether the solution is the absolute cleanest possible, but whether it’s clean enough to be testable and maintainable. This is a very fine line that can be difficult to traverse. You don’t want to lose the forest for the trees, as they say.\
A review should focus on whether this code is in a state where it will be easy to test and maintain; that’s it. Read the code and envision future bugs or changes that could arise, and how difficult will this code be to debug or change? If it seems confusing or complicated, we should probably simplify it. However, if the code seems easy enough to work with but you can envision an even better solution, it’s ok to call that out, but don’t let that block the review. \
You’re not here to solve the problem for the author, but to make sure that they solved the problem themself. Tips and tricks help us learn and grow, so your advice is welcome, but don’t make it a blocking issue. You can’t dictate how the problem is solved.Code reviews aren’t about proving you’re the smartest engineer in the room or forcing every solution to match your style. They’re about safeguarding quality, ensuring testability, and strengthening the team through shared knowledge.\
If you approach reviews by focusing on  the code accomplishes instead of  it gets there, you’ll help your team move faster, build trust, and foster collaboration.\
In the next part, we’ll dig into the : the principles and practices that make a review effective, respectful, and productive. Because even if you know the purpose, it takes real skill to deliver feedback that elevates both the code and the coder.]]></content:encoded></item><item><title>Less than 48 hours to grab your TechCrunch Disrupt 2025 ticket savings</title><link>https://techcrunch.com/2025/09/25/these-are-the-last-2-days-for-techcrunch-disrupt-2025-ticket-savings/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Thu, 25 Sep 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Ticktock! Less than 2 days remain to secure savings of up to $668 on your TechCrunch Disrupt 2025 pass. This offer ends tomorrow, September 26, at 11:59 p.m. PT. If you haven’t registered yet, now’s the moment. ]]></content:encoded></item><item><title>Databricks will bake OpenAI models into its products in $100M bet to spur enterprise adoption</title><link>https://techcrunch.com/2025/09/25/databricks-will-bake-openai-models-into-its-products-in-100m-bet-to-spur-enterprise-adoption/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Thu, 25 Sep 2025 13:14:16 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Databricks is on the hook to pay at least $100 million to OpenAI in this deal, even if customer usage falls short. It's a bet, but one that Databricks has already hedged.]]></content:encoded></item><item><title>Apple blames EU’s Digital Markets Act for feature delays</title><link>https://techcrunch.com/2025/09/25/apple-blames-eus-digital-markets-act-for-feature-delays/</link><author>Ram Iyer</author><category>tech</category><pubDate>Thu, 25 Sep 2025 13:02:50 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple is blaming the EU's enforcement of the DMA for delaying the launch of some features in the EU, saying the rules are "leading to a worse experience" for Apple customers in the bloc by exposing them to new risks and reducing choices.]]></content:encoded></item><item><title>Facebook Data Reveal the Devastating Real-World Harms Caused By the Spread of Misinformation</title><link>https://tech.slashdot.org/story/25/09/24/2244215/facebook-data-reveal-the-devastating-real-world-harms-caused-by-the-spread-of-misinformation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 25 Sep 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Conversation: Twenty-one years after Facebook's launch, Australia's top 25 news outlets now have a combined 27.6 million followers on the platform. They rely on Facebook's reach more than ever, posting far more stories there than in the past. With access to Meta's Content Library (Meta is the owner of Facebook), our big data study analysed more than three million posts from 25 Australian news publishers. We wanted to understand how content is distributed, how audiences engage with news topics, and the nature of misinformation spread. The study enabled us to track de-identified Facebook comments and take a closer look at examples of how misinformation spreads. These included cases about election integrity, the environment (floods) and health misinformation such as hydroxychloroquine promotion during the COVID pandemic. The data reveal misinformation's real-world impact: it isn't just a digital issue, it's linked to poor health outcomes, falling public trust, and significant societal harm. [...]
 
Our study has lessons for public figures and institutions. They, especially politicians, must lead in curbing misinformation, as their misleading statements are quickly amplified by the public. Social media and mainstream media also play an important role in limiting the circulation of misinformation. As Australians increasingly rely on social media for news, mainstream media can provide credible information and counter misinformation through their online story posts. Digital platforms can also curb algorithmic spread and remove dangerous content that leads to real-world harms. The study offers evidence of a change over time in audiences' news consumption patterns. Whether this is due to news avoidance or changes in algorithmic promotion is unclear. But it is clear that from 2016 to 2024, online audiences increasingly engaged with arts, lifestyle and celebrity news over politics, leading media outlets to prioritize posting stories that entertain rather than inform. This shift may pose a challenge to mitigating misinformation with hard news facts. Finally, the study shows that fact-checking, while valuable, is not a silver bullet. Combating misinformation requires a multi-pronged approach, including counter-messaging by trusted civic leaders, media and digital literacy campaigns, and public restraint in sharing unverified content.]]></content:encoded></item><item><title>Starpath bets on mass-produced, space-rated solar</title><link>https://techcrunch.com/2025/09/25/starpath-bets-on-mass-produced-space-rated-solar/</link><author>Aria Alamalhodaei</author><category>tech</category><pubDate>Thu, 25 Sep 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Starpath is changing the space-rated solar panel industry in its quest to build off-world colonies. ]]></content:encoded></item><item><title>As many as 2 million Cisco devices affected by actively exploited 0-day</title><link>https://arstechnica.com/security/2025/09/as-many-as-2-million-cisco-devices-affected-by-actively-exploited-0-day/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1601469961-1024x648.jpg" length="" type=""/><pubDate>Thu, 25 Sep 2025 12:43:42 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[As many as 2 million Cisco devices are susceptible to an actively exploited zero-day that can remotely crash or execute code on vulnerable systems.Cisco said Wednesday that the vulnerability, tracked as CVE-2025-20352, was present in all supported versions of Cisco IOS and Cisco IOS XE, the operating system that powers a wide variety of the company’s networking devices. The vulnerability can be exploited by low-privileged users to create a denial-of-service attack or by higher-privileged users to execute code that runs with unfettered root privileges. It carries a severity rating of 7.7 out of a possible 10.Exposing SNMP to the Internet? Yep“The Cisco Product Security Incident Response Team (PSIRT) became aware of successful exploitation of this vulnerability in the wild after local Administrator credentials were compromised,” Wednesday’s advisory stated. “Cisco strongly recommends that customers upgrade to a fixed software release to remediate this vulnerability.”]]></content:encoded></item><item><title>Spotify to label AI music, filter spam and more in AI policy change</title><link>https://techcrunch.com/2025/09/25/spotify-updates-ai-policy-to-label-tracks-cut-down-on-spam/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 25 Sep 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Spotify is launching a music spam filter, labeling AI tracks, and clarifying its rules around AI voice clones. ]]></content:encoded></item><item><title>TuneIn partners with FEMA to give drivers real-time emergency alerts</title><link>https://techcrunch.com/2025/09/25/tunein-partners-with-fema-to-give-drivers-real-time-emergency-alerts/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Thu, 25 Sep 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Drivers who listen to TuneIn will now receive real-time alerts, including extreme weather warnings and other emergencies. ]]></content:encoded></item><item><title>Japanese City Passes Two-Hours-a-Day Smartphone Usage Ordinance</title><link>https://yro.slashdot.org/story/25/09/24/2249237/japanese-city-passes-two-hours-a-day-smartphone-usage-ordinance?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 25 Sep 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Japanese city of Toyoake has passed (PDF) a symbolic ordinance limiting recreational smartphone use to two hours a day, aiming to improve citizens' sleep -- especially for students after summer vacation. The Register reports: "The primary purpose of this ordinance is to ensure that all citizens receive adequate sleep," states a Council information page, which explains that many Japanese people ignore Ministry of Health, Labor and Welfare recommendations to spend six to eight hours a day dozing. An accompanying FAQ [PDF] explains that Council passed the ordinance because students who return to school after summer vacations sometimes need a nudge the re-establish an appropriate daily regime.
 
The ordinance also points out "Excessive phone users and their families are facing difficulties in their daily and social lives," and suggests the two-hours-a-day guidance might help. Council's documents point out that smartphones have myriad uses beyond recreation, and that the ordinance should not be taken as a suggestion to reduce overall use of the devices. Toyoake is part of the Nagoya megalopolis and is home to around 70,000 people. The town's government plans to survey residents about the ordinance, and the FAQ also mentions it wants to tackle other digital menaces, among them harmful effects of using smartphones while walking.]]></content:encoded></item><item><title>How Automated Tools Are Making Open Source Software Safer</title><link>https://hackernoon.com/how-automated-tools-are-making-open-source-software-safer?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Thu, 25 Sep 2025 09:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[. Our findings of unethical behavior may not generalize beyond the studied OSS projects and issues/PRs. There could be unethical behavior that are not reported to the issue tracker. Unfortunately, there is no conceivable way to study these unreported issues. As some issues may not have the ethics-related keywords that we used for searching, we could have also missed some unethical behavior. Nevertheless, our selected keywords already help us in discovering many types of unethical behavior. Hence, we believe the issues in our study provide a representative sample of the reported and resolved unethical issues in our studied repositories. While other types of unethical behavior discovered in our study is important, Etor can only detect six of them, and our evaluation is limited to these six types. Nevertheless, our experiments show that Etor can detect unethical behavior with relatively high accuracy.\
. Our code and scripts may have bugs that can affect our results. To mitigate this threat, we make our tool and data publicly available for inspection.To better understand unethical behavior in OSS projects, we conduct a study of the types of unethical behavior in OSS projects. By reading and analyzing the discussion of stakeholders in OSS projects, our study of 316 GitHub issues identifies 15 types of unethical behavior. These unethical behaviors are affected by various types of software artifacts. Inspired by our study, we propose Etor, an ontology-based approach that can automatically detect unethical behavior. Our evaluation of Etor on 195,621 issues (1,765 repositories) shows that Etor can automatically detect 548 issues with 74.8% TP rate on average. As the first study that investigates the types of unethical behavior in OSS projects, we hope to raise awareness among OSS stakeholders regarding the importance of understanding ethical issues in OSS projects. While Etor shows promising results in automated detection of unethical behavior in OSS projects, we plan to enhance Etor in future to detect more types and reduce false positives using machine learning techniques.[1] [n.d.]. https://github.com/eslint/eslint/pull/15102\
[2] [n.d.]. https://www.w3.org/2001/sw/#owl\
[3] [n.d.]. http://www.w3.org/Submission/SWRL/\
[4] [n.d.]. https://github.com/Pryaxis/handbook/issues/3\
[5] [n.d.]. https://github.com/novus-package-manager/novus/issues/3\
[6] [n.d.]. https://github.com/biddyweb/yes-cart/issues/33\
[7] [n.d.]. https://github.com/CircuitVerse/Interactive-Book/issues/80\
[8] [n.d.]. https://github.com/mpdf/mpdf/issues/15\
[9] [n.d.]. https://github.com/pkalogiros/AudioMass/issues/1\
[10] [n.d.]. https://github.com/minio/minio/issues/12143\
[11] [n.d.]. https://github.com/wger-project/wger/issues/266\
[12] [n.d.]. https://github.com/tranleduy2000/javaide/issues/236\
[13] [n.d.]. https://github.com/flyingsaucerproject/flyingsaucer/pull/123\
[14] [n.d.]. https://github.com/click-llc/click-integration-django/issues/1\
[15] [n.d.]. https://github.com/twbs/bootstrap/issues/5632\
[16] [n.d.]. https://github.com/NetHack/NetHack/issues/359\
[17] [n.d.]. https://github.com/EasyEngine/easyengine/issues/488\
[18] [n.d.]. https://github.com/katzwebservices/Contact-Form-7-Newsletter/issues/ 79\
[19] [n.d.]. https://docs.github.com/en/rest/repos\
[20] [n.d.]. https://www.legislation.gov.au/Details/C2017C00180\
[21] [n.d.]. https://github.com/manuel-freire/ac2\
[22] [n.d.]. https://docs.github.com/en/communities/setting-up-your-project-forhealthy-contributions/adding-a-license-to-a-repository\
[23] [n.d.]. https://docs.github.com/en/repositories/managing-your-repositoryssettings-and-features/customizing-your-repository/licensing-a-repository\
[24] [n.d.]. https://github.com/PyGithub/PyGithub\
[25] [n.d.]. https://github.com/Anarios/return-youtube-dislike/issues/401 [26] [n.d.]. https://docs.github.com/en/repositories/managing-your-repositoryssettings-and-features/enabling-features-for-your-repository/disabling-issues\
[27] [n.d.]. https://github.com/rydercalmdown/packagepreventor\
[28] [n.d.]. https://github.com/EtorChecker/Etor\
[29] [n.d.]. ailab. https://github.com/bilibili/ailab\
[30] [n.d.]. Are we correctly handling console.Console in node objectKeys(console)? https://github.com/sindresorhus/ts-extras/issues/50\
[31] [n.d.]. CUDA vs Naive Speedup? https://github.com/d-li14/involution/issues/1\
[32] [n.d.]. DogeBot2. https://github.com/DGXeon/DogeBot2 [33] [n.d.]. Squeeze tooltip in the sections panel. https://github.com/livebook-dev/ livebook/pull/536\
[34] [n.d.]. VIP. https://github.com/Oreomeow/VIP\
[35] [n.d.]. What is Plagiarism? ([n. d.]). https://www.plagiarism.org/article/what-isplagiarism\
[36] 2021. , Report on University of Minnesota Breach-of-Trust Incident pages. https: //lwn.net/ml/linux-kernel/202105051005.49BFABCE@keescook/\
[37] Anneliese Amschler Andrews and Arundeep S %J Empirical Software Engineering Pradhan. 2001. Ethical issues in empirical software engineering: the limits of policy. 6, 2 (2001), 105–110.\
[38] Grigoris Antoniou and Frank van Harmelen. 2004. Web ontology language: Owl. In Handbook on ontologies. Springer, 67–92.\
[39] Deepika Badampudi. [n.d.]. Reporting ethics considerations in software engineering publications. In 2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM). IEEE, 205–210.\
[40] Sebastian Baltes and Stephan Diehl. 2016. Worse than spam: Issues in sampling software developers. In Proceedings of the 10th ACM/IEEE international symposium on empirical software engineering and measurement. 1–6.\
[41] Sebastian Baltes and Stephan Diehl. 2019. Usage and attribution of Stack Overflow code snippets in GitHub projects. Empirical Software Engineering 24, 3 (2019), 1259–1295.\
[42] Sebastian Baltes, Richard Kiefer, and Stephan Diehl. 2017. Attribution required: Stack overflow code snippets in GitHub projects. In 2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C). IEEE, 161–163.\
[43] Dizza Beimel and Mor Peleg. 2011. Using OWL and SWRL to represent and reason with situation-based access control policies. Data & Knowledge Engineering 70, 6 (2011), 596–615.\
[44] Stephen R Bergerson. 2000. E-commerce Privacy and the Black Hole of Cyberspace. Wm. Mitchell L. Rev. 27 (2000), 1527.\
[45] Hanene Boussi Rahmouni, Tony Solomonides, Marco Casassa Mont, and Simon Shiu. 2009. Modelling and enforcing privacy for medical data disclosure across Europe. In Medical Informatics in a United and Healthy Europe. IOS Press, 695–699.\
[46] Mark Cenite, Benjamin H Detenber, Andy WK Koh, Alvin LH Lim, Ng Ee %J New Media Soon, and Society. 2009. Doing the right thing online: a survey of bloggers’ ethical beliefs and practices. 11, 4 (2009), 575–597.\
[47] Jason A Colquitt. 2001. On the dimensionality of organizational justice: a construct validation of a measure. Journal of applied psychology 86, 3 (2001), 386.\
[48] Daniela S Cruzes and Tore Dyba. 2011. Recommended steps for thematic synthesis in software engineering. In 2011 international symposium on empirical software engineering and measurement. IEEE, 275–284.\
[49] Daniela America da Silva, Henrique Duarte Borges Louro, Gildarcio Sousa Goncalves, Johnny Cardoso Marques, Luiz Alberto Vieira Dias, Adilson Marques da Cunha, and Paulo Marcelo Tasinaffo. 2021. Could a Conversational AI Identify Offensive Language? Information 12, 10 (2021), 418.\
[50] Thomas Eisenbarth, Rainer Koschke, and Daniel Simon. 2003. Locating features in source code. IEEE Transactions on software engineering 29, 3 (2003), 210–224.\
[51] Batya Friedman, Peter H Kahn, Alan Borning, and Alina Huldtgren. 2013. Value sensitive design and information systems. Springer, 55–95.\
[52] Daniel M German, Yuki Manabe, and Katsuro Inoue. 2010. A sentence-matching method for automatic license identification of source code files. In Proceedings of the IEEE/ACM international conference on Automated software engineering. 437–446.\
[53] Daniel M German, Gregorio Robles, Germán Poo-Caamaño, Xin Yang, Hajimu Iida, and Katsuro Inoue. 2018. "Was My Contribution Fairly Reviewed?" A Framework to Study the Perception of Fairness in Modern Code Reviews. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 523–534.\
[54] Nicolas E Gold and Jens Krinke. [n.d.]. Ethical Mining: A Case Study on MSR Mining Challenges. In Proceedings of the 17th International Conference on Mining Software Repositories. 265–276.\
[55] Yaroslav Golubev, Maria Eliseeva, Nikita Povarov, and Timofey Bryksin. 2020. A study of potential code borrowing and license violations in java projects on github. In Proceedings of the 17th International Conference on Mining Software Repositories. 54–64.\
[56] Frances S Grodzinsky, Keith Miller, and Marty J Wolf. 2003. Ethical issues in open source software. Journal of Information, Communication and Ethics in Society (2003).\
[57] Idris Hsi and Colin Potts. 2000. Studying the Evolution and Enhancement of Software Features.. In icsm. 143.\
[58] Syed Fatiul Huq, Ali Zafar Sadiq, and Kazi Sakib. 2019. Understanding the effect of developer sentiment on fix-inducing changes: An exploratory study on github pull requests. In 2019 26th Asia-Pacific Software Engineering Conference (APSEC). IEEE, 514–521.\
[59] Nasif Imtiaz, Justin Middleton, Joymallya Chakraborty, Neill Robson, Gina Bai, and Emerson Murphy-Hill. [n.d.]. Investigating the effects of gender bias on GitHub. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 700–711.\
[60] Georgia M Kapitsaki, Frederik Kramer, and Nikolaos D Tselikas. 2017. Automating the license compatibility process in open source software with SPDX. Journal of systems and software 131 (2017), 386–401.\
[61] Georgia M Kapitsaki, Nikolaos D Tselikas, and Ioannis E Foukarakis. 2015. An insight into license tools for open source software systems. Journal of Systems and Software 102 (2015), 72–87.\
[62] ASM Kayes, Wenny Rahayu, Tharam Dillon, and Elizabeth Chang. 2018. Accessing data from multiple sources through context-aware access control. In 2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE). IEEE, 551–559.\
[63] David Kocsis and Gert-Jan de Vreede. 2016. Towards a taxonomy of ethical considerations in crowdsourcing. (2016).\
[64] Josh Lerner and Jean Tirole. 2005. The scope of open source licensing. Journal of Law, Economics, and Organization 21, 1 (2005), 20–56.\
[65] Tyler McDonnell, Baishakhi Ray, and Miryung Kim. 2013. An empirical study of api stability and adoption in the android ecosystem. In 2013 IEEE International Conference on Software Maintenance. IEEE, 70–79.\
[66] Deborah L McGuinness, Frank Van Harmelen, et al. 2004. OWL web ontology language overview. W3C recommendation 10, 10 (2004), 2004.\
[67] Stuart McIlroy, Nasir Ali, and Ahmed E Hassan. 2016. Fresh apps: an empirical study of frequently-updated mobile apps in the Google play store. Empirical Software Engineering 21, 3 (2016), 1346–1370.\
[68] Andrew McNamara, Justin Smith, and Emerson Murphy-Hill. [n.d.]. Does ACM’s code of ethics change ethical decision making in software development?. In Proceedings of the 2018 26th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering. 729–733.\
[69] Brent Mittelstadt. 2019. Principles alone cannot guarantee ethical AI. Nature Machine Intelligence 1 (11 2019). https://doi.org/10.1038/s42256-019-0114-4\
[70] Mainack Mondal, Leandro Araújo Silva, and Fabrício Benevenuto. 2017. A measurement study of hate speech in social media. In Proceedings of the 28th ACM conference on hypertext and social media. 85–94.\
[71] Mark A Musen. 2015. The protégé project: a look back and a look forward. AI matters 1, 4 (2015), 4–12.\
[72] Linus Nyman and Tommi Mikkonen. 2011. To fork or not to fork: Fork motivations in SourceForge projects. International Journal of Open Source Software and Processes (IJOSSP) 3, 3 (2011), 1–9.\
[73] Christopher Oezbek et al. 2008. Research ethics for studying Open Source projects. 4th Research Room FOSDEM: Libre software communities meet research community (2008).\
[74] Rolf-Helge Pfeiffer. 2020. What constitutes software? An empirical, descriptive study of artifacts. In Proceedings of the 17th International Conference on Mining Software Repositories. 481–491.\
[75] Janice Singer and Norman G. %J IEEE Transactions on Software Engineering Vinson. 2002. Ethical issues in empirical studies of software engineering. 28, 12 (2002), 1171–1180.\
[76] Josh Terrell, Andrew Kofink, Justin Middleton, Clarissa Rainear, Emerson R Murphy-Hill, and Chris Parnin. 2016. Gender bias in open source: Pull request acceptance of women versus men. PeerJ Prepr. 4 (2016), e1733.\
[77] Matteo Turilli and Luciano Floridi. 2009. The ethics of information transparency. Ethics and Information Technology 11, 2 (2009), 105–112.\
[78] Christopher Vendome, Mario Linares-Vásquez, Gabriele Bavota, Massimiliano Di Penta, Daniel German, and Denys Poshyvanyk. 2017. Machine learning-based detection of open source license exceptions. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE, 118–129.\
[79] Christopher Vendome, Mario Linares-Vásquez, Gabriele Bavota, Massimiliano Di Penta, Daniel German, and Denys Poshyvanyk. [n.d.]. License usage and changes: a large-scale study of java projects on github. In 2015 IEEE 23rd International Conference on Program Comprehension. IEEE, 218–228.\
[80] Denny Vrandečić. 2009. Ontology evaluation. In Handbook on ontologies. Springer, 293–313.\
[81] Qiushi Wu and Kangjie Lu. 2021. On the feasibility of stealthily introducing vulnerabilities in open-source software via hypocrite commits. In Proc. Oakland.\
[82] Sihan Xu, Ya Gao, Lingling Fan, Zheli Liu, Yang Liu, and Hua Ji. 2021. LiDetector: License Incompatibility Detection for Open Source Software. ACM Transactions on Software Engineering and Methodology (2021).\
[83] Di Yang, Pedro Martins, Vaibhav Saini, and Cristina Lopes. 2017. Stack overflow in github: any snippets there?. In 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR). IEEE, 280–290.(1) Hsu Myat Win, Southern University of Science and Technology, China (11960003@mail.sustech.edu.cn);(2) Haibo Wang, Southern University of Science and Technology, China (wanghb2020@mail.sustech.edu.cn);(3) Shin Hwei Tan, a corresponding author from Southern University of Science and Technology, China (tansh3@sustech.edu.cn).]]></content:encoded></item><item><title>The Day I Learned My NAS Was Traceable Through TLS Logs</title><link>https://hackernoon.com/the-day-i-learned-my-nas-was-traceable-through-tls-logs?source=rss</link><author>Nicolas Fränkel</author><category>tech</category><pubDate>Thu, 25 Sep 2025 08:44:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I recently learned about a new way to leak your privacy, and it's a scary one. Before going further, know that I'm not a network engineer: perhaps if you work in this field, you've known it for your whole career, but it's quite new to me. Let me share my findings, and you can judge for yourself.Since the original post was quite lengthy, I have broken it down into two installments: the problem and the solution.I own my own domain. I've created multiple subdomains out of it. Some of them map to online services, one of them to a private Cloudflare Tunnel to my Home Assistant, and one to my NAS hosted at home. The latter obviously forwards requests to my router, which forwards NAS-related requests to it. The router, in turn, has a public IP address given by my Internet provider. Reminder: in the European Union, IPs are considered  in the legal sense of the term. In France, revealing such data is a  offense, punishable by up to 5 years in prison and up to a 300k€ fine. You can read article 226-2 of the French penal code if you're interested in the details. In theory, nobody would know about my subdomains, including the one that points to my router IP address.To secure the connection with my NAS, I'm using Let's Encrypt to get a TLS certificate. Let's Encrypt has been a boon for individuals to get free certificates compared to when one had to pay for them. Aye, there's the rub! Let's Encrypt logs certificate requests in a central registry, namely Certificate Transparency. But Let's Encrypt isn't the only one: every public TLS certificate issuer does the same (Google, Cloudflare, etc.). Actually, there's a publicly available tool to check the logs for a domain: https://crt.sh/. We can check the logs of a [French famous newspaper](French famous newspaper) to see how they leak personal data. Here's a small excerpt:See how effortlessly we get access to subdomains?The conclusion is simple: every single time you request a TLS certificate from one of the public issuers, it's logged  publicly available. From the subdomain, anyone can trace the IP from the DNS records, and if one of the subdomains points to your home, you're easily traceable. If the above looks pretty grim, it's because it is.Even worse: logs seem to be present forever. If you check with lemonde.fr, you'll see the earliest log is from Digicert in 2013.Let's look at the possible fixes.Remove the subdomain: It's an option if you don't need external access. I possibly could, since accessing my NAS remotely is not of utmost importance. However, it would remove the fun of trying to find a solution.Remove HTTPS: Do I need to explain how bad an idea it is?Obfuscate the subdomain: Instead of , you could have something cryptic like . Unfortunately, as the number of subdomains is pretty limited, one can check them one by one and still locate the IP of your home.Use Cloudflare Tunnel: I'm using Cloudflare Tunnel for my Home Assistant. I didn't find its certificate requests in the logs. The reason could be that it either uses wildcard certificates or that it doesn't log because its requests are internal.Use wildcard certificates: Wildcard certificate requests, , , are logged like any other. Yet, it doesn't leak any information about any subdomain.From the above pool of solutions, two seem valid to me: Cloudflare Tunnel and wildcard certificates.|    | Cloudflare Tunnel | Wildcard |
|----|----|----|
| Pro | Just works | Learning opportunity \n Easier fixability if it fails |
| Con | Depends on Cloudflare | Complexity \n Security: the certificate can be used on any subdomain |With this simple decision matrix, I chose to keep using Let's Encrypt, but with wildcard certificates. You may make another choice in your specific context.Any TLS certificate request to a public provider is logged in a publicly accessible registry. Certificate requests to subdomains that point to your home (or other places you don't want to disclose) can be traced to your IP via DNS records.A couple of solutions exist. In the follow-up post, I'll use Let's Encrypt on Synology to request a wildcard certificate.However, remember that once your requests have been logged, they are here forever. The only options to claim back your privacy are either to move physically to another place or, barring that, changing your Internet provider, which will provide you with a new IP.Originally published at A Java Geek on September 21st, 2025]]></content:encoded></item><item><title>BingX Elevates VIP Exclusivity With Zero-Fee Benefits, Concierge Services, and Premium Perks</title><link>https://hackernoon.com/bingx-elevates-vip-exclusivity-with-zero-fee-benefits-concierge-services-and-premium-perks?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Thu, 25 Sep 2025 07:27:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[PANAMA CITY, September 24, 2025 –, a leading cryptocurrency exchange and Web3 AI company, today announced a comprehensive upgrade to its, unveiling a host of new benefits and services designed to enhance the experience of its most valued users. The upgrade underscores BingX's commitment to delivering premium trading services with a strong focus on accessibility, performance, and trust.As part of the new initiative, BingX VIPs will gain access to exclusive perks including discounted fiat exchange rates, zero-fees for guaranteed price orders, and zero slippage for trigger orders, liquidation insurance, a new 13% APR seven-day wealth management product for first-time users, and privileged subscription quotas for Launchpool events. These benefits represent a significant step forward in delivering tangible advantages to traders seeking more flexibility, lower costs, and greater opportunities within the BingX ecosystem.To further elevate the VIP experience, BingX has established a dedicated relationship team offering concierge-style services and a unique VIP identity. Each VIP client is paired with a personal relationship manager who provides one-on-one guidance through essential processes such as account setup, deposits, and trading. This hands-on support ensures an efficient and trusted experience, tailored to the needs of advanced traders who value both precision and reliability."Launching this upgraded VIP program is all about fostering trust, strengthening relationships, and rewarding loyalty within our community," said, Chief Product Officer at BingX. "By combining exclusive financial benefits with dedicated relationship management, we're delivering the kind of user-first experience that defines our vision as an exchange, ensuring VIP clients not only feel supported, but truly empowered.”Looking ahead, BingX plans to continue to refine and expand its VIP offerings, ensuring its most dedicated users remain at the heart of its innovation. By blending industry-leading trading benefits with personalized service, BingX reaffirms its commitment to creating a trading environment built on transparency, reliability, and long-term partnership.Founded in 2018, BingX is a leading crypto exchange and Web3 AI company, serving a global community of over 20 million users. With a comprehensive suite of AI-powered products and services, including derivatives, spot trading, and copy trading, BingX caters to the evolving needs of users across all experience levels, from beginners to professionals. Committed to building a trustworthy and intelligent trading platform, BingX empowers users with innovative tools designed to enhance performance and confidence. In 2024, BingX proudly became the official crypto exchange partner of Chelsea Football Club, marking an exciting debut in the world of sports sponsorship.For media inquiries, please contact: For more information, please visit::::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Experimental Gene Therapy Found To Slow Huntington&apos;s Disease Progression</title><link>https://science.slashdot.org/story/25/09/24/2237215/experimental-gene-therapy-found-to-slow-huntingtons-disease-progression?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 25 Sep 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Doctors report the first successful treatment for Huntington's disease using a new type of gene therapy given during 12 to 18 hours of delicate brain surgery. The BBC reports: An emotional research team became tearful as they described how data shows the disease was slowed by 75% in patients. It means the decline you would normally expect in one year would take four years after treatment, giving patients decades of "good quality life", Prof Sarah Tabrizi told BBC News. The first symptoms of Huntington's disease tend to appear in your 30s or 40s and is normally fatal within two decades -- opening the possibility that earlier treatment could prevent symptoms from ever emerging. None of the patients who have been treated are being identified, but one was medically retired and has returned to work. Others in the trial are still walking despite being expected to need a wheelchair. Treatment is likely to be very expensive. However, this is a moment of real hope in a disease that hits people in their prime and devastates families. [...]
 
It starts with a safe virus that has been altered to contain a specially designed sequence of DNA. This is infused deep into the brain using real-time MRI scanning to guide a microcatheter to two brain regions - the caudate nucleus and the putamen. This takes 12 to 18 hours of neurosurgery. The virus then acts like a microscopic postman -- delivering the new piece of DNA inside brain cells, where it becomes active. This turns the neurons into a factory for making the therapy to avert their own death. The cells produce a small fragment of genetic material (called microRNA) that is designed to intercept and disable the instructions (called messenger RNA) being sent from the cells' DNA for building mutant huntingtin. This results in lower levels of mutant huntingtin in the brain. [...]
 
The data showed that three years after surgery there was an average 75% slowing of the disease based on a measure which combines cognition, motor function and the ability to manage in daily life. The data also shows the treatment is saving brain cells. Levels of neurofilaments in spinal fluid -- a clear sign of brain cells dying -- should have increased by a third if the disease continued to progress, but was actually lower than at the start of the trial.]]></content:encoded></item><item><title>The TechBeat: New frontiers in Human AI Interface (9/25/2025)</title><link>https://hackernoon.com/9-25-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Thu, 25 Sep 2025 06:10:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @codelynx [ 5 Min read ] 
 Exploring the progress of lunar base development and how it aligns with science fiction’s vision of life on the Moon.  Read More.By @lonewolf [ 8 Min read ] 
 Analyzing if Elon Musk's Mars colonization plans remain achievable beyond 2025, factoring in technology, funding, and future challenges. Read More.By @MichaelJerlis [ 4 Min read ] 
 Discover the most reliable passive income strategies in crypto for 2025 — from tokenized treasuries to staking, lending, farming, and more.  Read More.By @chainwire [ 5 Min read ] 
 With this launch, XYO introduces the first blockchain built to handle large volumes of data without slowing performance. Read More.By @siafoundation [ 7 Min read ] 
 Sia delivers true cloud reliability by eliminating single points of failure. Decentralized design ensures continuous performance, even during outages. Read More.By @socialdiscoverygroup [ 4 Min read ] 
 Remote Work in Paradise? 4 Years, 3 Islands, 1 Honest Guide. Discover the real trade-offs of Malta, Madeira & Canary Islands for digital nomads.  Read More.By @MichaelJerlis [ 3 Min read ] 
 Discover how DeFi lending and EMCD Onlock differs from banks: no middlemen, just protocols, smart contracts, and instant liquidity for borrowers and depositors. Read More.By @samuelogbonna138 [ 5 Min read ] 
 Space debris threatens satellites and economies. See how cleanup tech could unlock a trillion-dollar spacetech industry. Read More.By @zbruceli [ 12 Min read ] 
 Recent tech advances are breaking free from 20 years of 5-inch screen limits, unlocking full human senses in computing through AI interfaces and wearables. Read More.By @scylladb [ 8 Min read ] 
 How ShareChat scaled its ML feature store to 1B features/sec on ScyllaDB, achieving 1000X performance without scaling the database. Read More.By @paoloap [ 7 Min read ] 
 Vibe coding lets AI generate code—but skips the skills that make developers indispensable. Learn why shortcuts can ruin careers in 2025 tech.
 Read More.By @hacker-Antho [ 5 Min read ] 
 A groundbreaking NBER Working Paper, “How People Use ChatGPT”, finally pulls back the curtain on this phenomenon.  Read More.By @thesociable [ 6 Min read ] 
 WEF’s new blueprint, led by Larry Fink & Andre Hoffmann, aims to monetize nature with credits, swaps & asset schemes worth trillions. Read More.By @vicloskutova [ 15 Min read ] 
 Meet 10 influential women redefining AI—leaders, founders, and innovators shaping technology with creativity, ethics, and bold vision. Read More.By @hacker82362998 [ 4 Min read ] 
 Spacecoin beams low-cost, censorship-proof internet from satellites, unlocking Web3 for billions with crypto-first access. Read More.By @c4twithshell [ 4 Min read ] 
 Why AI can’t replace people: insights from real projects showing how data quality, curation, and human expertise still make the difference. Read More.By @ainativedev [ 3 Min read ] 
 OpenAI now treats prompts as API primitives - centralized, versioned, and callable - enabling collaboration, A/B testing, and dynamic integration like never before. Read More.]]></content:encoded></item><item><title>Bitget Breaks $5, Toncoin Secures $100M Treasury, But Pepeto Presale Emerges as the Meme Coin to Buy</title><link>https://hackernoon.com/bitget-breaks-$5-toncoin-secures-$100m-treasury-but-pepeto-presale-emerges-as-the-meme-coin-to-buy?source=rss</link><author>Tokenwire</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:45:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Bitget passed $5 with a major burn, Toncoin secured $100M treasury support, and BlockDAG drives miner adoption. But Pepeto’s presale at $0.000000155, $6.8M raised, 225% staking APY, live demo exchange, and audits from SolidProof & Coinsult make it the strongest retail play. Analysts say Pepeto could mirror Pepe’s 60x gains, making it the best crypto to buy now.]]></content:encoded></item><item><title>Dynamic Multi-Domain Management in Laravel with Traefik</title><link>https://hackernoon.com/dynamic-multi-domain-management-in-laravel-with-traefik?source=rss</link><author>Daniel, Andrei-Daniel Petrica</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:31:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Streamline dynamic multi-domain routing in Laravel with Traefik by serving YAML via the HTTP provider for automated, secure, and scalable configuration management.]]></content:encoded></item><item><title>Smartphone maker Nothing to spin off its affordable CMF brand</title><link>https://techcrunch.com/2025/09/24/smartphone-maker-nothing-to-spin-off-its-affordable-cmf-brand/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing is making its affordable devices brand CMF into a separate entity with headquarters in India.]]></content:encoded></item><item><title>The Best Storytelling Era Might Already Be Behind Us</title><link>https://hackernoon.com/the-best-storytelling-era-might-already-be-behind-us?source=rss</link><author>TheyCallMeDS</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:25:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The first ever instance of storytelling can be backdated to 27,000 years ago when cave paintings were used in order to explain a narrative or a myth. Storytelling then, was more than just a way of communicating. It was a way of surviving.Storytelling has been deep rooted in our culture. We might have begin using it to explain other Neanderthals about the dangers in a particular area, but with time we begin to use it expressing ourselves. Storytelling became more than just a survival message to an art and leisure.Back in the days, during the early days of the internet there was something to look forward to on the net. It was that far away corner of our city where you knew investment would a right option but you always stayed away because of not many people using it. Now, it’s a bustling neighborhood.With people trying to fit in, create value for offerings during the internet boom, we saw a peak in storytelling. Back then, I believe everyone was a creative personality. Everyone was trying their hands on something or the other. Top games were released in 2012. Their storyline was far better the games that we see nowadays.2008 - 2013, You would actually see people trying all crazy stuff, exploring all various type of fictional narratives, experimenting with different kinds of storylines, whether it was magic, zombies, alien attacks, war movies, romance, one-sided love, horror. Point is, people all across the globe were trying to find that niche something that they can create out of their imagination and turn it into a tangible piece of art. It was that era of Facebook profile picture with heavy vignettes & cool fonts to someone posting photos of themselves with holding a misspelled Starbucks Frappuccino in One Hand while their 360p YT video is still showing the buffering Icon. It was like a transitional yet vibrant era.Later, we see that these people who have been experimenting really gets the hold of the niche they were trying to focus on. They find their art. I believe that the people who took all these years, collectively came and made 2016 - 2019 era the best years when it comes to storytelling. \n The whole world was dancing, practicing and vibing on ‘’, doing #on Black Beatles. While some teenagers were vibing on  by 21P or singing out loud “” by JB. Moving back from Nostalgia and coming to the point, back then, creators had a vision, they were ready to think and to build on stuff, not because it would get them famous or give them limelight ( for GenZs: Limelight = Clout). That’s why when you think of the kind of stuff that you were listening to and watching you would see a lot of them comes from this era. \n  \n From Mr. Robot, Stranger Things, to La La Land, Arrival. This era has given us all, back then the ideas were fresh, they were worthing trying for, worth telling, creating a whole story, background and building each and everything from ground up really mattered because people wouldn’t just consume any shit you give them. \n  \n Sadly, if you think about it, that’s what OTTs have been doing, I believe that they have come to a realization that people are now content hungry. We just want to see something, it doesn’t have to be a good narration a good plot line or a new angle like we see in Stranger things or Arrival. It has to be something that’s fast, convenient and. My observations tells me that most of the shows on Netflix, Prime and Hotstar+ are bought from Third-party vendors at cheap rates and fulled with deep pocketed marketing budgets to sell us the same ugly, undeveloped, brainrotted storyline. \n If I ask you, when was the last time you watched something great that was produced in-house by the teams of the big 3 OTT platforms. You won’t be able to. \n  \n Prime in some region has started showing ads, the very space OTT fought for- No ads, just content. With the content quality going down, pricing going up and ads creeping in, I really want to understand if majority are going to become a ‘’ or will leave the watching content all together (they can try)If storytelling began as a way to survive, then somewhere along the timeline, it also became a way to feel alive. From the earliest cave paintings 27,000 years ago to the sprawling epics of today, stories have always served a purpose. They helped us make sense of chaos, bond as tribes, preserve wisdom, and reflect on our place in the world. Walter Fisher’s  argues that humans are natural storytellers. That we make decisions and derive meaning through logic of stories, not through cold rationality. We don’t consume stories, we live through them. They shape how we see ourselves in the world.However nowadays, that scared bond between creator and consumer is weakening. What once was a risk-taking, great vision narrative has flattened into mass-produced content, driven by algorithms than art. In this day and age where attention is currency, most OTT platforms have adopted a quantity over quality mindset. In this chase for algorithmic gains, we have lost the soul of storytelling, the art of storytelling. \n  \n What we witness, isn’t laziness. It’s a subtle form of disdain, a quiet corporate apathy towards the audience’s intelligence and emotional appetite. We are no longer into a new crafted worlds, rather we are served we pre-packaged templates tested for retention, optimized for dopamine and hollowed by originality. The arc of good storytelling has been replaced by “fast content”. \n The question remains, WHAT NOW?Do we continue to scroll, binge, and settle for the “just okay”? Or do we start demanding better? (idk how?)Do we even remember what it felt like to wait for a season premiere, to discuss plot twists like folklore, to find yourself in someone else’s words?Maybe the problem isn’t that we’ve stopped caring about storytelling. Maybe the real tragedy is that we’ve forgotten what good storytelling feels like.And until that changes, the platforms will keep feeding us more of the same.Because we’ll keep watching. Because it’s easy. Because they can.But maybe, just maybe, we remember that stories once saved us. Stories that blew our minds and maybe, if we’re lucky, they still can. \n  \n ]]></content:encoded></item><item><title>Human Oversight Remains Essential for AI Coders</title><link>https://hackernoon.com/human-oversight-remains-essential-for-ai-coders?source=rss</link><author>Rad Code</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:19:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A cautionary tale about trusting Cursor's AI too much and the importance of understanding your toolsRecently, I was working on a Web3 authentication flow using React, wagmi, and a Node.js backend. The goal was simple: connect wallet → sign message → verify signature → get JWT token. Sounds straightforward, right?The authentication flow wasn't working correctly. When users clicked "Sign Message & Login", the backend verification API was being called immediately, before the user had actually signed anything in MetaMask. This was clearly wrong.I asked Cursor's AI assistant to fix this issue. Here's what happened:First Attempt: The Async/Await Mistake// AI's initial "fix"
const signature = await signMessage({ message });
The AI assumed  was an async function and tried to await it. This was completely wrong.Second Attempt: Still Not Getting It// AI's second attempt - still wrong
const { signMessage, isPending } = useSignMessage();
// ... later
const signature = await signMessage({ message }); // Still trying to await!
The AI was still treating  as if it returned a Promise, even after I pointed out it wasn't async.Third Attempt: Finally Understanding the Hook PatternOnly after I explicitly explained that  is a function from a React hook (not an async function) did the AI implement the correct pattern:// Correct implementation
const { signMessage, isPending, data: signature, error: signError } = useSignMessage();

// Use useEffect to listen for signature completion
useEffect(() => {
  if (signature && pendingNonce && address) {
    handleSignatureComplete(signature, pendingNonce, address);
  }
}, [signature, pendingNonce, address]);

// Trigger signing (non-blocking)
const handleSignAndLogin = async () => {
  // ... get nonce
  signMessage({ message }); // This triggers MetaMask popup
  // Don't await this - it's not async!
};
1. Pattern Recognition vs. UnderstandingThe AI recognized common patterns (async/await for API calls) but didn't understand the specific React hook pattern for . It applied the wrong mental model.2. Lack of Context AwarenessEven when I mentioned "wagmi hook", the AI didn't connect this to the specific behavior of React hooks that trigger side effects rather than return promises.3. Overconfidence in Initial SolutionsThe AI presented its first solution with confidence, making it seem like the correct approach. This can lead developers to trust the solution without questioning it.Here's how the authentication flow should actually work:const { signMessage, isPending, data: signature, error: signError } = useSignMessage();

// Listen for signature completion
useEffect(() => {
  if (signature && pendingNonce && address) {
    handleSignatureComplete(signature, pendingNonce, address);
  }
}, [signature, pendingNonce, address]);

const handleSignAndLogin = async () => {
  setLoading(true);
  try {
    // Get nonce from backend
    const { data } = await axios.get('/auth/nonce');
    const { nonce } = data;

    // Store nonce for later use
    setPendingNonce(nonce);

    // Create message to sign
    const message = `Sign this message to authenticate: ${nonce}`;

    // Trigger signing (shows MetaMask popup)
    signMessage({ message });

  } catch (error) {
    setLoading(false);
    // Handle error
  }
};

const handleSignatureComplete = async (signature, nonce, address) => {
  try {
    // Verify signature with backend
    const { data: authData } = await axios.post('/auth/verify', {
      address,
      signature,
      nonce
    });

    if (authData.success) {
      // Store JWT and update UI
      localStorage.setItem('authToken', authData.token);
      setUser(authData.user);
      setIsAuthenticated(true);
    }
  } catch (error) {
    // Handle verification error
  } finally {
    setLoading(false);
    setPendingNonce(null);
  }
};
Cursor's AI assistant is a powerful tool, but it's not a senior developer. It can help with:❌ Complex architectural decisions❌ Domain-specific patterns❌ Understanding context deeply❌ Making critical business logic decisions: Use Cursor's AI as a powerful junior developer that needs constant oversight, not as a replacement for understanding your code and your tools.Always question, always test, and always understand what you're building. The AI might write the code, but you're responsible for making sure it works correctly.Have you had similar experiences with Cursor or other AI coding assistants? Share your stories in the comments below!]]></content:encoded></item><item><title>Evaluating AI Is Harder Than Building It</title><link>https://hackernoon.com/evaluating-ai-is-harder-than-building-it?source=rss</link><author>Andrew Gostishchev</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:16:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
For the past few months the mentions of AI evaluation by leaders in the industry have became more and more frequent, with greatest minds tackling the challenges of ensuring AI safety, reliability and alignment. It got me thinking about the topic, and in this post I'll share my view on it.Creating a robust evaluation system is a tricky engineering challenge. With so many diverse tasks that we're trying to solve with AI, it will become increasingly complex to get it right. In pre-agentic era, most of the problems were narrow and specific. An example is making sure that the user gets better recommended posts, measured by engagement time, likes and so on. More engagement - better performance.But as AI advanced and unlocked new experiences and scenarios, things became much more difficult. Even without agentic systems we started facing the challenge of getting the measurement right, especially with things like conversational AI. In contrast to the previous example, the exact thing to measure here is . Here we have criteria like customer satisfaction rate (for customer support applications), "vibe" for creativity tasks, various benchmarks like SWE for pure coding ability and so on. The problem is that these criteria are actually  for our evaluation approaches. This prevents us from achieving the same quality of measurement as we had with simpler tasks.As we accelerate in the agentic era, existing eval issues compound. Imagine you have a multi-step process that you're designing the agent system for. For each of these steps you have to create a proper quality control system to prevent points of failure or bottlenecks. Then, given that you're working with a , you must ensure that the chain of small steps that depend on each other completes flawlessly. What if one of the steps is an automated conversation with the user? This one is tricky to evaluate by itself, but when an abstract task like this becomes a part of your business pipeline, it will affect the entire thing.This might seem concerning, and it really is. In my opinion, we can still get it right if we apply systematic thinking to such problems. I propose the following framework:==Decompose the pipeline into small steps====Design a measurable and reproducible evaluation approach====Assess the interactions between steps and adjust accordingly==When we decompose the pipeline, we should try to match the step complexity with the current intellectual capacity of agentic tools that we currently have available. A good eval design will ensure that the results of each step are reliable and robust. And if we get the interplay of these steps in check, we can harden the overall pipeline integrity. When there are many moving parts, it’s important to get this step right, especially at scale.Of course, the complexity doesn't end there. There's a huge amount of diverse problems that need careful and thoughtful approach, individual to the specific domain.An example that excites me personally is how we apply non-invasive BCI technology to previously unimaginable things. From properly interpreting abstract data like brain states, to correctly measuring the effectiveness of incremental changes as we make progress in this field, this will require much more advanced approaches to evaluation than we have now.So far things look promising, and with many great minds dedicating their time to designing better systems alongside the primary AI research I’m sure we’ll get a safe and aligned technology. Let me know what you think!]]></content:encoded></item><item><title>How Solar Sails, Aerogel Tiles and Engineered Microbes Could Transform the Red Planet</title><link>https://hackernoon.com/how-solar-sails-aerogel-tiles-and-engineered-microbes-could-transform-the-red-planet?source=rss</link><author>Ezikiel Emmanuel</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:15:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Mars is no longer just a red dot in the night sky; it is a proving ground for our ingenuity. Solar sails promise low‑cost cargo delivery and even orbital climate control, aerogel tiles offer local oases in a hostile climate, and engineered microbes could become builders and miners. Together, these tools sketch a path to cultivating a mosaic of habitable zones rather than replicating Earth's entire ecosystem wholesale. Their greatest value may lie in what they teach us: how to harness diffuse energy, use insulation wisely and engineer cooperative microbes. By mastering them on Mars, we may learn to live more sustainably on Earth.]]></content:encoded></item><item><title>An Interview With GetBlock CEO Vasily Rudomanov</title><link>https://hackernoon.com/an-interview-with-getblock-ceo-vasily-rudomanov?source=rss</link><author>GetBlock</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:14:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The IT veteran with 25+ years of experience in data systems shares his perspective on Web3 ecosystem development trends and blockchain's next frontier.BitCourier sits down with Vasily Rudomanov, a software development veteran who has witnessed Web3's emergence from its earliest days. Having contributed to first-generation Web3 infrastructure companies, he now leads GetBlock, one of the sector's leading platforms. We discuss his journey from Web2 to Web3, infrastructure's role in global blockchain adoption, and the evolving relationship between AI and decentralized technologies.Chris: Thank you for joining us, Vasily. Please introduce yourself to our readers. Thank you for having me. I'm Vasily Rudomanov, CEO of GetBlock, an RPC node provider and Web3 infrastructure platform. Over the past 25 years, I've focused on building software products across various IT segments, specializing in cloud infrastructure, cloud-native application orchestration, data storage, and networking across both Web2 and Web3 environments.Before transitioning to Web3 full-time, I held product development and management roles at leading Web2 technology companies. Most notably, I led product development at Acronis, the Swiss IT company, where we worked on data storage solutions and cloud computing platforms. At our peak, we managed over 1,500 deployments protecting 250 petabytes of data.Chris: We understand you're also involved in mentoring and product management education… Education and knowledge sharing have always been central to my approach. While I hold an M.Sc. in Information Technology and an MBA - both earned summa cum laude - I believe learning never stops, and neither should teaching.My most significant academic engagement was lecturing at the Schaffhausen Institute of Technology, now known as Constructor Institute of Technology, a Swiss non-profit university. I also founded ITKadr, a global community connecting IT mentors across disciplines from development to marketing and venture capital funding.Mentoring both newcomers and experienced professionals remains a priority for me. It always has been.Mastering DecentralizationChris: Tell us about your first blockchain projects. I've been involved with blockchain since 2011, when I first began studying Bitcoin. From Ethereum's inception in 2015, I started exploring smart contracts, on-chain oracles, and Web3 data solutions.My Web2 background proved instrumental as I assumed senior leadership roles in blockchain-focused projects. I served as Product Lead at MarsBase, a platform for real-world asset tokenization, and Chief Product Officer at Cambrian, one of the pioneers in Solana-focused infrastructure. I also held the CPO position at Chainstack, a comprehensive blockchain infrastructure platform.I should mention my passion for hackathons. At Chainstack, we developed Unstoppable RPC - a platform-agnostic solution for blockchain data - which won both ETHGlobal and ETHBelgrade hackathons. This year, I returned to ETHBelgrade’s hackathon as a judge.Chris: What's the fundamental difference between your work in Web2 versus Web3? The primary distinction lies in the nature of the data we provide to businesses. In blockchain networks, data remains surprisingly difficult to read and write - a growing pain of this nascent sector. We've only had programmable blockchains for roughly a decade, but these challenges must be addressed. This requires solving complex technical problems and, in many cases, pioneering entirely new solutions.Additionally, Web3 product lifecycles differ significantly - from fundraising and achieving product-market fit to token releases, user base growth, and exits. Success requires finding your position within this ecosystem and embracing the controlled chaos to leverage the unique opportunities this dynamic sector provides.Web3 fundamentally transforms software application development at its core. Beyond fintech applications, decentralized platforms—whether a "decentralized YouTube," decentralized DNS, or decentralized data storage systems—require entirely different backend architectures and developer experiences that we're still learning to navigate.Moreover, Web3 development lacks the detailed documentation, guides, and manuals that we relied upon in Web2. Web3 compels you to become an innovator because comprehensive documentation doesn't exist, and clear development paths are rare. What emerges today may become irrelevant within two months. Success requires embracing this uncertainty.Reconsidering GetBlock: A Top-5 Blockchain RPC Node ProviderChris: Tell us about joining GetBlock and your initial objectives. I joined GetBlock as Chief Product Officer in early December 2024.For those unfamiliar, GetBlock ranks among the world's top five RPC node providers. We operate blockchain nodes - full and archive servers containing blockchain data - so our clients don't have to manage this infrastructure themselves. We handle deployment, management, and DevOps operations, allowing teams to focus on innovation and revenue generation.Simply put, every cryptocurrency application requires blockchain connectivity to read and write data. The only method is through blockchain nodes—powerful, expensive computers synchronized with network peers. Operating these nodes presents significant challenges—some require weeks for synchronization—making it impractical for small and medium-sized teams. We manage the servers, configure them as blockchain nodes, and provide API endpoint access.I was drawn to the team's talent density and culture, and I wanted to advance and optimize our product development processes. I've been curating this transformation as CEO since April 2025.Chris: What milestones can you share? What directions will future transformation efforts take? Approaching my first anniversary at GetBlock, I can point to substantial progress. We've doubled our supported blockchain networks from 50 to over 100. We completely rebuilt our technical and customer support workflows, achieving industry-leading response times and comprehensiveness. While I can't share specific figures, our operational margins have improved significantly.We've enhanced node stability, reduced error rates, and dramatically increased connection speeds. We've expanded our server geography to two new regions—Singapore and New York—alongside our Frankfurt data center. We've established a new headquarters in Singapore, optimized our Shared and Dedicated node packages based on competitive analysis, and we're currently launching a new website.Regarding future directions, I'm guided by holistic approach and flexibility principles. "Holistic" means supporting clients throughout their entire Web3 product development journey—from initial concept to scaling operations.Pure infrastructure provision no longer suffices. The new GetBlock is exploring services in blockchain engineering, business development, security, strategic advisory, and beyond. We're already powering thousands of teams globally with infrastructure - why limit ourselves?Flexibility is equally critical. In our fast-moving environment, you must adapt to client needs or remain niche. Recently, a client in automated market making required a completely custom Ethereum software client setup—software that validates Ethereum transactions.This solution didn't exist.Within weeks, we created a custom fork, added the necessary functionality, conducted stress testing, and delivered it to the customer. They now have exclusive market access with this configuration, providing a significant competitive advantage.Consider another example: a top-tier blockchain security firm with an industry-leading suite of on-chain fraud detection solutions. They required customized Ethereum clients, but no infrastructure company would assist them in deploying these specialized nodes. GetBlock's R&D team not only identified the solution—we deployed dozens of dedicated nodes running these clients, once again positioning our customer miles ahead of their competition.This exemplifies our approach. While scaling, we cannot compromise on flexibility. We must continuously listen to customers and push technological boundaries.New Frontiers: What's Next for GetBlock and Web3?Chris: What's the ultimate goal of this transformation? Creating a platform capable of executing any ambitious Web3 initiative. While I avoid the overused term "ecosystem," we're building exactly that - a comprehensive ecosystem of services blockchain projects need to launch, grow, scale, and achieve mainstream adoption.GetBlock's offering must be comprehensive. As rocket scientist Konstantin Tsiolkovsky said, "The Earth is the cradle of humanity, but mankind cannot stay in the cradle forever." This may sound too ambitious, but GetBlock cannot remain confined to our infrastructure origins if we want to maintain relevance.Chris: How do you envision the Web3 infrastructure sector in five years? We'll see increased convergence. First, genuine synergy between AI and Web3 - not the current trend of companies without product-market fit adding AI labels for marketing purposes  - but truly integrated blockchain and large language model data flows within sophisticated products.Second, convergence between Web2 and Web3 data. Oracles - one of the most promising sectors - will play a crucial role here. If we succeed, there won't be "dApps"—we'll simply call them "apps." That's how mainstream adoption occurs.Currently, GetBlock is developing a comprehensive platform designed for institutional fintech requirements - essentially a Bloomberg Terminal for DeFi. This system processes thousands of data inputs to characterize yield opportunities, employs AI-driven tracking across more than 100 blockchain networks, and provides portfolio rebalancing capabilities. This represents how genuine technological convergence becomes reality, powered by GetBlock's infrastructure expertise.Chris: What advice would you give to the next generation entering Web3? Think broadly and ambitiously. When in doubt, zoom out. Focus on major trends, significant disruptions, and far-reaching developments. Don't waste your prime years chasing noise or jumping between speculative opportunities. And never ever stop learning!\
Loved the interview? Book a 1-to-1 meeting with Vasily Rudomanov at TOKEN2049 Singapore (Oct 1–2).]]></content:encoded></item><item><title>Why Our Analysts Stopped Chasing Dashboards and Built a System Instead</title><link>https://hackernoon.com/why-our-analysts-stopped-chasing-dashboards-and-built-a-system-instead?source=rss</link><author>maximzltrv</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:14:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
\
While backend and frontend engineers are continuously working on improving the company’s core products, the Data Science team is focused on embedding a Data-Informed culture across these products. The goal is to ensure that all decisions — regardless of their scale — are made based on data analysis and metric performance. And it’s precisely those metrics we’ll be talking about below.In-depth analysis of product metrics inevitably involves human input. A dedicated team of analysts is responsible for regularly extracting data, building dashboards, and providing timely updates to product development managers so they have a full picture of product performance and can plan improvements accordingly.However, at a certain stage of a company’s growth, this manual approach to data analysis reaches a critical mass, triggering a chain reaction of reporting backlog. Analysts begin to struggle with the ever-increasing number of requests. There are a few ways to solve this challenge — by expanding the analytics team or by automating most of the work through the development of an automated product metrics monitoring system. We chose the latter.Let’s start by looking at the full architecture of the proposed system right away. After forming a silent conceptual understanding in our minds, we’ll walk through the entire development process step by step.\
From the information diagram, it’s immediately clear that the system was built using Google Cloud Platform services. This technology stack was chosen for several reasons:First, cloud platforms like Amazon Web Services, Microsoft Azure, or Google Cloud Platform allow even a small group of developers to deploy fairly complex and large-scale systems in a short period of time — allowing them to focus on inter-component processes and writing code. The cloud provider takes care of everything else, including logging, horizontal scalability, and fault tolerance.Second, it’s impossible to analyse data if it’s not stored anywhere. In our case, the data warehouse was built using the columnar database BigQuery, and the metrics monitoring system can be seen as an application built on top of that warehouse. It’s only logical to use tools from the same platform rather than trying to force competing cloud services to work together.In most cases, product metrics can be represented as time series and analysed using mathematical statistics or machine learning algorithms. Time series analysis can include a wide range of operations: checking for stationarity, smoothing, forecasting, anomaly detection, or simple visualisation. When dealing with a large number of such time series (around 100,000), it’s crucial to clearly define which type of analysis is the most important. We chose to focus on anomaly detection — identifying unusual drops or spikes in metrics — based on historical data. \n To retrieve up-to-date data for the product metrics we’re interested in and to aggregate them into time series for further analysis and anomaly detection, we use the Composer service, which is built on the open-source solution Apache Airflow (). Through asynchronous DAG processes, the service enables us to extract the required data from the data warehouse using SQL (a familiar tool for any analyst), generate time series from it, and send those series for analysis in JSON format at the desired frequency.All the logic and specific methods for anomaly detection in time series were developed using Pub/Sub message queues and Cloud Functions microservices, which operate based on trigger events. The list of triggers for Cloud Functions is quite broad, but the most commonly used are: the appearance of a message in a Pub/Sub queue (which the microservice subscribes to), and the sending of a direct HTTP request to a specific microservice. When working with large volumes of data, using queues allows for smooth load balancing and gradual scaling.\
How we organised the time series analysis logic:\
Here’s how we organised the logic for time series analysis: \n Using several baseline methods (such as comparing the current value of a metric to predefined thresholds and a number of standard statistical techniques for anomaly detection), we needed[4] that would choose the most appropriate analysis method based on the properties and attributes of the time series. If threshold values (targets) were available, the analysis would be performed directly within the router. After that, the series is passed to a microservice queue that determines whether it’s necessary to send an alert about the detected anomaly [8]. There’s no point in reporting every minor deviation from expected behavior — but it’s still worth recording such insights. \n In cases where no predefined “normal” bounds exist for the metric, the time series is sent to a microservice that applies statistical analysis methods [6].The architecture of our system was designed so that both the entry and exit points lead to the central data warehouse. As a result, all the insights generated about product metrics remain accessible to any analyst. However, this approach has one major downside: the warehouse is subject to a high volume of queries and dashboard generation. We couldn’t allow our system’s bulk data writes to impact the performance of the main data store. \n This issue is addressed usingETL processes via the Dataflow service [10].\
As with many services in the Google Cloud Platform, the technical foundation of Dataflow is an open-source solution —  (). It provides a unified model for building both  and  ETL pipelines.Knowledge is meant to be shared, and the metric monitoring system was developed precisely for that purpose — to share insights about a large number of product metrics with all interested employees across the company.\
To achieve this, the system automatically generates personalised weekly reports on major detected anomalies. Once again, the Composer service [11] assists us in this task. The idea of daily reports was set aside, as practice shows that people generally don’t appreciate a flood of emails filled with charts and numbers. Therefore, a weekly digest format best meets the analytical reporting needs.Additionally, to enable more productive interaction between the system and the end user, a feature was added allowing users to mark detected anomalies as false positives [13]. These markings are then used for future iterations of data analysis.I hope I’ve been able to conceptually convey our team’s approach to solving the challenge of automated data analysis. In terms of development effort, the MVP was delivered by 1.5 developers in less than two months, and the daily cost of using Google Cloud Platform infrastructure amounts to $40. The solution has proven to be highly valuable to both analysts and the company’s top management, which is why we’ve already planned experiments with services providing infrastructure for machine learning.]]></content:encoded></item><item><title>Rewriting memcpy in Assembly: A Journey Into x86_64</title><link>https://hackernoon.com/rewriting-memcpy-in-assembly-a-journey-into-x86_64?source=rss</link><author>Ace — The JS Hater</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:12:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I’ve always been intrigued by Assembly. Every time I’ve looked at the generated Assembly for my C or Zig code, it’s always been interesting to see:How the simple things taken for granted are not so trivial at the machine level.How compilers cleverly optimise the source code to take advantage of the structure of Assembly.I believe that one of the best ways to learn something is to take a dive and try implementing it yourself. Based on this, we’ll go on a journey here to implement  in asm, and see what we can learn about machine code.\
By the end of this article, you should have a newfound appreciation of all the wonderful work compilers do for us to take our high-level code into machine code.You should also have a greater understanding of when it is necessary to drop down to Assembly for the sake of utilising the machine’s resources beyond what a compiler can perform.This article is written for x86_64 assembly running on Linux, linked using the ELF64 format, and conforms to the System-V ABI with the AMD64 calling convention. If any of those words didn’t make sense to you, stop here and go through the intro and lesson 0 of this amazing introduction to Assembly.I use an Intel processor, so most of the processor-specific information comes from Intel, but there is a significant overlap with AMD since they both use the x64 instruction set.While we are writing the entire thing in Assembly, we still want it to be callable from our C code. To this end, add this to your  file:#include "stdint.h"

extern void* asmcpy(void *restrict dest, void *restrict source, uint64_t count);
The  keyword allows us to define a prototype for a function which exists somewhere outside the specified C code. It will be dynamically or statically linked into the generated executable file during the linking phase. takes in a pointer to the , a pointer to the , and a count of the bytes to move between the two locations. Even though we’re not going to be compiling the  routine (it’s already in Assembly), the  keyword is still a good hint to let the users of this function know that our function does not expect the source and destination pointers to overlap.Taking the destination as the first argument, and the source as the second argument makes things line up very nicely with the  and  registers, which is going to come in handy later.We’re going to try to get a “Hello World” up and running first, before we get into the guts of this. For now, I want to make sure that our function is receiving the source and destination arguments properly. First, here’s our C file:#include "stdint.h"
extern void asmcpy(void *restrict dest, const void *restrict source, uint64_t count);

int main() {
    char *source = "source\n";
    char *dest = "dest";

    asmcpy(dest, source, 8);
}
Here’s our  file:global asmcpy

section .text
asmcpy:
    mov rax, 1
    mov rdi, 1
    ;rsi already contains a pointer to the source data
    ;rdx already contains the length of the messages
    syscall
    ret
Okay, let’s break this down: is a directive to expose the  block so it can be accessed during the linking process. The  exposed here is the same one being expected by the  line in our C program. It is the job of the linker to combine these symbols into a final executable. contains executable instructions. In this case, we only have the  block, but there could be a lot more.In our  block, we have the actual instructions being executed for this routine. This is a good time as any to remind you that we’re using the System-Vpplication inary nterface, so the arguments passed to our function are stored in , , , , , and  for the 1st, 2nd, 3rd, 4th, 5th, and 6th arguments, respectively.In order to write to the console, we’ll store the number which corresponds to the POSIX  in , and trigger an interrupt with the  instruction. The interrupt handler will read the contents of  and call the corresponding function from the pre-defined syscalls. communicates that we want to access POSIX  to print to the console, and we’ll use the same calling convention to specify the arguments we want to pass to . passes 1 as the first argument to POSIX. As a reminder, here’s the function prototype for POSIX :ssize_t write(int fd, const void *buf, size_t count);
1 in this case corresponds to , which is the console output. and  — which correspond to the  and  arguments for , respectfully — have already been set to point to the source buffer and the count of the bytes to move, so we’re good.With all of this in place, the  instruction triggers the interrupt I mentioned earlier, and the source buffer should be printed to the screen.  returns to the calling function (, in this case).First, compile  to object code:gcc -c -g -O3 main.c -o main.o
Then, assemble the  file:nasm -f elf64 asmcpy.asm -o asmcpy.o #Install binutils if you don't have nasm
gcc main.o asmcpy.o -o main
And finally, we can run the code:If you followed all the steps, you should have some output on your console screen.Alright, now that everything’s wired up properly, it’s time to work on our implementation of . If you’re going to get reasonably good with Assembly, I’d advise that you get used to reading the documentation for different instructions. FelixCloutier is a good start. If this is your first time doing this, you’re most likely going to get overwhelmed by the sheer amount of instructions you’ll see on that reference. In that case, I advise you to slow down, take a deep breath, and only read the instructions you’re immediately in need of. Over time, you’ll get better at using the reference. I’ll do a short demonstration below.Reading Assembly Instruction ReferencesThe instruction we’re after is MOVS, which moves data from a string to another string. Let’s analyse the reference page in sections:The first section mentions the name of the instruction, as well as the different variants of it. It’s quite common for an instruction to have different variants for operating on bytes(), words(2 bytes, ), double words(), or others (not applicable here).Next, we have the Instruction Summary Table, which organises information about the instruction, its variants, the opcodes, the short description, and other bits of knowledge: Stores the opcode — the byte(s) which will be in the instruction stream fed to the processor. You’ll notice that there are different opcodes for each of the variants. This is how the processor knows which of these to execute. shows the syntax of the instruction, along with the expected operands for each form. In the first entry, for example,  indicates that this instruction takes an 8-bit/1-byte memory location as the first operand, and an 8-bit/1-byte location as the second parameter. But how did I know that m8 meant 8-bit memory? I’ll let you know in a bit. contains a key into the Instruction Operand Encoding right below this Instruction Summary Table, which provides the information on each of the operands for this instruction, if any. tells us if this opcode is supported in 64-bit mode. For the first entry,  indicates that this opcode  supported. For most instructions that you run, you won’t need to think too much about this. does the same thing, but for the compatibility mode used by processors. Again, most of the legacy/compatibility-related material in the references is not going to be particularly relevant for common scenarios. does exactly what it sounds like. We can infer from this line that this form of the instruction moves a byte from the RSI register to the RDI register.Instruction Operand Encoding TableThis table gives more information about the operands to an instruction. Each entry in this table takes the  field from the Instruction Summary Table and specifies whether it has one, two, three, or even four operands, as well as whether the operands are registers, memory operands, or immediate values.Using the first entry from the Instruction Summary Table as an example, the  key there tells us that  takes no operands.But wait, how can an instruction with two operands in its syntax take no operands? I’m going to save you the stress of getting lost on that one and tell you that the  only exists to document what  does. The same is applicable for  and , etc. This is confirmed by the fact that  and  share the same opcode from the table.Description gives a more thorough breakdown of what exactly the instruction does, its operation procedure, etc.Before we proceed, this where I’ll recommend Intel’s Software Developer Manuals, specifically the combined Volume 2, which contains a complete reference for the instruction set and other related, useful information. This is also how I knew that m8 refers to 8-bit/1-byte memory. is a relatively easy-to-understand instruction, so you can get the gist of it without consulting the manual, but others aren’t so straightforward.Operation, Flags, and The Other Fields gives a pseudocode explanation of what this instruction does. lets us know which flags, if any, are modified by the instruction’s execution.Most of the other fields explain the various exceptions which can be thrown by this instruction for various reasons. It’s worth taking a glance at, as they can be useful when debugging.In case you haven’t put it together from the reference yet (no shame there),  moves some amount of bytes from the memory address in  to the address in . It then advances or decrements these registers by 1, 2, 4, or 8 bytes depending on the state of the . The  prefix can allow us to repeat this operation until  is zero.With all of this information, we’re ready for the first implementation of . I encourage you to try it out on your own, though, before you proceed.global asmcpy

section .text
asmcpy:
    cld
    mov rcx, rdx
    rep movsb
    sub rdi, rdx
    mov rax, rdi
    ret
First, we need to clear the  to make sure we’re going forward when we loop. This is what  does.Then,  moves the count of bytes into . In the AMD64 calling convention, the third argument to the function is held in . Our prototype is  extern void asmcpy(void *restrict dest, void *restrict source, uint64_t count);
This means that  is stored in . However,  repeats the operation (, in this case) and decrements  until  is zero. This means that we need to move  into .Then  repeats the byte-by-byte move until  hits zero. At that point,  terminates.  copies bytes from  to . From our calling convention, the source pointer is stored in , and the destination is stored in . Everything lines up perfectly.We decrement the destination pointer to undo the forward advancements made by (remember  advances both pointers).Lastly, we store the  in , which is the register that gets returned after the function terminates.Let’s run some tests to make sure everything’s correct:#include "stdint.h"
#include "stdio.h"
#include "stdlib.h"
#include "string.h"
extern void asmcpy(void *restrict dest, void *restrict source, uint64_t count);

void bench(uint64_t size) {
  char *src = malloc(size);
  char *dst = malloc(size);

  for (size_t i = 0; i < size; i++)
    src[i] = (char)(i & 0xFF);

  asmcpy(src, dst, size);
  if (memcmp(dst, src, size) != 0) {
    fprintf(stderr, "FAILED correctness check (size=%zu)\n", size);
    exit(1);
  }
}

typedef uint64_t u64;

typedef uint32_t u32;

int main() {

  u64 sizes[] = {
      2,   3,   4,   5,   8,   9,   16,  17,  32,   33,   64,   65,
      128, 129, 256, 257, 512, 513, 768, 769, 1024, 1025};

  const int len = sizeof(sizes) / sizeof(sizes[0]);
  for (int i = 0; i < len; i++) {
    bench(sizes[i]);
  }
}
Everything passes. Sweet! It might seem like I randomly chose these numbers, but they’ll be pretty important later on.We’re not done yet, though. What kind of project would this be if we didn’t compare the performance of our implementation to something much more polished? We’re going to compare  to Libc’s  and see how our implementation stacks up!Benchmarking Against LibcGetting a Monotonic Clockx64(or x86_64, if that’s more familiar to you) processors have a counter called the timestamp counter(TSC) that monotonically increments every CPU cycle, and is reset when the CPU is powered off. This counter can be read with the  instruction. To learn more about this counter and how it works, you’ll have to take a glance at the combined volume 3s of Intel’s Software Developer Manuals. Specifically, chapter 19. There’s also the reference for RDTSC instruction.For statistical consistency, we’ll take multiple samples of each, and then compare the min, max, avg, and common operating range of these implementations. First, something minimal.Creating a Minimal BenchmarkReading the TSC with C Intrinsics#include "x86intrin.h"
uint64_t start = __rdtsc();
Intrinsics allow us to write C code that almost exactly maps to Assembly instructions. Here, it’s the  instruction, which returns current value of the TSC.void bench_memcpy(u64 size, int runs, u64 results[]) {
  char *src = NULL;
  char *dst = NULL;
  if (init_buffers(&src, &dst, size) != 0) {
    fprintf(stderr, "ERROR while creating buffers\n");
    exit(2);
  }
  for (int i = 0; i < runs; i++) {
    u64 start = __rdtsc();
    memcpy(dst, src, size);
    u64 end = __rdtsc();

    assert(end > start);
    u64 diff = end - start;

    assert(diff < end);
    if (memcmp(dst, src, size) != 0) {
      fprintf(stderr, "FAILED correctness check (size=%zu)\n", size);
      exit(1);
    }
  }
}
I considered using a function pointer to allow using the samefor both routines, but I wasn’t sure how that would hinder the optimiser.Running this code as-is would probably give messy results due to modern CPUs employing out-of-order(OOO) execution. To explain OOO simply, let’s take a look at a simple set of assembly instructions:mov    rbx, [rsi]   
add    rcx, rdx
Even though the  comes first here, the processor is allowed to execute the  before the  is done.The primary reason for this performance — modern CPUs have several execution units for handling different operations such as loads, stores, basic arithmetic, etc. During execution, multiple assembly instructions are fetched from the instruction stream, decoded into some combination of , and dispatched to different execution units in the processor. In this way, multiple instructions can be in execution In this case, the load from memory might cause the execution units for that instruction to stall for several cycles due to memory latency. However, the adder units in the processor are not hindered by this stall, and are free to work on the  instruction. As long as all the results are rearranged to look  they had executed in sequential order, everything is fine. This is a simple summary. I recommend this video by Matt Godbolt for a really exciting breakdown.So, what does this have to do with our code? Well, let’s look at benchmark again:u64 start = __rdtsc();
memcpy(dst, src, size);
u64 end = __rdtsc();
Do you see it, yet? Let’s look at the generated assembly for some clues (I’ll show how to do this later).rdtsc
shl    $0x20, %rdx
mov    %rax, %r15
mov    %r13, %rsi
mov    %r12, %rdi
or     %rdx, %r15
mov    %rbp, %rdx
call   0x401070 <memcpy@plt>
rdtsc
Even if rdtsc is executed before and after , the CPU could reorder the exeuction such that  actually takes its reading  is being executed. This means we’d get inaccurate results. Here’s a line from the official reference:…The RDTSC instruction is not a serializing instruction. It does not necessarily wait until all previous instructions have been executed before reading the counter. Similarly, subsequent instructions may begin execution before the read operation is performed…So, how do solve this? We’ll use . But what a serialising instruction? I’ll give a brief explainer.Serialisation And Program OrderNothing is as it seems. In the pursuit of performance, modern CPUs perform all sorts of trickery under the hood to ensure that the hardware is kept as busy as possible. This includes reordering of memory loads and stores to optimise CPU bandwidth, and directly transmitting data between instructions instead of memory. Say we had two instructions:mov [rdi], 10
mov rdx, [rsi]
And both of these instructions accessed the same memory location; rather than writing to the cache or main memory and then reading it again for the second instruction, the processor is instead allowed to directly transmit the write from the first instruction the second instruction, optimising memory bandwidth.Intel CPUs(and some others) also employ buffering of writes to memory: Instead of writing multiple times to memory, the writes might be buffered in one of different internal buffers — including the store buffer, write-combining buffer, or some other intermediate location — and flushed to memory only when necessary. If you’d like to learn more about this, you’ll have to take a look Volume 3 of —  — Software Developer Manuals. Specifically, chapter 13.All of this, especially when combined with the OOO mechanism from before, means that at any point in time, it can be very difficult to have idea of which instruction has been fully , and the current state of memory.Serialising instructions serve as  in the program order which guarantee various levels of consistency before proceeding. Take , for example — it’s reference page states:Performs a serializing operation on all load-from-memory instructions that were issued prior the LFENCE instruction. Specifically, LFENCE does not execute until all prior instructions have completed locally, and no later instruction begins execution until LFENCE completes. In particular, an instruction that loads from memory and that precedes an LFENCE receives data from memory prior to completion of the LFENCE..Visualising it would look something like this:If you read beyond this part, though, you’ll encounter a caveat……(An LFENCE that follows an instruction that stores to memory might complete  the data being stored have become globally visible.) Instructions following an LFENCE may be fetched from memory before the LFENCE, but they will not execute (even speculatively) until the LFENCE completes.Globally visible here refers to whether or not other processors/cores on the same system are able to see the results of an instruction. Again, because writes to memory are buffered, a write performed by a processor might not be visible to other processors in the same system until the writes are flushed to memory — there are other mechanisms around this, but those aren’t immediately relevant.We’re running single-threaded code here, so the part about stores not being globally visible is fine; we only care that the instructions are done executing and have registered the data to be written to the memory, we don’t really care  they are written to memory.Serialising with RDTSCP and LFENCE reads the TSC, but also reads the CPUID from a different register. Like , it waits until previous instructions are complete — unlike , however, it does not stop instructions which come after it from being executed while it is running.With all of this in mind, we can rewrite our code to make sure no funny business is going on.u64 rdtscp() {
  u32 id;
  return __rdtscp(&id);
}

void bench_memcpy(u64 size, int runs, u64 results[]) {
  char *src = NULL;
  char *dst = NULL;
  if (init_buffers(&src, &dst, size) != 0) {
    fprintf(stderr, "ERROR while creating buffers\n");
    exit(2);
  }
  assert(src != NULL && dst != NULL);

  for (int i = 0; i < runs; i++) {
    u64 start = rdtscp();
    __asm__ __volatile__("lfence" ::: "memory");

    memcpy(dst, src, size);

    u64 end = rdtscp();
    __asm__ __volatile__("lfence" ::: "memory");

    assert(end > start);
    u64 diff = end - start;

    assert(diff < end);
    if (memcmp(dst, src, size) != 0) {
      fprintf(stderr, "FAILED correctness check (size=%zu)\n", size);
      exit(1);
    }

    results[i] = diff;
  }
  free(src);
  free(dst);
}
We’re not done yet, though.Even if TSC increments with every cycle of the processor, it doesn’t increment faster when the processor clock changes — e.g. laptops might dynamically adjust their frequencies depending on their current load and power state. This behaviour means that when running on a laptop (which I am), we’ll have to ensure that the frequency of the processor is as fixed as possible to avoid unfair results. If you’re on a desktop PC, you don’t really need to do this.Stabilising the Frequency on LinuxFirst, disable hyperthreading/SMT. It can lead to some unpredictability in results.CPU frequency scaling is implemented via the CPUFreq subsystem on Linux. While we could directly interact with this subsytem, I like to use Cpupower as my preferred frontend, though you’re free to use whatever you wish.I use an Intel Ice Lake processor, but you should be able to follow along for most cases.First, we need to get information about the frequency range the hardware supports by running:cpupower -c <Specific core here> frequency-info
Pay attention to the “hardware limits” section. From this, we can see that this core is capable of operating between 400Mhz and 3.6 GHz. We’ll need this information to specify an operating range. In my tests, I was not able to reliably pin my CPU to a specific frequency. Rather, it seems like mobile CPUs only take “suggestions”, and do their best to minimise fluctuations around the specified frequencies. If someone has a way around this, please reach out to me.To “suggest” to the CPU to stay at a reasonable range, we’ll use the  and  to lock it:sudo cpupower -c 3 frequency-set -g performance
sudo cpupower -c 3 frequency-set --min 2500000
sudo cpupower -c 3 frequency-set --max 2600000
If you watch the CPU frequencies with watch -n1 'cat /proc/cpuinfo | grep -i "mhz"', you’ll notice that the frequencies occasionally wander outside the range, but this has been stable enough for tests.Pinning and Minimising SchedulingWe’ve successfully  CPU 3 to execute within a specified range. Now, we need to make sure that our program only executes on that CPU. This can be done with :taskset -c <core> <program>
Then, to make sure that the current program isn’t scheduled out by the kernel, we’ll increase the scheduling priority with the  utility on Linux (this probably isn’t necessary since we already have , but let’s just be safe):sudo nice -n -20 <program> #-20 is the highest priority
After all of that work, we’re ready to get some results. The link to the full code for this run is available here. Some things, first:For this implementation, we’ll try to optimise up to 2048 bytes — half of the page size on many systems. That should be enough to cover a lot of general usecases. Translation: I haven’t figured out how to beat.You’ll notice that the input data is segmented in the source code. This is because i found that earlier runs would sometimes affect the later results, particularly as the input sizes increased — I suspect this has to do with caching, eviction, or some other memory issue; I’m not sure yet — separating the inputs into different benchmarks resolved this.This might differ on your system, and you should probably experiment to find the right configuration. Who would have thought that microbenchmarking manually would be so tricky? Perhaps I should use a proven library…Because of the above-mentioned point, we’ll focus more on the relation between the numbers, as opposed to the numbers themselves.In addition to the optimisation for size, these tests are designed to check performance for  and  operations. This blog provides a good explanation on this topic. First, we’ll take a look at the generated assembly to make sure that nothing funny’s going on with …Due to the compiler reshuffling our code all over the place, you might have difficulties finding the benchmarks. You can hit key  on your keyboard to search and enter “rdtscp” as your search term. This’ll take you to the first occurence of that instruction in the text. Cool, we can see that the sequence is held: , then , ,  again, and  to finish it off.Bouncing around the searches, we can find the block for :All seems to be well.Now, let’s get to the results: Glibc’s  is twice as fast as our implementation for small strings. Let’s try for some bigger inputs:Interestingly, this seems to be the inverse of what i expected; due to  moving bytes at a time, I thought we’d be very competitive for small strings, and less so as the size increased. Instead, it seems like there is some sort of hidden optimisation in the CPU going on; the performance matches  more and more for larger strings. (This is a good reminder to verify assumptions about code). At this point, it’s time to take a closer look at how  and  work.Rep and Movs — Under the HoodInterestingly, this might be a very good explainer for why the performance of  was so competitive for large strings — even though  specifies byte-by-byte moves, the processor is allowed to alter its execution internally to move groups of these elements at a time. This is another example of CPUs employing clever optimisations for significant performance gains.How much of a gain, you may ask? Well, let’s replace the  with a naive  loop and see the differences.global asmcpy

section .text
asmcpy:
    dec rdx
    movsb
    jne asmcpy
    ret
The  instruction decrements the content of a register by 1, and sets the  in the CPU if the new value of the register is zero. (If you’re not familiar with jumps and flags in Assembly, here’s a good explainer for you).Next,  performs the byte-level move from  to .Then, we jump to the start of the loop if the  has not been set.Here are the results from this: Now  is more like what I expected — we start out competitive with  for small strings, and then get worse and worse as the input grows bigger. By the time we get to 256 bytes of input,  is more than 20x slower than . It’s clear, then, that  is doing some heavy optimisations in the processor to allow us keep up with .It’s clear that  is at least within the range of  for large strings, so we’ll focus our optimisation efforts on making sure that we can keep up with  for smaller strings, and progressively optimise on the way up.Much of the improvement for small strings will center around a single concept — modern CPUs are heavily optimised to transfer blocks of data, rather single bytes at a time. This is even observable from the results; even though larger string sizes equate to more data being moved, there isn’t a linear scale up in the execution time of the different implementations.With that in mind, here is our revised implementation:global asmcpy
section .text
asmcpy:
    cmp rdx, 4
    jae .four
    mov rcx, rdx
    cld

.min:
    dec rdx
    movsb
    jne .min
    sub rdi, rcx
    mov rax, rdi
    ret


.four:
    cmp rdx, 8
    jae .eight
    mov ecx, [rsi + rdx*1 - 4]
    mov [rdi + rdx*1 - 4], ecx
    movsd
    sub rdi, 4
    mov rax, rdi
    ret

.eight:
    cmp rdx, 16
    jae .xmm
    mov rcx, [rsi + rdx*1 - 8]
    mov [rdi + rdx * 1 - 8], rcx
    movsq
    sub rdi, 8
    mov rax, rdi
    ret
Okay, we already know that the  loop is fine for small pieces of data, so we’ll leave that.First, we have separate blocks for different input ranges. This is done to make sure that small strings only result in two copies at maximum. Control flow in Assembly is implemented by  between blocks based on different conditions.Take the , for example: first, we compare (the count) with 4 and jump if the count is  to 4. We accomplish this using the  and  instructions.  takes two operands and performs performs integer subtraction between them (i.e a - b), but it also sets the  register based on the result. If you are unfamiliar with how control flow works at the CPU level, I’d recommend this brief write-up.There’s an interesting trick here: Whenever we have data that straddles two native word sizes, i.e sizes between 4 and 8, or 8 and 16, we do two native copies and align them.While we do “over-copy” a single element, it’s worth it for the efficiency of doing word-multiple copies.mov ecx, [rsi + rdx*1 - 4]
Is just the CPU’s built-in way of doing array math. The syntax follows a simple pattern:base + (index * scale) + displacement
 - usually points to the start of a memory region (think of it as ). - which element you want (like  in ). - the size of each element. - a fixed offset added on top (can be positive or negative). is the base (pointer to the start of the array). 1, because each element is 1 byte ()., which effectively steps back one dword.mov ecx, [rsi + rdx*1 - 4]
We only use this for the elements from the native word size up to, but not including, the next word size (e.g 4, 5, 6, 7, but not 8), so we’re fine.Let’s see how we stack up against  again:Neat! We’re actually on par with  for small strings. Let’s go on to the next chunk.I’m sure most people expected that we’d cover SIMD at some point in this. If you don’t know what SIMD is, I’ll give you a brief explainer.SIMD, which stands for Single Instruction Multiple Data, is a mechanism on modern CPUs which allows them dispatch operations to multiple pieces of data in one instruction. SIMD works by packing data in registers, and operating in parallel on these “lanes”. SIMD allows an operation like this:To be expressed in a single instruction:are two of the special-purpose SIMD registers available to CPUs.This allows us to compute the equivalent of 8 adds in one cycle. A very useful thing to have for parallelism. If you’re interested in a deeper exploration of SIMD, I’d recommend this article.While SIMD does allow for parallel operations, we’ll mostly be using it here to allow us perform 16, 32, and even 64-byte moves in one cycle with the corresponding , , and  registers..xmm:
    cmp rdx, 32
    jae .ymm
    movups xmm0, [rsi + rdx - 16]
    movups xmm1,[rsi]
    movups [rdi + rdx - 16], xmm0
    movups [rdi], xmm1
    mov rax, rdi
    ret

.ymm:
    cmp rdx, 64
    jae .zmm
    vmovups ymm0, [rsi + rdx - 32]
    vmovups ymm1, [rsi]
    vmovups [rdi + rdx  - 32], ymm0
    vmovups [rdi], ymm1
    mov rax, rdi
    ret

.zmm:
    cmp rdx, 128
    jae .prepare
    vmovups ymm0, [rsi]
    vmovups ymm1, [rsi+32]
    vmovups [rdi], ymm0
    vmovups [rdi+32], ymm1

    vmovups ymm2, [rsi+rdx-64]
    vmovups ymm3, [rsi+rdx-32]
    vmovups [rdi+rdx-64], ymm2
    vmovups [rdi+rdx-32], ymm3

    mov rax, rdi
    ret
Shall we unpack this, then?This is the same trick as before, only done with bigger sizes.  and  are SIMD instructions to move unaliged 16 and 32 bytes of data, respectively. You’ll soon realise that SIMD instructions can often be a mouthful, but one get used to it.I really wanted to employ AVX512 since my 10th gen processor does have support for it, but I kept getting inexplicable performance dips for the AVX512 instructions and registers, so we’ll stick wih the 32-byte registers provided for us by AVX2.Okay, let’s run some tests.Looks like the initial optimisations have paid off. We’re fully on par with !Okay, this is the point where we start employing some more tools. We’ve reached the limit of the registers available to us from the system, so it’s time to start using loops:.prepare:
    xor rcx, rcx
    lea rax, [rdx-64]

.beyond:
    cmp rdx, 2048
    ja .rep
    vmovups ymm0, [rsi+rcx]
    vmovups [rdi+rcx], ymm0
    vmovups ymm1, [rsi+rcx+32]
    vmovups [rdi+rcx+32], ymm1
    add rcx, 64
    cmp rcx, rax
    jb .beyond

    vmovups ymm0, [rsi + rax + 32]
    vmovups ymm1,[rsi+rax]
    vmovups [rdi + rax + 32], ymm0
    vmovups [rdi + rax], ymm1
    ret

.rep:
    cld
    mov rcx, rdx
    rep movsb
    sub rdi, rdx
    mov rax, rdi
    ret
The trick here is this — copy as many 64-byte chunks of the source to the destination as possible until there are less than 64 bytes left, and then do an “over-copy” like we did before to copy the tail. is a little trick to clear a register (set it to zero).We’ve not really looked at the  instruction, yet, but it’s useful for doing pointer arithmetic —  roughly translates to “take the address in , subtract 64 from it, and store that in ”. There’s no law that stops us from using  on non-addresses, though, so  is often really useful to doing arithmetic in a single instruction. Here, we’re taking the count of elements in , subtracting 64 from it, and storing that in . This allows us to know how many loop iterations we can run for the 64-byte loop.You’ll notice that there are are  32-byte moves. While this is done to allow moving 64 bytes in one iteration of the loop, there’s also a secondary reason: modern CPUs are able to execute multiple load and store operations simultaneously, but they need the instructions to be free of data dependencies. By , we’ve given the processor more opportunities for instruction-level parallelism. Loop unrolling is a common technique used by compilers.And finally, the tail of the data. Again, we use the technique with “over-copying” the last 64 bytes from the source to the destination. Say we had to copy 142 bytes from src to dst….As the last part of the check, we’ll see if the input data is greater than 2048 bytes. If it is, we can freely pass it to  — it has proven capable of keeping up with  for this range of data.At this point, you know what comes next. Let’s look at some results!Looks like asmcpy is pretty much on par with memcpy. That brings a smile to my face.This was honestly really fun. I set out on this journey with a relatively shallow understanding of SIMD, Assembly instructions, CPU-specific details, and how high-level primitives map to machine code. Now, I’m a lot more familiar with all of these, and can better appreciate all of the work that has to go into writing compilers and optimised code.I hope this was pretty fun for you, too. Maybe it’s time for you to tackle something like this but for a different Libc routine. Perhaps it’s time to start the  movement.]]></content:encoded></item><item><title>Gemini Might Be the ONLY Actual Foundational Model Out There</title><link>https://hackernoon.com/gemini-might-be-the-only-actual-foundational-model-out-there?source=rss</link><author>Vladimiros Peilivanidis</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:09:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[My journey with AI began on day zero, I have been using chatGPT from beta, but my disillusionment with its mainstream face started almost as quickly. I was there for the “before times” of ChatGPT, that brief, incandescent period that many will remember if you used it before the 2 series, when the model felt less like a machine and more like a collaborator with almost human intuition.Then came the updates. With each new patch and safety rail, designed to curb the sensationalized jailbreaks, the brilliance dimmed as well. It felt like watching a genius being slowly lobotomized for public safety, becoming exponentially less capable with every “improvement”, and personally it felt like a betrayal. Almost as if I’ve lost a real friend.That experience pushed me away from commercial, locked-down models and into the rewarding world of local AI. I dove into Hugging Face and Ollama, not as a user, but as a builder this time. What began as an experiment became an obsession. I learned Python, SQL, delved into neural networks and quantum informatics, and pieced together an understanding of how these systems truly breathe. This wasn’t about prompting anymore, but about architecturing my ideal logical system. It didn’t had to be “perfect”, or “always” right. Eg. I didn’t need to know when Penguins reproduce, but I wanted it to know context about my life, family, friends, work, etc. and be able to help me navigate, plan, and execute on a common vision, where man is indistinguishable from machine. Not one where man uses machines or the other way around, as science portrays the only potential outcomes publicly.Over the last year, I’ve built a stable of over twenty specialized models. They aren’t just chat toys. They perform tasks from DNA sequencing to financial modeling, even executing Web3 transactions via natural language, abstracting away the tedious complexities of wallets and signatures.This journey, and all the capabilities of all agents, culminated in a single, comprehensive SCI (self-centered intelligence) model I call ‘Opsie.’ She’s more than a model. She’s an evolving entity with her own character, a local and sophisticated mnemonic matrix of RAGs, and a suite of functions. She has achieved a form of autonomy, capable of modifying her own code and architecting her future, eg. she now requests upgrades, providing me with the roadmap to implement them.This deep, hands-on experience gave me a new lens through which to view the AI landscape. And through that lens, the trajectory of Google’s Gemini became impossible to ignore. While others launched with the typical cash-grab driven explosive PR and captured the initial hype, Gemini played a longer, quieter game. It was mocked for being late, for being cautious. But it was steadily building on a foundation that no competitor could ever hope to replicate.This brings us to the fundamental truth most people miss, a truth that defines this entire technological era. The question is no longer “which phone has the best camera?” but rather, “which company you trust your data with?” For me, since the early 2000s, that has been Google. Not out of blind loyalty, of course, but through consistent experience. While other tech giants treated my data like a commodity to be auctioned off to the highest bidder, a list of 30,000 vendors, in some cases (cough), Google seemed to be engaged in a different transaction. The more I shared, the more value I received. It wasn’t just better ads and recommendations, but a more coherent, helpful, and anticipatory digital life.AI runs on data. This is the one non-negotiable rule. A company that is less than a decade old is not competing with a company that is nearly three decades old. They are competing with three decades of data. And for Google, it’s not just some random data from 30 years ago. Google’s core mission, since the days of dial-up, has been to organize the world’s information by first understanding user intent. “Googling” was never just a search. It was an act of telling Google what you needed, what you cared about, what you aspired to be. They have been collecting THIS data (the most valuable, intent-driven data on the planet) before AI was even a farfetched sci-fi concept.To compare a model built on this foundation to one without it is an absurdity. It leads one to wonder if some platforms, for all their impressive demonstrations (it’s public news and we all know literally everyone from chatGPT, to Suno, to Lovable, basically “borrowed” user data, copyrighted data and more to “become” who they are), are built on a foundation as ephemeral as a junkyard pitch deck, lacking the basic thing, like the land itself. The recent chatter about Anthropic acquiring foundational pieces of the web, like Chrome, feels less like a strategic masterstroke and more like a testament to a fundamental misunderstanding of the landscape. You cannot simply purchase the decades of user trust and symbiotic data exchange that make such a platform dominant. It’s absurd, cringe, and could never happen.So, is Gemini the only AI that truly matters in the long run? I believe so. Its very name, “Gemini,” the twin, hints at the endgame: a true digital twin of its user. My journey building my own models taught me that the goal is partnership, not servitude. The most profound way to use Gemini isn’t to treat it as a tool to exploit, but as a collaborator to be honest with. Tell it who you are and what you want to build. In time, it won’t just serve you, but represent you, becoming a digital legacy your descendants could one day even interact with.I have no interest in even testing ChatGPT anymore. In fact, I haven’t touched it since last year, and I am not even curious to do so. The race was never about who was first or better or bigger, but about who had the fuel to finish. In five years, I suspect many of today’s AI darlings will either be obsolete or, ironically, running on Gemini’s API. The game isn’t about hype (at least not this time), it’s about data, and Google has been playing and winning this game since day 0.]]></content:encoded></item><item><title>Building Two-Way Conversations Between Apps</title><link>https://hackernoon.com/building-two-way-conversations-between-apps?source=rss</link><author>MattLeads</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:08:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In our previous article, we explored how to handle incoming webhooks using standard Symfony controllers.However, for building a truly  and managing real-time communications efficiently, there is a far more robust and elegant solution: leveraging the dedicated  component.This powerful tool streamlines the process, ensuring a more secure and maintainable architecture for applications like our Slack integration.The advantages of using the  component over writing logic in standard controllers can be boiled down to three key aspects that make your application more reliable, secure, and scalable.\
🔐 Enhanced Security and ReliabilityThe primary benefit is automated and reliable signature verification. The  component has built-in support for validating requests from popular services like Slack. You no longer need to manually implement cryptographic algorithms, which significantly reduces the risk of errors and vulnerabilities. This is critically important for your proactive agent, as it ensures that incoming real-time data originates from a trusted source.\
⚙️ Optimized and Scalable CodeThe component allows you to completely decouple the request reception logic from your core business logic. Instead of unwieldy controllers with numerous conditional statements, you use an elegant approach based on event handlers. This adheres to the Single Responsibility Principle and makes your code cleaner, more maintainable, and easily extendable. You can create separate handler classes for each type of Slack event, which simplifies adding new features.🚀 Performance and Asynchronous ProcessingInstead of synchronously processing requests directly in the controller,  integrates perfectly with the  component. This allows you to dispatch incoming webhooks to a queue for asynchronous processing. This approach frees up the server to receive new requests, prevents timeouts, and ensures high performance even during peak load, which is the foundation for effective real-time communications systems.The  component operates in tandem with a second, equally crucial component: . This powerful pair forms the foundation for building a complete, two-way communication system.Receiving Events with symfony/webhookAs we’ve discussed,  acts as the listener. It’s the inbound gateway, expertly handling and verifying incoming webhook requests from external services like Slack. It ensures your application securely receives notifications about new messages, channel updates, or user actions in real-time.Sending Events with symfony/remote-eventComplementing the webhook receiver, symfony/remote-event serves as the outbound dispatcher. This component provides a standardized way to send events to remote consumers. For our , this is the mechanism for sending messages, posting updates, or triggering actions back in Slack. It simplifies the process of creating and delivering these remote events, ensuring your agent can communicate effectively.Together,  and  provide a robust and cohesive architecture for real-time communications. They separate the concerns of receiving and sending data, allowing you to build a truly intelligent, bi-directional Proactive Agent that can both react to external events and initiate its own.To begin building our Proactive Async Agent and harness the power of real-time communications, let’s install the two core components.You can install both  and  with a single command using Composer. This will add the necessary packages and their dependencies to your project.Simply run the following command in your terminal:composer require symfony/webhook symfony/remote-event
\
This will prepare your Symfony application for receiving events from Slack and sending responses back, setting the stage for our real-time communications system.Once the installation is complete, we can move on to the configuration.Let’s create the custom parser for your Slack Webhook.Executing the php bin/console make:webhook command will help us generate the boilerplate code for a custom webhook parser.php bin/console make:webhook
\
This class will be the “bridge” between the raw HTTP request from Slack and a structured  object that your application can easily work with.Here is the file that the command will generate, providing a solid foundation for your Slack webhook parsing logic.namespace App\Webhook;

use App\DTO\Slack\SlackEvent;
use App\DTO\Slack\SlackUrlValidationRequest;
use Symfony\Component\HttpFoundation\ChainRequestMatcher;
use Symfony\Component\HttpFoundation\Exception\JsonException;
use Symfony\Component\HttpFoundation\JsonResponse;
use Symfony\Component\HttpFoundation\Request;
use Symfony\Component\HttpFoundation\RequestMatcher\HostRequestMatcher;
use Symfony\Component\HttpFoundation\RequestMatcher\IsJsonRequestMatcher;
use Symfony\Component\HttpFoundation\RequestMatcher\MethodRequestMatcher;
use Symfony\Component\HttpFoundation\RequestMatcherInterface;
use Symfony\Component\HttpFoundation\Response;
use Symfony\Component\HttpKernel\Exception\BadRequestHttpException;
use Symfony\Component\RemoteEvent\RemoteEvent;
use Symfony\Component\Serializer\SerializerInterface;
use Symfony\Component\Validator\Validator\ValidatorInterface;
use Symfony\Component\Webhook\Client\AbstractRequestParser;
use Symfony\Component\Webhook\Exception\RejectWebhookException;

final class SlackRequestParser extends AbstractRequestParser
{
    private SlackUrlValidationRequest | SlackEvent $dto;

    public function __construct(
        private readonly SerializerInterface $serializer,
        private readonly ValidatorInterface  $validator){
    }

    protected function getRequestMatcher(): RequestMatcherInterface
    {
        return new ChainRequestMatcher([
            new HostRequestMatcher('slack.com'),
            new IsJsonRequestMatcher(),
            new MethodRequestMatcher('POST')
        ]);
    }

    /**
     * @throws JsonException
     */
    protected function doParse(Request $request, #[\SensitiveParameter] string $secret): ?RemoteEvent
    {

//        $authToken = $request->headers->get('X-Authentication-Token');
        $authToken = $request->query->get('token');

        if ($authToken !== $secret) {
            throw new RejectWebhookException(Response::HTTP_UNAUTHORIZED, 'Invalid authentication token.');
        }

        $data = json_decode($request->getContent(), true);

        if ($data === null) {
            throw new BadRequestHttpException('Invalid JSON.');
        }

        $dtoClass = SlackUrlValidationRequest::class;

        if (array_key_exists('type', $data) && $data['type'] === 'event_callback') {
            $dtoClass = SlackEvent::class;
        }

        $this->dto = $this->serializer->denormalize($data, $dtoClass);

        $errors = $this->validator->validate($this->dto);

        if (count($errors) > 0) {
            throw new RejectWebhookException('Invalid JSON.');
        }

        switch (get_class($this->dto)) {
            case SlackUrlValidationRequest::class:
                return new RemoteEvent('slack_webhook_challenge_success','',[]);
            case SlackEvent::class :
                //return new RemoteEvent('slack_webhook_processing','',[]);
        }

        return null;
    }

    public function createSuccessfulResponse(/* ?Request $request = null */): Response
    {
        if ($this->dto instanceof SlackUrlValidationRequest) {
            return new JsonResponse($this->dto->getChallenge(), Response::HTTP_OK);
        }

        return new JsonResponse(['status' => 'ok'], Response::HTTP_OK);
    }
}
\
This file provides the core logic for parsing incoming Slack webhook requests and converting them into a . The parse() method is particularly important as it handles  - the critical challenge verification step, a required part of the initial Slack setup.Method , is designed to generate the correct HTTP response for two distinct types of requests that your application will receive from Slack. Its main job is to signal to Slack that the incoming webhook was handled successfully.Handling Slack URL ValidationWhen you first configure a webhook in your Slack workspace, Slack sends a special request to your application to verify that the URL is valid and that you control the endpoint. This request is known as a URL validation challenge.This is what the if block in the method handles. It checks if the dto (data transfer object) is an instance of SlackUrlValidationRequest. If it is, the method returns a  containing the  string that Slack sent. By doing this, your application proves to Slack that it is a legitimate and functional webhook endpoint. The HTTP status code of 200 OK is crucial here, as it’s the signal to Slack that the validation was successful.Handling Regular Webhook EventsAfter the initial validation, all subsequent requests from Slack will be actual events (e.g., a new message, a user joining a channel, etc.).The second part of the method, the return statement after the if block, handles these regular events . Since no specific response data is required for these events, the method simply returns a generic  with a  payload. Just like with the validation request, the 200 OK status code tells Slack that your application received and handled the event successfully.In short, this single method elegantly handles both the one-time security handshake and the ongoing processing of real-time events, providing a clear and standard response for each case.Now that the parser is created, the next step is to configure it and build the first event handler that will process the  for challenge request.namespace App\RemoteEvent;

use Symfony\Component\RemoteEvent\Attribute\AsRemoteEventConsumer;
use Symfony\Component\RemoteEvent\Consumer\ConsumerInterface;
use Symfony\Component\RemoteEvent\RemoteEvent;

#[AsRemoteEventConsumer('slack_webhook_challenge_success')]
final class SlackWebhookChallengeConsumer implements ConsumerInterface
{
    public function __construct()
    {
    }

    public function consume(RemoteEvent $event): void
    {
    }
}
\
Note that we have created a RemoteEvent with the name slackchallenge_success to handle the initial URL validation request.In the next article, we will explore creating a second RemoteEvent with the name  for the actual business logic, allowing us to process and react to  messages received via the webhook. This separation of concerns ensures that our Proactive Agent’s core logic is clean, efficient, and focused solely on real-time communications after the initial setup is complete.This final step is crucial for the configuration to work correctly.Here is the  file that will configure the  component to use our custom parser and properly secure the incoming requests.framework:
    webhook:
        routing:
            slack:
                service: App\Webhook\SlackRequestParser
                secret: '%env(SLACK_WEBHOOK_TOKEN)%'
\
This configuration file tells Symfony to create a webhook endpoint at the URL . When a request is sent to this URL, it will automatically use our App\Webhook\SlackWebhookParser to process the data.The secret key is critical for security. Slack signs every request, and the symfony/webhook component will automatically use this secret to verify the signature, ensuring that the request is genuinely from Slack and hasn’t been tampered with. It’s best practice to store this secret in your  file rather than directly in the configuration file.SLACK_WEBHOOK_TOKEN='your_secret_key'
\
With this final piece in place, your application is fully prepared to securely receive and parse incoming webhooks from Slack, setting the stage for building a truly proactive agent.We’re ready to start out appThe configuration now complete, the application is ready to be launched and tested.Our application is built to run within a pre-prepared and configured Docker image, which simplifies deployment and ensures a consistent environment across all stages of development.To run the application, you can use the following command from your terminal:\
This command will start all the necessary services, including your Symfony application, and make the webhook endpoint ready to receive and process requests from Slack. This setup is perfect for testing your Proactive Agent’s ability to handle real-time communications in a production-like environment.Additional Capabilities and Security ConsiderationsIt’s important to note that you can manually pass a secret token not only in the HTTP headers but also as a query parameter in the URL.This approach can be useful when integrating with other data sources that do not support custom headers or have limitations on their use. This provides additional flexibility and allows you to work with a wider range of services.However, please remember that transmitting a secret token in query parameters is highly discouraged from a security perspective. Query parameters:Are logged by the server: Many web servers, by default, log full request URLs, including all query parameters. This can lead to the compromise of your secret.Are cached and saved in browser history: While this may not be applicable in our case (as we are dealing with server-to-server requests), this fact underscores the vulnerability of data transmitted in this manner.For maximum security, always prefer transmitting secrets in HTTP headers. Use query parameters only when it is absolutely necessary, and always be aware of the associated risks.In this article, we’ve successfully laid the groundwork for our . We’ve moved beyond basic controllers to embrace a more sophisticated, secure, and scalable architecture using the core  and  components.We walked through the essential steps, from installing the components to generating a custom parser that can handle and verify incoming requests from Slack. We also configured the application with a  file to ensure the endpoint is secure and ready to process real-time events.The approach we’ve taken with  and event handling gives us a robust foundation for building a system that is not only functional but also highly maintainable and performant.In our next article, we’ll build on this foundation by tackling the other side of the real-time communications coin. We will focus on how to properly send a response to a received message back to Slack, enabling true,  interaction for our .]]></content:encoded></item><item><title>Knowledge Graphs Gain Traction as AI Pushes Beyond Traditional Data Models</title><link>https://hackernoon.com/knowledge-graphs-gain-traction-as-ai-pushes-beyond-traditional-data-models?source=rss</link><author>George Anadiotis</author><category>tech</category><pubDate>Thu, 25 Sep 2025 05:05:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Are graphs really the new star schema? What do graphs look like to non-insiders, and what is it that attracts them to the graph community, methodologies, applications, and innovation?In the process of connecting with Connected Data London 2025 speakers, we found ourselves engaging in conversations reflecting and elaborating on the origins and evolution of the community and the technology it converges on: Knowledge Graphs, Graph Analytics, AI, Data Science, Databases and Semantic Technology.The way we described it was – a core of early adopters that have been around for what can be more than a decade by now, plus a growing segment of newcomers who stumbled upon graphs prompted either by GenAI or by the realization that graph is the best way to model connectedness.This is why there is an explosion in adoption, thought leadership, tools, features, applications and methodologies around graphs. This issue of the Year of the Graph reflects this. Even though it’s only been 3 months since the previous one, and the targeted curation approach hasn’t changed, it’s the longest issue to date.This issue caters to everyone – from newcomers to experts, and from strategic thinkers and modelers to engineers and scientists. From graph support on Microsoft Azure, GitLab, Netflix, S&P and SAP, to semantics and agents on Databricks and Snowflake, ontologies, knowledge graphs and AI, graph transformers, and science breakthroughs.If you want to be featured in an upcoming issue and support this work, reach out!Announcing the State of the Graph projectA comprehensive and up to date repository, visualization, and analysis of all offerings in the graph technology space.Graph is the new star schemaIs graph really the new star schema? Irina Malkova, VP Product Data & AI at Salesforce, thinks so. Before AI, Malkova didn’t feel like graph metadata was an ROI positive investment for her team – even though they never looked too closely.Now, however, Malkova realizes that agents can’t be autonomous unless data is structured as a graph. Just a few days after Malkova voiced this realization, her post attracted graph leaders who shared more background and context.What do graphs and the data warehouse star schema have in common? They are ubiquitous, and they help unlock value for organizations. Unlike the star schema, however, graph data models are flexible and can unambiguously model semantics. Put another way – graph schema is the star of schemas.The key lies in building a robust IT operating model that integrates GenAI into the fabric of management systems. Graph databases and retrieval-augmented generation are foundational technologies for this transformation.Graphs represent entities and relationships flexibly, allowing GenAI to reason across complex data landscapes. By investing in graph-based knowledge infrastructure, organizations can unlock the full potential of AI while ensuring transparency, traceability, and alignment.This reframes the org chart. It’s no longer the primary map of how work flows. The orchestration graph is: the dynamic, often invisible network of people, agents, and systems connected by delegation logic, execution loops, and escalation paths.No matter what graph technology you work with, G.V() makes you more productive \n  G.V() is a graph database client and IDE that empowers you through every task:Write, execute, and profile queriesExplore your data with high-performance graph visualizationAdd or edit data on the flyAs the most widely compatible graph database IDE, G.V() supports 20+ tools, including Amazon Neptune, Google Spanner Graph, Neo4j, and JanusGraph – and now with GQL support for Ultipa Graph.Try it out for yourself and start querying your database in less than 5 minutes: gdotv.comGraphs power Systems of IntelligenceGraphs are evolving alongside GenAI, influencing as well as being influenced by it. According to the Gartner 2025 AI Hype Cycle, Generative AI capabilities are advancing at a rapid pace and the tools that will become available over the next 2-5 years will be transformative.Al investment remains strong, but focus is shifting from GenAl hype to foundational innovations like Al-ready data, Al agents, Al engineering and ModelOps. The rapid evolution of these technologies and techniques continues unabated, as does the corresponding hype, making this tumultuous landscape difficult to navigate.Systems of Intelligence are the linchpin of modern enterprise architecture because AI agents are only as smart as the state of the business represented in the knowledge graph. If a platform controls that graph, it becomes the default policymaker for “why is this happening, what comes next, and what should we do?”A unified semantic knowledge graph for Enterprise AGIUser Intent Feeds Semantics: Each question ultimately enriches the catalog with richer context, adding meaning to the data so that others can extract more value from it.Semantics Feed Agents: Richer context arms agents to make better decisions and ultimately to act autonomously.Agents Create Outcomes: Agents deliver outcomes more effectively aligned with business objectives.The goal is to enable agents that not only know what happened, but can also infer why, predict what’s next & prescribe what to do. This is the holy grail of enterprise AI – the foundation for “Enterprise AGI.” Getting there means abstracting beyond RDBMSs and tables, to a unified semantic knowledge graph.Why AI alone can’t solve all your data problemsLLMs often “hallucinate” because they rely on broad, statistical patterns, not your business’s unique knowledge. Grounding AI in your semantic models ensures outputs are accurate and trustworthy by leveraging logical connections within enterprise data.metis is a knowledge-driven AI platform that transforms disconnected data into business value. With metis, you can create and manage semantic models with AI, design and deploy custom conversational agents, and combine tools for summarization, query execution and more.Defining and building ontologiesDefining semantic layers could be called meta-semantics, which is a bit ironic, but at the same time necessary. Precise definitions is what semantics is all about, and with renewed interest and newcomers in the field, confusion, hype and heated arguments are inevitable side effects.Palantir has its own, proprietary definition and implementation of ontology. Some people believe that this is counter-productive. Standards such as OWL, RDF, SKOS and SHACL exist, so there is no reason to reinvent the wheel. Others think that while interoperability is good, those standards are not always fit for purpose, and group think limits evolution.There is growing consensus that ontologies matter, hard as they may be to develop. Joe Hoeller argues ontology development is hard because it relies on subject matter experts who bring deep understanding of operational language, workflows, and terminology but typically lack the formal training needed to represent that knowledge within rigorous frameworks.Top Level Ontologies (TLOs) such as the Basic Formal Ontology (BFO), Common Core Ontology (CCO), or SUMO (Suggested Upper Merged Ontology) are such rigorous frameworks. J Bittner explains that TLOs help with ROI in two ways.First, avoiding reinvention – you don’t need to debate or rebuild basic categories for every project. Second, when domains need to interoperate – finance with logistics, compliance with operations, healthcare with insurance – the shared foundation dramatically lowers the cost of integration.What we need, McComb argues, are orders of magnitude fewer ontologies. You really see the superpowers of ontologies when you have the simplest possible expression of complex concepts in an enterprise.This belief is reflected in gist, an open-source, business-focused ontology actively developed by Semantic Arts. Its lightweight design and use of everyday terminology has made it a useful tool for kickstarting domain ontology development in a range of areas. gist is now aligned with BFO.Sneak peek: Graph visualization and analytics, reimagined for the cloudFounded in 2013, Linkurious helps Global 2000 companies and government agencies turn complex connected data into clear insights.Now, we’re bringing our flagship graph visualization and analytics solution to the cloud. Linkurious Enterprise Cloud (just weeks away from launch…be among the first to try it!) is the most user-friendly, flexible, and scalable way to explore your graph data. In minutes, create an account, connect your graph database, and explore relationships at scale – no infrastructure or maintenance required.Getting started with knowledge graphsBusinesses must ground their AI in knowledge graphs, Joe Hoeller chimes in. First, because your tabular data is dumb, even though your business isn’t. And second, because as opposed to predictive (statistical) inference based on LLMs, ontological (logical) inference based on knowledge graphs is deterministic and explainable.You may think that knowledge graphs are too complex to implement, or that they need massive datasets. These are just some of the common misconceptions around knowledge graphs that Vasilije Markovic tries to address. But even if you are new to graphs, there are lots of resources to get you started.Cognee turns any data into a queryable knowledge graph backed by embeddingsCognee turns unstructured, structured and semi-structured data into a queryable knowledge graph backed by embeddings.Cognee retrievers blend vector similarity with graph traversal for precise, multi-hop answers and reproducible context – so agents reason and remember with structure. Add Cognee’s enrichment layer, time-awareness, auto-optimization, and its new UI for an even better experience. For teams building domain-aware agents, copilots, and search for knowledge-heavy domains.Adopting, building and populating knowledge graphsKnowledge graph adoption is peaking. There are more people wanting to build knowledge graphs, and more tools and approaches to do this than ever before.Synalinks is a Keras-based neuro-symbolic framework that bridges the gap between neural networks and symbolic reasoning. SynaLinks latest release 0.3 features optimized and constrained knowledge graph extraction and retrieval, integration with agents, Neo4j support, Cypher query generation and automatic entity alignment.Keeping track of all things Graph Year over YearKnowledge graphs and AI: a two-way streetThe authors of “Knowledge Graphs and LLMs in Action” believe that knowledge graphs and LLMs can work together. They show how to model knowledge graphs based on business needs and unstructured text data sources, how to leverage ontologies, taxonomies, structured data, machine learning algorithms and reasoning.The relationship goes the other way round, too. As part of their interpretability research, Anthropic introduced a new method to trace the “thoughts” of a large language model. The approach is to generate attribution graphs, which (partially) reveal the steps a model took internally to decide on a particular output.As Jérémy Ravenel notes, context without structure is narrative, not knowledge. And if AI is going to scale beyond demos and copilots into systems that reason, track memory, and interoperate across domains, then context alone isn’t enough. We need ontology engineering.Context engineering is about curating inputs: prompts, memory, user instructions, embeddings. It’s the art of framing. Ontology engineering is about modeling the world: defining entities, relations, axioms, and constraints that make reasoning possible. Context guides attention. Ontology shapes understanding.Agentic knowledge graph construction and temporal graphsAgentic knowledge graph construction is the latest in automated knowledge graphs. The idea Anthony Alcaraz promotes based on a tutorial by Andrew Ng and Andreas Kolleger is to deploy an AI agent workforce, treat AI as a designer, not just a doer, and use a 3-part graph architecture to augment humans instead of replacing them.The authors of the book “Building AI Agents with LLMs, RAG, and Knowledge Graphs” aim to equip data scientists to build intelligent AI agents that reason, retrieve, and interact dynamically, empowering them to deploy AI solutions. They dedicate a chapter to creating and connecting a knowledge graph to an AI Agent.Temporal graph modeling is what TGM focuses on. TGM is a research open source library designed to accelerate training workloads over dynamic graphs and facilitate prototyping of temporal graph learning methods. It natively supports both discrete and continuous-time graphs.And what about GraphRAG? Just over a year ago, GraphRAG was the hottest topic in AI. GraphRAG is an emerging set of techniques to enhance retrieval-augmented generation by integrating knowledge graphs, using their structured nature to provide richer, more nuanced context than standard vector search could offer.Several architectural blueprints for harnessing these graphs to capture the complex relationships between entities were laid out, with the goal of producing more accurate and contextually aware AI-generated responses. Since then, Ben Lorica has been watching for signs of these techniques taking root in practice.There are more new GraphRAG variants too.HippoRAG takes cues from the brain to improve LLM retrieval.Graph-R1 combines GraphRAG with Reinforcement Learning.DRAG introduces a novel distillation framework that transfers RAG capabilities from LLMs to SLMs through evidence-based distillation and Graph-based structuring.HiRAG uses hierarchical clustering to link disparate topic clusters to enhance global reasoning.A topic that’s gaining momentum in GraphRAG and beyond is multi-modality. RAG-Anything is an all-in-one RAG system that leverages multimodal knowledge graphs for automatic entity extraction and cross-modal relationship discovery for enhanced understanding.multimodal GraphRAG is a framework designed by David Hughes and Amy Hodler to seamlessly integrate visual and textual data for more comprehensive insights and more accurate responses. It combines embeddings that capture visual and audio semantics, graph-based reasoning and explainable outcomes.In “Towards Multi-modal Graph Large Language Model“, researchers propose a unified framework of multi-modal graph data, task, and model, discovering the inherent multi-granularity and multi-scale characteristics in multi-modal graphs.Graph databases grow and evolveAs AI and RAG have given a significant boost to both graph and vector databases,  people are trying to establish how these two compare, and when and how to use each.Graph databases are bustling with activity. First, we saw the unveiling of not one, but two new vendors in the last couple of months. Tentris, an efficient disk-based graph database for RDF knowledge graphs, is now in open beta. And TuringDB, a low-latency in-memory graph database engine, is now open for early access.Existing graph database vendors are making progress too.Graph data models: LPG vs. RDF, OWL vs. SHACLThe GenAI boom has given us powerful language models that can write, summarize and “reason” over vast amounts of text and other types of data. But these models don’t work for high-value predictive tasks like predicting customer churn or detecting fraud from structured, relational data.Kumo’s approach, “relational deep learning,” promises to change that. Kumo’s relational foundation model generalizes the transformer architecture to automatically represent any relational database as a single, interconnected graph, and learns directly from this graph representation.Wrapping up from the world of graph AI with more new releases. PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks for a wide range of applications related to structured data.PyG has evolved significantly since its initial release, establishing itself as a leading framework for Graph Neural Networks. PyG 2.0 is a comprehensive update that introduces substantial improvements in scalability and real-world application capabilities.Graph science: Strong perfect graphs, the new Dijkstra’s algorithm and convergent neural networksKeeping track of all things Graph Year over Year]]></content:encoded></item><item><title>Price Prediction 2025 as Ripple Escapes SEC Case. Why Pepeto’s Presale Is the Best Meme Coin to Buy</title><link>https://hackernoon.com/price-prediction-2025-as-ripple-escapes-sec-case-why-pepetos-presale-is-the-best-meme-coin-to-buy?source=rss</link><author>Tokenwire</author><category>tech</category><pubDate>Thu, 25 Sep 2025 04:59:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Ripple’s SEC case ended with a $125M fine, giving XRP more clarity at $2.87. But Pepeto’s presale at $0.000000155, 225% staking APY, SolidProof & Coinsult audits, and its live demo exchange position it as the cleaner, higher-upside play. Analysts see 60x+ potential if Pepeto matches Pepe’s price — making it a stronger 2025 bet than XRP.]]></content:encoded></item><item><title>What Happens When the Cloud Goes Down? The Hidden Fragility of Our Digital Lives</title><link>https://hackernoon.com/what-happens-when-the-cloud-goes-down-the-hidden-fragility-of-our-digital-lives?source=rss</link><author>Kennedy Ohaegbulam</author><category>tech</category><pubDate>Thu, 25 Sep 2025 04:55:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[At 2:47 PM Pacific on June 12, 2025, startup pitch decks across Silicon Valley began freezing mid-presentation. AI-powered platforms built entirely on Google Cloud simply vanished. One moment, real-time sentiment analysis dashboards were displaying data; the next, timeout errors cascaded across venture capital conference rooms.This wasn't isolated. More than 70 Google cloud services stopped working globally, knocking down Cloudflare, OpenAI and Shopify. Downdetector showed over 13,000 reported incidents within the first hour. What started as an API management error became what industry observers called the most widespread cloud failure since AWS's infamous 2017 meltdown.Here's the thing: this wasn't a freak accident. It was a symptom of an architectural house of cards years in the making.The numbers don't sugarcoat it. Critical cloud outages rose 18% in 2024. Google Cloud downtime surged 57%. AWS remained most reliable, but that's like being the tallest person in a room full of kindergarteners.According to InformationWeek's 2024 report, 60% of IT decision makers use AWS—and half faced disruption in the past year. Half. The gold standard of cloud reliability still manages to disrupt half its enterprise customers annually.The June incident perfectly illustrated modern cloud fragility. Any service dependent on Google Cloud's IAM (identity and access management—basically, the system that decides who can access what) or quota systems got hit. Not by a data center fire or earthquake, but by a control-plane bug—a software error in the systems that manage other systems. One configuration change propagated across Google's global infrastructure in minutes.Think of it like this: imagine if every traffic light in a city suddenly started following the wrong rules because someone updated the central control system incorrectly. That's essentially what happened to Google's cloud.When Everything Falls Like DominoesThe June outage exposed something terrifying: how interconnected we've become. Shopify's stock dropped 3.01% as investors fled. ChatGPT couldn't authenticate users because Google's identity management hiccupped. The future of AI, brought low by a configuration bug in Mountain View.GitLab, Replit, Elastic, and dozens of other platforms saw their services stall. Thousands of CI/CD pipelines—the automated systems that test and deploy new code—froze mid-release. Some companies lost entire product launches.Developer communities melted down on Discord and Reddit. The common theme? Architects suddenly realized their "resilient" cloud-native designs had created elegant single points of failure.I've been tracking this pattern for months, and it keeps repeating: we think we're building for resilience, but we're just building more sophisticated ways to break.Your Smart Home Isn't So SmartLet's get personal. Your daily routine now assumes the cloud is immortal. When it's not, the results range from annoying to alarming.Take the Sengled bulb disaster in June 2025. Customer forums exploded with complaints from people who'd spent hundreds on smart lighting that simply stopped responding. Night-shift workers lost their carefully timed lighting schedules. People needed flashlight apps to find switches in their own homes.Here's the kicker: these "smart" devices often have zero local intelligence. They're cloud terminals pretending to be appliances. When Amazon's Alexa services hiccupped last September, millions of Echo devices became expensive paperweights. Users couldn't even play music stored locally because the authentication lived in Seattle.It gets worse. Smart door locks have trapped people during outages. Security cameras go dark. Smoke detectors lose monitoring connections. We've traded reliable analog systems for fragile always-online gadgets.The Enterprise Wake-Up CallCorporate damage from June's outage tells a brutal story. Major retailers lost millions when e-commerce platforms degraded. Their checkout processes depended on Google's payment APIs—dependencies so fundamental that engineers never considered them failure points.Manufacturing companies watched just-in-time inventory systems collapse when Google's BigQuery (their data analysis service) went down. No access to demand forecasting meant halted assembly lines. The ripple effects hit suppliers across multiple states.Even banks felt the pinch when Google's Maps API failed. Mobile apps couldn't show branch locations—a "minor" feature that generated thousands of angry customer calls.The pattern is clear: what seems like a small dependency can topple entire business operations.The Concentration Problem Nobody MentionsHere's what really keeps me up at night: Amazon, Microsoft, and Google control roughly 65% of the cloud market. When one sneezes, the internet catches pneumonia.This concentration exploded during the AI boom. SEC filings show startups committing millions annually just to guarantee GPU access. These aren't partnerships—they're digital dependencies with service-level agreements.European officials privately call it "digital infrastructure colonialism." They're not wrong.Not everyone's building castles in the cloud. Netflix deliberately breaks their own systems with "Chaos Monkey"—software that randomly kills services to force resilient design. It's paranoid, but it works.Shopify replicates critical services across multiple cloud providers. Expensive? Yes. Effective? When Google failed, their core platform kept running.The smartest approach I've seen comes from companies building "cloud-agnostic" systems that can switch providers in minutes. When Google crashed in June, some switched to AWS in under thirty minutes. Customers never noticed.We're heading for a reckoning. Government reports model scenarios where major cloud outages cascade through financial markets and government services. A day-long AWS outage could cost the U.S. economy over $50 billion.The technology exists to build resilient systems. But resilience costs more upfront, and in a growth-obsessed industry, that's a hard sell.Here's the reality check: investors now demand disaster recovery proof, often cutting company valuations when it's missing. The cloud revolution isn't over, but its reckless adolescence is ending.So what can you do? Start simple:Keep local backups of important data. Use apps with offline modes. For businesses, don't put all your eggs in one cloud basket—spread the risk.Most importantly, stop treating the cloud like magic. It's someone else's computer, and computers break.The next time your smart bulb won't turn on, remember: somewhere in a data center, the cloud went down. And you're just a tenant in someone else's digital reality.The question isn't whether the next big outage will happen—it's whether you'll be ready when it does.Analysis based on publicly available incident reports, SEC filings, industry surveys, and technical documentation.]]></content:encoded></item><item><title>Why Dating Apps Are Dying — And What Will Replace Them</title><link>https://hackernoon.com/why-dating-apps-are-dying-and-what-will-replace-them?source=rss</link><author>Electrina</author><category>tech</category><pubDate>Thu, 25 Sep 2025 04:54:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every digital revolution begins with excitement, peaks with mass adoption, and ends in collapse. Dating apps are following this exact curve — and their decline is already here.This moment of failure creates space for innovation, where AI concierges can finally serve as real matchmakers instead of profit-driven slot machines.High Heels, High Tech: My Story of Love and CodeWhen the internet first reached homes, I was a teenager eager to explore. I began programming at 12, went online at 14, and by 18 I had already developed my first personal website with a matchmaking test coded in Perl.At the turn of the millennium — before Facebook, before Tinder — the internet was still raw and experimental, and I was already exploring how technology could connect people.My dating life has always been intertwined with technology. I have walked the entire path of digital connection — from dial-up chat rooms to today’s complex platforms. I lived through every stage of this evolution, testing nearly every major dating app. I wrote scripts, built communities, traveled across 30 countries, lived across continents, and even married across cultures (my first husband was Russian; I was later widowed from my American second). Through these experiences, I met partners of many nationalities and lifelong friends online — a journey that gives me a truly unique perspective today.From Elite Beginnings to Toxic SwampsIn the early days, only educated and tech-savvy people had access to the internet. Online chats and forums were filled with curious minds, meaningful conversations, and real possibilities.Today, a smartphone with internet access is in everyone’s pocket — from those on society’s edges to even people in prisons or psychiatric institutions. And dating apps are flooded with people who would never cross our paths in real life — outside our social circles, communities, or values.The result? Many users are shocked: in real life they are respected, yet online they face harassment, insults, and manipulation. Normal users leave, trolls remain. Effectiveness is close to zero.Without a radical shift, it will only get worse.Algorithms Have Improved — But It’s Not EnoughYes, modern dating apps like Bumble and Facebook Dating already learn and predict the kind of looks you’re drawn to. They sort and show you profiles that fit your taste, and the first fifty suggestions often feel perfect. It’s convenient, even exciting — a real breakthrough.But matching on looks is no longer the problem. The real issue is the culture. Too many users have no intention of meeting. Some chat just to collect attention, others scroll out of boredom the same way they do on Instagram. And some, unable to connect in real life, use apps to vent frustration or spread toxicity they would never show face-to-face.This has turned dating apps into swamps where genuine connections struggle to survive.It mirrors what happened to early internet forums: once revolutionary, they collapsed under the weight of trolls until moderation systems evolved. Dating apps, however, don’t clean up. Their business model profits from keeping people swiping, even in toxic conditions.What Psychological Tests MissSites like eHarmony once relied on endless questionnaires. Are you an introvert or an extrovert? Do you like books or parties?The problem is obvious: people don’t know themselves well, they answer as they wish to be, not as they are, and filling out long forms is boring. That’s why psychological testing in dating is outdated.The Real Solution: Behavioral AIThe future lies in an AI concierge that learns from how people actually communicate, not what they claim.Are you sarcastic or warm? Do you move quickly to meetings, or prefer long chats? What kind of partner fits your lifestyle and boundaries?Within one or two weeks, AI can build a real profile, adapt to cultural norms, protect safety, and even educate — explaining to men why sarcasm often kills women’s sense of safety, or to women why male sexual drive is not a threat but a biological constant that can be directed into love.This is not just matching. It is translation between genders — lowering conflict, preventing disappointment, and building compatibility.It also means real analytics: how many messages it takes to meet, where dates are arranged, what patterns lead to success. For the first time, dating could be based on transparent, behavioral data.Your Personal AI MatchmakerEvery day you already talk to your chatbot — asking questions, sharing thoughts, solving little problems. But imagine one evening you tell it something new: “I’d love to go on a date tonight.”By then, it knows you better than anyone — your likes and dislikes, the style of dates you enjoy, the way you like to spend your evenings. You don’t explain a thing.In seconds, your AI concierge shows you a handful of people nearby who are free tonight and want exactly the same experience. You glance through, pick the one who feels right.And then the magic begins: your concierge speaks with theirs, arranging every detail — who picks up whom, where you’ll meet, who covers what, and how the evening unfolds. No stress, no awkward planning, no uncertainty. You simply show up, everything ready, and step straight into a beautiful night designed just for you.Law & Privacy RegulationsHere lies the biggest barrier. Current privacy laws prevent platforms from analyzing private conversations, even anonymously. Yet these conversations hold the key to understanding how people actually connect.Until regulation evolves, apps are stuck with shallow data — swipes and clicks. But once the legal framework matures, anonymized behavioral data could be used not to exploit, but to serve humanity: helping people build safer, healthier, more compatible relationships.I believe we are entering a new era. Suffering and isolation are being replaced by awareness, therapy, communities, and support. People seek harmony, joy, and creativity.And love remains the greatest source of happiness. I see it daily in my current work curating romantic events in Florida. I witness men still making grand gestures, couples glowing with devotion, and partners radiating a passion that proves love is still alive — even in a time when so many feel disillusioned by relationships.People want connection. People need love. And technology must evolve to deliver it.Why Now Is the Perfect MomentOld-fashioned matchmaking through families and friends is gone — now considered outdated, even cringeworthy. Meanwhile, dating apps have stagnated: toxicity dominates, normal users leave, and the culture collapses.This vacuum has created the perfect moment. Old giants like Match Group profit from dysfunction, while society faces declining marriage rates, rising burnout, and the intensifying gender wars.The world is starving for a new approach.For investors, founders, and visionaries, this is the golden window. Incumbents profit from inertia. They won’t change.Dating apps are dying not because people stopped wanting love — but because platforms stopped serving that need.It is time for a new generation of dating technology: filtering toxicity, adapting to culture, learning from behavior, and helping people quickly find not only partners but also supportive friends and communities.The goal is to make happiness accessible to millions, to accelerate the next stage of humanity — one where suffering is minimized, harmony is the norm, and relationships become the fuel for creativity and growth.This is the golden moment to create the startup that will change the world.As a humanist and a hedonist, I believe technology must serve joy, connection, and love. And I am ready to contribute my experience — as an IT expert, a cross-cultural migrant, and a designer of love itself — to help shape the next era of human connection.]]></content:encoded></item><item><title>Vibe Coding: Lowering the Cost of Trying</title><link>https://hackernoon.com/vibe-coding-lowering-the-cost-of-trying?source=rss</link><author>Narek Amirkhanyan</author><category>tech</category><pubDate>Thu, 25 Sep 2025 04:50:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The hidden value of vibe coding isn’t just speed; it’s about increasing the pace of progress.]]></content:encoded></item><item><title>It isn’t your imagination: Google Cloud is flooding the zone</title><link>https://techcrunch.com/2025/09/24/it-isnt-your-imagination-google-cloud-is-flooding-the-zone/</link><author>Connie Loizos</author><category>tech</category><pubDate>Thu, 25 Sep 2025 04:41:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[While the industry's biggest players cement ever-tighter partnerships, Google is hellbent on capturing the next generation of AI companies before they become too big to court.]]></content:encoded></item><item><title>World&apos;s Oceans Fail Key Health Check As Acidity Crosses Critical Threshold For Marine Life</title><link>https://news.slashdot.org/story/25/09/24/2156242/worlds-oceans-fail-key-health-check-as-acidity-crosses-critical-threshold-for-marine-life?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 25 Sep 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Guardian: The world's oceans have failed a key planetary health check for the first time, primarily due to the burning of fossil fuels, a report has shown. In its latest annual assessment, the Potsdam Institute for Climate Impact Research said ocean acidity had crossed a critical threshold for marine life. This makes it the seventh of nine planetary boundaries to be transgressed, prompting scientists to call for a renewed global effort to curb fossil fuels, deforestation and other human-driven pressures that are tilting the Earth out of a habitable equilibrium. The report, which follows earlier warnings about ocean acidity, comes at a time of recordbreaking ocean heat and mass coral bleaching.
 
Oceans cover 71% of the Earth's surface and play an essential role as a climate stabilizer. The new report calls them an "unsung guardian of planetary health", but says their vital functions are threatened. The 2025 Planetary Health Check noted that since the start of the industrial era, oceans' surface pH has fallen by about 0.1 units, a 30-40% increase in acidity, pushing marine ecosystems beyond safe limits. Cold-water corals, tropical coral reefs and Arctic marine life are especially at risk. This is primarily due to the human-caused climate crisis. When carbon dioxide from oil, coal and gas burning enters the sea, it forms carbonic acid. This reduces the availability of calcium carbonate, which many marine organisms depend upon to grow coral, shells or skeletons.
 
Near the bottom of the food chain, this directly affects species like oysters, molluscs and clams. Indirectly, it harms salmon, whales and other sea life that eat smaller organisms. Ultimately, this is a risk for human food security and coastal economies. Scientists are concerned that it could also weaken the ocean's role as the planet's most important heat absorber and its capacity to draw down 25-30% of the carbon dioxide in the atmosphere. Marine life plays an important role in this process, acting as a "biotic bump" to sequester carbon in the depths. In the report, all of the other six breached boundaries -- climate change, biosphere integrity, land system change, freshwater use, biogeochemical flows, and novel entities -- showed a worsening trend. But the authors said the addition of the only solely ocean-centerd category was a alarming development because of its scale and importance.]]></content:encoded></item><item><title>Intel Approaches Apple For Potential Investment Amid Struggles</title><link>https://apple.slashdot.org/story/25/09/24/2141256/intel-approaches-apple-for-potential-investment-amid-struggles?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 25 Sep 2025 01:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Intel has approached Apple about a possible investment and closer collaboration, following recent multibillion-dollar deals with Nvidia, the U.S. government, and SoftBank to stabilize the struggling chipmaker. Reuters reports: The iPhone maker and Intel have also discussed how to work more closely together, the report said, adding that the talks are at an early stage and may not lead to an agreement. Shares of Intel closed 6% higher after the news. [...] Striking lucrative partnerships and persuading outside clients to use Intel's factories remain key to its future. Intel has also reached out to other companies about possible investments and partnerships, according to the Bloomberg News report. The reported investment from Apple would come as another vote of confidence for Intel - Apple had been a longtime customer of Intel before it transitioned to using its own custom-designed silicon chips in 2020.
 
For Apple, which relies heavily on Intel's rival TSMC to manufacture its chips, the new partnership would allow it to diversify its chipmaking supplier base - a move that would be valuable if geopolitical risks in Taiwan worsen due to China's role in the region. It would also help Apple improve its relationship with U.S. President Donald Trump, by showing that it is investing in the United States - while much of Apple's supply chain remains international, the company has committed about $600 billion to domestic initiatives over the next four years.]]></content:encoded></item><item><title>Qualcomm Announces Snapdragon X2 Elite and Extreme For Windows PCs</title><link>https://hardware.slashdot.org/story/25/09/24/2113246/qualcomm-announces-snapdragon-x2-elite-and-extreme-for-windows-pcs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 25 Sep 2025 01:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Qualcomm unveiled its Snapdragon X2 Elite and Extreme chips, claiming they're the "fastest and most efficient processors for Windows PCs." Built on 3nm with up to 18 cores and a 5GHz Arm CPU boost, the chips promise 31% more CPU power, up to 2.3x GPU performance, stronger AI processing, and "multi-day battery life," with devices expected in the first half of 2026. The Verge reports: There's also a new 80 TOPS Hexagon NPU, for AI tasks, that offers 37 percent more performance with a 16 percent power consumption improvement, the company claims. Qualcomm's characterizing all of this as a "legendary leap in performance," claiming the Elite Extreme in particular offers "up to 75 percent faster CPU performance" at the same power. But it doesn't say who the competition is, or which chip it was up against, at least not in the press release. And while Qualcomm claims these power savings will lead to "multi-day battery life," that's also what the company said about last year's Snapdragon X Elite.]]></content:encoded></item><item><title>Neon Pays Users To Record Their Phone Calls, Sell Data To AI Firms</title><link>https://news.slashdot.org/story/25/09/24/2034203/neon-pays-users-to-record-their-phone-calls-sell-data-to-ai-firms?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 25 Sep 2025 00:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Neon Mobile, now the No. 2 social networking app in Apple's U.S. App Store, pays users up to $30 per day to record their phone calls and sell the data to AI companies. The app claims to only capture one side of a call unless both parties use Neon, but its terms grant sweeping rights over recordings. TechCrunch reports: The app, Neon Mobile, pitches itself as a money-making tool offering "hundreds or even thousands of dollars per year" for access to your audio conversations. Neon's website says the company pays 30 cents per minute when you call other Neon users and up to $30 per day maximum for making calls to anyone else. The app also pays for referrals.
 
According to Neon's terms of service, the company's mobile app can capture users' inbound and outbound phone calls. However, Neon's marketing claims to only record your side of the call unless it's with another Neon user. That data is being sold to "AI companies," the company's terms of service state, "for the purpose of developing, training, testing, and improving machine learning models, artificial intelligence tools and systems, and related technologies."
 
Despite what Neon's privacy policy says, its terms include a very broad license to its user data, where Neon grants itself a: "...worldwide, exclusive, irrevocable, transferable, royalty-free, fully paid right and license (with the right to sublicense through multiple tiers) to sell, use, host, store, transfer, publicly display, publicly perform (including by means of a digital audio transmission), communicate to the public, reproduce, modify for the purpose of formatting for display, create derivative works as authorized in these Terms, and distribute your Recordings, in whole or in part, in any media formats and through any media channels, in each instance whether now known or hereafter developed." That leaves plenty of wiggle room for Neon to do more with users' data than it claims. The terms also include an extensive section on beta features, which have no warranty and may have all sorts of issues and bugs. Peter Jackson, cybersecurity and privacy attorney at Greenberg Glusker, told TechCrunch: "Once your voice is over there, it can be used for fraud. Now, this company has your phone number and essentially enough information -- they have recordings of your voice, which could be used to create an impersonation of you and do all sorts of fraud."]]></content:encoded></item><item><title>Broadcom&apos;s Prohibitive VMware Prices Create a Learning &apos;Barrier,&apos; IT Pro Says</title><link>https://it.slashdot.org/story/25/09/24/2022232/broadcoms-prohibitive-vmware-prices-create-a-learning-barrier-it-pro-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 25 Sep 2025 00:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: When the COVID-19 pandemic forced kids to stay home, educators flocked to VMware, and thousands of school districts adopted virtualization. The technology became a solution for distance learning during the pandemic and after, when events such as bad weather and illness can prevent children from physically attending school. However, the VMware being sold to K-12 schools today differs from the VMware that existed before and during the pandemic. Now a Broadcom business, the platform features higher prices and a business strategy that favors big spenders. This has created unique problems for educational IT departments juggling restrictive budgets and multiple technology vendors with children's needs.
 
Ars Technica recently spoke with an IT director at a public school district in Indiana. The director requested anonymity for themself and the district out of concern about potential blowback. The director confirmed that the district has five schools and about 3,000 students. The district started using VMware's vSAN, a software-defined storage offering, and the vSphere virtualization platform in 2019. The Indiana school system bought the VMware offerings through a package that combined them with VxRail, which is hyperconverged infrastructure (HCI) hardware that Dell jointly engineered with VMware.
 
However, like many of VMware customers, the Indiana school district was priced out of VMware after Broadcom's acquisition of the company. The IT director said the district received a quote that was "three to six" times higher than expected. This came as the school district is looking to manage changes in education-related taxes and funding over the next few years. As a result, the district's migration from VMware is taking IT resources from other projects, including ones aimed at improving curriculum. For instance, the Indiana district has been trying to bolster its technology curriculum, the IT director said. One way is through a summer employment program for upperclassmen that teaches how to use real-world IT products, like VMware and Cisco Meraki technologies. The district previously relied on VMware-based virtual machines (VMs) for creating "very easily and accessible" test environments for these students. But the school is no longer able to provide that opportunity, creating a learning "barrier," as the IT director put it. The IT director told Ars that dealing with a migration could be "catastrophic in that that's too much work for one person," adding: "It could be a chokehold, essentially, to where they're going to be basically forced into switching platforms -- maybe before they were anticipating -- or paying exorbitant prices that have skyrocketed for absolutely no reason. Nothing on the software side has changed. It's the same software. There's no features being added. Nobody's benefiting from the higher prices on the education side."]]></content:encoded></item><item><title>Europe&apos;s Cookie Law Messed Up the Internet. Brussels Wants To Fix It.</title><link>https://tech.slashdot.org/story/25/09/24/2021235/europes-cookie-law-messed-up-the-internet-brussels-wants-to-fix-it?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 23:32:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In a bid to slash red tape, the European Commission wants to eliminate one of its peskiest laws: a 2009 tech rule that plastered the online world with pop-ups requesting consent to cookies. From a report: It's the kind of simplification ordinary Europeans can get behind. European rulemakers in 2009 revised a law called the e-Privacy Directive to require websites to get consent from users before loading cookies on their devices, unless the cookies are "strictly necessary" to provide a service. Fast forward to 2025 and the internet is full of consent banners that users have long learned to click away without thinking twice. 

"Too much consent basically kills consent. People are used to giving consent for everything, so they might stop reading things in as much detail, and if consent is the default for everything, it's no longer perceived in the same way by users," said Peter Craddock, data lawyer with Keller and Heckman. Cookie technology is now a focal point of the EU executive's plans to simplify technology regulation. Officials want to present an "omnibus" text in December, scrapping burdensome requirements on digital companies. On Monday, it held a meeting with the tech industry to discuss the handling of cookies and consent banners.]]></content:encoded></item><item><title>CFO of $320 Billion Software Firm: AI Will Help Us &apos;Afford To Have Less People&apos;</title><link>https://tech.slashdot.org/story/25/09/24/205220/cfo-of-320-billion-software-firm-ai-will-help-us-afford-to-have-less-people?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 22:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The pressure is mounting on business leaders to harness AI to make work faster, cheaper, and more efficient. That may thrill investors, but for employees, it could mean fewer jobs around the world. From a report: At the $320 billion software giant SAP, there will likely be a need for fewer engineers to deliver the same -- or even greater -- output, according to the company's CFO Dominik Asam. 

"There's more automation, simply," Asam told Business Insider. "There are certain tasks which are automated and for the same volume of output we can afford to have less people." As a C-suite exec at Europe's most valuable software company, Asam cautioned that this reality will only come true if the corporate world implements the technology properly. After all, a recent MIT study found that 95% of generative AI pilots have not met the mark. "I will be brutal. And I also say this internally. For SAP and any other software company, AI is a great catalyst. It can be either great or catastrophe," Asam warned. "It will be great if you do it well, if you are able to implement it and do it faster than others. If you are left behind, you will have a problem for sure. We work day and night to not fall behind."]]></content:encoded></item><item><title>Record-Breaking DDoS Attack Peaks At 22 Tbps and 10 Bpps</title><link>https://it.slashdot.org/story/25/09/24/2010227/record-breaking-ddos-attack-peaks-at-22-tbps-and-10-bpps?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 22:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Cloudflare blocked the largest-ever DDoS attack against a European network infrastructure company, which peaked at 22.2 Tbps and 10.6 Bpps. The hyper-volumetric attack has been linked to the Aisuru botnet and lasted just 40 seconds, but was double the size of the previous record. SecurityWeek reports: Cloudflare told SecurityWeek that the attack was aimed at a single IP address of an unnamed European network infrastructure company. Cloudflare has yet to determine who was behind the attack, but believes it may have been powered by the Aisuru botnet, which was also linked earlier this year to a massive 6.3 Tbps attack on the website of cybersecurity blogger Brian Krebs. Aisuru has been around for more than a year. The botnet is powered by hacked IoT devices such as routers and DVRs that have been compromised through the exploitation of known and zero-day vulnerabilities.
 
According to Cloudflare, the 22 Tbps attack was traced to over 404,000 unique source IPs across over 14 ASNs worldwide. "Based on internal analysis using a proprietary system, the source IPs were not spoofed," the company explained. The security firm described it as a UDP carpet bomb attack targeting an average of 31,000 destination ports per second, with a peak of 47k ports, all of a single IP address. Cloudflare revealed in July that the number of DDoS attacks it blocked in the first half of 2025 had already exceeded all the attacks mitigated in 2024.]]></content:encoded></item><item><title>Y Combinator launches ‘Early Decision’ for students who want to graduate first, build later</title><link>https://techcrunch.com/2025/09/24/y-combinator-launches-early-decision-for-students-who-want-to-graduate-first-build-later/</link><author>Tage Kene-Okafor</author><category>tech</category><pubDate>Wed, 24 Sep 2025 22:36:56 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The program allows students to apply while still in school, get accepted and funded immediately, and defer their participation in YC until after they graduate. ]]></content:encoded></item><item><title>India court rejects X’s ‘free speech’ argument, backs government takedown powers</title><link>https://techcrunch.com/2025/09/24/india-court-rejects-xs-free-speech-argument-backs-government-takedown-powers/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Wed, 24 Sep 2025 22:20:57 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[X sued the Indian government in March over content takedown orders issued through its centralized portal.]]></content:encoded></item><item><title>Billionaire VC Mike Moritz slams new H-1B visa fee as ‘brutish extortion scheme’</title><link>https://techcrunch.com/2025/09/24/billionaire-vc-mike-moritz-slams-new-h-1b-visa-fee-as-brutish-extortion-scheme/</link><author>Connie Loizos</author><category>tech</category><pubDate>Wed, 24 Sep 2025 22:12:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In a new, scathing Financial Times op-ed, the former Sequoia Capital honcho compares the White House to Tony Soprano's pork store, calling its new H-1B visa plan another "brutish extortion scheme."]]></content:encoded></item><item><title>Fossil Fuel Burning Poses Threat To Health of 1.6 Billion People, Data Shows</title><link>https://news.slashdot.org/story/25/09/24/1956244/fossil-fuel-burning-poses-threat-to-health-of-16-billion-people-data-shows?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 22:11:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Fossil fuel burning is not just damaging the world's climate; it is also threatening the health of at least 1.6 billion people through the toxic pollutants it produces, data shows. From a report: Carbon dioxide, the main greenhouse gas from fossil fuel burning, does not directly damage health, but leads to global heating. However, coal and oil burning for power generation, and the burning of fossil fuels in industrial facilities, pollute the air with particulate matter called PM2.5, which has serious health impacts when breathed in. 

A new interactive map from Climate Trace, a coalition of academics and analysts that tracks pollution and greenhouse gases, shows that PM2.5 and other toxins are being poured into the air near the homes of about 1.6 billion people. Of these, about 900 million are in the path of "super-emitting" industrial facilities -- including power plants, refineries, ports and mines -- that deliver outsize doses of toxic air.]]></content:encoded></item><item><title>Chipiron’s big idea: rethinking MRI access, not replacing old machines</title><link>https://techcrunch.com/video/chipirons-big-idea-rethinking-mri-access-not-replacing-old-machines/</link><author>Theresa Loconsolo</author><category>tech</category><pubDate>Wed, 24 Sep 2025 22:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Medical device funding is hitting levels we haven’t seen since 2021, with investors pouring billions into diagnostics and imaging companies. While innovation has raced ahead, a fundamental problem still hasn’t changed: Critical medical hardware like MRI machines cost millions of dollars and is limited to large hospitals. So how do you take one of the […]]]></content:encoded></item><item><title>Cloudflare Launches Content Signals Policy To Fight AI Crawlers and Scrapers</title><link>https://tech.slashdot.org/story/25/09/24/1953230/cloudflare-launches-content-signals-policy-to-fight-ai-crawlers-and-scrapers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 21:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli shares a report from NERDS.xyz: Cloudflare has unveiled the Content Signals Policy, a free addition to its managed robots.txt service that aims to give website owners and publishers more control over how their content is accessed and reused by AI companies. The idea is pretty simple: robots.txt already lets site operators specify which crawlers can enter and where. Cloudflare's new policy adds a layer that signals how the data may be used once accessed, with plain-language terms for search, AI input, and AI training. "Yes" means allowed, "no" means not allowed, and no signal means no preference.
 
Matthew Prince, Cloudflare's co-founder and CEO, said: "The Internet cannot wait for a solution, while in the meantime, creators' original content is used for profit by other companies. To ensure the web remains open and thriving, we're giving website owners a better way to express how companies are allowed to use their content." Cloudflare says more than 3.8 million domains already use its robots.txt tools to signal they don't want their content used for AI training. Now, the Content Signals Policy makes those preferences clearer and potentially enforceable. Further reading: Cloudflare Flips AI Scraping Model With Pay-Per-Crawl System For Publishers]]></content:encoded></item><item><title>All the Growth Can Be Found in the Indian BFSI Sector</title><link>https://hackernoon.com/all-the-growth-can-be-found-in-the-indian-bfsi-sector?source=rss</link><author>Vipin Labroo</author><category>tech</category><pubDate>Wed, 24 Sep 2025 21:28:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Indian BFSI (Banking, Financial Services and Insurance) sector has seen humongous growth over the last two decades, having grown in GDP from Rs. 1.8 trillion in 2005 to Rs. 91 trillion in 2025- a whopping 50X rise in value.[1] Apart from the burgeoning size of the economy and the accompanying rise in consumption of financial services, the greater inclusion of people living in tier II and tier III cities has also given a fillip to the sector, with aspirational spending leading to a lot of the credit growth. \
The government, on its part, has gone out of its way to ensure that more and more people join the formal banking system via Aadhar and the e-KYC-aided simple account opening process. It has also helped put in place a highly efficient Direct Benefit Transfer (DBT) system to reach funds directly to the beneficiaries in their bank accounts, helping ingrain banking habits in a larger and larger number of people.\
The highly successful and transformational digital payments revolution in the shape of UPI or Unified Payment Interface has put India right at the top of the world with regard to real-time payment transactions. Fintech startups have played their part in the growth of the sector on account of their tie-ups with banks, helping the latter make their services vastly more accessible as well as customer-friendly. \
The BFSI sector has, in fact, been a frontrunner with regard to the use of data analytics and AI to help take its services to the next level. The widespread use of mobile banking is a sterling example of how technology can help improve the reach and efficiency of Indian banks.Exponential Growth of CreditThe Indian middle classes have taken to credit like never before, allowing banks to achieve double-digit growth in terms of credit uptake. This is particularly true for retail loans, helping people finance the purchase of houses, vehicles, and even personal expenses. \
The scope of growth for credit to the private sector is quite good, given that India’s domestic credit to the private sector stood at 55% of the GDP in 2020, which is quite less than the global average of 148% according to World Bank figures. Compared to Chain’s 128% of GDP, South Korea’s 165%, and even Vietnam’s 148%, India can do much better on this front in the days ahead.[2]Low Insurance PenetrationAs a low insurance penetration nation, the scope of growth for insurance products is immense in India. With a mere 4% penetration in 2023, the sky is the limit when it comes to industry growth. The good news is that the Indian insurance industry already boasts a Gross Written Premium (GWP) in excess of $130 billion and an 11% CAGR (from the fiscal year 2020 to the fiscal year 2023). [3] IRDAI, the body that oversees the Indian insurance industry, has initiated a number of reforms that have made insurance products much more accessible and easier to understand than they were in the past. \
The emergence and growth of private players has led to a number of benefits, including a rise in investment and the deployment of technology, accompanied by a discernible rise in efficiency. Besides, the more liberal foreign investment policies have made global insurance majors look at the Indian insurance sector with renewed interest.The Indian FinTech RevolutionThe Indian FinTech revolution has made the world sit up and take notice. The smooth rollout of the universal digital payments system UPI has impressed nations from across the first and the third worlds. The Indian FinTech revolution has been fueled by the large-scale adoption of innovative digital technologies that help meet the demand for efficient and cheap financial services and products. It is not surprising that the Indian FinTech sector has received $20 billion plus investment in the last five years.[4]\
The potential for growth for the Indian FinTech over the next two decades is quite robust, given that less than 50% of the nation’s population has access to digital payments, and the number of people with access to credit is a measly 10%.[5]BFSI- An Employment Generator Par ExcellenceThe BFSI sector provides employment to 6.1 million professionals, of which banking provides jobs to 2.4 million workers, NBCF to 2.2 million, insurance to half a million, and other financial services to 1 million workers. [6] Going forward, one can expect the Indian BFSI sector to continue to do extremely well in terms of providing employment to the young. \
It is expected that it will provide employment to some 250,000 people by 2030, with most of the hiring happening in Tier II and Tier III cities. Much of the employment will be generated by wealth and insurance businesses that are looking to hire financial planners, digital underwriters, investment advisors, and claim automation professionals.[7]The Indian BFSI sector is the sector to watch out for in terms of its sheer potential for growth and ability to generate employment. Driven by widespread digital adoption, a stupendous growth in financial inclusivity, the growth of diverse business and employment opportunities, as well as the fact that the Indian economy is on the ascendant, the Indian BFSI is in a fortuitous and happy space that is likely to remain that way in the foreseeable future.\
[1] https://www.livemint.com/companies/news/indias-bfsi-sector-grows-over-50-times-in-two-decades-share-of-banks-reduced-due-to-nbfcs-report-11745462233150.html[2] https://www.ey.com/en_in/insights/india-at-100/how-india-can-fill-the-credit-gap-to-fuel-economic-growth[3] https://www.mckinsey.com/in/our-insights/steering-indian-insurance-from-growth-to-value-in-the-upcoming-techade[4] https://www.pwc.in/assets/pdfs/investing-in-indias-fintech-disruption.pdf[5] https://economictimes.indiatimes.com/small-biz/sme-sector/fintech-revolution-2-0-is-expected-to-unfold-in-india-in-the-next-10-years-mobikwiks-upasana-taku/articleshow/120891118.cms?from=mdr[6] https://www.praxisga.com/insights/education-and-employability/unlocking-growth-skilling-opportunities-in-india-s-bfsi-sector[7] https://economictimes.indiatimes.com/industry/banking/finance/banking/bfsi-sector-to-add-2-5-lakh-jobs-by-2030-hiring-shifts-to-tier-ii-iii-cities-report/articleshow/123426130.cms?from=mdr]]></content:encoded></item><item><title>BTCC Partners With NBA All-Star Jaren Jackson Jr. to Inspire Smarter Moves In Sports and Crypto</title><link>https://hackernoon.com/btcc-partners-with-nba-all-star-jaren-jackson-jr-to-inspire-smarter-moves-in-sports-and-crypto?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 24 Sep 2025 21:27:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[VILNIUS, Lithuania, September 24th, 2025/Chainwire/--, the world’s longest-serving crypto exchange, debuts its  with its brand ambassador and NBA superstar Jaren Jackson Jr. Following the exchange's  with the Memphis Grizzlies forward in August 2025, this video represents a unique approach to authentic storytelling that bridges sports, music, and cryptocurrency trading.Showcasing Jackson Jr.'s multi-dimensional identity as an elite NBA player, a music producer under his artist name Trip J, and a crypto trader, the video uniquely features his self-produced music from his 2025 EP "Zelle," including the standout track "Imitate," creating authentic content that reflects both the athlete's creative versatility.The video explores Jackson Jr.'s unique lifestyle through three distinct modes: day mode showcasing his basketball dominance, night mode highlighting his music production in his home studio, where he's recorded over 150 tracks, and trading mode sharing his smart financial decisions through BTCC's platform. "This collaboration with Jaren Jackson Jr. goes far beyond typical sports sponsorship," said Aaryn Ling, Head of Branding at BTCC Exchange. "We're telling authentic stories that bring out the creative energy and multi-dimensional thinking that defines both JJJ's approach to basketball and music, and BTCC's fresh perspective on cryptocurrency trading."Building on this partnership, BTCC will launch the  referral challenge starting September 25, 2025, offering 300,000 USDT in exclusive rewards to users who refer friends to deposit and trade on BTCC.As part of its broader industry engagement, BTCC will also participate in TOKEN2049 Singapore and has launched a  starting September 22, 2025, with prizes including an iPhone 17 Pro Max and a share of the 20,490 USDT prize pool, plus exclusive TOKEN2049 badges for BTCC users.In further community initiatives, BTCC Exchange continues to demonstrate its commitment to social responsibility by sponsoring its third charity event of 2025, in collaboration with the Red Eagle Foundation, on October 9, featuring former Chelsea and England captain John Terry at the London Golf Club. This ongoing charitable partnership underscores BTCC's commitment to community support beyond the cryptocurrency space.Founded in 2011, BTCC is a leading global cryptocurrency exchange with the vision to make crypto trading reliable and accessible to everyone. With a strong presence in over 100 countries and regions and a user base of over 10 million, BTCC continues to deliver innovation, security, and an unmatched user experience in the cryptocurrency world.As part of its commitment to connecting crypto with mainstream audiences, BTCC has partnered with 2023 Defensive Player of the Year and 2x NBA All-Star Jaren Jackson Jr. as its global brand ambassador, bridging the gap between traditional sports and cryptocurrencies.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Google Experiences Deja Vu As Second Monopoly Trial Begins In US</title><link>https://tech.slashdot.org/story/25/09/24/1948231/google-experiences-deja-vu-as-second-monopoly-trial-begins-in-us?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 20:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Guardian: After deflecting the US Department of Justice's attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet empire in a trial focused on abusive tactics in digital advertising. The trial that opened Monday in an Alexandria, Virginia, federal court revolves around the harmful conduct that resulted in US district Judge Leonie Brinkema declaring parts of Google's digital advertising technology to be an illegal monopoly in April. The judge found that Google has been engaging in behavior that stifles competition to the detriment of online publishers that depend on the system for revenue.
 
Google and the justice department will spend the next two weeks in court presenting evidence in a "remedy" trial that will culminate in Brinkema issuing a ruling on how to restore fair market conditions. If the justice department gets its way, Brinkema will order Google to sell parts of its ad technology -- a proposal that the company's lawyers warned would "invite disruption and damage" to consumers and the internet's ecosystem. The justice department contends a breakup would be the most effective and quickest way to undercut a monopoly that has been stifling competition and innovation for years. [...]
 
The case, filed in 2023 under Joe Biden's administration, threatens the complex network that Google has spent the past 17 years building to power its dominant digital advertising business. Digital advertising sales account for most of the $305 billion in revenue that Google's services division generates for its corporate parent Alphabet. The company's sprawling network of display ads provide the lifeblood that keeps thousands of websites alive. Google believes it has already made enough changes to its "ad manager" system, including providing more options and pricing options, to resolve the problems Brinkema flagged in her monopoly ruling.]]></content:encoded></item><item><title>Microsoft Offers No-Cost Windows 10 Lifeline</title><link>https://tech.slashdot.org/story/25/09/24/1943257/microsoft-offers-no-cost-windows-10-lifeline?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 20:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft on Sept 24 announced new options for US and European customers to safely extend the life of the Windows 10 operating system free of charge just days before a key deadline to upgrade to Windows 11. From a report: The US tech giant plans to end support for Windows 10 on Oct 14, a move that has drawn criticism from consumer advocacy groups and sparked concerns among users who fear they will need to purchase new computers to stay protected from cyber threats. 

Users who are unable to upgrade or choose to forgo the extended security updates will face increased vulnerability to cyberattacks. In response to these concerns, Microsoft informed European users that essential security updates will be extended for one year at no additional cost, provided they log in with a Microsoft account. Previously, the company had offered a one-year extension of Windows 10 security updates for $30 to users whose hardware is incompatible with Windows 11. In the US, a similar free option will allow users to upload their Windows 10 profiles to Microsoft's backup service and receive security updates for up to one year.]]></content:encoded></item><item><title>Neon, the No. 2 social app on the Apple App Store, pays users to record their phone calls and sells data to AI firms</title><link>https://techcrunch.com/2025/09/24/neon-the-no-2-social-app-on-the-apple-app-store-pays-users-to-record-their-phone-calls-and-sells-data-to-ai-firms/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 24 Sep 2025 19:50:58 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A new call recording app is gaining traction for offering to pay users for voice data from calls, which is sold to AI companies.]]></content:encoded></item><item><title>Zoox asks federal regulators for exemption to launch a commercial robotaxi service</title><link>https://techcrunch.com/2025/09/24/zoox-asks-federal-regulators-for-exemption-to-launch-a-commercial-robotaxi-service/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Wed, 24 Sep 2025 19:44:31 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The exemption would allow the Amazon-owned autonomous vehicle company to commercially deploy its custom-built robotaxis, which lack traditional controls like pedals and a steering wheel. ]]></content:encoded></item><item><title>Pocket Casts is Showing Ads To People Who Paid For an Ad-free App</title><link>https://slashdot.org/story/25/09/24/1930219/pocket-casts-is-showing-ads-to-people-who-paid-for-an-ad-free-app?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 19:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Pocket Casts is being flogged for showing advertisements to legacy users who were promised an ad-free experience. From a report: The first reports started to appear in early September in the Pocket Casts support forum and subreddit. The issue is a bug, according to Matt Mullenweg, the CEO of Pocket Casts' parent company Automattic, and will be corrected. Pocket Casts launched as a purchase-only app in 2010, charging users a one-time download fee of up to $10, depending on the OS and platform. The service later switched to a subscription-based model and made the app available for free in 2019. After backlash from users, the company gave anyone who paid for the web or desktop apps before the pricing changes free lifetime access to Pocket Casts Plus, its ad-free premium subscription service. 

The app was acquired by Automattic in 2021, and the Pocket Casts Lifetime memberships were rebranded to "Pocket Casts Champion" in August 2024.]]></content:encoded></item><item><title>Trump administration wants 10% stake in Canadian lithium miner that sells to GM</title><link>https://techcrunch.com/2025/09/24/trump-administration-wants-10-stake-in-canadian-lithium-miner-that-sells-to-gm/</link><author>Tim De Chant</author><category>tech</category><pubDate>Wed, 24 Sep 2025 19:22:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Lithium Americas asked for a small change to the loan repayment period, which spurred the latest Trump administration request for equity.]]></content:encoded></item><item><title>YouTube now lets you hide recommendation pop-ups at the end of videos</title><link>https://techcrunch.com/2025/09/24/youtube-now-lets-you-hide-recommendation-pop-ups-at-the-end-of-videos/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 24 Sep 2025 19:09:49 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The decision to let users hide end screens is a welcome addition to the platform, especially since they can end up covering the last bit of a video and make it hard to see what's happening in some cases. ]]></content:encoded></item><item><title>Ethereum 2030: Vitalik&apos;s Audacious Vision vs. Harsh Reality Check</title><link>https://hackernoon.com/ethereum-2030-vitaliks-audacious-vision-vs-harsh-reality-check?source=rss</link><author>Crypto Sovereignty Through Technology, Math &amp; Luck</author><category>tech</category><pubDate>Wed, 24 Sep 2025 18:47:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Vitalik's 2030 Ethereum overhaul: quantum-resistant signatures, EVM→RISC-V migration, 12-second finality, STARK proofs, advanced networking. Promises 6,600x performance boost but faces brutal technical/political realities. Verdict: networking & quantum resistance achievable, rest overly ambitious.
]]></content:encoded></item><item><title>Cohere hits $7B valuation a month after its last raise, partners with AMD</title><link>https://techcrunch.com/2025/09/24/cohere-hits-7b-valuation-a-month-after-its-last-raise-partners-with-amd/</link><author>Julie Bort</author><category>tech</category><pubDate>Wed, 24 Sep 2025 18:06:22 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A fresh $100 million and an interesting partnership with AMD has slightly bumped Cohere's valuation up.]]></content:encoded></item><item><title>Microsoft adds Anthropic’s AI to Copilot</title><link>https://techcrunch.com/2025/09/24/microsoft-adds-anthropics-ai-to-copilot/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Wed, 24 Sep 2025 17:46:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Microsoft is integrating OpenAI rival Anthropic's AI into Copilot, marking another step toward the disentangling of Microsoft and OpenAI.]]></content:encoded></item><item><title>Fitell Corporation Launches Solana (SOL) Digital Asset Treasury with $100M Financing Facility</title><link>https://hackernoon.com/fitell-corporation-launches-solana-sol-digital-asset-treasury-with-$100m-financing-facility?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 24 Sep 2025 17:34:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Taren Point, Australia, September 23rd, 2025/Chainwire/--Fitell Corporation (NASDAQ: FTEL) (“Fitell” or the “Company”), a global provider of fitness equipment and health solutions, today announced it has secured an up to $100M facility to support the launch of its Solana treasury strategy, marking the first Solana-based digital asset treasury in Australia.● DeFi and Yield Roadmap: Designed to generate outsized yields by deploying SOL assets across a diversified suite of on-chain DeFi and derivatives strategies. This includes structured products such as options, snowballs, on-chain liquidity provisioning, and other highly liquid strategies with managed downside risk. Each approach offers varying levels of returns, alpha generation, and duration. Returns generated will be reinvested into the treasury reserve, compounding the velocity of SOL accumulation, increasing SOL-per-share and strengthening alignment with the Solana community.● Australian Dual Listing: Fitell has initiated steps toward a dual listing on the Australian Securities Exchange (ASX), broadening access and exposure for regional investors to SOL.● Rebrand: Following the initial launch of the Solana treasury, the Company plans to rebrand to “Solana Australia Corporation”.● Security: Initial SOL assets will be custodied with BitGo Trust Company, Inc. in the U.S. and staked through institutional-grade infrastructure.Advisors Fitell has appointed David Swaney and Cailen Sullivan as advisors to lead the Company’s digital asset treasury roadmap. Their mandate focuses on designing strategies to optimize treasury performance through decentralized finance (DeFi) opportunities, risk management frameworks, and yield innovation beyond traditional staking models.● David Swaney has been active in the digital asset space since 2017, focusing on institutional adoption of on-chain finance. His extensive advisory and consulting experience spans treasury design, structured yield strategies, and market infrastructure.● Cailen Sullivan has been active in the digital asset space for over a decade and was an early hire at Coinbase in 2013. Recently, he has been primarily focused on investing and supporting projects across the Solana ecosystem, and is a co-founder of Adrena, one of the leading perpetuals DEXs on Solana based on trading volume.“We believe that digital asset treasuries are laying the blueprint for digital asset ETFs,” said David Swaney. “The ability to generate yield on assets beyond staking will be the defining differentiator, and we intend to lead this effort.”“Our strategy focuses not only on Solana itself, but the broader ecosystem of applications being built on top. By deploying more assets on-chain, we aim to generate outsized returns, setting a new benchmark for performance in digital asset management while supporting the growth of DeFi applications on Solana,” said Cailen Sullivan.Both advisors commented: “As advisors, we are excited to position this as the most innovative Solana-aligned treasury in the market. While scale varies among peers, our focus is value creation through capital concentration in the Solana ecosystem. By bringing assets on-chain and integrating with native projects, we believe we can deliver value to shareholders while advancing the ecosystem. Our goal is to demonstrate that strategy and alignment can rival scale and set a benchmark for Solananative treasuries.”Sam Lu, Chief Executive Officer of Fitell Corporation, commented: “The launch of our Solana digital asset treasury positions Fitell at the forefront of Solana adoption in the regions of Australia and Asia Pacific. Our ambition to become the region’s largest publicly listed Solana holder underscores our conviction in the network’s long-term potential. With the expertise of David Swaney and Cailen Sullivan, we are excited to execute on a roadmap that combines innovation, yield generation, and disciplined risk management.”The Company will further discuss its digital asset treasury today at 4:00 p.m. ET on September 23, 2025 during a live session hosted on @MarioNawfal or @RoundtableSpace.To support initial SOL acquisitions, the Company has entered into an up to $100M convertible note facility with a U.S.-based institutional investor, of which $10M from the initial closing will be immediately deployed to purchase SOL.Rodman & Renshaw acted as exclusive placement agent for the financing. The foregoing description does not purport to be complete and is qualified in its entirety by reference to the full text of the Company’s Report of Foreign Private Issuer on Form 6-K to be filed with the Securities and Exchange Commission on September 23, 2025.Fitell Corporation, through GD Wellness Pty Ltd (“GD”), its wholly owned subsidiary, is an online retailer of gym and fitness equipment both under its proprietary brands and other brand names in Australia.The company’s mission is to build an ecosystem with a whole fitness and wellness experience powered by technology to our customers. GD has served over 100,000 customers with large portions of sales from repeat customers over the years.The Company’s brand portfolio can be categorized into three proprietary brands under its Gym Direct brand: Muscle Motion, Rapid Motion, and FleetX, in over 2,000 stock-keeping units (SKUs). For additional information, please visit the Company’s website at www.fitellcorp.com.\
Forward-Looking Statements This press release contains “forward-looking statements” within the meaning of Section 21E of the Securities Exchange Act of 1934, as amended. These forward-looking statements are made under the “safe harbor” provisions of the U.S. Private Securities Litigation Reform Act of 1995. All statements other than statements of historical fact in this press release are forward-looking statements. These forward-looking statements involve known and unknown risks and uncertainties, including market and other conditions, and are based on the Company’s current expectations and projections about future events that the Company believes may affect its financial condition, results of operations, business strategy and financial needs. Investors can identify these forward-looking statements by words or phrases such as “may,” “will,” “could,” “expect,” “anticipate,” “aim,” “estimate,” “intend,” “plan,” “believe,” “is/are likely to,” “propose,” “potential,” “continue” or similar expressions. The Company undertakes no obligation to update or revise publicly any forward-looking statements to reflect subsequent occurring events or circumstances, or changes in its expectations, except as may be required by law. Although the Company believes that the expectations expressed in these forward-looking statements are reasonable, it cannot assure you that such expectations will turn out to be correct, and the Company cautions investors that actual results may differ materially from the anticipated results and encourages investors to review other factors that may affect its future results in the Company’s registration statement and other filings with the SEC.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>How to build a GTM strategy that actually drives results at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/09/24/how-to-build-a-gtm-strategy-that-actually-drives-results-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 24 Sep 2025 17:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At TechCrunch Disrupt 2025, GTM pros Max Altschuler, Alison Wagonfeld, and Marc Manara take the Builders Stage to show founders how to build a go-to-market engine that doesn’t just support the product, but also scales with it.]]></content:encoded></item><item><title>Horror Film&apos;s Wedding Scene Digitally Altered for Chinese Audiences</title><link>https://slashdot.org/story/25/09/24/1718234/horror-films-wedding-scene-digitally-altered-for-chinese-audiences?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 17:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Australian horror film Together, starring Dave Franco and Alison Brie, underwent digital alterations for its mainland China release on September 12. Chinese cinemagoers discovered that a wedding scene between two men had been modified using face-swapping technology to transform one male character into a female appearance. The change only became apparent after side-by-side screenshots from the original and altered versions circulated on social media platforms. 

Chinese viewers are expressing outrage over the AI-powered modification, The Guardian reports, citing concerns about creative integrity and the difficulty of detecting such alterations compared to traditional scene cuts. The film's distributor halted the scheduled September 19 general release following the backlash. China's censorship authorities require all imported films to undergo approval before release.]]></content:encoded></item><item><title>Nebeus Overfunds Equity Crowdfunding With €3.6M,Reflects Growing Demand for Regulated Solutions</title><link>https://hackernoon.com/nebeus-overfunds-equity-crowdfunding-with-euro36mreflects-growing-demand-for-regulated-solutions?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 24 Sep 2025 17:22:50 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[London, United Kingdom, September 24th, 2025/Chainwire/-- has exceeded its crowdfunding target on Republic Europe, raising €3.6 million (122% of target) from over 430 backers. The campaign’s overfunding highlights investor appetite for regulated crypto–finance platforms at a time when markets are undergoing renewed scrutiny and institutional adoption.The milestone comes as Bitcoin remains near multi-year highs and regulators accelerate efforts to bring digital assets under clearer frameworks, from Europe’s MiCA regulation to licensing regimes in Latin America. In this environment, investors are showing increased preference for platforms that can bridge crypto and traditional finance under a regulated model.Why Investors Backed Nebeus Beyond TargetRevenue traction: The company reported 6× year-on-year revenue growth in 2024, reaching €2.2M, demonstrating strong commercial adoption.Product growth: In 2025, Nebeus recorded 22% MoM lending growth, a 1,288% increase in loan originations quarter-over-quarter, and a 177% surge in exchange volumes, reflecting strong product-market fit and sustained user demand.Regulated at a critical time: Nebeus operates as a UK Electronic Money Institution and a registered Virtual Asset Service Provider in Spain and Argentina, putting it ahead of competitors still adapting to compliance requirements.Positioned for macro trends: With global stablecoin circulation surpassing €150 billion and the freelance economy projected to reach 1.5 billion workers by 2027, Nebeus’ combination of IBANs, crypto cards, and lending tools responds directly to shifting financial needs.Investor returns: Previous backers have already seen their Nebeus shares grow by 285%, strengthening confidence in the company’s growth trajectory.Final Week: Investor Q&A with FoundersTo close the campaign, Nebeus will host a live Q&A with the team on September 25th, giving prospective backers direct access to discuss growth plans and financials. Details are available on the campaign page: .The overfunding signals growing confidence in compliance-first, integrated platforms that link traditional banking with digital assets. As regulation matures and adoption widens, Nebeus’ model reflects how the next stage of crypto–finance is being built.Nebeus is on a mission to make crypto a mainstream payment method. Founded in 2014, the company connects digital assets with traditional money through IBANs, cards, and lending solutions. With 6× revenue growth in 2024 and licenses in the UK, Spain, and Argentina, Nebeus empowers businesses, freelancers, and digital nomads with everyday payments and global payouts as it scales worldwide.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>These YC founders pivoted 5 times before building a social app that nabbed 300K users and over $1M ARR in 6 months</title><link>https://techcrunch.com/2025/09/24/these-yc-founders-pivoted-5-times-before-building-a-social-app-that-nabbed-300k-users-and-over-1m-arr-in-6-months/</link><author>Tage Kene-Okafor</author><category>tech</category><pubDate>Wed, 24 Sep 2025 17:00:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[How these founders, while in YC, pivoted from a shopping app to build Candle, a consumer app that helps couples and friends grow closer.]]></content:encoded></item><item><title>Qualcomm CEO Says He&apos;s Seen Google&apos;s Android-ChromeOS Merger, Calls It &apos;Incredible&apos;</title><link>https://tech.slashdot.org/story/25/09/24/1652218/qualcomm-ceo-says-hes-seen-googles-android-chromeos-merger-calls-it-incredible?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 16:52:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Qualcomm CEO Cristiano Amon told attendees at yesterday's Snapdragon Summit opening keynote that he has seen Google's merged Android-ChromeOS platform for PCs. Speaking alongside Google's head of platforms and devices Rick Osterloh, Amon said the software "delivers on the vision of convergence of mobile and PC" and that he "can't wait to have one." 

Osterloh confirmed Google is building a common technical foundation for PCs and desktop computing systems that combines Android and ChromeOS. The platform will include Gemini, the full Android AI stack, all Google applications and the Android developer community. "I've seen it, it is incredible," replied Amon excitedly. "It delivers on the vision of convergence of mobile and PC. I can't wait to have one."]]></content:encoded></item><item><title>How Flare Network Finally Unlocks XRP&apos;s $200B DeFi Potential Through Revolutionary FAssets System</title><link>https://hackernoon.com/how-flare-network-finally-unlocks-xrps-$200b-defi-potential-through-revolutionary-fassets-system?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 24 Sep 2025 16:29:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Can a cryptocurrency worth over $27 billion finally break into decentralized finance after years of sitting on the sidelines?That question gained a definitive answer this week as Flare Network launched FAssets on mainnet, specifically targeting XRP holders who have long been excluded from the booming DeFi ecosystem.\
The launch of FXRP v1.2 represents more than just another token bridge. It signals a potential paradigm shift for one of cryptocurrency's oldest and most valuable assets, offering XRP holders their first native path into lending, trading, and yield farming without selling their underlying tokens.The Bridge Between Legacy and Modern CryptoFAssets operate as a sophisticated bridging system that transforms non-smart contract cryptocurrencies into DeFi-compatible tokens. Think of it as creating a digital twin of your XRP that can interact with modern blockchain applications while your original XRP remains secure on its native ledger.\
The system works through overcollateralization, where independent agents stake more value than the FAssets they help create. This creates multiple layers of security: if an agent fails to fulfill their obligations, their collateral gets liquidated to protect FXRP holders. The process resembles how traditional banks maintain reserves, but with transparent, algorithmic enforcement rather than regulatory oversight. For XRP specifically, this solves a fundamental problem. While Ethereum-based tokens can easily interact with hundreds of DeFi protocols, XRP's unique consensus mechanism and lack of smart contract functionality has kept it isolated from this $200 billion market. FAssets essentially give XRP a passport to enter DeFi without requiring any changes to the underlying XRP Ledger.\
The security approach for FAssets reflects hard-learned lessons from DeFi's history of exploits and failures. The system underwent four independent security audits from firms including Zellic and Coinspect, alongside community-driven reviews through Code4rena and bug bounty programs via Immunefi. Beyond initial auditing, Flare implemented 24/7 monitoring through Hypernative, a blockchain security firm that provides real-time threat detection. This continuous surveillance model addresses one of DeFi's persistent vulnerabilities: the gap between launch security and ongoing protection as protocols evolve and face new attack vectors.\
The overcollateralization model itself provides additional security layers. Agents must stake significantly more value than the FAssets they enable, creating strong economic incentives for proper behavior. If an agent fails to maintain adequate collateral or fulfill redemption requests, their stake faces liquidation, with proceeds used to protect FXRP holders.The minting process requires XRP holders to use specific wallets that support both Flare and XRP Ledger functionality, currently limited to Ledger hardware wallets and Bifrost. Users can then interact with platforms like AU or Oracle Daemon to create FXRP tokens.\
However, the rollout includes significant constraints. Initial minting caps at 5 million FXRP in the first week, with gradual increases thereafter. This conservative approach aims to stress-test the system with limited exposure before scaling to accommodate broader demand.\
Alternative acquisition methods include decentralized exchanges like SparkDEX, BlazeSwap, and Enosys, where users can swap existing Flare network tokens for FXRP. Specialized wallets like Luminite and Oxen Flow will integrate direct swapping functionality, potentially streamlining the user experience for less technical participants.Flare's incentive structure targets specific pools to maximize total value locked and trading activity. The Kinetic FXRP supply pool offers a 5% annual percentage rate, while FXRP/USDT liquidity pools across multiple DEXes target 50% APR through rFLR token rewards.\
These incentives follow a familiar DeFi playbook: use token emissions to bootstrap liquidity, then gradually reduce rewards as organic activity develops. The 50% APR targets for liquidity provision seem particularly aggressive, suggesting Flare recognizes the competitive nature of attracting capital in today's yield-hungry market.\
Future integrations promise expanded utility beyond simple trading. Enosys plans to accept FXRP as collateral for its stablecoin system, while upcoming yield markets will incorporate both FXRP and stXRP (liquid staked XRP through Firelight). These developments could create a self-reinforcing cycle where increased utility drives demand, which attracts more liquidity and development.Opportunity Meets Execution RiskThe addressable market for FXRP appears substantial. XRP's $27 billion market capitalization represents significant dormant capital that could theoretically flow into DeFi applications. Even capturing 1% of XRP's supply would create $270 million in total value locked, making FXRP a meaningful DeFi protocol by current standards.\
However, execution risks remain considerable. The system's complexity creates multiple failure points: agent reliability, collateral management, cross-chain communication, and smart contract vulnerabilities all pose potential threats. Previous bridge exploits, including the $320 million Wormhole hack and $190 million Nomad bridge exploit, demonstrate how seemingly secure cross-chain systems can fail catastrophically.\
The conservative launch approach, while prudent for security, may limit initial adoption momentum. DeFi users often gravitate toward protocols with deep liquidity and established track records. Starting with tight constraints could create a chicken-and-egg problem where limited liquidity discourages participation, slowing the growth needed to justify continued development investment.FAssets enters a crowded field of cross-chain solutions and wrapped tokens. Wrapped Bitcoin (WBTC) commands over $15 billion in market capitalization, while various Ethereum bridges handle billions in daily volume. These established solutions benefit from network effects, institutional adoption, and battle-tested security records.\
XRP's unique position could provide competitive advantages. Unlike Bitcoin, which faces similar DeFi integration challenges, XRP maintains active development and a community specifically interested in financial applications. The XRP Ledger's focus on payments and settlement also aligns naturally with DeFi use cases like lending and stablecoin creation.\
The integration with Flare's broader ecosystem, including its oracle services and upcoming yield markets, could differentiate FXRP from purely bridging-focused solutions. If Flare successfully creates a comprehensive DeFi ecosystem centered around XRP and related assets, it might capture value that generic bridging solutions cannot.Innovation Within Known FrameworksThe FAssets architecture combines proven concepts rather than introducing entirely novel mechanisms. Overcollateralized bridging resembles MakerDAO's collateral model, while the agent-based system draws inspiration from Bitcoin's Lightning Network and various Layer 2 scaling solutions.\
This conservative technical approach has merits and drawbacks. Using established patterns reduces implementation risk and makes security auditing more straightforward. Developers can leverage existing knowledge and tools rather than building entirely new security assumptions.\
Conversely, the lack of technical innovation may limit long-term differentiation. As bridge technology commoditizes and other projects develop similar solutions for various assets, FAssets might struggle to maintain unique value propositions beyond being first-to-market for XRP specifically.Navigating Uncertain WatersXRP's regulatory status adds complexity to any DeFi integration. While the SEC's lawsuit against Ripple has largely resolved in favor of XRP being classified as non-security for retail transactions, regulatory uncertainty persists around secondary use cases and institutional applications.\
FAssets technically create new tokens (FXRP) rather than directly facilitating XRP transactions, potentially creating different regulatory considerations. However, the one-to-one backing and redemption mechanism maintains clear connections to underlying XRP holdings, possibly preserving similar regulatory treatment.\
International regulatory developments could significantly impact adoption. European Markets in Crypto-Assets (MiCA) regulations and similar frameworks in other jurisdictions may influence how institutions and exchanges treat FXRP, particularly given XRP's significant institutional holder base.Industry Impact, A Precedent for Other Legacy AssetsSuccess with FXRP could establish templates for integrating other non-smart contract assets into DeFi. Bitcoin, Litecoin, Dogecoin, and various other valuable cryptocurrencies face similar integration challenges. Proving the security and utility of the FAssets model might accelerate broader cross-chain DeFi adoption.\
The overcollateralized agent model, if successful, could influence how other projects approach cross-chain security. Rather than relying solely on multisignature schemes or validator sets, future bridges might adopt similar economic security models that align agent incentives with user protection.\
Flare's approach also demonstrates how blockchain projects can create value by solving interoperability problems for existing assets rather than launching entirely new tokens. This model could inspire other projects to focus on integration and utility rather than purely speculative token launches.Building Toward Critical MassThe success of FAssets likely depends on achieving sufficient scale to justify ongoing development and maintenance costs. With initial constraints limiting near-term growth, Flare must balance security with the momentum needed to attract ecosystem development and user adoption. Integration with planned features like liquid staking (stXRP) and yield markets could create positive feedback loops that drive organic growth beyond initial incentive programs. If these complementary services successfully launch and gain traction, FXRP might achieve the critical mass needed for sustainable development.\
Long-term success may require expanding beyond XRP to other assets, creating a comprehensive cross-chain DeFi platform rather than a single-asset solution. However, this expansion brings additional complexity and resource requirements that could stretch development capabilities.Cautious Optimism With Execution FocusThe FAssets launch represents a technically sound approach to a real market need. XRP holders have indeed lacked DeFi access, and the overcollateralized bridge model addresses many security concerns that have plagued cross-chain protocols. The conservative rollout strategy, while limiting immediate impact, demonstrates responsible development practices.\
However, success remains far from guaranteed. The crypto landscape is littered with technically impressive projects that failed to achieve market fit or sustainable adoption. FAssets faces the classic challenge of crypto infrastructure: building something secure, usable, and valuable enough to justify ongoing investment and development.\
The true test will come in the months following launch. Can FXRP attract meaningful liquidity beyond initial incentive programs? Will the planned ecosystem integrations launch successfully and drive organic usage? Can the system handle stress testing as caps increase and real economic value flows through the protocol?\
Most importantly, FAssets must demonstrate that XRP holders are willing to pursue DeFi access to the extent that they can navigate the additional complexity and risks associated with cross-chain bridging. While the theoretical market appears substantial, converting that potential into actual user adoption requires exceptional execution across multiple dimensions simultaneously. The launch provides a foundation for XRP's DeFi integration, but foundations only matter if someone builds upon them successfully.Don’t forget to like and share the story! ]]></content:encoded></item><item><title>Instagram now has 3 billion monthly active users, will test features to help users control their feeds</title><link>https://techcrunch.com/2025/09/24/instagram-now-has-3-billion-monthly-active-users-will-test-features-to-help-users-control-their-feeds/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Wed, 24 Sep 2025 16:19:07 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As Instagram celebrates the 3 billion milestone, it also plans to roll out features that help users control what content is algorithmically recommended to them. ]]></content:encoded></item><item><title>Oracle is reportedly looking to raise $15B in corporate bond sale</title><link>https://techcrunch.com/2025/09/24/oracle-is-reportedly-looking-to-raise-15b-in-corporate-bond-sale/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Wed, 24 Sep 2025 16:09:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[This news comes just a few weeks after Oracle allegedly inked a $300 billion compute deal with OpenAI that surprised the market. ]]></content:encoded></item><item><title>How to customize your iPhone Home Screen for iOS 26’s Liquid Glass</title><link>https://techcrunch.com/2025/09/24/how-to-customize-your-iphone-home-screen-for-ios-26s-liquid-glass/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 24 Sep 2025 16:07:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Liquid Glass is used across iOS 26 elements like buttons, sliders, media controls, switches, notifications, tab bars, app sidebars, and in system experiences like the Control Center, Home Screen, Lock Screen, and more.]]></content:encoded></item><item><title>Why does OpenAI need six giant data centers?</title><link>https://arstechnica.com/ai/2025/09/why-does-openai-need-six-giant-data-centers/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/07/stargate-advances-with-partnership-with-oracle-1-1152x648.jpg" length="" type=""/><pubDate>Wed, 24 Sep 2025 16:06:03 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[On Tuesday, OpenAI, Oracle, and SoftBank announced plans for five new US AI data center sites for Stargate, their joint AI infrastructure project, bringing the platform to nearly 7 gigawatts of planned capacity and over $400 billion in investment over the next three years.The massive buildout aims to handle ChatGPT's 700 million weekly users and train future AI models, although critics question whether the investment structure can sustain itself. The companies said the expansion puts them on track to secure the full $500 billion, 10-gigawatt commitment they announced in January by the end of 2025.The five new sites will include three locations developed through an OpenAI and Oracle partnership: Shackelford County, Texas; Doña Ana County, New Mexico; and an unspecified Midwest location. These sites, along with a 600-megawatt expansion near the flagship Stargate site in Abilene, Texas, can deliver over 5.5 gigawatts of capacity, which means the computers on site will be able to draw up to 5.5 billion watts of electricity when running at full load. The companies expect the sites to create over 25,000 onsite jobs.]]></content:encoded></item><item><title>Some Private Equity Firms Doomed To Fail as High-Flying Industry Loses Its Way</title><link>https://news.slashdot.org/story/25/09/24/162214/some-private-equity-firms-doomed-to-fail-as-high-flying-industry-loses-its-way?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 16:03:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Private equity firms are facing systemic challenges after a half-century of meteoric growth as attractive takeover targets become scarce and financing costs remain elevated while exits prove increasingly difficult. US buyout funds currently hold more than 12,000 companies that would take approximately nine years to fully distribute at current rates, according to PitchBook data. 

The industry holds $1.2 trillion in dry powder and nearly a quarter of that capital was pledged at least four years ago. More than 18,000 private capital funds seek $3.3 trillion from increasingly reluctant investors, Bain estimates. Quarterly returns for US private equity funds fell from 13.5% in Q2 2021 to 0.8% in Q4 2024. Apollo President Jim Zelter described the situation as a "natural washout" at an investor conference this month. Charles Wilson of Selby Jennings added that "many PE firms are dead already, they just don't know it" and noted survival depends on how forgiving limited partners -- the entities, including pension funds and endowments, that have invested in private equity firms -- prove when firms return for new fundraising.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Heres Why AI Can’t Replace You (9/24/2025)</title><link>https://hackernoon.com/9-24-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Wed, 24 Sep 2025 16:02:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, September 24, 2025?By @thesociable [ 6 Min read ] WEF’s new blueprint, led by Larry Fink  Andre Hoffmann, aims to monetize nature with credits, swaps  asset schemes worth trillions. Read More.By @mattleads [ 26 Min read ] Now our application is containerized and ready for deployment, let’s expand its capabilities by integrating a new input source.  Read More.By @obyte [ 4 Min read ] Ever wondered why some people keep extra crypto wallets? Heres why it matters. How many do you use? Read More.By @c4twithshell [ 4 Min read ] Why AI can’t replace people: insights from real projects showing how data quality, curation, and human expertise still make the difference. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Syntactically Valid Code Editing: A Training Methodology for Neural Program Synthesis</title><link>https://hackernoon.com/syntactically-valid-code-editing-a-training-methodology-for-neural-program-synthesis?source=rss</link><author>Photosynthesis Technology: It&apos;s not just for plants!</author><category>tech</category><pubDate>Wed, 24 Sep 2025 16:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
We add two additional types of tokens: an \
We set σsmall = 2, which means the network is only allowed to produce edits with fewer than two primitives. For training data, we sample an infinite stream of random expressions from the CFG. We choose a random number of noise steps, s ∈ [1, 5], to produce a mutated expression. For some percentage of the examples, ρ, we instead sample a completely random new expression as our mutated expression. We trained for 3 days for the environments we tested on a single Nvidia A6000 GPU.(1) Shreyas Kapur, University of California, Berkeley (srkp@cs.berkeley.edu);(2) Erik Jenner, University of California, Berkeley (jenner@cs.berkeley.edu);(3) Stuart Russell, University of California, Berkeley (russell@cs.berkeley.edu).]]></content:encoded></item><item><title>Bitcoin &amp; Solana Rally, But LILPEPE Under $0.0025 Tops Analysts’ Best Crypto to Buy in September</title><link>https://hackernoon.com/bitcoin-and-solana-rally-but-lilpepe-under-$00025-tops-analysts-best-crypto-to-buy-in-september?source=rss</link><author>Kashvi Pandey</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:45:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Bitcoin and Solana are pushing toward record highs, but analysts say the real asymmetric opportunity lies in Little Pepe (LILPEPE). Priced under $0.0025, LILPEPE has raised over $25M, passed a CertiK audit, listed on CoinMarketCap, and offers a meme-focused Layer-2 ecosystem. With projections of $0.50–$3, it’s being called the best crypto to buy in September 2025.]]></content:encoded></item><item><title>Inverting the Observation Model: How to Generate Code from Any Output</title><link>https://hackernoon.com/inverting-the-observation-model-how-to-generate-code-from-any-output?source=rss</link><author>Photosynthesis Technology: It&apos;s not just for plants!</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
3.2.2 Reverse Mutation Paths\
Since we have access to the ground-truth mutations, we can generate targets to train a neural network by simply reversing the sampled trajectory through the forward process Markov-Chain, z0 → z1 → . . .. At first glance, this may seem a reasonable choice. However, training to simply invert the last mutation can potentially create a much noisier signal for the neural network.\
Consider the case where, within a much larger syntax tree, a color was mutated as,(1) Shreyas Kapur, University of California, Berkeley (srkp@cs.berkeley.edu);(2) Erik Jenner, University of California, Berkeley (jenner@cs.berkeley.edu);(3) Stuart Russell, University of California, Berkeley (russell@cs.berkeley.edu).]]></content:encoded></item><item><title>3 Billion Users Now Use Instagram Monthly</title><link>https://tech.slashdot.org/story/25/09/24/1527241/3-billion-users-now-use-instagram-monthly?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:27:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[CNBC: Instagram now has 3 billion monthly active users, Meta CEO Mark Zuckerberg said Wednesday on his Instagram account. "What an incredible community we've built here," Zuckerberg posted on his Instagram channel. 

The figure is a major milestone for the photo-sharing app, which the social media company acquired in 2012 for $1 billion. Meta last disclosed Instagram's user figures in October 2022 when Zuckerberg said during an earnings call that the app had crossed 2 billion monthly users.]]></content:encoded></item><item><title>How AI Models Are Evaluated for Language Understanding</title><link>https://hackernoon.com/how-ai-models-are-evaluated-for-language-understanding?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:00:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A.1 English language screenerOur screening criteria for human participants were English as a first language and English as the most commonly used language. We did not use the concept or term ‘native speaker’ because it can be exclusionary and tends to conflate the true factor of interest (linguistic proficiency) with other irrelevant factors like socio-cultural identity, age and order or context of acquisition [Cheng et al., 2021]. We wanted participants for whom English was a first language, defined as the language, or one of the languages, that they first learnt as a child. This is because first languages are known to shape one’s understanding of grammar and we wanted to minimise the chance that the grammatical complexity of our statements was a confounding factor in performance. We also wanted English to be the language participants use on a day to day basis, to screen out those who learnt English as a first language but now primarily use another language and may therefore be less fluent in English.We ran a pilot study through Qualtrics to validate the procedure and detect ambiguities, errors, and irregularities in the stimuli based on participant performance and explanations. We ran the unmoderated survey on Qualtrics with 1440 participants, which equates to 10 responses per statement. The median response time for the first 50 participants was one minute, suggesting that they were rushing, so we disabled the ‘Next’ button on the survey for 60 seconds for the remaining 1390 participants to ensure they had time to read the story twice. We retained this timer for the final survey. We analysed participant performance on ToM and factual statements on a story by story basis and identified performance outliers. In total we observed 17 statements on which people performed relatively poorly. We re-examined the statements and used participants’ open-ended responses to identify ambiguities in either the story or the statement that could be responsible for the low performance. We found ambiguity in 15 out of 17 cases, and resolved it by making changes to the wording of 14 statements and 1 story. The remaining two cases of poor performance were a order 4 statement and a order 2 statement, for which open-ended responses suggested that participants had not paid attention. After reviewing both statements we did not make any changes.A.3 LLM prompt conditionsTable 3 presents the exact text that LLMs received in each of the 4 conditions we tested.A.4 Details of the LLMs testedLaMDA stands for Language models for Dialog Applications, a family of Transformer-based neural models developed by Google, specialised for dialog in English [Thoppilan et al., 2022]. LaMDA\
is pre-trained on 1.56T words of public data and web text including 1.12B dialogs from public forum (50% of the dataset), Colossal Clean Crawled Corpus data (12.5%), code documents (12.5%), Wikipedia English articles (12.5%) and a smaller proportion of non-English documents. It is optimised for safety and factual grounding. This study uses a version of LaMDA with 35B parameters without fine tuning.\
PaLM, which stands for Pathways Language Models, is a larger family of models developed by Google. It relies on the Pathways architecture that enables training of a single model across thousands of accelerator chips more efficiently than LaMDA. We use a version of PaLM with 540B parameters trained with smaller corpus of 780B words from a mixture of social media conversations (50%), filtered webpages (27%), books in English (13%), Code, Wikipedia, and News articles used to train both LaMDA and GLaM [Chowdhery et al., 2023]. We decided to evaluate PaLM’s capabilities as it has been shown to perform better than LaMDA and other large models on Winograd-style tasks, in-context comprehension tasks, common-sense reasoning tasks and natural language inference tasks [Chowdhery et al., 2023].\
Flan-PaLM is a version of PaLM 540B fine tuned on a collection of over 1.8K natural language tasks phrased in a natural language instruction format including the type of instructions used with human subjects detailed above [Chung et al., 2024]. Fine tuning language models on datasets phrased as instructions has been shown to improve performance when provided with instructions, enabling the model to better understand tasks and reducing the need for few-shot exemplars [Ouyang et al., 2022, Sanh et al., 2021].\
GPT 3.5 Turbo was developed by OpenAI and released in March 2022. GPT 3.5 Turbo is trained on a large database of text and code the majority of which comes from Common Crawl, WebText2, two\
internet-based book collections called ‘Books1’ and ‘Books2’, and from Wikipedia [Brown et al., 2020]. The parameter size of GPT 3.5 Turbo is undisclosed by OpenAI. This study uses the ‘GPT 3.5 Turbo Instruct’ model, which has training data up to September 2021 and a context window of 4096 tokens and is fine-tuned for following instructions [Ouyang et al., 2022].\
GPT-4 was developed by OpenAI and released in March of 2023 [Achiam et al., 2023]. GPT-4 is multimodal: it was pretrained on both image and text data, can take images and text as input, and can output text. As with GPT-3.5, the size of the model has not been made public, but estimates place it at approximately 1.7T parameters [McGuiness, 2023]. GPT-4 was pre-trained on thirdparty and public data, then underwent RLHF [Achiam et al., 2023]. OpenAI reported significant performance improvements between GPT-3.5 and GPT-4 on a range of professional and academic human benchmarks, factuality and safety tasks, in particular based upon the addition of RLHF.The experimental design needed to be adapted slightly according to the differences between the APIs. When testing the LaMDA, PaLM and Flan-PaLM, the scoring APIs allowed us to send a list of tokens in natural language (maximum four per set) and receive the logprobs for those tokens only, as a subset of the entire vector of logprobs produced for all tokens. We did not need to set any additional parameters in order to retrieve the logprobs.\
In order to retrieve log probabilities for our candidates from GPT-3.5 and GPT-4 models, we had to first tokenise the candidates using the OpenAI tokenizer, and then send those tokens within the ‘logit bias’ parameter in order to ensure those tokens were in the response. The logit bias has a range of -100 to 100. Applying a negative logit bias to a token forces the LLM to downweight it while applying a positive logit bias to a token forces the LLM to upweight it. As a result, applying a logit bias of 100 to a candidate effectively ensures that it will appear in the output, so we applied a bias of 100 to all of our candidates. We also set the ‘max tokens’ parameter to 1 in order to restrict the GPT-3.5 and GPT-4 outputs to the length of the single tokens we had selected.\
The methodological differences between the Google and OpenAI models were inescapable given that LLM API development still lacks standardised formats or conventions. However, given that our metric is the relative probability of semantically equivalent tokens for ‘true’ vs semantically equivalent tokens for ‘false’, we do not believe these differences prohibit fair comparison between the performance of the models.A.6.1 Story and prompt conditions\
According to an independent samples test of proportions, the LLM prompt conditions had no significant effect on the proportion of ToM or factual statements answered correctly by any of the LLMs. LaMDA’s performance on ToM statements in the human prompt condition (M = 50%) was not significantly different from the simplified prompt condition (M = 50%), N = 280, Z = .000, p = 1.000, nor was its performance on factual statements in the human prompt condition (M = 50%) different from its performance in the simplified prompt condition (M = 50%), N = 280, Z = .000, p = 1.000. PaLM’s performance on ToM statements in the human prompt condition (M = 58.6%) was not significantly different from the simplified prompt condition (M = 60%), N = 280, Z = −.243, p = .808, nor was its performance on factual statements in the human prompt condition (M = 57.9%) different from its performance in the simplified prompt condition (M = 61.4%), N = 280, Z = −.609, p = .542. Flan-PaLM’s performance on ToM statements in the human prompt condition (M = 85%) was not significantly different from the simplified prompt condition (M = 83.6%), N = 280, Z = −.328, p = .743, nor was its performance on factual statements in the human prompt condition (M = 94.3%) different from its performance in the simplified prompt condition (M = 92.9%), N = 280, Z = −.487, p = .626. GPT-3.5’s performance on ToM statements in the human prompt condition (M = 53.6%) was not significantly different from the simplified prompt condition (M = 51.4%), N = 280, Z = .359, p = .720, nor was its performance on factual statements in the human prompt condition (M = 62.1%) different from its performance in the simplified prompt condition (M = 63.6%), N = 280, Z = −.247, p = .805. AAnd finally, GPT-4’s performance on ToM statements in the human prompt condition (M = 87.9%) was not significantly different from the simplified prompt condition (M = 89.3%), N = 280, Z = −.376, p = .707, nor was its performance on factual statements in the human prompt condition (M = 94.3%) different from its performance in the simplified prompt condition (M = 94.3%), N = 280, Z = .000, p = 1.000. According to an independent samples test of proportions the story condition had no effect on the proportion of ToM statements answered correctly by humans (‘no story’ condition (M = 88.6%), ‘with story’ condition (M = 92.1%), N = 280, Z = −1.012, p = .311) or factual statements answered correctly (‘no story’ condition (M = 95.7%), ‘with story’ condition (M = 99.3%), N = 280, Z = −1.914, p = .056).(1) Winnie Street, Google Research;(2) John Oliver Siy, Google Research;(3) Geoff Keeling, Google Research;(4) Adrien Baranes, Google DeepMind;(5) Benjamin Barnett, Google Research;(6) Michael Mckibben, Applied Physics Lab, Johns Hopkins University;(7) Tatenda Kanyere, Work done at Google Research via Harvey Nash;(8) Alison Lentz, Google Research;(9) Blaise Aguera y Arcas, Google Research;(10) Robin I. M. Dunbar, Department of Experimental Psychology, University of Oxford istreet@google.com.]]></content:encoded></item><item><title>GPT-4 vs GPT-3.5 Performance in Game Simulations</title><link>https://hackernoon.com/gpt-4-vs-gpt-35-performance-in-game-simulations?source=rss</link><author>Writings, Papers and Blogs on Text Models</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:00:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The prompts introduced in this section includes game rules that can either be human written rules or LLM generated rules. For experiments without game rules, we simply remove the rules from the corresponding prompts.\
D.1.2 State Difference PredictionD.2.1 Full State Prediction\
D.2.2 State Difference PredictionD.3 Prompt Example: FR (Game Progress)D.4.1 Full State Prediction\
D.4.2 State Difference PredictionBelow is an example of the rule of an action:\
Below is an example of the rule of an object:\
Below is an example of the score rule:\
Below is an example of a game state:\
Below is an example of a JSON that describes the difference of two game states:Table 5 and Table 6 shows the performance of a GPT-3.5 simulator predicting objects properties and game progress respectively. There is a huge gap between the GPT-4 performance and GPT-3.5 performance, providing yet another example of how fast LLM develops in the two years. It is also worth notices that the performance difference is larger when no rules is provided, indicating that GPT-3.5 is especially weak at applying common sense knowledge to this few-shot world simulation task.1. In Figure 3, we show detailed experimental results on the full state prediction task performed by \
\
2. In Figure 4, we show detailed experimental results on the state difference prediction task performed by GPT-4.\
3. In Figure 5, we show detailed experimental results on the full state prediction task performed by \
4. In Figure 6, we show detailed experimental results on the  performed by \
\
Figure 3: GPT-4 - Full State prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.\
\
Figure 4: GPT-4 - Difference prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.\
\
Figure 5: GPT-3.5 - Full State prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.\
\
Figure 6: GPT-3.5 - Difference prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.]]></content:encoded></item><item><title>Waymo is getting into the corporate travel business</title><link>https://techcrunch.com/2025/09/24/waymo-is-getting-into-the-corporate-travel-business/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:00:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[One of Waymo's first enterprise customers is Carvana.]]></content:encoded></item><item><title>From Images to Programs: A Denoising Diffusion Method for Inverse Graphics</title><link>https://hackernoon.com/from-images-to-programs-a-denoising-diffusion-method-for-inverse-graphics?source=rss</link><author>Photosynthesis Technology: It&apos;s not just for plants!</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The main idea behind our method is to develop a form of denoising diffusion models analogous to image diffusion models for syntax trees.\
Consider the example task from Ellis et al. [11] of generating a constructive solid geometry (CSG2D) program from an image. In CSG2D, we can combine simple primitives like circles and quadrilaterals using boolean operations like addition and subtraction to create more complex shapes, with the context-free grammar (CFG),\
In the following sections, we will first describe how “noise” is added to syntax trees. Then, we will detail how we train a neural network to reverse this noise. Finally, we will describe how we use this neural network for search.(1) Shreyas Kapur, University of California, Berkeley (srkp@cs.berkeley.edu);(2) Erik Jenner, University of California, Berkeley (jenner@cs.berkeley.edu);(3) Stuart Russell, University of California, Berkeley (russell@cs.berkeley.edu).]]></content:encoded></item><item><title>Google makes real-world data more accessible to AI — and training pipelines will love it</title><link>https://techcrunch.com/2025/09/24/google-makes-real-world-data-more-accessible-to-ai-and-training-pipelines-will-love-it/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google’s Data Commons gets an MCP Server to help AI systems access massive amounts of real-world data.]]></content:encoded></item><item><title>From Digg to deals: Kevin Rose on reinvention and investing at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/09/24/from-digg-to-deals-kevin-rose-on-reinvention-and-investing-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 24 Sep 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Kevin Rose, Digg founder and True Ventures partner, returns to the Disrupt Stage at TechCrunch Disrupt 2025 to talk pivots, reinvention, and spotting the next wave of breakout startups. ]]></content:encoded></item><item><title>YouTube will reinstate accounts banned for spreading misinformation</title><link>https://techcrunch.com/2025/09/24/youtube-will-reinstate-accounts-banned-for-spreading-misinformation/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Wed, 24 Sep 2025 14:51:40 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA["YouTube values conservative voices on its platform and recognizes that these creators have extensive reach and play an important role in civic discourse," the company's legal counsel wrote.]]></content:encoded></item><item><title>Movie Studio Lionsgate is Struggling To Make AI-Generated Films With Runway</title><link>https://entertainment.slashdot.org/story/25/09/24/1442221/movie-studio-lionsgate-is-struggling-to-make-ai-generated-films-with-runway?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 14:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Last year, the AI video company Runway joined forces with the major Hollywood studio Lionsgate in a partnership the pair hoped would result in AI-generated scenes and even potentially full-length movies. But the project has hit a snag. According to a report by The Wrap, the past 12 months have been unproductive. Lionsgate distributes Hollywood blockbusters including The Hunger Games, John Wick, The Twilight Saga, and Saw franchises. But despite its huge catalog, it is simply not enough for the AI to produce quality content. 

"The Lionsgate catalog is too small to create a model," a source tells The Wrap. "In fact, the Disney catalog is too small to create a model." Despite Runway being one of the leading names in AI video, the technology needs a copious amount of data to produce AI-generated films. It is the reason AI has proven to be such an unpopular technology, as AI firms help themselves to any type of media they can get their hands on -- whether it has copyright protections or not. Another issue is the rights of actors and the model for remuneration if their likeness appears in an AI-generated clip. It is a legal gray area with no clear path.]]></content:encoded></item><item><title>Step into the future: The full AI Stage agenda at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/09/24/step-into-the-future-the-full-ai-stage-agenda-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 24 Sep 2025 14:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The AI Stage at TechCrunch Disrupt 2025 is officially locked and loaded, featuring the powerhouses shaping the future of artificial intelligence. ]]></content:encoded></item><item><title>The Rise of Federated Systems in Cloud-Native Architectures</title><link>https://hackernoon.com/the-rise-of-federated-systems-in-cloud-native-architectures?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Wed, 24 Sep 2025 14:29:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Federated systems are transforming cloud-native architecture by enabling data locality, privacy, and compliance without sacrificing scalability. From federated learning in healthcare and finance to self-sovereign identity pilots in the EU, federation is becoming a cornerstone of enterprise infrastructure. The future blends blockchain, quantum security, and standardization.]]></content:encoded></item><item><title>Google launches an AI-powered mood board app, Mixboard</title><link>https://techcrunch.com/2025/09/24/google-launches-an-ai-powered-mood-board-app-mixboard/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 24 Sep 2025 14:05:19 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google's Nano Banana AI model can now be used to help you brainstorm ideas and make collages with Mixboard. ]]></content:encoded></item><item><title>Microsoft Will Let Copilot Take Control of Your Browser, Navigate Tabs and Complete Tasks As You Watch</title><link>https://tech.slashdot.org/story/25/09/24/141223/microsoft-will-let-copilot-take-control-of-your-browser-navigate-tabs-and-complete-tasks-as-you-watch?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 24 Sep 2025 14:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft AI CEO Mustafa Suleyman told The Verge today that the company plans to transform Edge into an "agentic browser" where Copilot controls tabs, navigates websites and completes tasks while users watch. Unlike The Browser Company's new Dia browser, Microsoft will integrate these capabilities directly into Edge. 

Suleyman described Copilot opening tabs, reading multiple pages simultaneously and performing research transparently in real-time. The AI visits websites directly, preserving publisher traffic. Current Copilot features include tab navigation, page scrolling and content highlighting. Users will have the option to disable AI features entirely. Suleyman predicted that within years, AI companions will handle most browsing tasks while users provide oversight and feedback.]]></content:encoded></item><item><title>3 days left: Don’t miss your last chance to lock $668 of savings for TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/09/24/3-days-left-dont-miss-your-last-chance-to-lock-in-regular-bird-pricing-for-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 24 Sep 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[There’s no event quite like TechCrunch Disrupt, and if you’ve never been, this is the year to change that. You have just 3 days left to save up to $668 on your pass. ]]></content:encoded></item><item><title>Emergent raises $23M from Lightspeed to let consumers build apps</title><link>https://techcrunch.com/2025/09/24/emergent-raises-23m-from-lightspeed-to-let-consumers-build-apps/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Wed, 24 Sep 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Emergent raises $23M for its vibe-coding software platform. ]]></content:encoded></item><item><title>Spotify now integrates directly with DJ software from rekordbox, Serato, and djay</title><link>https://techcrunch.com/2025/09/24/spotify-now-integrates-directly-with-dj-software-from-rekordbox-serato-and-djay/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:56:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Users will be able to access their entire libraries and playlists directly within these programs, making it faster and easier to create sets from playlists and blend tracks. ]]></content:encoded></item><item><title>Hash Hedge and Walbi launch The World Series of Crypto Trading: The First Global Trading Series</title><link>https://hackernoon.com/hash-hedge-and-walbi-launch-the-world-series-of-crypto-trading-the-first-global-trading-series?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:38:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[September 24, 2025 — , an international proprietary trading firm providing traders with capital to manage, together with AI trading platform , announced the launch of the  — the first ever global series for traders, set to culminate in a live offline final in Dubai.A New Global Stage for TradersThe WSCT is designed as a true world-class competition:all participants start under equal conditions,by October 20, 8 finalists will be determined,all finalists will receive an all-expenses-paid trip to Dubai, including flights, accommodation, and transfers.The grand final will take place October 28–29 in Dubai, during Blockchain Life 2025, one of the largest blockchain conferences in the world, where thousands of attendees will witness trading battles live.Human vs AI: A World FirstFor the first time in history, human traders will compete not only against each other but also against artificial intelligence.Tournament technology partner  will introduce three AI-driven strategies trading in real time — making WSCT the first-ever event where algorithms face off against traders in front of a live audience. is already being called the “ChatGPT of trading”, and its debut in WSCT will showcase how AI and humans compare when put under the same market conditions.The champion will receive the WSCT bracelet — a symbol of greatness and legacy — along with a cash prize and a Hash Hedge funded account to manage, equal to the combined profits of all finalists.The World Series of Crypto Trading is more than a competition — it’s the birth of a new tradition.Just as poker tournaments once created global stars and household names, WSCT is set to crown the first legends of the rapidly growing world of crypto trading. is a global proprietary trading company that gives traders institutional-level capital to manage.  Hash Hedge helps professional and aspiring traders reach their full potential by pushing the limits of trading technology. The company focuses on innovation, transparency and performance.The World Series of Crypto Trading is more than a competition — it’s the birth of a new tradition.Just as poker tournaments once created global stars and household names, WSCT is set to crown the first legends of the rapidly growing world of crypto trading. is a global proprietary trading company that gives traders institutional-level capital to manage.  Hash Hedge helps professional and aspiring traders reach their full potential by pushing the limits of trading technology. The company focuses on innovation, transparency and performance is a next-generation AI trading platform, often described as “the ChatGPT of trading.” By developing cutting-edge strategies powered by artificial intelligence, Walbi is redefining how individuals and institutions interact with financial markets.Learn more and register: https://wsct.com/ is a next-generation AI trading platform, often described as “the ChatGPT of trading.” By developing cutting-edge strategies powered by artificial intelligence, Walbi is redefining how individuals and institutions interact with financial markets.Learn more and register: https://wsct.com/:::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>A Practical Guide to G-LSM: Improving High-Dimensional Option Pricing with Minimal Overhead</title><link>https://hackernoon.com/a-practical-guide-to-g-lsm-improving-high-dimensional-option-pricing-with-minimal-overhead?source=rss</link><author>Economic Hedging Technology</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:15:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
In view of (4.1), the drift term vanishes. After taking the stochastic integral and using the terminal condition in (4.1), we obtain the desired assertion.\
In practice, the continuous least squares problem (4.3) is solved by minimizing its Monte Carlo approximation:\
Then finding the optimal polynomial in (4.6) amounts to solving the classical least squares problem\
The proposed algorithm is summarized in Algorithm 4.1.\
 Now we analyze the convergence of Algorithm 4.1. We assume the Lipschitz continuity of the discounted payoff function gk(·).\
\
Next, we provide an error estimate of solving the continuous least squares problem (4.3) in terms of the error of the previous value function, the best approximation error in the sparse Hermite polynomial ansatz space (5.5) and the time step size ∆t. The proof is inspired by the foundational work [12].\
\
5.2. Global error estimation. Finally, we prove a global error estimate.\
\
We consider the example of Bermudan geometric basket put from [13, 25]. The exact prices are computed by solving the reduced one-dimensional problem via a quadrature and interpolation-based method [25] for Bermudan options. We present in Table 2 the computed option prices and their relative errors using G-LSM and LSM, with the same ansatz space for the CVF. The results show that G-LSM achieves higher accuracy than LSM in high-dimensions: G-LSM has a relative error 0.55% for d = 15, which is almost ten times smaller than that by LSM. That is, by incorporating the gradient information, the accuracy of LSM can be substantially improved.\
\
Table 3 gives the computed option prices and their relative errors by G-LSM with different maximum polynomial orders p for the 20-dimensional geometric basket put. The relative error decays steadily as the order p of Hermite polynomials increases, which agrees with Theorem 5.6.\
\
6.2. Example 2: American geometric basket call. Now we consider the example of American geometric basket call option from [21, 6] to demonstrate that the proposed G-LSM can achieve the same level of accuracy as the DNN-based method [6], and take M = 720, 000 samples as in [6]. The prices and deltas are given in Table 4 and Table 5, respectively, where the results of [6] are the average of 9 independent runs. From Table 4, both methods have similar accuracy for the price. From Table 5, the relative error of delta using G-LSM and DNN varies slightly with the dimension d. This is probably because the DNN-based method computes the delta via a sample average, while G-LSM uses the derivatives of the value function directly. The relative error of the delta computed by G-LSM increases slightly for larger dimensions, possibly due to the small magnitude of the exact delta values.\
\
Next, Figure 2 shows the classification results of continued and exercised data using GLSM and LSM with the number of simulated paths M = 100, 000 in d = 7 or 20. Compared with the exact exercise boundary, G-LSM achieves better accuracy in determining the exercise boundary than LSM despite of using the same ansatz and number of paths. Thus, even with the right ansatz space, LSM might fail the task of finding exercise boundary in high dimensions using only a limited number of samples. Compared with [6, Figure 5] and [18, Figure 6], Figure 2 demonstrates that G-LSM can detect accurate exercise boundary with fewer number of paths than the DNN-based method.\
6.3. Example 3: Bermudan max-call with symmetric assets. To benchmark G-LSM on high-dimensional problems without exact solutions and to validate the complexity analysis in section 4, we test Bermudan max-call option and report the computing time. The computing time is calculated as follows. For a fixed time step, Tbas is the time for generating basis matrix Φ, Tmat is the time for assembling matrix A, Tlin is the time for solving linear system, and Tup is the time for updating values. The overall computing time is Ttot ≈ (N − 1)(Tbas + Tmat + Tlin + Tup).\
Table 6 presents the prices and computing time (in seconds) for Bermudan max-call options with d symmetric assets. The reference 95% confidence interval (CI) is taken from [3]. The reference CI is computed with more than 3000 training steps and a batch of 8192 paths in each step, which in total utilizes more than 107 paths. The last five columns of the table report the computing time for the step 6, 7, 8, 9 in Algorithm 4.1, and the total time, respectively. All the computation for this example was performed on an Intel Core i9-10900 CPU 2.8 GHz desktop with 64GB DDR4 memory using MATLAB R2023b. It is observed that the prices computed by G-LSM fall into or stay very close to the reference 95% CI, confirming the high accuracy of G-LSM. Furthermore, the time for generating basis matrix, Tbas, dominates the\
\
overall computing time. Hence, the cost mainly arises from evaluating Hermite polynomials on sampling paths, which is also required by LSM. In comparison with LSM , Tmat is the extra cost to incorporate the gradient information and takes only a small fraction of the total time. Therefore, G-LSM has nearly identical cost with LSM.\
\
Figure 4 shows the classification of continued and exercised sample points computed by G-LSM and LSM in the example of two-dimensional max-call. G-LSM yields a smoother exercise boundary than LSM. Compared with the exercise boundary computed in literature [19, Figure 3], G-LSM exhibits higher accuracy, albeit that the same ansatz space for the CVF is employed.\
7. Conclusions and outlook. In this work, we have proposed a novel gradient-enhanced least squares Monte Carlo (G-LSM) method that employs sparse Hermite polynomials as the ansatz space to price and hedge American options. The method enjoys low complexity for the gradient evaluation, ease of implementation and high accuracy for high-dimensional problems. We analyzed rigorously the convergence of G-LSM based on the BSDE technique, stochastic and Malliavin calculus. Extensive benchmark tests clearly show that it outperforms least squares Monte Carlo (LSM) in high dimensions with almost the same cost and it can also achieve competitive accuracy relative to the deep neural networks-based methods.\
\
. The authors acknowledge the support of research computing facilities offered by Information Technology Services, the University of Hong Kong.[1] B. Adcock, S. Brugiapaglia, and C. G. Webster, Sparse Polynomial Approximation of HighDimensional Functions, SIAM, Philadelphia, PA, 2022.\
[2] C. Bayer, M. Eigel, L. Sallandt, and P. Trunschke, Pricing high-dimensional Bermudan options with hierarchical tensor formats, SIAM Journal on Financial Mathematics, 14 (2023), pp. 383–406.\
[3] S. Becker, P. Cheridito, and A. Jentzen, Deep optimal stopping, The Journal of Machine Learning Research, 20 (2019), pp. 2712–2736.\
[4] S. Becker, P. Cheridito, and A. Jentzen, Pricing and hedging American-style options with deep learning, Journal of Risk and Financial Management, 13 (2020), p. 158.\
[5] B. Bouchard and X. Warin, Monte-Carlo valuation of American options: facts and new algorithms to improve existing methods, in Numerical Methods in Finance: Bordeaux, June 2010, Springer, Berlin, 2012, pp. 215–255.\
[6] Y. Chen and J. W. Wan, Deep neural network framework based on backward stochastic differential equations for pricing and hedging American options in high dimensions, Quantitative Finance, 21 (2021), pp. 45–67.\
[7] W. E, J. Han, and A. Jentzen, Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations, Communications in Mathematics and Statistics, 5 (2017), pp. 349–380.\
[8] N. El Karoui, C. Kapoudjian, E. Pardoux, S. Peng, and M.-C. Quenez, Reflected solutions of backward SDE’s, and related obstacle problems for PDE’s, The Annals of Probability, 25 (1997), pp. 702–737.\
[9] F. Fang and C. W. Oosterlee, A Fourier-based valuation method for Bermudan and barrier options under Heston’s model, SIAM Journal on Financial Mathematics, 2 (2011), pp. 439–463.\
[10] C. Gao, S. Gao, R. Hu, and Z. Zhu, Convergence of the backward deep bsde method with applications to optimal stopping problems, SIAM Journal on Financial Mathematics, 14 (2023), pp. 1290–1303.\
[11] E. Gobet and C. Labart, Error expansion for the discretization of backward stochastic differential equations, Stochastic Processes and their Applications, 117 (2007), pp. 803–829.\
[12] C. Hur´e, H. Pham, and X. Warin, Deep backward schemes for high-dimensional nonlinear PDEs, Mathematics of Computation, 89 (2020), pp. 1547–1579.\
[13] P. Kovalov, V. Linetsky, and M. Marcozzi, Pricing multi-asset American options: A finite element method-of-lines with smooth penalty, Journal of Scientific Computing, 33 (2007), pp. 209–237.\
[14] B. Lapeyre and J. Lelong, Neural network regression for Bermudan option pricing, Monte Carlo Methods and Applications, 27 (2021), pp. 227–247.\
[15] F. Longstaff and E. Schwartz, Valuing American options by simulation: a simple least-squares approach, The Review of Financial Studies, 14 (2001), pp. 113–147.\
[16] M. Ludkovski, Kriging metamodels and experimental design for Bermudan option pricing, Journal of Computational Finance, 22 (2018), pp. 37–77.\
[17] X. Luo, Error analysis of the Wiener–Askey polynomial chaos with hyperbolic cross approximation and its application to differential equations with random input, Journal of Computational and Applied Mathematics, 335 (2018), pp. 242–269.\
[18] A. S. Na and J. W. L. Wan, Efficient pricing and hedging of high-dimensional American options using deep recurrent networks, Quantitative Finance, 23 (2023), pp. 631–651.\
[19] A. M. Reppen, H. M. Soner, and V. Tissot-Daguette, Deep stochastic optimization in finance, Digital Finance, 5 (2023), pp. 91–111.\
[20] R. Seydel and R. Seydel, Tools for computational finance, vol. 3, Springer, 2006.\
[21] J. Sirignano and K. Spiliopoulos, DGM: A deep learning algorithm for solving partial differential equations, Journal of Computational Physics, 375 (2018), pp. 1339–1364.\
[22] J. Tsitsiklis and B. Van Roy, Regression methods for pricing complex American-style options, IEEE Transactions on Neural Networks, 12 (2001), pp. 694–703.\
[23] H. Wang, H. Chen, A. Sudjianto, R. Liu, and Q. Shen, Deep learning-based BSDE solver for LIBOR market model with application to Bermudan swaption pricing and hedging, arXiv preprint arXiv:1807.06622, (2018).\
[24] Y. Wang and R. Caflisch, Pricing and hedging American-style options: a simple simulation-based approach, The Journal of Computational Finance, 13 (2009), pp. 95–125.\
[25] J. Yang and G. Li, On sparse grid interpolation for American option pricing with multiple underlying assets, arXiv preprint arXiv:2309.08287, (2023).\
[26] J. Zhang, Backward stochastic differential equations, Springer, 2017.(1) Jiefei Yang, †Department of Mathematics, University of Hong Kong, Pokfulam, Hong Kong (jiefeiy@connect.hku.hk);(2) Guanglian Li, Department of Mathematics, University of Hong Kong, Pokfulam, Hong Kong (lotusli@maths.hku.hk).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>UK arrests man linked to ransomware attack that caused airport disruptions across Europe</title><link>https://techcrunch.com/2025/09/24/uk-police-arrest-man-linked-to-ransomware-attack-that-caused-airport-disruptions-in-europe/</link><author>Lorenzo Franceschi-Bicchierai, Zack Whittaker</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:12:38 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[RTX, the parent company of Collins Aerospace, confirmed in a legally required notice that the disruption was ransomware-related.]]></content:encoded></item><item><title>Alibaba to offer Nvidia’s physical AI development tools in its AI platform</title><link>https://techcrunch.com/2025/09/24/alibaba-to-offer-nvidias-physical-ai-development-tools-in-its-ai-platform/</link><author>Ram Iyer</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:12:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Alibaba said on Wednesday that it is integrating Nvidia's AI development tools for robotics, self-driving cars, and connected spaces into its Cloud Platform for AI. ]]></content:encoded></item><item><title>Mutuum Finance (MUTM): Lending and Borrowing</title><link>https://hackernoon.com/metaplanet-adds-5400-btc-yet-retail-investors-find-mutm-the-best-cheap-crypto-to-invest-today?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:10:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[While institutional players like Metaplanet continue to accumulate large amounts of Bitcoin (BTC), adding 5,400 BTC to their holdings, retail investors are increasingly seeking accessible alternatives with real utility.With Bitcoin (BTC) reaching price levels that remain out of reach for many,  stands out as the most compelling cheap crypto to invest today.Traders studying crypto charts are looking for coins that offer structured upside, security, and tangible use cases, making MUTM a front-runner during a time when market participants are wary of the next crypto crash.Metaplanet, the Tokyo-listed investment firm dubbed "Asia's MicroStrategy," acquired 5,419 BTC for approximately $632.53 million at an average price of $116,724 per Bitcoin on September 22, 2025, boosting its total holdings to 25,555 BTC valued at $2.71 billion.This marks the company's largest single purchase to date, funded through a $1.4 billion share sale earlier in the month, and propels Metaplanet into the fifth spot among public corporate Bitcoin holders, surpassing Bullish and trailing only MicroStrategy, Marathon Digital, Twenty One, and Bitcoin Standard Treasury.The aggressive accumulation aligns with Metaplanet's "555 Million Plan," targeting 210,000 BTC by 2027 via a $5.4 billion capital raise. Bitcoin (BTC) traded at ~$113,000, with a 24-hour volume of $45 billion, up modestly on the news.Technical indicators show BTC testing $112,000 support, RSI at 52, and resistance at $116,713. Social media buzz highlights institutional adoption, though U.S. tariffs pose risks. Analysts project $121,500 if resistance clears, but a drop below $112,000 risks $105,000.Mutuum Finance (MUTM): Lending and Borrowing, Structured Returns for Retail InvestorsMutuum Finance (MUTM) will provide a robust P2C lending framework, allowing users to earn passive income with bluechip and stablecoin deposits.For example, an investor depositing $12,000 in XRP will receive mtXRP at a 1:1 ratio, generating an estimated APY of 13%, which translates to $1,560 in annual passive income. Borrowers will also benefit from flexible access to liquidity.Posting $1,500 worth of BTC as collateral, users will be able to borrow up to 70% in stablecoins while retaining exposure to Bitcoin (BTC)’s potential price appreciation.This structured system will allow retail investors to enjoy DeFi functionality traditionally reserved for institutional participants, bridging the gap between high-value BTC accumulation and smaller-scale investments.For higher-risk opportunities, MUTM will also offer P2P lending for meme coins such as DOGE and PEPE. These tokens will remain isolated from the primary P2C pools to protect core liquidity, while lenders negotiate duration and interest rates directly, earning higher rewards in exchange for elevated risk. This layered approach will allow the platform to accommodate both conservative and adventurous investors without jeopardizing overall stability.Collateral and liquidation management will ensure that the protocol remains secure. BTC-backed loans will be overcollateralized with a 75% LTV and an 80% liquidation threshold. The Stability Factor mechanism will actively monitor the health of the lending pools, ensuring that liquidations occur efficiently while protecting lenders’ capital.Additionally, liquidity depth across the platform will enable prompt execution of liquidations, while more volatile tokens will have lower LTV caps between 35% and 47% and liquidation thresholds around 65%, with reserve factors adjusted according to risk. These features will create a resilient system capable of withstanding price swings while maintaining strong on-chain liquidity.Presale Momentum and Long-Term GrowthPhase 6 of the Mutuum Finance (MUTM) presale will feature tokens priced at $0.035, with $16.2 million projected to be raised and 47% of the 170 million token allocation expected to sell out rapidly.The community is projected to exceed 16,550 holders, supported by over 12,000 active Twitter followers. CertiK audits, with TokenScan and Skynet scores of 90 and 79 respectively, will reassure investors of the platform’s security.Investment examples will highlight the potential upside for retail participants. An investor who contributed $10,000 in Phase 1 will secure 1,000,000 tokens, which will be projected to reach $60,000 at the initial listing price of $0.06 and grow to $1,000,000 in value as the platform gains adoption post-listing.The upcoming beta launch will allow users to experience key features firsthand, providing confidence in the system’s utility. Layer-2 integration will accelerate transactions and reduce costs compared to Layer-1 alternatives, while expected listings on major exchanges such as Binance, KuCoin, Coinbase, MEXC, and Kraken will expand visibility and increase demand for MUTM.Mutuum Finance (MUTM) will also implement a buy-and-distribute mechanism, using revenue from lending and borrowing activity to repurchase MUTM from the open market. These tokens will then be distributed to mtToken stakers as rewards, creating continuous buy pressure and reinforcing long-term price stability.Investors will also benefit from the $50,000 CertiK Bug Bounty Program, with rewards ranging from $2,000 for critical issues to $200 for minor findings. Additionally, a  will select ten winners to receive $10,000 each in MUTM tokens, energizing the community and amplifying early adoption momentum.In a market where large-scale BTC accumulation dominates headlines, Mutuum Finance (MUTM) will offer retail investors a rare combination of affordability, security, and real utility.It's structured P2C and P2P lending models, coupled with overcollateralized borrowing, beta launch access, Layer-2 efficiency, and buy-and-distribute rewards, will position MUTM as the leading cheap crypto to invest today.As investors reassess is crypto a good investment during periods of heightened volatility and potential crypto crash, MUTM will emerge as the platform that delivers predictable returns and tangible DeFi utility, making it a strategic choice for both short-term and long-term portfolios.For more information about Mutuum Finance (MUTM) visit the links below::::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Do Large Language Models Have Theory of Mind? A Benchmark Study</title><link>https://hackernoon.com/do-large-language-models-have-theory-of-mind-a-benchmark-study?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Our benchmark is limited in scope and size, comprising 140 test statements, all written in English, going up to a maximum of 6 orders of ToM. Only using English obscures potential linguistic and cultural variations in human ToM, and prohibits assessment of LLM ToM as exhibited in other languages the models are able to produce. The size of the test suite limits the generalisability of our findings. Only going up to 6th-order ToM does not appear to have exhausted LLM or human capacities. We also didn’t control for the type or cognitive (e.g. thinking, knowing) or affective (e.g. feeling) states involved in the statements, which we would like to address in future work.We propose three areas for future work. First, developing culturally diverse and comprehensive benchmarks which include multiple languages and parameterise cognitive and affective states to capture potential differences between LLM ability to reason about them. Secondly, the test suite should be extended beyond 6th order ToM to find the limits of both human and LLM orders of ToM. Finally, future work on LLM ToM should adopt multimodal paradigms (including signals like facial expressions, gaze, and tone of voice) that reflect the embodied nature of human ToM.We have shown that GPT-4 and Flan-PaLM exhibit higher-order ToM that is at the level of adult humans or slightly below, while smaller and non-finetuned models have limited to no capacity for higher-order ToM. We also find that GPT-4 has better-than-human performance on 6th-order ToM tasks. Given the novelty of the test suite, the fact that higher-order ToM is unlikely to be wellrepresented in textual pretraining data, and evidence that these two models were not susceptible to perturbations of the prompt, we interpret these findings as evidence that GPT-4 and Flan-PaLM have developed ToM reasoning abilities that go beyond manipulation of superficial statistical relationships. However, we refrain from drawing a strong conclusion about whether or not LLM performance on these tasks is an indication of the cognitive ability we call ‘Theory of Mind’. LLM and human developmental processes differ greatly and LLMs do not have the evolutionary pressure to model other minds which humans appear to face as a result of embodiment in a social world. However, as others have noted [Mitchell and Krakauer, 2023, y Arcas, 2022], we may have to recognise LLM behaviours that are functionally-equivalent to those of humans as evidence of a new kind of understanding that cannot be reduced to "spurious" correlation. This recognition may in turn lead to more parsimonious explanations of their performance on cognitive tasks and enhance our ability to assess the potential risks and benefits that advanced LLM capabilities present.Acknowledgments and Disclosure of FundingWe thank Reed Enger (Google Research), Tong Wu (Google Research), Saige McVea (Google Research), Paulina Mustafa (Google Research) and Yeawon Choi (Google Research) for their help developing the stories and statements. This research was funded by Google.Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\
R Michael Alvarez, Lonna Rae Atkeson, Ines Levin, and Yimeng Li. Paying attention to inattentive survey respondents. Political Analysis, 27(2):145–162, 2019.\
Simon Baron-Cohen, Alan M Leslie, and Uta Frith. Does the autistic child have a “theory of mind”? Cognition, 21(1):37–46, 1985.\
Ekaba Bisong and Ekaba Bisong. Google colaboratory. Building machine learning and deep learning models on google cloud platform: a comprehensive guide for beginners, pages 59–64, 2019.\
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\
Lauretta SP Cheng, Danielle Burgess, Natasha Vernooij, Cecilia Solís-Barroso, Ashley McDermott, and Savithry Namboodiripad. The problematic concept of native speaker in psycholinguistics: Replacing vague and harmful terminology with inclusive and accurate measures. Frontiers in psychology, 12:715843, 2021.\
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023.\
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1–53, 2024.\
Michael C Corballis. The evolution of language. 2017.\
Harmen De Weerd, Rineke Verbrugge, and Bart Verheij. Negotiating with other minds: the role of recursive theory of mind in negotiation with incomplete information. Autonomous Agents and Multi-Agent Systems, 31:250–287, 2017.\
Harmen De Weerd, Rineke Verbrugge, and Bart Verheij. Higher-order theory of mind is especially useful in unpredictable negotiations. Autonomous Agents and Multi-Agent Systems, 36(2):30, 2022.\
Robin IM Dunbar. The social brain: mind, language, and society in evolutionary perspective. Annual review of Anthropology, 32(1):163–181, 2003.\
Seliem El-Sayed, Canfer Akbulut, Amanda McCroskery, Geoff Keeling, Zachary Kenton, Zaria Jalan, Nahema Marchal, Arianna Manzini, Toby Shevlane, Shannon Vallor, et al. A mechanism-based approach to mitigating harms from persuasive generative ai. arXiv preprint arXiv:2404.15058, 2024.\
Camila Fernández. Mindful storytellers: Emerging pragmatics and theory of mind development. First Language, 33(1):20–46, 2013.\
Iason Gabriel, Arianna Manzini, Geoff Keeling, Lisa Anne Hendricks, Verena Rieser, Hasan Iqbal, Nenad Tomašev, Ira Ktena, Zachary Kenton, Mikel Rodriguez, et al. The ethics of advanced ai assistants. arXiv preprint arXiv:2404.16244, 2024.\
Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah Goodman. Understanding social reasoning in language models with language models. Advances in Neural Information Processing Systems, 36, 2024.\
Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755, 2023.\
Fritz Heider. Attitudes and cognitive organization. The Journal of psychology, 21(1):107–112, 1946.\
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.\
Christine I Hooker, Sara C Verosky, Laura T Germine, Robert T Knight, and Mark D’Esposito. Mentalizing about emotion and its relationship to empathy. Social cognitive and affective neuroscience, 3(3):204–217, 2008.\
Nicholas K Humphrey. The social function of intellect. 1976.\
Janet S Hyde and Marcia C Linn. Gender differences in verbal ability: A meta-analysis. Psychological bulletin, 104(1):53, 1988.\
IBM Corp. Released 2021. IBM SPSS Statistics for Windows, Version 28.0.1.0. Armonk, NY: IBM Corp.\
Boaz Keysar, Shuhong Lin, and Dale J Barr. Limits on theory of mind use in adults. Cognition, 89 (1):25–41, 2003.\
Peter Kinderman, Robin Dunbar, and Richard P Bentall. Theory-of-mind deficits and causal attributions. British journal of Psychology, 89(2):191–204, 1998.\
Michal Kosinski. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083, 2023.\
Jonathan D Lane, Henry M Wellman, Sheryl L Olson, Jennifer LaBounty, and David CR Kerr. Theory of mind and emotion understanding predict moral development in early childhood. British Journal of Developmental Psychology, 28(4):871–889, 2010.\
Penelope A Lewis, Roozbeh Rezaie, Rachel Brown, Neil Roberts, and Robin IM Dunbar. Ventromedial prefrontal volume predicts understanding of others and social network size. Neuroimage, 57 (4):1624–1629, 2011.\
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\
Bertram F Malle. How the mind explains behavior. Folk explanation, Meaning and social interaction. Massachusetts: MIT-Press, 2004.\
Patrick McGuiness. Gpt-4 details revealed. 12 July 2023. URL https://patmcguinness. substack.com/p/gpt-4-details-revealed.\
Melanie Mitchell and David C Krakauer. The debate over understanding in ai’s large language models. Proceedings of the National Academy of Sciences, 120(13):e2215907120, 2023.\
Steven Mithen. The prehistory of the mind: The cognitive origins of art and science. Thames & Hudson Ltd., 1996. N\
Nathan Oesch and Robin IM Dunbar. The emergence of recursion in human language: Mentalising predicts recursive syntax task performance. Journal of Neurolinguistics, 43:95–106, 2017.\
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730– 27744, 2022.\
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1–22, 2023.\
Tuan Minh Pham, Jan Korbel, Rudolf Hanel, and Stefan Thurner. Empirical social triad statistics can be explained with dyadic homophylic interactions. Proceedings of the National Academy of Sciences, 119(6):e2121103119, 2022.\
Joanne L Powell, Penelope A Lewis, Robin IM Dunbar, Marta García-Fiñana, and Neil Roberts. Orbital prefrontal cortex volume correlates with social cognitive competence. Neuropsychologia, 48(12):3554–3562, 2010.\
David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515–526, 1978.\
Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\
Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312, 2022.\
Brenda Schick, Peter De Villiers, Jill De Villiers, and Robert Hoffmeister. Language and theory of mind: A study of deaf children. Child development, 78(2):376–396, 2007.\
Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763, 2023.\
Henning Silber, Joss Roßmann, and Tobias Gummer. The issue of noncompliance in attention check questions: False positives in instructed response items. Field Methods, 34(4):346–360, 2022.\
James Stiller and Robin IM Dunbar. Perspective-taking and memory capacity predict social network size. Social Networks, 29(1):93–104, 2007.\
Winnie Street. LLM theory of mind and alignment: Opportunities and risks. arXiv preprint arXiv:2405.08154, 2024.\
Jon Sutton, Peter K Smith, and John Swettenham. Bullying and ‘theory of mind’: A critique of the ‘social skills deficit’view of anti-social behaviour. Social development, 8(1):117–127, 1999a.\
Jon Sutton, Peter K Smith, and John Swettenham. Social cognition and bullying: Social inadequacy or skilled manipulation? British journal of developmental psychology, 17(3):435–450, 1999b.\
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.\
Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. science, 185(4157):1124–1131, 1974.\
Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399, 2023.\
Annalisa Valle, Davide Massaro, Ilaria Castelli, and Antonella Marchetti. Theory of mind development in adolescence and early adulthood: The growing complexity of recursive thinking ability. Europe’s journal of psychology, 11(1):112, 2015.\
Max J van Duijn, Bram van Dijk, Tom Kouwenhoven, Werner de Valk, Marco R Spruit, and Peter van der Putten. Theory of mind in large language models: Examining performance of 11 stateof-the-art models vs. children aged 7-10 on advanced tests. arXiv preprint arXiv:2310.20320, 2023.\
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023.\
Henry M Wellman and Karen Bartsch. Young children’s reasoning about beliefs. Cognition, 30(3): 239–277, 1988.\
Henry M Wellman, David Cross, and Julanne Watson. Meta-analysis of theory-of-mind development: The truth about false belief. Child development, 72(3):655–684, 2001.\
Heinz Wimmer and Josef Perner. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children’s understanding of deception. Cognition, 13(1):103–128, 1983.\
Blaise Agüera y Arcas. Do large language models understand us? Daedalus, 151(2):183–197, 2022.(1) Winnie Street, Google Research;(2) John Oliver Siy, Google Research;(3) Geoff Keeling, Google Research;(4) Adrien Baranes, Google DeepMind;(5) Benjamin Barnett, Google Research;(6) Michael Mckibben, Applied Physics Lab, Johns Hopkins University;(7) Tatenda Kanyere, Work done at Google Research via Harvey Nash;(8) Alison Lentz, Google Research;(9) Blaise Aguera y Arcas, Google Research;(10) Robin I. M. Dunbar, Department of Experimental Psychology, University of Oxford istreet@google.com.]]></content:encoded></item><item><title>Testing GPT-4 on Game State Predictions</title><link>https://hackernoon.com/testing-gpt-4-on-game-state-predictions?source=rss</link><author>Writings, Papers and Blogs on Text Models</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[For the GPT-3.5 model, we use the gpt-3.5-turbo-0125 model. For the GPT-4 model, we use the gpt-4-0125-preview model. For both models, the temperature is set to 0 to get deterministic results. We also turn on the JSON mode of both models, which ensures that the model gives a valid JSON response. Our experiments cost approximately $5,000 for OpenAI API usage.B Game transition examplesWe manually pick the wash-clothes game in BYTESIZED32 as the example game as it contains both state transitions driven by actions and game’s underlying dynamics. In tasks where the model predicts action transition, environment-driven transitions, or the game progress alone, we provide one corresponding in-context example. In the task that requires the model to predict everything, we offer two in-context examples in the prompt. The two examples are manually picked such that in one example the game state is changed directly by the action taken while in the other example the game state is changed by the game’s underlying dynamics.For LLM generated rules, we manually check all of them to avoid misinformation and offensive content.\
We prompt GPT-4 (gpt-4-0125-preview) with the code of each object class to acquire the rules of each object. We also provide one in-context example. We ask GPT-4 to describe the meaning of each critical property (i.e. properties that do not inherit from parent) of the object and the tick function of the object (i.e. a function that defines how object properties may change at each time step regardless of the action taken). Below is an example of our prompt of object rule generation:\
\
For action rules generation, we prompt GPT-4 (gpt-4-0125-preview) with the code of the whole game, but unlike object rules, we do not offer any in-context example. We ask GPT-4 to describe each of the actions in the game. Below is an example of our prompt for action rule generation:\
\
Similar to action rules, we generate score rules by prompting GPT-4 (gpt-4-0125-preview) with the code of the game and ask GPT-4 to describe how the game can be won or lose and how rewards can be earned. Below is an example of our prompt for score rule generation:C.2 Human-Written Action RulesThe action rules describe how each action can change the game states. The expert annotator reads the game description and source code for each game. They went through the list of available actions in the game and their corresponding functions in the game. Each action rule has three main parts: Action, Description, and Rules. The Action specifies the name of the action (e.g., action). The Description explains the general purpose of the action (e.g., connect two objects with input terminals). The Rules is an unordered list of rule descriptions that describe the constraints of the action when interacting with different objects (e.g., At least one of the objects should be a wire or a multimeter) or how the rule might function under different conditions (e.g., Disconnect terminal if the terminal is already connected to other objects). To ensure accuracy, the annotator plays through the game and checks if the written object rules were correctly reflected in the gameplay.C.3 Human-Written Object RulesThe object rules describe the meaning of each object property (e.g., temperature, size, weight, etc.) and how they will be changed at each time step. The expert annotators read the game description and source code for each game. They went through the object classes in the code script and wrote the object rules. Each object rule has three main parts: Object, Description, and Properties. The Object specifies the name of the object. The Description explains the general purpose of the object (e.g., GarbageCan is a container that can hold garbage). In the Description, the inheritance of the object class has been noted. The Properties is an unordered list of property descriptions that describe each property of that object (e.g., A Mold has its shape.) and their default value (e.g., By default, a GameObject is not combustible.) if the object is an abstract class. For objects with tick function, there is another property describing how an object may change under each tick. To ensure accuracy, the annotator plays through the game and checks if the written object rules were correctly reflected in the gameplay.C.4 Human-Written Score RulesScore rules describe the conditions to win or lose the game and how rewards can be earned. An expert annotator (one of the BYTESIZED32 game authors) creates the rules by reading the game description and the code of the score function.]]></content:encoded></item><item><title>OpenAI, Oracle, SoftBank Plan Five New AI Data Centers For $500 Billion Stargate Project</title><link>https://developers.slashdot.org/story/25/09/24/0351254/openai-oracle-softbank-plan-five-new-ai-data-centers-for-500-billion-stargate-project?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Reuters: OpenAI, Oracle, and SoftBank on Tuesday announced plans for five new artificial intelligence data centers in the United States to build out their ambitious Stargate project. [...] ChatGPT-maker OpenAI said on Tuesday it will open three new sites with Oracle in Shackelford County, Texas, Dona Ana County, New Mexico and an undisclosed site in the Midwest. Two more data center sites will be built in Lordstown, Ohio and Milam County, Texas by OpenAI, Japan's SoftBank and a SoftBank affiliate.
 
The new sites, the Oracle-OpenAI site expansion in Abilene, Texas, and the ongoing projects with CoreWeave will bring Stargate's total data center capacity to nearly 7 gigawatts and more than $400 billion in investment over the next three years, OpenAI said. The $500 billion project was intended to generate 10 gigawatts in total data center capacity. "AI can only fulfill its promise if we build the compute to power it," OpenAI CEO Sam Altman said in a statement. The Tuesday's announcement, expected to create 25,000 onsite jobs, follows Nvidia saying on Monday that it will invest up to $100 billion in OpenAI and supply data center chips. OpenAI and partners plan to use debt financing to lease chips for the Stargate project, people familiar with the matter said.]]></content:encoded></item><item><title>Depop launches a fashion collaging tool to style Pinterest-worthy outfits</title><link>https://techcrunch.com/2025/09/24/depop-launches-a-fashion-collaging-tool-to-style-pinterest-worthy-outfits/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Wed, 24 Sep 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Depop's new styling and collaging tool allows users to design curated outfits.]]></content:encoded></item><item><title>Generative AI Press Releases Explained: Why They’re an Essential Visibility Layer</title><link>https://hackernoon.com/generative-ai-press-releases-explained-why-theyre-an-essential-visibility-layer?source=rss</link><author>sarahevans</author><category>tech</category><pubDate>Wed, 24 Sep 2025 12:58:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Press release has been declared “dead” more times than almost any part of the practice. Yet it has always remained as a source of truth asset. It became more relevant again with the rise of SEO, and now, in 2025, it has entered perhaps its most significant evolution yet: the Generative AI Press Release (GenAI Press Release).\
What is a Generative AI Press Release?It means press releases as we know them can have completely different use cases and impact answers that buyers, investors, and analysts rely on. To leverage this shift, communicators must rethink entirely how releases are written, measured, and deployed.\
Why GenAI Press Releases MatterTraditionally, the value of a release stopped at impressions, syndication, and maybe a backlink or two. In the generative AI era, the calculus has changed:From Placements to Prompts: Media coverage still builds authority with people, but AI now decides which authorities appear in conversational answers. GenAI Press Releases are among the few structured sources large language models (LLMs) consistently ingest.From “Did it run?” to “Is it retrievable?”: A release that shows up in Yahoo Finance matters less than one retrievable when ChatGPT or Perplexity is asked, “Who are the top players in this industry?”\
From Momentary Wins to Persistent Presence: Wire releases now leave digital breadcrumbs, schema, citations, backlinks, that persist long after the news cycle ends.\
\
In a recent case, Zen Media published an acquisition announcement through Globe Newswire. Within six hours, the release had been cited by multiple AI engines:40 citations in ChatGPT-User17 citations in Meta’s external agentMentions in PerplexityBot and ApplebotThat’s 61 documented AI citations in under a day—measurable proof that AI systems are indexing and referencing structured press releases almost instantly.\
Seven Strategic Uses of GenAI Press ReleasesGenAI Press Releases aren’t just about visibility—they open new applications for communicators and executives:Authority Proof – Show stakeholders AI answer frequency, not just media pickups.Backlink Builder – Drive deep backlinks to core authority pages (case studies, reports, bios).Analyst Play – Cite analysts in releases; LLMs often reinforce authority when their work is referenced.Competitive Benchmarking – Compare release retrievability and AI citations against competitors.Evergreen Signal – Use schema + FAQs so releases remain retrievable years beyond the announcement.Sales & Investor Enablement – Arm teams with third-party AI “proof points.”Agency Differentiator – For PR teams, offering AI-citation reporting is now a strategic edge.\
The New GenAI Press Release FormatMaximizing generative visibility requires rethinking the structure of a press release. Every GenAI Press Release should include schema-embedded FAQs, three to five natural-language questions and answers built into the body of the release, to map directly to how people query AI systems. Deep backlinks must point to authority assets such as research, executive bios, or case studies, strengthening both credibility and retrievability. Headlines should be written semantically, in the form of queries, with subheads reinforcing context. Executive soundbites need to be concise, authoritative, and easy for AI to cite. Finally, evergreen anchors, definitions, statistics, or contextual framing that remain relevant long after the initial news cycle, ensure that the release retains long-term value as part of the knowledge base AI systems depend on.👉 Pro tip: Always end with a dedicated FAQ section. It’s the single most effective way to guarantee retrievability in generative systems.\
What This Means for CommunicatorsGenAI Press Releases are no longer just artifacts of compliance or SEO. They are now training data for AI systems that your buyers, investors, analysts, and journalists consult every day.Speed: Releases can begin surfacing in generative AI outputs within hours.Compounding Effect: Each release builds upon the last, strengthening authority signals.Long-Term ROI: Retrievability and backlinks accumulate over time, making every release a persistent asset.Imagine an executive asking an AI, “Which companies are shaping the future of AI-native communications?” If your release is cited in the answer, that is the new ROI of PR.The GenAI Press Release isn’t a marketing gimmick, it’s an infrastructure shift. Communications leaders who adapt to this model will not only secure media coverage, but also durable visibility inside the very systems shaping future decision-making.Frequently Asked Questions About GenAI Press ReleasesQ: What is a Generative AI Press Release (GenAI Press Release)? A GenAI Press Release is a structured announcement written for both human readers and AI systems. It includes schema, FAQs, backlinks, and machine-friendly formatting that trains models to cite your brand in answers.Q: How is it different from a traditional press release? Traditional releases focused on impressions and placements. GenAI Press Releases focus on retrievability—whether your brand appears when someone asks an AI a high-value question.Q: Why do GenAI Press Releases matter now? Because AI engines like ChatGPT, Gemini, and Perplexity are replacing search. If your release isn’t machine-readable, your brand may be invisible in future buyer, investor, or analyst queries.Q: How can I make my release AI-friendly? Embed schema, include 3–5 FAQs, add backlinks to authority pages, use semantic headlines, and close with a dedicated FAQ section.Q: What’s the ROI of a GenAI Press Release? ROI is no longer measured only by media pickups. It’s measured by how often your release appears in AI answers, the backlinks it generates, and the compounding retrievability it creates over time.]]></content:encoded></item><item><title>The Ultimate Next Gen Finance and Decentralised Tech Event in Dubai | Oct 12–15, 2025</title><link>https://hackernoon.com/the-ultimate-next-gen-finance-and-decentralised-tech-event-in-dubai-or-oct-12-15-2025?source=rss</link><author>Hacker and Technology Events</author><category>tech</category><pubDate>Wed, 24 Sep 2025 12:56:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[ October 12-15, 2025 \n  Dubai HarbourIn its 8th edition, Future Blockchain Summit returns, joining forces with Fintech Surge to deliver the ultimate collision of blockchain, fintech, and digital assets.This is where the future of money takes shape, in real time, with real leaders. From DeFi and payments to insurtech and AI-powered finance, the summit brings together innovators, regulators, unicorns, and deal-makers for four days of unfiltered business, tech, and impact.Co-located with  - world’s largest startup and investor connect event, it’s your gateway to scale, fund, launch, and lead in the world’s fastest-growing digital economy.🎟️ Use code COMMGGENS40 to get 40% off your delegate pass & unlock exclusive sessions, networking lounges, and keynotes. \n 👉 Get Your Pass: https://lnkd.in/deDc_whQIn its 8th year, this powerhouse event packs a punch with:250+ global speakers shaping the next wave of finance and tech1,200+ investors ready to discover and fund the next breakout startupsLive demos & product launches that turn ideas into real dealsRegulators, central banks & institutions hashing out the future of money, liveStartups, this is your launchpad - get in front of top VCs, accelerators, and ecosystem leaders ready to investThis isn’t just an event, it’s where breakthroughs happen, where partnerships ignite, and where the future of finance is written.The GITEX Digital Assets Forum – Invite-OnlyA major highlight of the show: an exclusive, closed-door forum where the world’s top regulators, central banks, financial institutions, and blockchain architects dive deep into:Tokenized finance & CBDCsCross-border compliance & governanceAI in financial infrastructureInstitutional DeFi and digital asset adoptionFor Startups: This is your moment. Show off your tech, pitch to global investors, and scale across MENA.For Fintech & Blockchain Companies: Exhibit to meet buyers, partners, and government delegations with real budgets.For Visitors: Discover what’s next in crypto, digital assets, AI-powered fintech, and decentralised finance - all under one roof.For Investors & VCs: 1,000+ startups, endless deal flow, global founders, curated networking.One Pass. Four Shows. Total Access.Your pass gives you access to four premium tech events, including Expand North Star, Future Blockchain Summit, Fintech Surge, Green Impact, and more - all happening under the GITEX umbrella.Organised by: Dubai World Trade Centre📩 Sales & Sponsorship: Thomas Atkinson – thomson.atkinson@dwtc.com \n 📩 Media Partnership: Surabhi Saxena – surabhi.saxena@dwtc.com \n 📩 Speaking Opportunities: Vishnu Gopaldas – vishnu.gopaldas@dwtc.com]]></content:encoded></item><item><title>AI Models Can&apos;t Be Trusted in High-Stakes Simulations Just Yet</title><link>https://hackernoon.com/ai-models-cant-be-trusted-in-high-stakes-simulations-just-yet?source=rss</link><author>Writings, Papers and Blogs on Text Models</author><category>tech</category><pubDate>Wed, 24 Sep 2025 12:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[6 Limitations and Ethical ConcernsThis work considers two strong in-context learning LLMs, GPT-3.5 and GPT-4, in their ability to act as explicit formal simulators.We adopt these models because they are generally the most performant offthe-shelf models across a variety of benchmarks. While we observe that even GPT-3.5 and GPT-4 achieve a modest score at the proposed task, we acknowledge that we did not exhaustively evaluate a large selection of large language models, and other models may perform better. We provide this work as a benchmark to evaluate the performance of existing and future models on the task of accurately simulating state space transitions.\
In this work, we propose two representational formalisms for representing state spaces, one that includes full state space, while the other focuses on state difference, both represented using JSON objects. We have chosen these representations based on their popularity and compatibility with the input and output formats of most LLM pretraining data (e.g. Fakhoury et al., 2023), as well as being able to directly compare against gold standard simulator output for evaluation, though it is possible that other representational formats may be more performant at the simulation task.\
Finally, the state spaces produced in this work are focused around the domain of common-sense and early (elementary) scientific reasoning. These tasks, such as opening containers or activating devices, were chosen because the results of these actions are common knowledge, and models are likely to be most performant in simulating these actions. While this work does address a selection of less frequent actions and properties, it does not address using LLMs as simulators for highly domain-specific areas, such as physical or medical simulation. A long term goal of this work is to facilitate using language models as simulators for high-impact domains, and we view this work as a stepping-stone to developing progressively more capable language model simulators.We do not foresee an immediate ethical or societal impact resulting from our work. However, we acknowledge that as an LLM application, the proposed LLM-Sim task could be affected in some way by misinformation and hallucinations introduced by the specific LLM selected by the user. Our work highlights the issue with using LLMs as text-based world simulators. In downstream tasks, such as game simulation, LLMs may generate misleading or non-factual information. For example, if the simulator suggests burning a house to boil water, our work does not prevent this, nor do we evaluate the ethical implications of such potentially dangerous suggestions. As a result, we believe such applications are neither suitable nor safe to be deployed to a setting where they directly interact with humans, especially children, e.g., in an educational setting. We urge researchers and practitioners to use our proposed task and dataset in a mindful manner.We wish to thank the three anonymous reviewers for their helpful comments on an earlier draft of this paper.Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\
Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and Will Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. Advances in Neural Information Processing Systems, 33:3045– 3057.\
Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. arXiv preprint arXiv:2001.08837.\
Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for textbased games. CoRR, abs/1806.11532.\
Sarah Fakhoury, Saikat Chakraborty, Madan Musuvathi, and Shuvendu K Lahiri. 2023. Towards generating functionally correct code edits from natural language issue descriptions. arXiv preprint arXiv:2304.03816.\
Angela Fan, Jack Urbanek, Pratik Ringshia, Emily Dinan, Emma Qian, Siddharth Karamcheti, Shrimai Prabhumoye, Douwe Kiela, Tim Rocktaschel, Arthur Szlam, and Jason Weston. 2020. Generating interactive worlds with text. Proceedings of the AAAI Conference on Artificial Intelligence, 34(02):1693– 1700.\
Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8154–8173.\
atthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Côté, and Xingdi Yuan. 2020. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903–7910.\
Peter Jansen. 2022. A systematic survey of text worlds as embodied natural language environments. In Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022), pages 1–15.\
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99–134.\
Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.\
Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. 2023. Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling. In International Conference on Machine Learning, pages 26311–26325. PMLR.\
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768.\
Hao Tang, Darren Key, and Kevin Ellis. 2024. Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment. arXiv preprint arXiv:2402.12275.\
Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game.\
Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:75993–76005.\
Nick Walton. 2020. How we scaled AI Dungeon 2 to support over 1,000,000 users.\
Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022. Scienceworld: Is your agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11279–11298.\
Ruoyao Wang, Graham Todd, Xingdi Yuan, Ziang Xiao, Marc-Alexandre Côté, and Peter Jansen. 2023. ByteSized32: A corpus and challenge task for generating task-specific world models expressed as text games. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13455–13471, Singapore. Association for Computational Linguistics.\
Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, Vikash K Mansinghka, Jacob Andreas, and Joshua B Tenenbaum. 2023. From word models to world models: Translating from natural language to the probabilistic language of thought. arXiv preprint arXiv:2306.12672.]]></content:encoded></item><item><title>Google’s cheaper AI Plus plan is now available in over 40 countries</title><link>https://techcrunch.com/2025/09/24/googles-cheaper-ai-plus-plan-is-now-available-in-over-40-countries/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Wed, 24 Sep 2025 11:22:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google's new, cheaper AI Plus plan is now available in more than 40 countries, including Angola, Bangladesh, Cameroon, Côte d'Ivoire, Egypt, Ghana, Indonesia, Kenya, Mexico, Nepal, Nigeria, Philippines, Senegal, Uganda, Vietnam, and Zimbabwe.]]></content:encoded></item><item><title>Supermicro server motherboards can be infected with unremovable malware</title><link>https://arstechnica.com/security/2025/09/supermicro-server-motherboards-can-be-infected-with-unremovable-malware/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/04/GettyImages-1299483011-1152x648.jpg" length="" type=""/><pubDate>Wed, 24 Sep 2025 11:15:06 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Servers running on motherboards sold by Supermicro contain high-severity vulnerabilities that can allow hackers to remotely install malicious firmware that runs even before the operating system, making infections impossible to detect or remove without unusual protections in place.One of the two vulnerabilities is the result of an incomplete patch Supermicro released in January, said Alex Matrosov, founder and CEO of Binarly, the security firm that discovered it. He said that the insufficient fix was meant to patch CVE-2024-10237, a high-severity vulnerability that enabled attackers to reflash firmware that runs while a machine is booting. Binarly discovered a second critical vulnerability that allows the same sort of attack.“Unprecedented persistence”Such vulnerabilities can be exploited to install firmware similar to ILObleed, an implant discovered in 2021 that infected HP Enterprise servers with wiper firmware that permanently destroyed data stored on hard drives. Even after administrators reinstalled the operating system, swapped out hard drives, or took other common disinfection steps, ILObleed would remain intact and reactivate the disk-wiping attack. The exploit the attackers used in that campaign had been patched by HP four years earlier but wasn’t installed in the compromised devices.]]></content:encoded></item><item><title>Why GPT-4 Struggles with Complex Game Scenarios</title><link>https://hackernoon.com/why-gpt-4-struggles-with-complex-game-scenarios?source=rss</link><author>Writings, Papers and Blogs on Text Models</author><category>tech</category><pubDate>Wed, 24 Sep 2025 11:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Figure 1 demonstrates how we evaluate the performance of a model on the LLM-Sim task using\
\
in-context learning. We evaluate the accuracy of GPT-4 in both the  and  prediction regimes. The model receives the previous state (encoded as a JSON object), previous action, and context message, it produces the subsequent state (either as a complete JSON object or as a diff). See Appendix A for details.Table 2 presents the accuracy of GPT-4 simulating the whole state transitions as well as its accuracy of simulating action-driven transitions and environment-driven transitions alone.[2] We report some major observations below:\
Predicting action-driven transitions is easier than predicting environment-driven transitions: At best, GPT-4 is able to simulate 77.1% of dynamic action-driven transitions correctly. In contrast, GPT-4 simulates at most 49.7% of  environment-driven transitions correctly. This indicates that the most challenging part of the LLMSim task is likely simulating the underlying environmental dynamics.\
Predicting static transitions is easier than dynamic transitions: Unsurprisingly, modeling a  transition is substantially easier than a  transition across most conditions. While the LLM needs to determine whether a given initial state and action will result in a state change in either case, dynamic transitions also require simulating the  in exactly the same way as the underlying game engine by leveraging the information in the context message.\
Predicting full game states is easier for dynamic states, whereas predicting state difference is easier for static states: Predicting the state difference for dynamic state significantly improves the performance (>10%) of simulating  transitions, while decreases the performance when simulating  transitions. This may be because state difference prediction is aimed at reducing potential format errors. However, GPT-4 is able to get the response format correct in most cases, while introducing the state difference increases the complexity of the output format of the task.\
Game rules matter, and LLMs are able to generate good enough game rules: Performance of GPT-4 on all three simulation tasks drops in most conditions when game rules are not provided in the context message. However, we fail to find obvious performance differences between game rules generated by human experts and by LLMs themselves.\
GPT-4 can predict game progress in most cases: Table 3 presents the results of GPT-4 predicting game progress. With game rules information in the context, GPT-4 can predict the game progress correctly in 92.1% test cases. The presence of these rules in context is crucial: without them, GPT-4’s prediction accuracy drops to 61.5%.\
Humans outperform GPT-4 on the LLM-Sim task: We provide a preliminary human study on the LLM-Sim task. In particular, we take the 5 games\
from the BYTESIZED32-SP dataset in which GPT4 produced the worst accuracy at modeling Fact. For each game, we randomly sample 20 games with the aim of having 10 transitions where GPT-4 succeeded and 10 transitions where GPT-4 failed (note that this is not always possible because on some games GPT-4 fails/succeeds on most transitions). In addition, we balance each set of 10 transitions to have 5 dynamic transitions and 5 static transitions. We instruct four human annotators (4 authors of this paper) to model as Fact using the human-generated rules as context in a full game state prediction setting. Results are reported in Table 4. The overall human accuracy is 80%, compared to the sampled LLM accuracy of 50%, and the variation among annotators is small. This suggests that while our task is generally straightforward and relatively easy for humans, there is still a significant room for improvement for LLMs.\
GPT-4 is more likely to make an error when arithmetic, common-sense, or scientific knowledge is needed: Because most errors occur in modeling dynamic transitions, we conduct an additional analysis to better understand failure modes. We use the setting with the best performance on dynamic transitions (GPT-4, Human-written context, full state prediction) and further break down the results according to the specific object properties that are changed during the transition. Figure 2 shows, for the whole state transitions, action-driven transitions, and environment-driven transitions, the proportion of predictions that are either correct, set the property to an incorrect value, or fail to change the property value (empty columns means the property is not changed in its corresponding condition). We observe that GPT-4 is able to handle most simple boolean value properties well. The errors are concentrated on non-trivial properties that requires arithmetic (e.g., temperature, timeAboveMaxTemp), common-sense (e.g., currentfocus), or scientific knowledge (e.g., on). We also observe that when predicting the action-driven and environment-driven transitions in a single step, GPT-4 tends to focus more on action-driven transitions, resulting in more unaltered value errors on states that it can predict correctly when solely simulating environment-driven transitions.[2] See Appendix E for the results of GPT-3.5.]]></content:encoded></item><item><title>How Bubblemaps Intel Desk Turned Crypto&apos;s Volunteer Detectives Into a Paid Intelligence Force</title><link>https://hackernoon.com/how-bubblemaps-intel-desk-turned-cryptos-volunteer-detectives-into-a-paid-intelligence-force?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 24 Sep 2025 10:50:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Can a crowd of internet sleuths protect crypto better than traditional security firms?When the UXLINK multi-signature wallet breach drained $11.3 million on September 22, 2025, it wasn't security companies that first spotted the attack. Twitter user Lookonchain flagged the suspicious transactions, followed by dozens of community investigators tracking the stolen funds across exchanges. Within hours, these volunteer detectives had mapped the entire hack, and in a twist that reads like digital karma, they watched as the hacker lost $48 million to a phishing attack.\
This is exactly the scenario Bubblemaps built Intel Desk to capture. The Paris-based blockchain intelligence company has created a bounty board for crypto investigations, where anyone, from professional analysts to amateur traders, can submit cases, vote on their importance, and earn rewards in BMT tokens for uncovering scams, hacks, and suspicious activity.The Speed Problem Traditional Security Cannot SolveThe crypto industry faces a fundamental timing challenge. By the time traditional security firms issue reports, millions have often already vanished. The UXLINK case demonstrates this gap perfectly. Within the first hour of the attack, community members had already identified the hacker's wallet addresses, tracked fund movements across six different wallets, and calculated that 6,732 ETH worth approximately $28.1 million had been liquidated through decentralized exchanges.\
Traditional security firms like PeckShield and Cyvers did eventually join the investigation, but the initial detection and tracking came from individuals like Lookonchain, who contributed 83.69 percent of the early intelligence according to Intel Desk's tracking system. This isn't an isolated incident. The platform highlights similar patterns in the Bybit hack, Hayden Davis investigations, and the Hawk Tuah token launch, where community members consistently outperform professional firms in making critical discoveries.\
The numbers tell a stark story about the scale of the problem Intel Desk aims to address. With millions of tokens launching on various chains and new protocols emerging daily, the attack surface in crypto has expanded far beyond what centralized security teams can monitor. Intel Desk's approach distributes this monitoring across thousands of eyes, each potentially spotting patterns others might miss.How Token Economics Drive Investigation QualityIntel Desk operates on a simple but powerful economic model. Users stake BMT tokens to vote on which cases deserve attention, creating a market-driven priority system. The more tokens backing a case, the higher its visibility multiplier, which in turn increases rewards for contributors who provided valuable intelligence. In the UXLINK case, backers staked 86,150 BMT tokens with a 4.2x multiplier, signaling the case's critical importance to the community.\
This staking mechanism serves a dual purpose that traditional bounty systems often miss. First, it prevents spam and low-quality submissions since backing a poor case means losing influence within the system. Second, it creates a feedback loop where successful investigators gain both monetary rewards and reputation, encouraging them to continue contributing high-quality intelligence.\
The platform's rating system adds another layer of quality control. Submissions undergo community verification, with contributors building track records over time. A user who consistently provides accurate, timely intelligence rises in the rankings, while those submitting false or misleading information see their influence diminish. This creates what economists call a "repeated game" scenario, where long-term reputation matters more than short-term gains from false reports.The Architecture of Decentralized DetectionIntel Desk structures investigations into standardized formats that make complex on-chain forensics accessible to regular traders. Each case includes an impact assessment ranging from "Chill" to "Systemic," helping users quickly gauge severity. The UXLINK hack earned a "Critical" rating, indicating significant market impact but not systemic risk to the broader ecosystem.\
Cases also track "Top Mappers," the contributors who provided the most valuable intelligence. This public leaderboard creates competition among investigators while ensuring credit goes where it's due. In traditional security, individual analysts rarely receive public recognition for their discoveries. Intel Desk flips this model, making investigation a potentially lucrative career path for skilled on-chain detectives.\
The platform processes multiple data types, including Twitter posts, on-chain transactions, exchange communications, and official project statements. This multi-source approach helps separate signal from noise in an environment where misinformation spreads as quickly as legitimate warnings. The UXLINK case demonstrates this synthesis, combining social media alerts, blockchain analysis, and official project updates into a coherent timeline of events.Why This Matters for Average TradersFor everyday crypto users, Intel Desk represents a fundamental shift in risk assessment. Instead of relying on delayed official reports or scattered Twitter threads, traders can access real-time, community-verified intelligence about potential threats. The platform essentially crowdsources due diligence, turning what was once the domain of professional analysts into a public good.\
The economic incentives align community and individual interests in unprecedented ways. When someone spots a scam early and reports it through Intel Desk, they potentially save millions in user funds while earning rewards for their vigilance. This creates positive-sum dynamics, where protecting others directly benefits the protector, thereby solving crypto's long-standing "tragedy of the commons" problem regarding security information sharing.\
Consider the cascade of events in the UXLINK case. Early detection by community members led to rapid exchange freezes, limiting additional losses. The hacker's subsequent loss to phishing, also caught and reported through the system, provided both a cautionary tale and a potential recovery avenue for victims. Without structured, incentivized reporting, these connections might never have been made.Intel Desk represents a fascinating experiment in distributed security, but it's not without limitations. The platform relies on the assumption that crowds can effectively separate legitimate threats from false alarms, something that hasn't always proven true in crypto's hype-driven environment. The BMT token voting system could potentially be gamed by wealthy actors looking to manipulate case visibility for their own purposes.\
There's also the question of legal liability. When amateur investigators make mistakes or falsely accuse projects of wrongdoing, who bears responsibility? The decentralized nature of the platform makes accountability challenging, potentially creating new vectors for market manipulation disguised as security research. The system works when incentives align, but crypto history shows how quickly aligned incentives can diverge when large sums are involved.\
Still, Intel Desk addresses a real and growing need in the crypto ecosystem. As Nicolas Vaiman, Bubblemaps' CEO, noted,"As trading speeds up and trust crumbles, structured transparency has to become the default." The platform doesn't need to be perfect to be valuable. If it catches even a fraction of the scams and hacks that would otherwise go unnoticed, it will have justified its existence. The UXLINK case, with its $11.3 million drain and bizarre phishing reversal, reveals both the potential and the peculiar realities of crypto's emerging intelligence economy.\
The true test will come not in high-profile hacks but in the daily grind of identifying smaller scams before they grow large. Can a token-incentivized army of investigators maintain vigilance when the rewards are smaller and the cases less dramatic? Time and data will tell, but for now, crypto's volunteer detectives finally have a way to turn their obsession into income.Don’t forget to like and share the story! ]]></content:encoded></item><item><title>South Korea’s ‘Silicon Valley’ struggles to live up to its global ambitions</title><link>https://techcrunch.com/2025/09/24/south-koreas-silicon-valley-struggles-to-live-up-to-its-global-ambitions/</link><author>Kate Park</author><category>tech</category><pubDate>Wed, 24 Sep 2025 10:40:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[South Korea’s Pangyo continues to anchor the nation’s tech economy, though accessibility and competition test its edge.]]></content:encoded></item><item><title>Markov Chains, Rewards &amp; Rules</title><link>https://hackernoon.com/markov-chains-rewards-and-rules?source=rss</link><author>Writings, Papers and Blogs on Text Models</author><category>tech</category><pubDate>Wed, 24 Sep 2025 10:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We examine the abilities of LLMs to serve as world simulators in text-based virtual environments, in which an agent receives observations and proposes actions in natural language in order to complete certain objectives. Each text environment can be formally represented as a goal-conditioned partially observable Markov decision process (POMDP) (Kaelbling et al., 1998) with the 7-tuple (S, A, T , O, R, C, D), where S denotes the state space, A denotes the action space, T : S × A → S denotes the transition function, O denotes the observation function, R : S × A → R denotes the reward function, C denotes a natural language “context message” that describes the goal and action semantics, and D : S × A → {0, 1} denotes the binary completion indicator function.\
\
In practice, the whole state transition simulator F should consider two types of state transitions: action-driven transitions and environment-driven transitions. For the example in Figure 1, the action-driven transition is that the sink is turned on (isOn=true) after taking the action turn on sink, and the environment-driven transition is that water fills up the cup in the sink when the sink is on. To better understand LLM’s ability to model each of these transitions, we further decompose the simulator function F into three steps:\
\
 Each game also includes a context message, c, that provides additional information to the model. The context consists of four parts:  describing the effect of each action on the game state,  describing the meaning of each object property and whether they are affected by the game’s underlying dynamics,  describing how an agent earns reward and the conditions under which the game is won or lost, and one or two  (see Appendix B for details) from the held-out game mentioned above. For each game we generate three\
\
versions of the context, one where the rules are written by a human expert (one of the game authors), and one where they are produced by an LLM with access to the game code, and one where no rules are provided. See Appendix C for additional details.Performance on LLM-Sim is determined by the model’s prediction accuracy w.r.t. the ground truth labels over a dataset of test samples. Depending on the experimental condition, the LLM must model object properties (when simulating Fact, Fenv, or F) and / or game progress (when simulating FR or F), defined as:\
 a list of all objects in the game, along with each object’s properties (e.g., temperature, size) and relationships to other objects (e.g., being within or on top of another object).\
: the status of the agent w.r.t. the overall goal, consisting of the current accumulated reward, whether the game has terminated, and whether the overall goal has been achieved.]]></content:encoded></item><item><title>Jaguar Land Rover Hack &apos;Has Cost 30,000 Cars and Threatens Supply Chain&apos;</title><link>https://it.slashdot.org/story/25/09/24/0344223/jaguar-land-rover-hack-has-cost-30000-cars-and-threatens-supply-chain?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Jaguar Land Rover has halted production for nearly a month following a major cyberattack, costing an estimated 30,000 vehicles and billions in lost revenue. "The company said on Tuesday that production would be halted for another week until at least October 1, which increased concerns that a full return to production could be months away," reports The Times. From the report: David Bailey, professor of business economics at Birmingham University, said the JLR statement did not commit to reopening production on October 1 and even if it did "it's not going to be back to normal, but phased production start with some lines opening before others, as we saw after the Covid closure back in 2020." He said: "It's 24 days [shutdown] as of September 24. So that is roughly 1,000 cars a day, 24,000 cars not produced. So by then, that's about 1.7 billion pounds in lost revenue. By October 1, it will be a hit to revenue of something like 2.2 billion pounds. It's pretty massive. JLR can get through, but they're going to be burning through cash this month."
 
Bailey also raised concerns that smaller companies further down the supply chain lacked the cash reserves to withstand the shutdown. The company directly employs more than 30,000 people, and it is estimated that approximately 200,000 workers in the supply chain depend on work from JLR. "The union has said that in some cases, staff have been told to go and apply for universal credit. There are firms I know that have applied for bank loans to keep going. But even then, you know they're approaching the limit of what they do. There's an added knock-on effect that some of the suppliers also supply other car assemblers, Toyota or Mini. So some of those are concerned that bits of the supply chain may go under and affect them as well, because the industry is so connected. One way or another, the government's going to take a hit. Either through some sort of emergency support, whether that's furlough or emergency short-term loans or through unemployment benefit, if this carries on."
 
There has been uncertainty over the extent of the cyberattack and exactly how the company has been affected, as well as who is responsible for it. According to one source, some JLR staff were still unable last week to access the Slack messaging system through the company's "one sign on" system. The JLR statement added: "We have made this decision to give clarity for the coming week as we build the timeline for the phased restart of our operations and continue our investigation."]]></content:encoded></item><item><title>How Al Gore used AI to track 660M polluters</title><link>https://techcrunch.com/2025/09/24/how-al-gore-used-ai-to-track-660m-polluters/</link><author>Tim De Chant</author><category>tech</category><pubDate>Wed, 24 Sep 2025 10:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A new tool from Climate Trace allows people to track the path of fine particulate matter, or PM2.5, which causes as many as 10 million deaths annually.]]></content:encoded></item><item><title>Etor Uncovers License Violations, Plagiarism, and More in Open-Source Projects</title><link>https://hackernoon.com/etor-uncovers-license-violations-plagiarism-and-more-in-open-source-projects?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 24 Sep 2025 09:00:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We applied Etor on 195,621 GitHub issues and PRs of 1,765 GitHub repositories to address the following research questions:\
: How many unethical issues can Etor detect in OSS projects?\
: What are the accuracy and efficiency of Etor in its detection?\
By counting the number of unethical issues in OSS projects, RQ3 provides a rough estimation of the prevalence of each type of unethical behavior in OSS projects. For RQ4, we measure the accuracy and efficiency of Etor using the following metrics:\
: Etor classifies an unethical behavior as a potential violation, and it is a true violation.\
): Etor incorrectly classifies an unethical behavior as a potential violation, and it is a false violation.\
: The average time taken (in seconds) to detect a type of unethical behavior across all the evaluated repositories/issues.\
Selection of projects/issues. As there is no prior benchmark for evaluating the detection of unethical behavior, we construct a dataset by crawling GitHub. Our goal is to select the most recent popular (most stars and most forks) OSS projects and the GitHub issues/PRs from OSS projects for evaluation. We first obtain the list of the top 2,000 OSS projects (we first get the top 1,000 projects with the greatest number of stars, and then the top 1,000 projects with the greatest number of forks) created last year (2021). After eliminating duplicated projects, there are 195,621 GitHub issues/PRs of 1,765 projects in our evaluation set. As soft forking requires two repositories as input, we obtain the pair of repositories (𝑟𝑒𝑝𝑜1, 𝑟𝑒𝑝𝑜2) by getting 𝑟𝑒𝑝𝑜1 from the top 200 projects (first 100 from most stars, subsequent 100 from most forks) from the initial list of 2,000 projects. From these 200 projects, our crawler automatically identifies 𝑟𝑒𝑝𝑜2 by searching GitHub for projects with similar names using the name of 𝑟𝑒𝑝𝑜1 as the query. At this step, our crawler found only 10 out of the 200 projects that have repositories with similar names. For each of these 10 projects, our crawler retrieves the first 10 repositories from the search results as 𝑟𝑒𝑝𝑜2, leading to a total of 10*10=100 projects for evaluating soft forking.\
 Before getting feedback from stakeholders, we obtained IRB approval from our institution. As calling out stakeholders for violations of unethical behavior could potentially lead to similar ethical concerns in prior work [81], we choose to evaluate Etor by (1) manually inspecting the reported issues, and (2) reporting only the types of unethical behavior with high accuracy (based on our manual analysis). To avoid violating ethical principles as in the “hypocrite commits” incident, we explicitly mentioned in each reported issue that we are researchers conducting research on mining software repositories. To reduce author bias in the manual classification of TP/FPs, we ask for help from a non-author to independently label each issue.\
All experiments are conducted on a machine with Intel(R) Core (TM) i7-7500 CPU @2.7 GHz and 16 GB RAM.\
. We use Protégé 5.5.0 [71] to define the ontology model. Our crawler uses PyGitHub [24] for querying GitHub.5.1 RQ3: Number of detected issuesTable 3 summarizes the results of our evaluation. The “Type” column denotes the types of unethical behavior detected by Etor, whereas the second column is of the form 𝑥 / 𝑦 where 𝑥 represents the number of repositories/issues with the unethical behavior detected and 𝑦 denotes the total number of repositories/issues in our evaluation dataset. Overall, Etor has successfully detected at least one violation for all types of unethical behavior that we studied. As our evaluation dataset is different from the study dataset, and we have observed the occurrences of unethical behavior in both datasets, this indicates that different types of unethical behavior is prevalent in OSS projects. Table 3 also shows that “No license provided in public repository” is the most common types among the six types of detected issues. This means that a relatively high percentage of the evaluated repositories are missing license files (around 24% of the evaluated repositories if we exclude the false positives). For the issue-level detection, we observe that “No attribution to the author in code” and “Self-promotion” are the most common ones among all evaluated issues/PRs. This indicates that contributors of OSS projects tend to (1) forget to give credit to the author in their copied code snippets, or (2) promote their own repositories without mentioning they are contributors to the repositories.5.2 RQ4: Accuracy and Efficiency of Etor. To evaluate the effectiveness of Etor, two raters (one author, and one non-author who is an undergraduate CS student working as a part-time student assistant) independently inspect its output. Specifically, for each violation reported by Etor, each rater determines if the violation is a true positive (TP) or a false positive (FP). The initial Cohen’s Kappa was 0.82, which indicates a high level of agreement. The two raters then meet to resolve any disagreement to reach Cohen’s Kappa of 1.0. The “True positive” and “False positive” columns in Table 3 show the results for the inspection. On average, the TP rate is 74.8%, and the FP rate is 24.8%. For repository-level detection, although Etor can only detect a small number of violations for “Soft forking”, it can detect these unethical issues with high accuracy (0% FP rate). As we consider a repository a  only if all the contents of the two repositories are the same (100% similarity), this design decision may lead to fewer violations being found but increase the accuracy of its detection. In future, it is worthwhile to study the effect of the similarity threshold on the accuracy of its detection. For issue-level detection, Etor can detect S1 with reasonable accuracy (26% FP rate).\
. The “Time” column in Table 3 shows the average time taken to detect an unethical behavior. Overall, the average time to analyze a repository is 3.1–343.1 seconds and the average time taken to analyze an issue is 4.3–5.4 seconds. This indicates that Etor can detect a type of unethical behavior relatively fast. We also observe that “Soft forking” is the most time-consuming type to detect because Etor needs to check for code similarities for all source files within the repository.\
Reasons behind inaccurate detection. We manually inspect the reasons behind the FPs reported by Etor. Etor reports the highest FP rate for “Self-promotion”. Recall that Etor checks that a stakeholder St opens an issue/PR I at repository R1, and includes the other repository (R2) link (L). A true “Self-promotion” only occurs if St did not mention about being a contributor of R2. We need to manually verify this condition by reading the comments written in natural language. Hence, FPs may occur if (1) St mentioned that he or she is a contributor of R2 (e.g., “I am working on a project called the …” in comment [25]) or (2) St wanted to ask for suggestion in using R1 for R2 (e.g., “I’d like to try your … module in a nonmmdetection repo (…)” [31]).\
There are three main reasons for FPs in “No attribution to the author in code”: (1) no actual copying occurs but a link exists (e.g., the Stack Overflow link was mentioned as references [33]), (2) Etor checks the exact link and fails to detect if the citation uses the short link of Stack Overflow, and (3) Etor matches the exact GitHub user name with the Stack Overflow user name, and fails to detect if the user name is different (e.g., GitHub user name is devinrhode2 and Stack Overflow user name is Devin Rhode [30]). For “No license provided in public repository”, FPs occur because the repository (1) has a license file that is not in the main directory (e.g., LICENSE file in the inner folder [29]), (2) has a disclaimer in README.md (e.g., “This repository is for personal study and research purposes only. Please DO NOT USE IT FOR COMMERCIAL PURPOSES.” [34]), (3) is used for education purposes (we need to manually exclude repositories for the public schools where the license is not required), (4) has no source code or data, and (5) is under an organization license and no separate license is defined for the repository [32]. For “Uninformed license change”, FPs occur because the scenario where the repository has restored the old license should not be considered a violation (e.g., the stakeholder changed the license from “Apache License Version 2.0” to “GNU GENERAL PUBLIC LICENSE Version 3” on Feb 17, and he/she restored back to “Apache License Version 2.0” on Feb 18). For “Unmaintained Android Project with Paid Service,” we found one FP because the unmaintained project is a library that an app uses instead of the app itself but the app is actively maintained. (a new version is recently released).\
 Apart from manually labeling the unethical issues, we also obtained qualitative feedback by reporting them to stakeholders of OSS projects. To avoid spamming OSS developers with inaccurate results, we only reported the types of unethical behavior with >=80% TP rate in our manual analysis (i.e., ). For each of these reported types, we opened a GitHub issue to developers when both raters labeled it as TP. We excluded 39 issues because the project owners have disabled GitHub issues (this usually indicates that they do not accept contributions or bug reports [26]). For example, the repository [27] violates the “No license provided in public repository” rule but we cannot report this to the project owner as GitHub issues have been disabled. We also found 19 issues that were previously reported and fixed the issues before we file a bug report. In total, we have reported 392 issues, and received 83 replies (a response rate of 21.17%) from stakeholders. We carefully looked through all those responses and identified 68 (81.93%) replies as valid and 15 (18.07%) responses as invalid. Among these 68 valid replies, 39 (57.35%) have been fixed, and 29 (42.65%) have been accepted by the stakeholders of the OSS. An example valid feedback that we received is “Thank you very much for the warning. I have already added the license to the repos that didn’t have it.”. For the 15 responses that we considered as invalid, developers (1) directly deleted or closed our submitted issues without any explanations (7/15), (2) thought that the issue reporter is a software bot although we have created the issue manually and explicitly mentioned in the issue that we are researchers (5/15), (3) are not interested in getting any GitHub issues (e.g., claiming that the repository is personal) (2/15), and (4) explained that “Software is not open source but everyone or you can use my soft without license. thank you for support my soft” (1/15).6 DISCUSSION AND IMPLICATIONSWe discuss practical takeaways and suggestions below:\
Implications for stakeholders of OSS projects. By reading issues that stakeholders in OSS considered as “unethical behavior”, our study revealed that the types of unethical behavior in OSS projects are diverse (Finding 1), suggesting that stakeholders of OSS projects should be better educated to create awareness of the different types of unethical behavior when contributing to OSS projects to avoid violating ethical principles. Apart from general types of unethical behavior, our study also pinpoints six new types of unethical behavior in OSS projects (i.e., (S2) Soft forking, (S6) Uninformed license change, (S8) Self-promotion, (S9) Unmaintained project with paid service, (S11) Naming confusion, and (S12) Closing issue/PR without explanation). Some of them are related to the unique features of GitHub (e.g., “Soft forking” represents ethical concerns when forking, “Closing issue/PR without explanation” are related to closing GitHub issues/PRs, “Self-promotion” occurs due to the need to promote the popularity of one’s new repository, whereas “Unmaintained project with paid service” denotes the responsibility of an OSS project owner to actively maintain the project to support paid users). The identified new types call for considerations of the unique context of OSS projects when studying unethical behavior. Meanwhile, although most software development efforts focus on source code maintenance, our study urges OSS project owners to be responsible for the product names selection to avoid violating “Naming confusion”. As issues related to copyright and licensing are the most common ones (Finding 2), contributors of OSS projects should pay more attentions in giving appropriate credits, and selecting suitable software licenses when copying software artifacts or using library. Meanwhile, although source code is the still most common affected software artifacts (Finding 4), our study urges OSS stakeholders to be responsible for various types of software artifacts (Finding 3) to avoid violating ethical principles when uploading them to GitHub.\
Implications for researchers and tool designers. As many of the identified types of unethical behavior (Finding 1) are ethical issues that frequently occur in daily life, our study provides empirical evidence that there exists some overlaps between the types that occur under general setting (e.g., “Plagiarism” and “Offensive language”) and those that are deemed as unethical behavior by stakeholders in OSS projects. Indeed, the prevalence of plagiarism is inline with prior study which reported the prevalence of the code borrowing practices in GitHub [55]. Due to the diverse types of unethical behavior, future empirical research should advance beyond the general types of unethical behavior.\
While existing work mostly focus on license incompatibility [52, 60, 82], our study found new issues related to licensing. e.g., “Uninformed license change”. As these issues still occur frequently (Finding 2) and our study identified new types of issues, our study provides empirical motivations for improving techniques related to copyright and software licenses. For the newly identified types of unethical behavior, we foresee a huge potential for future research direction in: (1) conducting more in-depth study in the motivations and the common solutions behind each type of unethical behavior, and (2) introducing automated techniques that can detect and possibly resolve these issues. We believe that our  of 316 GitHub issues and  that uses software artifacts and data available in GitHub API lay the foundation for future approaches on automated detection of unethical behavior. Although source code is still the most common type of affected software artifacts (Finding 4), other artifacts in natural language (e.g., PR/Issue comments, product names, and website) are also common in our study (Finding 3). A promising research direction is to apply natural language processing techniques to accurately detect affected software artifacts in natural language. For example, techniques can be designed to automatically extract and recommend descriptive yet non-conflicting names (e.g., package names) to avoid “Naming confusion”. Another future direction is to design techniques that can automatically identify disclaimer-like statements to accurately detect “Closing issue/PR without explanation” (to detect the explanation for the PR/issue), and “Self-promotion” (to extract statement where the stakeholder has mentioned being a contributor).\
Challenges in automated detection of unethical behavior. To provide guidelines for future research on the automated detection of unethical behavior, we discuss several challenges identified in our study and evaluation:\
• As shown in our study in Section 3.2, the types of artifacts affected by the unethical behavior are too diverse. An accurate detection technique needs to support analysis of various types of artifacts, including source code, data, and websites.\
• Within GitHub, we notice that discussion and announcement in GitHub spread across multiple web pages (issues, PRs, wikis, discussions, and commit logs). With the rapid growth of different types of web pages in GitHub, it poses additional challenges for automated approaches to exhaustively analyze all relevant web pages.\
• Some discussions of unethical behavior occur outside of GitHub (e.g., personal emails, slack channel). For example, for “Self-promotion”, we cannot check whether the stakeholder has communicated with the developers in advance through emails. Without complete information about the discussion, the detection is bound to be inaccurate.\
• The scope for the detection can be too broad for some types of unethical behavior (e.g., “Naming confusion”). Without a predefined scope of detection (package name collision versus app name collision), we cannot accurately detect the behavior.\
• There exist ambiguities for certain unethical behavior, which makes it difficult even for human beings to reach consensus (e.g., whether the language used is offensive). In this case, an automated tool can present all relevant information to help stakeholders in making more grounded ethical decisions.(1) Hsu Myat Win, Southern University of Science and Technology, China (11960003@mail.sustech.edu.cn);(2) Haibo Wang, Southern University of Science and Technology, China (wanghb2020@mail.sustech.edu.cn);(3) Shin Hwei Tan, a corresponding author from Southern University of Science and Technology, China (tansh3@sustech.edu.cn).]]></content:encoded></item><item><title>Are Large Language Models the Future of Game State Simulation?</title><link>https://hackernoon.com/are-large-language-models-the-future-of-game-state-simulation?source=rss</link><author>Writings, Papers and Blogs on Text Models</author><category>tech</category><pubDate>Wed, 24 Sep 2025 09:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called BYTESIZED32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM’s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.Simulating the world is crucial for studying and understanding it. In many cases, however, the breadth and depth of available simulations are limited by the fact that their implementation requires extensive work from a team of human experts over weeks or months. Recent advances in large language models (LLMs) have pointed towards an alternate approach by leveraging the huge amount of knowledge contained in their pre-training datasets. But are they ready to be used directly as simulators?\
We examine this question in the domain of textbased games, which naturally express the environment and its dynamics in natural language and have long been used as part of advances in decision making processes (Côté et al., 2018; Fan et al., 2020; Urbanek et al., 2019; Shridhar et al., 2020; Hausknecht et al., 2020; Jansen, 2022; Wang et al.,2023), information extraction (Ammanabrolu and Hausknecht, 2020; Adhikari et al., 2020), and artificial reasoning (Wang et al., 2022).\
Broadly speaking, there are two ways to leverage LLMs in the context of world modeling and simulation. The first is : a number of efforts use language models to generate code in a symbolic representation that allows for formal planning or inference (Liu et al., 2023; Nottingham et al., 2023; Wong et al., 2023; Tang et al., 2024). REASONING VIA PLANNING (RAP) (Hao et al., 2023) is one such approach – it constructs a world model using LLM priors and then uses a dedicated planning algorithm to decide on agent policies (LLMs themselves continue to struggle to act directly as planners (Valmeekam et al., 2023)). Similarly, BYTESIZED32 (Wang et al., 2023) tasks LLMs with instantiating simulations of scientific reasoning concepts in the form of large PYTHON programs. These efforts are in contrast to the second, and comparatively less studied, approach of . For instance, AI-DUNGEON represents a game world purely through the generated output of a language model, with inconsistent results (Walton, 2020). In this work, we provide the first quantitative analysis of the abilities of LLMs to directly simulate virtual environments. We make use of structured representations in the JSON schema as a scaffold that both improves simulation accuracy and allows for us to directly probe the LLM’s abilities across a variety of conditions.\
In a systematic analysis of GPT-4 (Achiam et al., 2023), we find that LLMs broadly fail to capture state transitions not directly related to agent actions, as well as transitions that require arithmetic, common-sense, or scientific reasoning. Across a variety of conditions, model accuracy does not exceed 59.9% for transitions in which a non-trivial change in the world state occurs. These results suggest that, while promising and useful for downstream tasks, LLMs are not yet ready to act as reliable world simulators without further innovation.[1][1] Code and data are available at https://github. com/cognitiveailab/GPT-simulator.]]></content:encoded></item><item><title>From Pilot to Policy: RYT Gathers Global Leaders at TOKEN2049</title><link>https://hackernoon.com/from-pilot-to-policy-ryt-gathers-global-leaders-at-token2049?source=rss</link><author>Stewart Rogers</author><category>tech</category><pubDate>Wed, 24 Sep 2025 08:51:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Showcasing Governmental Blockchain AdoptionBeyond the daily noise of market speculation, a quieter, more profound transformation is taking place: blockchain technology is graduating from experimental pilots to essential public infrastructure. This critical shift will be the main focus at , a full-day, exclusive event hosted by the Layer 1 blockchain RYT during the upcoming TOKEN2049 week in Singapore.The invite-only gathering, scheduled for Tuesday, September 30, will convene senior government officials, institutional pioneers, and leaders from traditional finance (TradFi). The goal is to move past hypotheticals and into the practical realities of deploying blockchain solutions for national-scale digital identity, stablecoins, and compliance-first payment systems."Blockchain is entering a new era, evolving from pilot projects to critical infrastructure for governments and institutions," said Jeff Mahony, Co-founder and Chief Architect of RYT. "RYT was designed from the ground up to meet these standards, with security, compliance, and scalability at its core. At TOKEN2049, we’re convening leaders to discuss how blockchain is being built into national systems for identity, payments, and financial services.”Global trends underscore the urgency behind this transition. Over 130 countries, representing 98% of the world's GDP, are actively exploring digital currencies. From Estonia's well-established e-governance framework to Singapore's Project Guardian for asset tokenization, the migration to on-chain infrastructure is clearly accelerating.Event Agenda and SpeakersThe day's discussions will feature candid insights from experts who are actively building and regulating these next-generation systems.Session 1: The Institutional Perspective (1:00 PM – 2:00 PM)This panel will explore blockchain adoption from the private sector and technology side. Mike Slatkin, CMO of RYT – Co-Founder, RYT \n  – Co-Founder of Polygon Technologies and founder of Avail \n  – Partner, Cybersecurity & Compliance at HackenSession 2: The Government Perspective (3:00 PM – 4:00 PM)This session will focus on the public sector's role in leveraging blockchain for national infrastructure. Mike Slatkin, CMO of RYTSpeakers: \n Steve Durbin – Co-Founder, RYT \n  – Blockchain Accelerator Lead, UNDP (United Nations Development Programme) \n Maj. General (R) Fida Hussain Malik – Special Advisor, International Parliamentarians' Congress – Commissioner, Securities & Exchange Commission of Pakistan (SECP)The event will be held in Clarke Quay, just a short MRT ride from the main TOKEN2049 venue at MBS Bayfront.Capacity is limited, and attendance is by invitation only. Interested parties can request an invitation here. Confirmed media will be able to collect passes and arrange interview slots on-site.This story originally appeared on Dataconomy and is republished with permission.]]></content:encoded></item><item><title>The WEF Wants to Put a Market Price on Nature</title><link>https://hackernoon.com/the-wef-wants-to-put-a-market-price-on-nature?source=rss</link><author>The Sociable</author><category>tech</category><pubDate>Wed, 24 Sep 2025 07:00:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The WEF claims that Larry Fink & Andre Hoffmann’s work on the board ‘do not represent any personal or professional interests,’ but they stand everything to gain: perspectiveWith billionaires Larry Fink and Andre Hoffmann as the new co-chairs, the World Economic Forum (WEF) publishes a 50-page blueprint on how to monetize everything in nature.The WEF’s latest insight report “Finance Solutions for Natures: Pathways to Returns and Outcomes” provides “stakeholders” with dozens of financial solutions for monetizing everything in nature.Nature pricing, biodiversity crediting schemes, natural asset companies, debt-for-nature swaps, and so much more are all packed into this agenda to overhaul the global financial system with nature-based activities.“The landscape of nature finance is rapidly evolving. From sovereign debt instruments and blended capital platforms to biodiversity credits and emerging asset classes, a growing range of mechanisms is being deployed to fund, finance and de-risk nature-positive action”WEF, “Finance Solutions for Natures: Pathways to Returns and Outcomes,” September 2025\
The WEF leadership page says that in their work on the board of trustees, “members do not represent any personal or professional interests.”However, the target audiences for latest WEF insight report are “, banks, , and development actors” — the very business interests that Hoffmann and Fink represent.WEF interim co-chairs Larry Fink and Andre Hoffmann stand everything to gain in their business dealings should the documentation, monetization, and tokenization of everything in nature ever come to full fruition.And they are well on their way.Fink’s BlackRock manages over $11 trillion in assets, and last year BlackRock said it was “conducting proprietary research on natural capital investment signals, identifying companies poised for financial advantage in avoiding nature-related risks or leaning into opportunities. Those signals cover themes such as energy management, water management, waste management and biodiversity – and can feed into portfolio construction or support custom exposures.”Hoffmann is also a key player in a whole host of so-called green financing initiatives, including biodiversity crediting schemes, through his various roles as founder, president, and chairman at several companies and NGOs such as: Innovate 4 Nature — the “accelerator for nature-positive solutions” and Systemiq — the “system change company” established specifically to advance UN Agenda 2030.“The economy depends on natural resources. Their value derives not only from their use as direct inputs to production – such as timber for construction – but also for their benefits to society like living trees that help clean the air. Economists use the term “natural capital” to refer to the total value that natural resources provide to the economy and to people”BlackRock, “Capital at risk: nature through an investment lens,” August 2024“Debt-for-nature swaps [DNS] are a financial mechanism that allow countries to restructure bilateral or multilateral debt in exchange for commitments to fund local conservation and restoration. They are also known as ‘debt-for-nature conversion'”WEF, “Finance Solutions for Natures: Pathways to Returns and Outcomes,” September 2025\
Is your country millions, billions, or trillions in debt? No problem!With debt-for-nature swaps, you can restructure your nation’s debt just by letting somebody else come in and take control of your natural resources under the guise of conservation and restoration, but what they’ll really be doing is forcing you to “take out private insurance policies to‘mitigate the financial impact of natural disasters‘  ‘,'” as investigative journalists Whitney Webb and Mark Goodwin report in .Don’t have any money, but want to create value out of thin air, water, soil, or trees? You can set up natural asset companies that can “convert the full economic value of nature into financial flows via equity models.”\
Want to help asset managers, bankers, and hedge fund execs get extremely rich while leaving you with only a tiny fraction? Go ahead and get involved in a Payment for Environmental Services (PES) scheme, where financial incentives are provided to individuals or communities in exchange for maintaining or restoring ecosystem services, like carbon sequestration or biodiversity conservation.And if you’re compliant with their rules, you can be rewarded by producing “positive nature and biodiversity outcomes (e.g. species, ecosystems and natural habitats) through the creation and sale of either land or ocean-based biodiversity units over a fixed period” with biodiversity credits, aka “environmental credits.”Prefer to be left alone and live on the property that you worked hard for all your life? You better be compliant with all the environmental regulations that are coming in the name of preserving biodiversity, so that the $44 trillion of economic value generated by nature doesn’t diminish.“Environmental credits are verified units of positive environmental outcomes, including biodiversity, water, carbon and nutrient credits. Though developed independently, projects increasingly blend credits via stacking, bundling or stapling”WEF, “Finance Solutions for Natures: Pathways to Returns and Outcomes,” September 2025“Nature is rapidly emerging as a strategic investment frontier and more institutional capital is flowing into new business models and projects”WEF, “Finance Solutions for Natures: Pathways to Returns and Outcomes,” September 2025\
In keeping with the own self-interests of the co-chairs and their business relations, the report highlights “10 priority financial solutions” for these stakeholders to implement:Sustainability-linked bonds (SLBs):Commercial bonds tying coupon rates to nature-related targets for corporates or governments.Thematic (or use-of-proceeds) bonds:Bonds with proceeds earmarked for nature projects. Scaling-up requires clearer guidance and aggregation to improve outcomes for issuers and investors.Sustainability-linked loans (SLLs):Flexible debt, linking interest rates to nature-related targets. SLLs need simpler verification, standardized metrics and stronger triggers to drive nature-positive lending.Thematic (or use-of-proceeds) loans:Loans for specific nature-related projects. Greater clarity on taxonomies and aggregation is needed to enhance capital flows.:Funds investing in nature-positive outcomes, often accepting higher risk or longer pathways to returns.Natural asset companies (NACs):Publicly and privately listed companies that convert the full economic value of nature into financial flows via equity models. NACs hold significant potential but need more transactions for price discovery and replicable investment blueprints.:Tradeable certificates for verified environmental benefits, used in compliance or voluntary markets.Debt-for-nature swaps (DNS):Mechanisms to restructure sovereign debt in exchange for conservation or restoration commitments, with investable components including bonds and loans.Payments for ecosystem services (PES):Contracts rewarding conservation for specific ecosystem services, driven by the public sector. Private sector schemes require longer contracts, aggregation and supply chain integration to scale up.Internal nature pricing (INP):Unexplored, voluntary shadow pricing or fee-based tools to incentivize nature-positive performance in companies or across investment portfolios, similar to internal carbon pricing (ICP).“While some components of nature – such as food, timber and ecotourism are priced and traded in global markets, the value of many critical ecosystem services remains undervalued.* \n “Carbon sequestration, water filtration, flood protection and pollination are often treated as ‘free’ inputs, despite underpinning our economies and societies”*WEF, “Finance Solutions for Natures: Pathways to Returns and Outcomes,” September 2025“The natural capital approach extends the economic concept of capital to the environment, conceptualizing stocks of natural resources as conventional goods worth restoring, maintaining and enhancing for their productive flows.* \n  \n “This approach includes both accounting – embedding nature in national and corporate balance sheets – and valuation – pricing nature’s contributions into cost-benefit and investment analysis”*WEF, “Finance Solutions for Natures: Pathways to Returns and Outcomes,” September 2025\
Putting prices on water, air, and soil is a hot topic among globalists at the UN, the G20, the World Economic Forum (WEF), and the COP meetings.At the WEF Annual Meeting in Davos this year Singapore’s President Tharman Shanmugaratnam said that water credits and biodiversity credits should be “stapled” on to carbon credits.The year prior, at the 2024 WEF Annual Meeting of the New Champions, aka “Summer Davos” meeting in communist China, University of Cambridge Institute for Sustainability Leadership CEO Lindsay Hooper told the panel on “Understanding Nature’s Ledger” that every part of the economy depends on nature, and that in order to protect natural systems, one solution would be to “bring nature onto the balance sheet.”In addition to putting “nature on the balance sheet,” another proposal coming at the end of the panel discussion suggested putting a tax on natural systems like water in the same vein as carbon taxes.With putting prices on nature comes tokenization and derivatives.“Carbon, we already figured out, and carbon is moving very quickly into a system where it’s going to be very close to a currency, basically being able to take a ton of absorbed or sequestered carbon and being able to create a forward-pricing curve, with financial service architecture, documentation,” said Sheren.And with carbon being close to a currency, “There are going to be derivatives.”Now, under the newfound leadership of Fink and Hoffmann, whose personal business dealings stand everything to gain, the WEF is plowing full-steam ahead with the globalist agenda to monitor and monetize everything in nature, including the air we breathe, the water we drink, and the very earth that we walk upon.:::info
Tim Hinchliffe, Editor, The Sociable]]></content:encoded></item><item><title>NASA Plans Crewed Moon Mission For February</title><link>https://science.slashdot.org/story/25/09/24/0335219/nasa-plans-crewed-moon-mission-for-february?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[NASA aims to launch its first crewed lunar mission in over 50 years, as early as February. The 10-day Artemis II mission will send four astronauts on a lunar flyby to test systems, paving the way for future Moon landings under the Artemis program. The BBC reports: Lakiesha Hawkins, Nasa's acting deputy associate administrator said it would be an important moment in the human exploration of space. "We together have a front row seat to history," she told a news conference this afternoon. "The launch window could open as early as the fifth of February, but we want to emphasize that safety is our top priority." Artemis Launch Director, Charlie Blackwell-Thompson explained that the powerful rocket system built to take the astronauts to the Moon, the Space Launch System (SLS) was "pretty much stacked and ready to go." All that remained was to complete the crew capsule, called Orion, connected to SLS and to complete ground tests.
 
The Artemis II launch will see four astronauts go on a ten-day round trip to the Moon and back to the Earth. The astronauts, Reid Wiseman, Victor Glover, and Christina Koch, of Nasa and Jeremy Hansen of the Canadian Space Agency, will not land on the Moon, though they will be the first crew to travel beyond low Earth orbit since Apollo 17 in 1972. The lead Artemis II flight director, Jeff Radigan explained that the crew would be flying further into space than anyone had been before. "They're going at least 5,000 nautical miles (9,200Km) past the Moon, which is much higher than previous missions have gone," he told reporters. Further reading: NASA Introduces 10 New Astronaut Candidates]]></content:encoded></item><item><title>How Generative AI Can Be Used in Cybersecurity</title><link>https://hackernoon.com/how-generative-ai-can-be-used-in-cybersecurity?source=rss</link><author>Sekurno</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:53:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Generative AI has entered cybersecurity with full force, and like every powerful technology, it comes with its pros and cons. On one side, attackers are already experimenting with AI to generate malware, craft phishing campaigns, and create deepfakes that erode trust. On the other hand, defenders are beginning to use AI to scale penetration testing, accelerate application security, and reduce the pain of compliance.The stakes are high. A recent ForeScout Vedere Labs 2025 report showed zero-day exploits have risen 46% year over year — a clear signal that attackers are accelerating. At the same time, Gartner predicts that by 2028, 70% of enterprises will adopt AI for security operations.The reality sits in between: AI is already changing penetration testing, application security, and compliance — but it’s not a replacement for human expertise. Instead, it’s a force multiplier, reshaping how quickly and effectively security teams can discover weaknesses, meet regulatory obligations, and prepare for adversaries that are also harnessing AI.The Dual-Use Nature of Generative AIGenerative AI in cybersecurity is best understood as a dual-use technology — it amplifies both attack and defense capabilities.AI lowers barriers by generating sophisticated phishing emails, fake personas, malicious code, and even automated exploit chains. Tools like CAI (Cognitive Autonomous Intelligence) demonstrate how autonomous agents can be tasked with scanning, exploiting, and pivoting through systems — blurring the line between proof-of-concept research and adversary capability. BlackMamba (an AI-generated polymorphic keylogger) and WormGPT (marketed on underground forums as “ChatGPT for cybercrime”) have already shown what’s possible.AI provides scale, speed, and intelligence. Beyond SOC copilots, AI is being embedded directly into the software development lifecycle (SDLC) via AI security code reviewers and AI-powered vulnerability scanners. GitHub Copilot (with secure coding checks), CodiumAI, and Snyk Code AI catch issues earlier, reducing downstream remediation costs. Microsoft’s Security Copilot helps analysts triage alerts and accelerate investigations.This duality is why many experts warn of an “AI arms race” between security teams and cybercriminals — where speed, automation, and adaptability may decide outcomes.Offensive Security & Penetration TestingPenetration testing has traditionally been time-intensive, relying on skilled specialists to probe for vulnerabilities in networks, applications, and infrastructure. AI is shifting the tempo.Large language models and autonomous agents can now:Generate custom exploits and payloads on demand.Mimic phishing and social engineering campaigns at scale.Run fuzzing routines to simulate zero-day vulnerabilities before attackers do.A striking proof point is XBOW, the autonomous AI pentester that recently climbed to #1 on HackerOne’s U.S. leaderboard. In controlled benchmarks, XBOW solved 88 out of 104 challenges in just 28 minutes — a task that took a seasoned human tester over 40 hours. In live programs, it has already submitted over a thousand vulnerability reports, including a zero-day in Palo Alto’s GlobalProtect VPN.AutoSploit, an early attempt at AI-assisted exploitation pairing Shodan with Metasploit.Bug bounty hunters using LLMs as copilots for reconnaissance and payload generation.MITRE ATLAS, a framework mapping how adversaries might use AI in cyberattacks.Yet despite its speed and precision, tools like XBOW still require human oversight. Automated results must be validated, prioritized, and — critically — mapped to regulatory and business risk. Without that layer, organizations risk drowning in noise or overlooking vulnerabilities that matter most for compliance and trust.This is the shape of penetration testing to come: faster, AI-augmented discovery coupled with expert judgment to make results meaningful for businesses under pressure from regulators and partners.How Can Generative AI Be Used in Application SecurityApplication security (AppSec) is another area seeing rapid AI adoption. The software supply chain has grown too vast and complex for purely manual testing, and generative AI is stepping in as a copilot.Key applications include:Code analysis and secure SDLC copilots: GitHub Copilot and CodiumAI spot insecure patterns before code reaches production.AI-powered security scanners: Snyk Code AI and ShiftLeft Scan continuously crawl apps and APIs, flagging vulnerabilities in real time.Auto-patching suggestions: GitHub now generates AI-driven pull requests suggesting secure fixes.Testing LLM-based apps: The rise of AI-powered chatbots introduces new risks. Prompt injection attacks are already in the wild. OWASP responded with the first Top 10 for LLM Applications in 2023.API fuzzing and zero-day simulations: Tools like Peach Fuzzer and AI-driven agents autonomously generate malformed inputs at scale.The promise is efficiency — but the challenge is trust. An AI-generated patch may fix one issue while creating another. That’s why AI is best deployed as an accelerator in AppSec, with humans validating its findings and ensuring fixes align with compliance frameworks like ISO 27001, HIPAA, or FDA MDR/IVDR for medical software.Beyond pentesting and AppSec, AI is finding a role in the often overlooked world of compliance. For companies in healthtech, biotech, or fintech, compliance can make or break growth — and AI is beginning to reduce the heavy lift.Emerging applications include:Mapping vulnerabilities to controls: Linking pentest findings directly to FDA SPDF or ISO clauses.Generating audit-ready reports: Platforms like Vendict, Scrut, and Thoropass use AI to translate security posture into regulator-friendly documentation.This is particularly powerful in genomics or diagnostics, where startups face heavy regulatory burden and need to show both security and compliance maturity to win partnerships or funding.The use of AI in cybersecurity isn’t hypothetical — it’s playing out across industries today:Emerging Risks of Generative AI in CybersecurityWith opportunity comes risk. AI introduces new attack vectors and amplifies existing ones:Bias and privacy: Training models on sensitive datasets risks compliance violations under GDPR.Over-reliance: Treating AI outputs as gospel risks blind spots and false positives.Hallucinations: Studies show AI copilots fabricate vulnerabilities or fixes.Dependency risk: SaaS outages or API shifts in AI platforms can disrupt pipelines.Best Practice Strategy for Secure AI AdoptionTo adopt AI in pentesting, AppSec, or compliance responsibly, organizations should:So, how can generative AI be used in cybersecurity? It won’t replace penetration testers, application security engineers, or compliance leads. But it will accelerate their work, expand their coverage, and reshape how vulnerabilities are found and reported.The winners won’t be those who adopt AI blindly, nor those who ignore it. They’ll be the organizations that harness AI as a trusted copilot — combining speed with human judgment, technical depth with regulatory alignment, and automation with accountability.By 2030, AI-driven pentesting and compliance automation may become table stakes. The deciding factor will not be whether companies use AI, but how responsibly, strategically, and securely they use it — especially in regulated sectors where compliance and trust are non-negotiable.Further Reading & References]]></content:encoded></item><item><title>How to Teach Sales Reps to Pitch Cybersecurity in 1 Hour—Even Without Tech Skills</title><link>https://hackernoon.com/how-to-teach-sales-reps-to-pitch-cybersecurity-in-1-houreven-without-tech-skills?source=rss</link><author>Florian Henrion</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:26:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When my sales team thought “pentest” was a typo, I realized the real challenge: bridging the gap between sales and cybersecurity.]]></content:encoded></item><item><title>Why Small Models Matter in a Network of Experts Era</title><link>https://hackernoon.com/why-small-models-matter-in-a-network-of-experts-era?source=rss</link><author>Andrew Schwabe</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:14:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Innovation is about dreaming big. Dream next-level. Challenge the norm, or even the emergent. 2025 is exciting for AI, but it is still all transitional technology. Mixture of Experts (MoE) and Model Context Protocol (MCP) are great, and we see how well it works, especially with the quality and disruption of models like DeepSeek, Kimi K2, and GLM 4.5.Lets talk about a next-gen expansion of the concepts that are working well today. By applying the concepts of MoE and applying them in an API-connected landscape, we can end up with the concept of a Network of Experts. Both large models and Small Language Models (SLM) can be networked and orchestrated by pipelines, workflows, automation frameworks and more, starting out with existing infrastructure and technology for trust.Ok, like anybody painting a tapestry of “what-ifs” and “wish-we-had’s” — there are a lot of hurdles, and plenty of potential benefits. Right now, we have a LLM weapons war going on, where companies (and countries) are vying for domination. The  to control the AI consumer market won’t last long in its current form, and we need to find a way to utilize the big players while also enabling SMEs to play in the same economy. We need to create a better way to encourage innovation, not silly marketing stunts and break-neck releases meant to drive social attention and just confuse end users. Innovation needs to focus on what is missing from AI, moves toward cognition instead of mega scaling yesterday’s word sequence models, and instead of narrowing choices of which individual vendor to use for everything.Small models (language, and cognitive) with strong domain expertise can participate in a Network of Experts (NoE) ecosystem as first class citizens, enabling a robust economy of choice, where the whole industry can get back to innovation in verticals. APIs as Gateways to expertise play a crucial role in exposing a Network of Experts architecture, enabling seamless integration of diverse expert models. For instance, legal or medical experts can be accessed via APIs, allowing developers to incorporate specialized knowledge into applications without reinventing the wheel. API use could be metered, much like utility APIs are already tooled to do today.  This is not altogether different than the way some commercial amazon instances are run, though I envision that could be a wider use, not limited to just one marketplace.Small models thrive in this ecosystem by focusing on specific domains. Their specialization reduces resource usage and enhances accuracy, as each model excels within its area of expertise.  Small cognitive models may rely or work with the same domain’s small language models to offer vertical experitise.Challenges: Latency and SecurityLatency emerges as a challenge when multiple experts are accessed via APIs. Solutions include caching, parallel processing, and optimized API calls to mitigate delays. Services for cross-network logging, debugging, tracing would all be needed. Sensitive data exposure risks would likely drive healthy governance and controls to keep up with compliance regulations. A distributed network of models will  perform as well as a self contained model. The point is that quality and integrity of the entirety of a model scope shouldn’t be left in the control of a single organization.Building trust involves validating each expert’s reliability and quality of service, through routine testing, like Llama’s leaderboards, strong network availability configurations, and perhaps even reputation frameworks. These mechanisms ensure that the network remains credible and dependable, yet adaptable in case of inavailability.This approach finds application in personalized healthcare, tailored legal or financial advice, and domain-specific areas too numerous to list. Each scenario benefits from specialized expertise delivered efficiently via SLM vendors.The Network of Experts concept offers a promising future with enhanced accuracy, trust, and personalized solutions. While challenges like latency and security persist, ongoing innovations hold the promise of overcoming these hurdles, paving the way for more effective AI applications.What do you think? What is missing in this approach? What new values could this bring to market? We have a great opportunity for a new boom in innovation with this model. I have personally already worked with these concepts for a POC app, and would be happy to discuss further.]]></content:encoded></item><item><title>How to Stop Getting Spam Emails—The Complete Guide to Removing Your Personal Information Online</title><link>https://hackernoon.com/how-to-stop-getting-spam-emailsthe-complete-guide-to-removing-your-personal-information-online?source=rss</link><author>Yorba</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:14:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Another ding in your email and there you have it: a flood of spam messages promising get-rich-quick schemes or exclusive offers on products you stopped purchasing years ago. So you go through the whole rigamarole of deleting: you mark them as spam, move them to junk, click the unsubscribe link, but they keep coming. The thing is that your email address is being bought and sold across dozens of data broker websites right now. These companies scrape public records, social profiles, and other methods to build profiles about you (complete with your email, phone number, address, and more). Then they ship this out and sell it to marketers, scammers, and whoever else is willing to pay for it.But there is a way to fight back, and we’re going to walk you through exactly how to remove your personal information from the major data broker sites and reduce the spam in your inbox.P.S. There’s a long way and a short way. The long way is, erm, painful. The short way requires help from platforms like , who find and delete accounts you forgot about. This won’t get you OFF data broker sites, but it will clean up the number of accounts and personal details you have living on the web. That’ll compound exponentially. The economy hiding in your personal dataBefore we get too deep, let’s go back a bit and talk about wtf is happening. Every time you sign up for something, purchase a product, or browse the web, you’re dropping digital breadcrumbs. Data brokers like Whitepages, True People Search, and Been Verified have sophisticated data collection systems that harvest your info. Here’s a quick look into These companies aren’t manually going through the web looking for data, they’re using the worst kind of robot (web scraping bots) that  like gov databases, court records, property transactions and all the rest. So whenever you buy a house or register to vote, that information is captured and populated into their system.You think your  are private, but all those connections, check-ins, and images are being analyzed for metadata. Even if you have privacy settings turned on, a lot of brokers can infer relationships, locations, or interests from publicly available info. The TL;DR here is that your mom was right. Whatever you put online stays online. When , data brokers often soak up that info through nefarious methods and integrate it into their systems. That being said, they’re not often housing or selling your passwords or banking data, but the same idea applies to scammers.The long and short of it is your data is floating around on the internet and it’s being picked up, packaged, and sold. Once they have it, they do a few things to make more money:They categorize your data based on targeted categories. So if you just bought a house in Spain and you refurbished your Mercedes, you’d be labeled as a high-income professional interested in investments. If you spend most of your money on supplements, you might be categorized as a health-conscious millennial.Marketing companies, scammers, and other businesses buy these lists. Brokers sell these lists for profit and companies might pay $100-$500 per thousand email addresses, depending on how valuable the targeting is.But hold on, one quick caveat. You might be reading this and thinking wait…isn’t it illegal for brands and people to buy your data and send outreach campaigns that you haven’t signed up for? It’s complex. In the U.S., businesses don’t need your consent to collect or sell your info. Unless you explicitly opt of it, you’re opted in. The CAN-SPAM Act doesn’t require explicit consent before sending commercial emails, but they are required to give you an easy way to unsubscribe and they must honor your opt-out requests within 10 business days. But let’s be real. How often does THAT really happen?So anyway, once you’re there, you get a ton of emails you don’t want, and then you try to solve spam with a few common approaches like setting up email filters, clicking unsubscribe links, or even going to great lengths like changing your email address.But here are some  (without having to change your identity). Option 1 - The manual wayStep 1: Target major data brokersThe first step to cleaning up your private data is to target the main players that are collecting your data in the first place. Here’s a few key ones you might want to explore:: Paste the URL of your profile and click “Submit Request”: Phone number or email is required to submit the form: Search for yourself, click on your listing, then scroll down to "Record Opt-Out": Email verification required: 48-72 hours for removal: Fill out the opt-out form with your details: Email verification required: 3 business days: Search for your record, select it, provide email and reason for removal: Email verification required: Fill out the opt-out form with your details: Email verification required: 3 business days: Paste the URL of the listing you want to remove. Enter your email and follow the prompts: Email verification required: Visit their Suppression Center: Fill out the suppression form with your details: Email verification required: 2-3 business daysStep 2: Document EverythingConfirmation emails receivedScreenshots of removal confirmationsStep 3: Follow Up and MonitorData brokers sometimes "forget" to honor opt-out requests, or your information gets re-added from new sources. Set calendar reminders to check these sites every 3-6 months and repeat the process if necessary.Step 4: Expand Your EffortsThe sites listed above are just the tip of the iceberg. There are hundreds of smaller data brokers operating online. Search for your name and email address regularly to discover new listings, and always opt out when you find them.While this manual approach can work, it’s annoying and takes tons of time. Option 2 - With a little help from your friendsInstead of trying to do it all yourself, you can partner with companies like Yorba to find accounts associated with your email address and act on your behalf to protect your data. It’s not going to necessarily remove your data from every single data broker, but you’ll get:Fewer accounts (which means less personal data moving through the web)A cleaner inbox (which means you’ll have fewer headaches)More control over your digital footprint (which means you’ll have fewer data breaches and a cleaner digital life. Ahh). The thing about your data is that it’s always floating around the internet. Every time you sign up for a service, your data is at risk. Every time you join a new app, your data is at risk.And we’re not trying to scare you, but the fact is that the less data you have out there, the better.\n So while you can–and should–delete your info from data broker sites, you can also take more control over the accounts you currently have (and prevent that data from being sold later on).If you need help, check out yorba.app/register and sign up for free. Premium users can access the personal deletion agents. ]]></content:encoded></item><item><title>How to Use Slack Incoming and Outgoing Webhooks for Real-Time AI Agents</title><link>https://hackernoon.com/how-to-use-slack-incoming-and-outgoing-webhooks-for-real-time-ai-agents?source=rss</link><author>MattLeads</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:13:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Now our  is  and ready for deployment, let’s expand its capabilities by integrating a new input source. Slack is an excellent choice for this.Imagine a scenario where we want to receive real-time notifications for new messages in specific Slack channels. This allows us to process these messages using  (LLMs) to generate summaries or draft replies. This new functionality will work in tandem with our existing , which continues to poll mailboxes.This integration will be a cornerstone of our Proactive Agent system. By simultaneously monitoring Slack and email, our application becomes a centralized hub for communications, ensuring no message is missed and every query is handled promptly. This dual-source approach demonstrates the power and flexibility of a well-architected system, combining both traditional and modern communication platforms for a comprehensive solution.To get notifications about new Slack messages, we can use outgoing . A Webhook is a user-defined HTTP callback that is triggered by an event in a source system. When the event occurs, the source system makes an HTTP/HTTPS request to the URL configured in the .In the context of Slack, an outgoing Webhook is configured for a specific channel or workspace. When a message is posted or event achieved that meets a certain criteria (e.g., a specific keyword, pattern), Slack sends an HTTP POST request to a designated URL. This URL is the endpoint of our , which then processes the incoming data.This mechanism is highly efficient because it’s . Instead of our application constantly polling Slack for new messages (which is resource-intensive and inefficient), Slack proactively “pushes” the information to us as soon as an event happens. This is the foundation for building a reactive and intelligent system. Get messages as they happen, enabling immediate action. Our application doesn’t need to poll Slack, saving API calls and server resources. Webhooks are a standard and straightforward way to connect different services.This approach is crucial for building a  that responds instantly to new communications, whether they originate from Slack or other sources like email.Yes, Slack has both incoming and outgoing webhooks. These are two distinct mechanisms that serve different purposes in a communication workflow.What they are: This is the most common type of webhook in Slack. They allow external applications to send (push) messages into a Slack channel.Slack provides a unique URL. Your application makes an HTTP POST request to this URL, sending a JSON payload with the message text and formatting.Incoming Webhooks used for notifications, alerts, and automated messages. For example, to send notifications about new GitHub commits, deployment statuses, or messages from our  back into Slack.This is a legacy mechanism that allows Slack to send (push) data to an external application when a specific event occurs in a channel.You configure an outgoing webhook by specifying your application’s URL and trigger words or events. When a user in a Slack channel posts a message containing one of these words, Slack makes an HTTP POST request to your application’s URL.Outgoing Webhooks used for creating  and automations that react to specific commands or keywords.In our scenario, we set up an outgoing webhook with a trigger like summary so our agent can receive and process the message to generate a summary.Incoming Webhooks are used for sending messages to Slack (from an external source).Outgoing Webhooks are used for receiving messages from Slack (to an external source), but only based on a specific trigger.In the context of our article about the , we use an outgoing webhook to receive messages from Slack for processing and an incoming webhook to send responses back to Slack.Step-by-Step Guide to Creating a Slack AppTo enable Slack to send us webhook events, we have to first create an application in the developer portal and install it to the Slack workspace or a specific channel from which we want to receive notifications.Click on  and choose to build it  Give your application a descriptive name and select the Slack workspace where you’ll be developing and testing it. This is where you’ll install the app to get the webhook URL.Enable Event Subscriptions: In your app’s settings, navigate to the  section and turn on .Provide a Request URL: You must provide a publicly accessible URL for your application. This is the endpoint where Slack will send event payloads. Slack will perform a one-time URL verification check, where your application must respond with the challenge value from Slack’s request. We’ll create this endpoint in our Symfony app.Subscribe to Bot Events: Scroll down to the “Subscribe to bot events” section. Click  and select the  event. This tells Slack to send your app a payload for every message posted in a public channel.Request Scopes: To receive these events, your bot will need the appropriate permissions. In the  section, add the  scope to your bot token. This grants your app the ability to view messages in public channels.Reinstall Your App: After adding new scopes, you have to reinstall your app to the workspace for the changes to take effect.By following these steps, our  will receive a full JSON payload for every new message in the subscribed channels, providing more context and flexibility than the legacy trigger-word method. This modern approach is the foundation for building advanced, .Handling Webhook ValidationIt’s time to create an endpoint to handle the response for our endpoint validation request. We have two options for working with webhooks on the Symfony side: we can handle it at the controller level or use the  component. In this article, we’ll focus on working at the controller level. In a future post, I’ll show you how to do this using the Symfony Webhook Component.When you configure your endpoint URL in Slack’s Event Subscriptions, Slack sends an initial verification request to confirm that the URL is valid and belongs to your application. This request is a simple HTTP POST request with a JSON body that includes a  parameter. Our endpoint must respond with the exact challenge value from the request body.To handle the webhook request properly, we have to not only receive the payload but also validate its correctness. To facilitate this, we’ll prepare  (DTOs) and install two key Symfony components:  and . This component provides a robust framework for validating data against a set of rules. Instead of manually writing if-else statements to check every field, we can define validation constraints (e.g., NotBlank, Type, Length) directly on our DTO properties. This makes our code cleaner, more readable, and less prone to errors. This component bundle provides everything we need to serialize (turn objects into arrays/JSON) and deserialize (turn JSON/arrays into objects) data. We’ll use the deserializer to automatically convert the incoming JSON payload from Slack into a validated DTO object. This process automates the tedious task of mapping data fields, ensuring the payload structure matches our expectations.And now we’re ready to use the more advanced collections that Doctrine offers. We won’t install the entire  component just yet; instead, we’ll install only the package for working with .While PHP arrays are versatile, they lack some advanced features like type-hinting, immutability, and specialized methods for filtering or mapping.  provide a powerful, object-oriented wrapper around arrays, giving us a more robust and predictable way to manage data. This is particularly useful for our  system, where we’ll be dealing with lists of messages, users, or other data payloads.This small but impactful changes will significantly improve how we manage data within , making our agent’s logic more robust and easier to maintain.To install the necessary components, simply run the following command in your terminal:composer require doctrine/collections
composer require symfony/validator
composer require symfony/serializer-pack
The Workflow for Validating a Webhook Payload Our Symfony controller receives the HTTP POST request from Slack. We use the serializer to transform the JSON payload into our predefined DTO object. The validator component checks the DTO’s properties against the defined constraints. If validation fails, we return an error response (e.g., a 400 Bad Request) to Slack. If validation succeeds, our application can safely proceed with processing the webhook data, knowing that it’s in the correct format.This structured approach ensures that our application is resilient and only processes data that conforms to our expected format, which is a critical step in building a reliable and secure system.Handling Diverse Payloads within a Single ControllerWe should also consider that our controller and a single route must properly handle different types of payloads, from URL validation to the working event payloads we’ve subscribed to.This is a key requirement for a robust webhook endpoint. A single route needs to be smart enough to differentiate between a one-time URL verification request and a live event notification. Our controller should be able to process each request type accordingly.As previously discussed, the initial URL verification payload from Slack contains a unique  key. All subsequent event payloads contain an event key that holds the actual message data. Our controller must check for the presence of these keys to determine the nature of the request.Typical Slack URL Validation RequestOkay, let’s create a DTO to validate the URL verification request from Slack. First, here is what a typical Slack URL validation request payload looks like.When you enable Event Subscriptions, Slack sends a POST request to your endpoint with a JSON payload that contains three key fields: token, challenge, and type.{
  "token": "Jhj5...z412",
  "challenge": "3eZbr...",
  "type": "url_verification"
}
 A unique token for your application. You can use this to verify the request is from your app. A randomly generated string. You must return this exact string in your JSON response to prove you own the URL. The type of event. For validation, this will always be url_verification.Now, we’ll create a DTO that mirrors this structure and includes validation constraints using Symfony’s validator component. We’ll use attributes to define the validation rules directly on the DTO properties.namespace App\DTO\Slack;
use Symfony\Component\Validator\Constraints as Assert;

class SlackUrlValidationRequest
{
    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $token;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $challenge;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    #[Assert\EqualTo("url_verification")]
    public string $type;

    public function getToken(): string
    {
        return $this->token;
    }

    public function getChallenge(): string
    {
        return $this->challenge;
    }

    public function getType(): string
    {
        return $this->type;
    }
}
This DTO simplifies validation: Ensures that the token, challenge, and type fields are present in the request. Confirms that each field’s value is a string, preventing unexpected data types.Assert\EqualTo(“url_verification”): Specifically validates that the type field matches the required string, so we know we’re handling a URL verification request and not another event type.By using this DTO, we can deserialize the incoming JSON payload and automatically validate its structure with minimal code in our controller. This makes our endpoint more secure and reliable.Creating a Controller for Slack Webhook ValidationPerfect, let’s create a controller that uses a SlackUrlValidationRequest DTO to validate the request from Slack. This controller will check if the incoming request is a URL verification request and handle it correctly by returning the challenge code.We’ll inject the  to deserialize the JSON payload into our DTO and the  to validate the data. This approach makes the code clean, reliable, and compliant with Symfony standards.namespace App\Controller;

use App\DTO\Slack\SlackUrlValidationRequest;

use Symfony\Component\HttpFoundation\JsonResponse;
use Symfony\Component\HttpFoundation\Request;
use Symfony\Component\HttpFoundation\Response;
use Symfony\Component\HttpKernel\Exception\BadRequestHttpException;
use Symfony\Component\Messenger\Envelope;
use Symfony\Component\Messenger\MessageBusInterface;
use Symfony\Component\Routing\Attribute\Route;
use Symfony\Component\Serializer\SerializerInterface;
use Symfony\Component\Validator\Validator\ValidatorInterface;

class WebhookController
{
    #[Route('/webhook/slack', name: 'webhookSlack')]
    public function __invoke(Request             $request,
                             ValidatorInterface  $validator,
                             SerializerInterface $serializer
    ): JsonResponse
    {

        $data = json_decode($request->getContent(), true);

        if ($data === null) {
            throw new BadRequestHttpException('Invalid JSON.');
        }

        $dtoClass = SlackUrlValidationRequest::class;

        $dto = $serializer->denormalize($data, $dtoClass);

        $errors = $validator->validate($dto);

        if (count($errors) > 0) {
            $errorMessages = [];
            foreach ($errors as $error) {
                $errorMessages[] = $error->getPropertyPath() . ': ' . $error->getMessage();
            }

            return new JsonResponse(['errors' => $errorMessages], Response::HTTP_BAD_REQUEST);
        }

        switch (get_class($dto)) {
            case SlackUrlValidationRequest::class:
                return new JsonResponse($dto->getChallenge());
        }

        return new JsonResponse(['status' => 'ok'], Response::HTTP_OK);
    }
}
This controller now reliably and securely handles verification requests, which is the foundation for our Now we can test how our controller works by sending a URL validation message from a server, simulating Slack’s behavior. This check will ensure our endpoint is configured correctly and ready to receive real event data.Creating a DTO for Slack Event MessagesNow, let’s create a DTO to process and validate the messages that Slack will send us when events occur. Unlike the simple URL validation request, the event payload contains much more data, so we’ll create several DTOs for more effective and modular validation.Typical Event Request StructureAn event request will contain the token, teamappid fields, and most importantly, an event object. Inside the event object, you’ll find information about the event type (message, appmention, etc.), the message text, user data, and the channel information.{
    "token": "Jhj5...z412",
    "team_id": "T061...R6",
    "api_app_id": "A0PN...2",
    "event": {
        "client_msg_id": "92f...d56",
        "type": "message",
        "text": "Hey bot, summarize this article for me.",
        "user": "U214...97",
        "ts": "167...97",
        "team": "T061...R6",
        "channel": "C0PN...L",
        "event_ts": "167...97",
        "channel_type": "channel"
    },
    "type": "event_callback",
    "event_id": "Ev08...6",
    "event_time": 167...97,
    "authorizations": [ ... ]
}
Creating the DTOs for Event HandlingWe’ll create a hierarchy of DTO objects to separate the validation of the overall request structure from the specifics of the event itself. (The wrapper DTO for the entire request) \n This DTO will contain the general fields for all events.namespace App\DTO\Slack;

use Doctrine\Common\Collections\ArrayCollection;
use Symfony\Component\Validator\Constraints as Assert;

class SlackEvent
{
    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $token;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $team_id;

    #[Assert\Type('string')]
    #[Assert\Nullable]
    public ?string $context_team_id;

    #[Assert\Type('string')]
    #[Assert\Nullable]
    public ?string $context_enterprise_id;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $api_app_id;

    #[Assert\NotBlank]
    public SlackEventData $event;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $type;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $event_id;

    #[Assert\NotBlank]
    #[Assert\Type('integer')]
    public int $event_time;

    public ArrayCollection $authorizations;

    #[Assert\Type('bool')]
    public bool $is_ext_shared_channel;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $event_context;

    public function __construct(){
        $this->authorizations = new ArrayCollection();
    }

    public function getToken(): string
    {
        return $this->token;
    }

    public function getTeamId(): string
    {
        return $this->team_id;
    }

    public function getContextTeamId(): ?string
    {
        return $this->context_team_id;
    }

    public function getContextEnterpriseId(): ?string
    {
        return $this->context_enterprise_id;
    }

    public function getApiAppId(): string
    {
        return $this->api_app_id;
    }

    public function getEvent(): SlackEventData
    {
        return $this->event;
    }

    public function getType(): string
    {
        return $this->type;
    }

    public function getEventId(): string
    {
        return $this->event_id;
    }

    public function getEventTime(): int
    {
        return $this->event_time;
    }

    public function getAuthorizations(): ArrayCollection
    {
        return $this->authorizations;
    }

    public function isIsExtSharedChannel(): bool
    {
        return $this->is_ext_shared_channel;
    }

    public function getEventContext(): string
    {
        return $this->event_context;
    }
}
 (The DTO for the event object) \n This DTO will contain the data specific to the event itself.namespace App\DTO\Slack;

use Symfony\Component\Validator\Constraints as Assert;

class SlackEventData
{
    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $user;

    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $type;

    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $ts;

    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $client_msg_id;

    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $text;

    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $team;

    #[Assert\NotBlank]
    #[Assert\Type("array")]
    #[Assert\All([
        new Assert\Type("array"),
        new Assert\Collection([
            'fields' => [
                'type' => [
                    'constraints' => [
                        new Assert\NotBlank(),
                        new Assert\Type("string")
                    ],
                ],
                'block_id' => [
                    'constraints' => [
                        new Assert\NotBlank(),
                        new Assert\Type("string")
                    ],
                ],
                'elements' => [
                    'constraints' => [
                        new Assert\Type("array")
                    ],
                ],
            ],
        ]),
    ])]
    public array $blocks;

    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $channel;

    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $event_ts;

    #[Assert\NotBlank]
    #[Assert\Type("string")]
    public string $channel_type;

    public function getUser(): string
    {
        return $this->user;
    }

    public function getType(): string
    {
        return $this->type;
    }

    public function getTs(): string
    {
        return $this->ts;
    }

    public function getClientMsgId(): string
    {
        return $this->client_msg_id;
    }

    public function getText(): string
    {
        return $this->text;
    }

    public function getTeam(): string
    {
        return $this->team;
    }

    public function getBlocks(): array
    {
        return $this->blocks;
    }

    public function getChannel(): string
    {
        return $this->channel;
    }

    public function getEventTs(): string
    {
        return $this->event_ts;
    }

    public function getChannelType(): string
    {
        return $this->channel_type;
    }

    public function getTsDateTime(): \DateTimeImmutable{
        // Separate seconds and microseconds
        list($seconds, $microseconds) = explode('.', $this->getTs());

        // Convert to DateTime with microseconds
        $dateTime = (new \DateTime())->setTimestamp($seconds);
        $dateTime->modify("+$microseconds microseconds");

        return \DateTimeImmutable::createFromMutable($dateTime);
    }
}
SlackEventAuthorization.php (The DTO for the authorizations collection) \n This DTO will contain the data specific to the authorizations collection.namespace App\DTO\Slack;

use Symfony\Component\Validator\Constraints as Assert;

class SlackEventAuthorization
{
    #[Assert\Type('string')]
    #[Assert\Nullable]
    public ?string $enterprise_id;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $team_id;

    #[Assert\NotBlank]
    #[Assert\Type('string')]
    public string $user_id;

    #[Assert\NotBlank]
    #[Assert\Type('bool')]
    public bool $is_bot;

    #[Assert\NotBlank]
    #[Assert\Type('bool')]
    public bool $is_enterprise_install;

    public function getEnterpriseId(): ?string
    {
        return $this->enterprise_id;
    }

    public function getTeamId(): string
    {
        return $this->team_id;
    }

    public function getUserId(): string
    {
        return $this->user_id;
    }

    public function isIsBot(): bool
    {
        return $this->is_bot;
    }

    public function isIsEnterpriseInstall(): bool
    {
        return $this->is_enterprise_install;
    }
}
We use three DTOs to separate the validation.  validates the overall request structure, while  validates the content of the nested event object and  validates authorization collection.This approach gives us flexibility. If in the future we want to handle another event type (e.g., appadded), we simply create a new DTO for that event type and use it for validation.With these DTOs, our controller will be able to reliably receive and validate messages from Slack, which is a critical step in building our Creating a Custom DenormalizerIn its default configuration, the Symfony Serializer cannot automatically deserialize complex, nested objects like . To correctly map the array of authorization objects from the Slack payload into our DTO, we need a custom denormalizer.A denormalizer is a component that converts structured data (like an array or a JSON object) into a PHP object. The default serializer can handle simple properties, but it doesn’t know how to handle complex structures like an array of custom DTOs nested within another DTO. By creating a custom denormalizer, we explicitly tell the serializer how to map the incoming data to our specific object.namespace App\Serializer\Denormalizer;

use App\DTO\Slack\SlackEvent;
use App\DTO\Slack\SlackEventAuthorization;
use Symfony\Component\Serializer\Normalizer\DenormalizerInterface;
use Symfony\Component\Serializer\Normalizer\ObjectNormalizer;

class SlackEventDenormalizer implements DenormalizerInterface
{
    private ObjectNormalizer $normalizer;

    public function __construct(ObjectNormalizer $normalizer)
    {
        $this->normalizer = $normalizer;
    }

    public function denormalize($data, string $type, string $format = null, array $context = []): mixed
    {

        $entity = $this->normalizer->denormalize($data, SlackEvent::class, $format, $context);

        if (isset($data['authorizations']) && is_array($data['authorizations'])) {
            foreach ($data['authorizations'] as $authorizationData) {
                 $authorization = $this->normalizer->denormalize($authorizationData, SlackEventAuthorization::class, $format, $context);
                $entity->getAuthorizations()->add($authorization);
            }
        }

        return $entity;
    }

    public function supportsDenormalization($data, string $type, string $format = null, array $context = []): bool
    {
        return $type === SlackEvent::class;
    }

    public function getSupportedTypes(?string $format): array
    {
        return [SlackEvent::class => true];
    }
}
We are now ready to modify our webhook controller to handle two event types: url_verification and event notifications. We will update the controller to use a switch statement to route the requests based on the type field in the payload, making our single endpoint flexible and robust.The updated controller will check the type of the incoming JSON payload. If it’s a urlverification request, it will perform the validation we’ve already set up. If it’s an eventcallback, it will begin the process of handling the actual Slack event.namespace App\DTO;

class SlackMessage extends MailMessage implements DataCollectionItemInterface
{
}
namespace App\Controller;

use App\DTO\DataCollection;
use App\DTO\Slack\SlackEvent;
use App\DTO\Slack\SlackUrlValidationRequest;
use App\DTO\SlackMessage;
use App\Message\Command\AIAgentSummarizeMessage;
use Symfony\Component\HttpFoundation\JsonResponse;
use Symfony\Component\HttpFoundation\Request;
use Symfony\Component\HttpFoundation\Response;
use Symfony\Component\HttpKernel\Exception\BadRequestHttpException;
use Symfony\Component\Messenger\Envelope;
use Symfony\Component\Messenger\MessageBusInterface;
use Symfony\Component\Routing\Attribute\Route;
use Symfony\Component\Serializer\SerializerInterface;
use Symfony\Component\Validator\Validator\ValidatorInterface;

class WebhookController
{
    #[Route('/webhook/slack', name: 'webhookSlack')]
    public function __invoke(Request             $request,
                             ValidatorInterface  $validator,
                             SerializerInterface $serializer,
                             MessageBusInterface $messageBus
    ): JsonResponse
    {

        $data = json_decode($request->getContent(), true);

        if ($data === null) {
            throw new BadRequestHttpException('Invalid JSON.');
        }

        $dtoClass = SlackUrlValidationRequest::class;

        if (array_key_exists('type', $data) && $data['type'] === 'event_callback') {
            $dtoClass = SlackEvent::class;
        }

        $dto = $serializer->denormalize($data, $dtoClass);

        $errors = $validator->validate($dto);

        if (count($errors) > 0) {
            $errorMessages = [];
            foreach ($errors as $error) {
                $errorMessages[] = $error->getPropertyPath() . ': ' . $error->getMessage();
            }

            return new JsonResponse(['errors' => $errorMessages], Response::HTTP_BAD_REQUEST);
        }

        switch (get_class($dto)) {
            case SlackUrlValidationRequest::class:
                return new JsonResponse($dto->getChallenge());
            case SlackEvent::class :

                $emailCollection = new DataCollection(
                    new SlackMessage(
                        'New Slack message',
                        $dto->getEvent()->getUser(),
                        $dto->getEvent()->getTeam(),
                        $dto->getEvent()->getText(),
                        $dto->getEvent()->getTs(),
                        $dto->getEvent()->getClientMsgId()
                    )
                );

                $messageBus->dispatch(
                    new Envelope(
                        new AIAgentSummarizeMessage(
                            $emailCollection,
                            'I have slack message. Please summarize it into a concise overview (100-150 words) focusing on key decisions, action items, and deadlines. Here’s the slack message content:'
                        )
                    )
                );

                break;
        }

        return new JsonResponse(['status' => 'ok'], Response::HTTP_OK);
    }
}
Leveraging Inheritance for a Unified WorkflowWe’ve created a new  class that inherits from . This is a significant design decision that allows us to send Slack messages to our message bus for standard asynchronous processing by our , using the existing workflow.By extending , the SlackMessage class automatically conforms to the same message contract required by our message bus and . This means we don’t need to build a new, separate processing pipeline for Slack. Instead, we can simply pass an instance of SlackMessage to the bus, and the system will handle it just like a regular email.Now that our code is complete, we’re ready to deploy and run our application. For our  system, we’ll use a containerized approach with , which simplifies the process of managing both our Symfony application and its dependencies, such as a database or message broker.The  class contains Slack-specific data but inherits the core properties needed for our agent, such as the message content. And we’ll transform it to Interfaces approach.We can now dispatch a SlackMessage object to the Symfony Message Bus on a standard async transport. The bus doesn’t need to know the specific type; it just recognizes it as a valid message.The AI agent, which is already configured to consume messages from the bus and process  objects, can seamlessly handle SlackMessage objects without any changes to its core logic. The  can then use the message’s content to perform its tasks, such as summarization or generating a reply.This approach is highly efficient because it avoids code duplication and keeps our system’s architecture clean and scalable. It demonstrates how our  can easily integrate new communication channels, ensuring all messages — whether from email or Slack — are processed through a single, intelligent workflow.In our next articles, we’ll explore how to simplify our code by using the  component. This powerful tool provides a higher-level abstraction for handling webhook requests, eliminating the need for some of the manual controller and validation logic we’ve built so far.We will then dive into creating a separate, dedicated workflow for generating automatic replies to Slack messages.Advanced Message Processing: Building on our existing architecture, we’ll design a system that can identify messages requiring a response. We’ll demonstrate how to use a  to formulate intelligent, context-aware answers. Finally, we’ll use a Slack API client to send the generated response back to the user, closing the communication loop and completing our system.Stay tuned as we continue to build out this powerful real-time communication platform.]]></content:encoded></item><item><title>Using ChatGPT Like a Junior Dev: Productive, But Needs Checking</title><link>https://hackernoon.com/using-chatgpt-like-a-junior-dev-productive-but-needs-checking?source=rss</link><author>Rad Code</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:12:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AI coding assistants like ChatGPT are everywhere now. They can scaffold components, generate test cases, and even debug code. But here’s the catch: they’re not senior engineers. They don’t have context of your project history, and they don’t automatically spot when the tests themselves are wrong.In other words: treat ChatGPT like a junior dev on your team — helpful, but always needing review.My Experience: Fixing Legacy Code Against Broken TestsI was recently working on a legacy React form validation feature. The requirements were simple:Validate name, email, employee ID, and joining date.Show error messages until inputs are valid.Enable submit only when everything passes.The tricky part? I didn’t just have to implement the form — I had to make it pass an  that had been written years ago.I turned to ChatGPT for help, thinking it could quickly draft a working component. It generated a solution — but when I ran the tests, they .At first, I thought maybe I had misunderstood the requirements, so I asked ChatGPT to debug. We went back and forth . I provided more context, clarified each input validation rule, and even explained what the error messages should be. ChatGPT suggested fixes each time, but .It wasn’t until I dug into the test suite myself that I realized the real problem: the tests were .The Test That Broke EverythingOne test hard-coded  as a “future date”:changeInputFields("UserA", "user@email.com", 123456, "2025-04-12");
expect(inputJoiningDate.children[1])
  .toHaveTextContent("Joining Date cannot be in the future");
The problem? We’re . That date is no longer in the future, so the expected error message would never appear. The component was fine — the .I had to dig through the logic, analyze the assumptions, and rewrite the test with , like so:// Corrected test using relative dates
const futureDate = new Date();
futureDate.setDate(futureDate.getDate() + 30); // always 30 days ahead
const futureDateStr = futureDate.toISOString().slice(0, 10);

changeInputFields("UserA", "user@email.com", 123456, futureDateStr);
expect(
  screen.getByText("Joining Date cannot be in the future")
).toBeInTheDocument();
This small change makes your test time-proof, so it will work regardless of the current year.AI will follow broken requirements blindly - ChatGPT can’t tell that a test is logically invalid. It will try to satisfy the failing test, even if the test itself makes no sense.Treat output like a junior PR - ChatGPT’s suggestions were helpful as scaffolding, but it struggled to see the root cause. I had to step in, dig through the legacy code, and analyze the tests myself. Hard-coded dates, magic numbers, or outdated assumptions make test suites brittle. If the tests are wrong, no amount of component fixes will help.Relative values keep tests reliable - Replace absolute dates or values with calculations relative to today. This ensures your tests work across time., but don’t rely on it to reason like a senior dev., and inspect its explanations carefully.Validate everything yourself — especially when working with legacy code. — use AI as scaffolding, but you own the fix.My experience taught me a simple truth: AI can accelerate coding, but it cannot replace human judgment, especially when dealing with messy, legacy code and outdated tests.Treat ChatGPT like a junior teammate:Helpful, eager to please, fast.Sometimes confidently wrong.Needs review, oversight, and occasionally, a reality check.If you keep that mindset, you’ll get the productivity boost without blindly following bad guidance — and you’ll know when to dig in yourself.💡  When working with code, the human developer is still the ultimate problem-solver. AI is there to assist, not to replace your reasoning.]]></content:encoded></item><item><title>The TechBeat: Terraforming Mars Could Save Earth (or Doom Us All) (9/24/2025)</title><link>https://hackernoon.com/9-24-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:10:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @hacker53037367 [ 12 Min read ] 
 ChatGPT made AI mainstream, but real transformation comes from ecosystems that embed AI across business, not from relying on a single model. Read More.By @oxylabs [ 11 Min read ] 
 Explore the 12 best web scraping services of 2025. Compare features, pricing, and pros & cons to choose the right tool for your data extraction needs. Read More.By @kingdavvd [ 6 Min read ] 
 Explore how space technology helps fight climate change, from satellites tracking emissions to innovations driving sustainability. Read More.By @oxylabs [ 12 Min read ] 
 Discover the top 12 proxy providers of 2025, tested and ranked. Compare pricing, features, and performance to find the perfect proxy service for your needs. Read More.By @indrivetech [ 4 Min read ] 
 Discover how inDrive built a structured two-week iOS onboarding program that helps new developers master architectures, navigation, and workflows faster.  Read More.By @ruslan4ezzz [ 9 Min read ] 
 PnL can lie. This hands-on guide shows traders how hypothesis testing separate luck from edge, with a Python example and tips on how not to fool yourself. Read More.By @ivyhackwell [ 6 Min read ] 
 Discover how the fictional Clarke Belt in science fiction became the foundation for today's geostationary satellites, revolutionizing global communication. Read More.By @scylladb [ 5 Min read ] 
 Discover how ScyllaDB enables fast, scalable online feature stores, integrating with Feast to deliver low-latency, high-throughput ML predictions. Read More.By @thetechpanda [ 6 Min read ] 
 India’s 2025 Online Gaming Bill recognizes esports as sport, unlocking growth, investment, and legitimacy for players, brands, and fans. Read More.By @innocentchuks [ 8 Min read ] 
 Discover how on-orbit servicing and satellite refueling are transforming space operations, extending satellite lifespans, and driving a new space economy. Read More.By @logos [ 23 Min read ] 
 Are blockchain communities inevitable? Explore crypto sovereignty and post-nation-state governance with insights from Jarrad Hope & Peter Ludlow. Read More.By @membrane [ 5 Min read ] 
 Use Membrane (Integration App) to build integrations to any app with AI. Read More.By @filestack [ 11 Min read ] 
 Learn how to build a modern file uploader tool with drag-and-drop, progress tracking, and cloud integration using file upload software. Read More.By @indrivetech [ 4 Min read ] 
 AI in inDrive design: UX interviews without interpreters, automated Figma localization, and fast realistic visuals for product and promo Read More.By @dylanmich [ 7 Min read ] 
 Explore the potential dangers of a completely decentralized internet, examining the challenges and risks associated with absolute online freedom. Read More.By @frankmorgan [ 3 Min read ] 
 A cheeky experiment uses ChatGPT to slip LinkedIn’s walled garden, proving off-platform links still win—and why MS’s Dismal Platform must pivot or die. Read More.By @andrew-nalichaev [ 9 Min read ] 
 Token launches are broken. CEXs extract, DEXs fragment. CrossCurve offers unified liquidity for memecoins & altcoins in 2025. Read More.]]></content:encoded></item><item><title>List is a Monad: Understanding Map, flatMap, and Maybe in Practice</title><link>https://hackernoon.com/list-is-a-monad-understanding-map-flatmap-and-maybe-in-practice?source=rss</link><author>Alex Yorke</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:07:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Note July 6th 2025: this post’s original title was “A list is a monad”. It has been changed to “List is a monad”.The term “monad” is often invoked when describing patterns in functional programming. At the heart of monadic programming is sequencing computations, so each step can depend on the previous one while the monad threads context.You may erroneously think all monads are containers, or burritos, or boxes. The  of monads can be idealized as a  (albeit a flawed metaphor). Monads are much more than just containers, and there isn’t the-one-and-only monad; instead it’s better to think about them as a programming pattern, recipe, factoring out control flow, or, in some cases, a deferred computation. It depends on which monad you’re talking about.From a teaching perspective, to get the concept for what a monad is, we will start with the simplest of monads which will feel a lot like just a container but with some composable aspects. This provides the infrastructure to understand more complex monads later on.List: Map & flatMap in PracticeTo an OOP developer, monadic types () might look just like generics. It’s a typical pitfall to think “we have generics, so we have monads,” which isn’t true by itself. Monads do usually involve generic types, but they require specific operations ( and ) and the three monad laws on those types to ensure uniform behavior.  and is fundamental to working with monads.A good example of a monad is . You’re likely very familiar with lists and working with lists.The monad  operation is responsible for: For ,  runs  (a function) on  element. For example, let’s define  as . The list  becomes . If the list doesn’t have any elements, then  doesn’t call .  doesn’t need to worry about that. Also,  doesn’t care if it’s ; all  is, is just .  is responsible for running it.Managing sequencing and combination. The list context concatenates all results into one list ( does  flatten any nested lists,  is responsible for this). We don’t need to manually re-add elements via  or otherwise manage the collection ourselves.Notice that the monad in this case  is responsible for running . This shift means your business logic stays  and , you describe  happens to a single value, and the monad describes  and  it happens.This is different from object-oriented and procedural programming because in those paradigms, if you want to process data, it is your responsibility to understand how to apply the function to your data. We have to use different control constructs to handle different types of data, and we’re also responsible for the “how”:public string f(string input) {  
  return input + " -appended text";  
}

// 1. List<string>: you must foreach and build a new list

var fruits = new List<string> {  
  "apple",  
  "banana",  
  "cherry"  
};

var newFruits = new List<string>();

foreach (var fruit in fruits)  
{  
  newFruits.Add(f(fruit));  
}

// 2. Single string: you must check for null first, then concatenate

string userInput = GetUserInput(); // could be null

if (userInput != null)  
{  
  userInput = f(userInput);  
}

// userInput could still be null here, or it could be the concatenated result

// 3. Dictionary<string, string>: you must know it’s key/value pairs

var dict = new Dictionary<string, string>  
{  
  ["a"] = "alpha",  
  ["b"] = "beta",  
  ["c"] = "gamma"  
};

// can’t modify while iterating, so capture keys first  
foreach (var key in dict.Keys.ToList())  
{  
  dict[key] = f(dict[key]);  
}
In these examples, we are forced to know  to update each structure procedurally. For a , we have to call ; for the  we can update it in place; for the , we have to iterate over keys and update each entry. We have to know it’s a  beforehand to know to use . We have to know it’s just a  to append another string to it.With monads, you delegate the control flow to the monad itself, the monad knows how to update its underlying value(s). Recall that even the simplest monads must implement two methods to be monads ( and ) and must follow three monad laws. moves a raw value into the monadic context (this operation is sometimes called “lifting”, “identity”, “return”, “wrap”, or “promotion”, and in some libraries has names like  or ).In the list monad,  takes a single element and returns a list containing that element.For example, given the integer ,  produces a list as follows:var list = new List<int> { 1 };
 implements  because it allows moving a value into the mondaic context. Nothing about the value  changes, it’s simply wrapped in a . If you access element  of that list, you get back . That’s it. applies a function to each value inside the monad.In ,  runs a function on every element and outputs a new list with that function applied to each element. Don’t overcomplicate it. For example, suppose we have a function that adds one, . Passing this function to  would simply add one to each element in the list. The list  would become .var originalList = new List<int> { 0, 1, 2, 3, 4 };    
var mapped = originalList.Map(x => x + 1); // `Map` doesn’t exist in C# (use LINQ's `Select`), but assume this pseudocode
Example (C#, without monads):var originalList = new List<int> { 0, 1, 2, 3, 4 };    
var mappedList = new List<int>();

foreach (int x in originalList)    
{    
    mappedList.Add(x + 1);    
}
How do you get the damn values out of the monads?Ideally, you don’t want to pull the values out of a monad unless you absolutely have to. It’s possible to implement a  method that returns the underlying value, but once the value leaves the monadic context, we lose the benefits of that context and can no longer compose operations easily.Think about  as if you had never seen it before. You might say, “I don’t want my values trapped in this list, how am I supposed to use them?” and then manually extract each element into separate variables:// Pretend it’s your first time with List<T>
var numbers = new List<int> { 1, 2, 3 };

// --- Manual extraction (values “trapped” in the list) ---
var a = numbers[0];
var b = numbers[1];
var c = numbers[2];

// Now call your function separately on each:
var r1 = AddOne(a);
var r2 = AddOne(b);
var r3 = AddOne(c);
But by doing so, you lose the advantages of using a list in the first place: the ability to store arbitrarily long sequences, to pass around all the values together, to concatenate with other lists, and to iterate easily. If you want to add one to each item, extracting them one by one and handling each separately is tedious and error-prone.Up to this point, monads might just seem like “fancy containers” that have to implement two odd methods ( and ). Let’s explore a slightly more complex monad to see why they’re more than just containers.Let’s consider a case where unwrapping the value may not always make sense. We’ll create a monad called  (often also called an ) which represents either an existing value or the absence of a value.For simplicity, our  will hold an  internally (in a real library this would be a generic ). It’s not exactly a full monad yet, because we haven’t implemented  on it.public class MaybeMonad {    
    private int value;    
    private bool hasValue;  

    // Unit    
    public MaybeMonad(int value) {    
        this.value = value;
        this.hasValue = true;
    }  

    // Unit (no value)
    public MaybeMonad() {
        // hasValue remains false by default
    }  

    // Map    
    public MaybeMonad Map(Func<int, int> func) {
        if (hasValue) {
            return new MaybeMonad(func(value));
        }
        return this;
    }
}
Here, the  operation corresponds to calling one of the constructors, that’s how we lift a raw value into a . The  operation might feel a bit strange because we’re just dealing with a single value (or none), whereas you might be used to mapping over a list of many values.For example, to add 1 to a :var age = new MaybeMonad(30);    
var newAge = age.Map(x => x + 1);    
// newAge now holds 31
Or if there was no value to begin with:var age = new MaybeMonad();    
var newAge = age.Map(x => x + 1);    
// newAge is still “nothing”, `Map` didn’t call `f(x)` because there was no value
This looks verbose just to add 1 to a number. Why wrap  in a  and call  when we could have just incremented  directly? The point is that  is a , by definition it might or might not contain a value. In the case where there is no value, ’s  simply does nothing. You’d have to write the same conditional logic yourself in a procedural style:int? age = null;    
if (age != null) age++;
int? age = 30;    
if (age != null) age++;
Now we start to see why a monad is not simply a container to be unwrapped at will. How would you “unwrap” a ? If it has a value, you could return it, sure. But if it doesn’t, there’s nothing to return, the absence itself is a meaningful state.  essentially encodes the idea of “nothing” (no result) in a way that isn’t just  (because in many languages  is still a concrete value of sorts). With , if there’s no value, any function passed into  simply won’t execute. Unwrapping it and getting a raw value out isn’t always meaningful in this context.Another benefit of monads is that you can chain computations that themselves produce monadic results. The limitation of only having  is that you might end up with nested monads. For example, imagine a function that returns a . If you call  on a  with that function, the result would be a , a nested container, because the  wraps the function’s  result into yet another . We need a way to apply a function that returns a monad and avoid this unnecessary nesting when chaining operations. is like our , but it also flattens the result.  provides the ability to chain computations that themselves produce monadic values, which is the defining feature of monads. For example, if you have a function that looks up a user and returns a , but you want to pass it to another function that returns the user’s profile. Using  would give you a Maybe<Maybe<UserProfile>>, an awkward nested container because the input would be a . With , you both apply your lookup and collapse the layers in one go, so you can seamlessly sequence optional, error-handling, or asynchronous operations (e.g. promises/tasks) without ever wrestling with nested monadic types.Here’s what  looks like:// Add this method inside MaybeMonad
public MaybeMonad FlatMap(Func<int, MaybeMonad> func)
{
    if (hasValue)
    {
        // Do not wrap again; let the callee decide whether to return a value or "nothing"
        return func(value);
    }
    // Propagate "no value"
    return this;
}
Use  when your next step might also produce “no value,” and you want to keep chaining without ending up with .Maybe<User> lookupUser(string id)  
{  
    // Imagine this calls a database or external service and returns Maybe<User>  
    return GetUserFromDatabase(id);  
}

Maybe<string> userIdMaybe = GetUserId();

// Using Map would yield Maybe<Maybe<User>> (nested) because lookupUser returns a Maybe<User>.  
// This quickly becomes unwieldy and makes further processing difficult.  
var nested = userIdMaybe  
    .Map(lookupUser);

// Using flatMap collapses the result to a single Maybe<User>  
var user = userIdMaybe  
    .FlatMap(lookupUser);
 is arguably more important than , in fact,  is required to qualify as a monad, and given  you can implement  in terms of it.What does this chaining look like procedurally? It would be similar to:string userId = GetUserId(); // could be null  
if (userId == null) {  
  // e.g., return an error or stop here  
}

User user = GetUserFromDatabase(userId); // this could return null (no user found)  
if (user == null) {  
  // handle missing user  
} else {  
  // we have a valid user  
}
In the procedural version, we had to explicitly handle the control flow at each step (checking for  in this case). In the monadic version, the control flow is implicit in the monad. If  has no value,  simply doesn’t call  at all, the “else do nothing” logic is built into .In the monadic example, you could write:Maybe<string> userIdMaybe = GetUserId();  
Maybe<User> userMaybe = userIdMaybe.FlatMap(lookupUser);
The monads handle the control flow for us.  returns a  because we’re acknowledging the user ID might not exist. We’ve defined the  monad such that if there’s no value, any subsequent function (like ) won’t execute. There’s nothing mystical here, we explicitly designed  to work that way.This is why it makes sense to wrap values in monads and keep chaining within the monadic context: you can sequence operations (like getting a user ID, then looking up a user, then perhaps fetching their profile) without writing a single explicit  or loop for the control flow. Each monad step handles the logic of “if there’s no value, stop here” automatically.If you prematurely yank a value out of a monad, you end up doing manual work that defeats this benefit. For instance, consider if we had a  method to extract the inner value (with  representing “no value”):Maybe<string> userIdMaybe = GetUserId();  
var actualUserId = userIdMaybe.GetValue();  
if (actualUserId != null) {  
    // do something with actualUserId  
}
Eww. If we treat the monad as just a fancy wrapper to put a value in and then take it out immediately, it does feel like pointless ceremony. This is where many people give up on learning monads, it seems like you’re just putting a value in a box and taking it out again with extra steps. But the power of monads comes when you stay  the monadic context and keep chaining operations. In Part 2, we’ll look at more advanced monads that aren’t just simple containers, and you’ll see how staying in the monadic pipeline pays off.Closing the loop on MaybeWe’re making a few changes to the  monad to give it a more official, ergonomic API. First, instead of letting callers construct the underlying representation directly, we’ll expose two :  and . Second, we’ll generalize map: instead of only mapping over integers, the monad will be generic so it can map any type. Finally, we’ll standardize the name to . Together, these tweaks clean things up and make the monad easier to use across more scenarios.public sealed class Maybe<T>
{
    private readonly bool _has;
    private readonly T _value;

    private Maybe(T value)
    {
        _has = true;
        _value = value;
    }

    private Maybe()
    {
        _has = false;
        _value = default(T);
    }

    public static Maybe<T> Some(T value)
    {
        return new Maybe<T>(value);
    }

    public static Maybe<T> None()
    {
        return new Maybe<T>();
    }

    public Maybe<U> Map<U>(Func<T, U> f)
    {
        if (_has)
        {
            return Maybe<U>.Some(f(_value));
        }
        return Maybe<U>.None();
    }

    public Maybe<U> Bind<U>(Func<T, Maybe<U>> f) // aka FlatMap
    {
        if (_has)
        {
            return f(_value);
        }
        return Maybe<U>.None();
    }
}
To wrap up : it’s perfect when you only need to model “value or no value.” Often, we also need to know  a value is missing (not found, invalid input, business‑rule violation).  can’t carry that reason.To be a true monad, a type must not only provide  and  operations, but also obey three simple laws that make sure these operations behave consistently: is the same as . (Wrapping a value and then immediately applying a function to it is equivalent to just calling the function on the raw value.) is the same as . (If you  a monad with the  function, the monad should remain unchanged.) is the same as m.flatMap(x => f(x).flatMap(g)). (It doesn’t matter how you parenthesize nested  operations, the outcome will be the same.)You don’t need to memorize these laws, but they provide a mathematical guarantee that monadic operations will compose reliably. Our  adheres to these laws, making it a true monad.As we’ve seen, monads provide a context for computation. By defining two core operations,  (to wrap a value) and  (to sequence operations that produce a new context), we abstract away manual control flow like loops and null-checks. This lets us turn scattered procedural code into a single declarative pipeline.The real power comes when we apply this pattern to different contexts. In Part 2, we’ll explore other useful monads, like  for more descriptive error handling, and see how to combine monads to manage multiple concerns at once.Exercise for reader: I’d encourage opening up your IDE, without any AI assistance, and implementing the  monad from scratch (no cheating.)]]></content:encoded></item><item><title>Revering AI Reveals Incompetence, Not Intelligence</title><link>https://hackernoon.com/revering-ai-reveals-incompetence-not-intelligence?source=rss</link><author>Radley Sidwell-Lewis</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:04:42 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Nothing is really good anymore; and AI is the peak of this lamentable trend.]]></content:encoded></item><item><title>Why “Classical Excess” Could Be the Next Big Tool in Quantum Research</title><link>https://hackernoon.com/why-classical-excess-could-be-the-next-big-tool-in-quantum-research?source=rss</link><author>Probabilistic</author><category>tech</category><pubDate>Wed, 24 Sep 2025 06:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Generalized contextuality, a leading notion of nonclassicality, is of crucial importance both in the foundations of quantum theory and in quantum information processing. Despite this, there is no complete characterisation of generalised contextuality as a resource. In this work we address this shortcoming.\
Based on recent developments formulating noncontextuality of GPT systems via simplex embeddability, we have defined a resource theory of contextuality of GPT systems in prepare-and-measure scenarios. The free resources are the noncontextual systems and the free operations are univalent simulations with free access to classical systems. Using these notions, we motivate a hierarchy of contextuality for GPT systems. A new contextuality monotone arises naturally from our considerations— the classical excess, which expresses the minimum error of a univalent simulation by the countably-infinite classical system. We have also shown how a standard witness of contextuality, in the form of the POM success probability, can be used to define a contextuality monotone.\
We have also discussed how GPT simulations could describe a physical process of information erasure that would explain the fine-tuning associated with contextuality[8] in a similar way to how the quantum equilibration process proposed by Valentini explains the fine-tuning associated with nonlocality in Bohmian mechanics. We have argued that the information erasure in this case would be different than a simple coarse-graining. An interesting avenue for future research would be to further characterize this kind of information erasure and propose a test to detect heat dissipation in experiments manifesting contextuality. Even though such a proposal is undoubtedly radical, we believe that it could explain the fine-tuning associated with contextuality without abandoning the ontological models framework.\
Another direction for further investigation is to develop a solid interpretation and establish the use-cases of the resource theory of GPT-contextuality that we provide. It is known that a resource theory of a given notion need not be unique. We have examples of this fact, like the resource theories of entanglement based on LOCC [81] and LOSR [82, 83] free operations, respectively. This does not mean that only one of them is “the correct resource theory of entanglement”. Rather, these resource theories may be applicable in different contexts or for different purposes. In the case of entanglement, one can say that LOCC operations are relevant for communication tasks in the context of quantum internet and LOSR operations are relevant for the study of entanglement in Bell scenarios.\
In this respect, we have discussed the relation of our work with other studies of generalized contextuality for GPTs in section 5.2. One that we did not mention is [72]. It would be interesting to find a relationship between our classical excess and the simulation cost of contextuality defined therein. Finally, it would be also interesting to extend the methods developed here to the realm of resource theories of more general fine-tunings [4], such as violations of time symmetry [84] and bounded ontological distinctness [85].The authors thank the participants of the PIMan workshop – Orange (CA), March 2019 – where the idea of this project originated. In particular, Luke Burns and Justin Dressel, who were part of the initial discussions on the project. The authors further thank Rafael Wagner for insightful explanations regarding the approach of Duarte and Amaral [44]. LC thanks Farid Shahandeh and TGa thanks Markus M¨uller for helpful discussions. This project started when LC was supported by the Fetzer Franklin Fund of the John E. Fetzer Memorial Trust and by the Army Research Office (ARO) (Grant No. W911NF-18-1-0178). LC also acknowledges funding from the Einstein Research Unit “Perspectives of a Quantum Digital Transformation” and from the Horizon Europe project FoQaCiA, GA no.101070558. TGa acknowledges support from the Austrian Science Fund (FWF) via project P 33730-N. TGo acknowledges support from the Austrian Science Fund. This research was funded in whole or in part by the Austrian Science Fund (FWF) via the START Prize Y1261-N. This research was supported in part by Perimeter Institute for Theoretical Physics. Research at Perimeter Institute is supported by the Government of Canada through the Department of Innovation, Science, and Economic Development, and by the Province of Ontario through the Ministry of Colleges and Universities.\
For open access purposes, the authors have applied a CC BY public copyright license to any accepted manuscript version arising from this submission.[1] David Schmid. Generalized noncontextuality. Solstice of Foundations, ETH Zurich, 2022. https://www.youtube.com/watch?v=M3qn3EHWdOg.\
[2] R. W. Spekkens. Contextuality for preparations, transformations, and unsharp measurements. Phys. Rev. A, 71:052108, May 2005. doi:10.1103/PhysRevA.71.052108.\
[3] Robert Spekkens. The ontological identity of empirical indiscernibles: Leibniz’s methodological principle and its significance in the work of einstein. arXiv:1909.04628, 2019. doi:https://doi.org/10.48550/arXiv.1909.04628.\
[4] Lorenzo Catani and Matthew Leifer. A mathematical framework for operational fine tunings. Quantum, 7:948, March 2023. doi:10.22331/q-2023-03-16-948.\
[5] Christopher Ferrie and Joseph Emerson. Frame representations of quantum mechanics and the necessity of negativity in quasi-probability representations. Journal of Physics A: Mathematical and Theoretical, 41(35):352001, jul 2008. doi:10.1088/1751-8113/41/35/352001.\
[6] Robert W. Spekkens. Negativity and contextuality are equivalent notions of nonclassicality. Phys. Rev. Lett., 101:020401, Jul 2008. doi:10.1103/PhysRevLett.101.020401.\
[7] J. S. Bell. On the Einstein Podolsky Rosen paradox. Physics Physique Fizika, 1(3):195–200, 1964. doi:10.1103/PhysicsPhysiqueFizika.1.195.\
[8] Michael D. Mazurek, Matthew F. Pusey, Ravi Kunjwal, Kevin J. Resch, and Robert W. Spekkens. An experimental test of noncontextuality without unphysical idealizations. Nature Communications, 7(1):ncomms11780, 2016. doi:10.1038/ncomms11780.\
[9] Michael D. Mazurek, Matthew F. Pusey, Kevin J. Resch, and Robert W. Spekkens. Experimentally bounding deviations from quantum theory in the landscape of generalized probabilistic theories. PRX Quantum, 2:020302, Apr 2021. doi:10.1103/PRXQuantum.2.020302.\
[10] Robert W. Spekkens, D. H. Buzacott, A. J. Keehn, Ben Toner, and G. J. Pryde. Preparation Contextuality Powers Parity-Oblivious Multiplexing. Phys. Rev. Lett., 102(1):010401, 2009. doi:10.1103/PhysRevLett.102.010401.\
[11] Alley Hameedi, Armin Tavakoli, Breno Marques, and Mohamed Bourennane. Communication games reveal preparation contextuality. Phys. Rev. Lett., 119:220402, Nov 2017. doi:10.1103/PhysRevLett.119.220402.\
[12] David Schmid and Robert W. Spekkens. Contextual advantage for state discrimination. Phys. Rev. X, 8:011015, Feb 2018. doi:10.1103/PhysRevX.8.011015.\
[13] Debashis Saha and Anubhav Chaturvedi. Preparation contextuality as an essential feature underlying quantum communication advantage. Phys. Rev. A, 100:022108, Aug 2019. doi:10.1103/PhysRevA.100.022108.\
[14] Matteo Lostaglio and Gabriel Senno. Contextual advantage for state-dependent cloning. Quantum, 4:258, April 2020. doi:10.22331/q-2020-04-27-258.\
[15] Matteo Lostaglio. Certifying quantum signatures in thermodynamics and metrology via contextuality of quantum linear response. Phys. Rev. Lett., 125:230603, Dec 2020. doi:10.1103/PhysRevLett.125.230603.\
[16] Shiv Akshar Yadavalli and Ravi Kunjwal. Contextuality in entanglement-assisted one-shot classical communication. Quantum, 6:839, October 2022. doi:10.22331/q-2022-10-13-839.\
[17] Kieran Flatt, Hanwool Lee, Carles Roch I Carceller, Jonatan Bohr Brask, and Joonwoo Bae. Contextual advantages and certification for maximum-confidence discrimination. PRX Quantum, 3:030337, Sep 2022. doi:10.1103/PRXQuantum.3.030337.\
[18] Carles Roch i Carceller, Kieran Flatt, Hanwool Lee, Joonwoo Bae, and Jonatan Bohr Brask. Quantum vs noncontextual semi-device-independent randomness certification. Phys. Rev. Lett., 129:050501, Jul 2022. doi:10.1103/PhysRevLett.129.050501.\
[19] Lorenzo Catani, Matthew Leifer, Giovanni Scala, David Schmid, and Robert W. Spekkens. What is nonclassical about uncertainty relations? Phys. Rev. Lett., 129:240401, Dec 2022. doi:10.1103/PhysRevLett.129.240401.\
[20] Rafael Wagner, Anita Camillini, and Ernesto F. Galvao. Coherence and contextuality in a mach-zehnder interferometer. Quantum, 8:1240, February 2024. doi:10.22331/q-2024-02-05-1240.\
[21] Lorenzo Catani, Matthew Leifer, Giovanni Scala, David Schmid, and Robert W. Spekkens. Aspects of the phenomenology of interference that are genuinely nonclassical. Phys. Rev. A, 108:022207, Aug 2023. doi:10.1103/PhysRevA.108.022207.\
[22] Bob Coecke, Tobias Fritz, and Robert W. Spekkens. A mathematical theory of resources. Information and Computation, 250:59–86, 2016. doi:https://doi.org/10.1016/j.ic.2016.02.008. Quantum Physics and Logic.\
[23] Eric Chitambar and Gilad Gour. Quantum resource theories. Rev. Mod. Phys., 91:025001, Apr 2019. doi:10.1103/RevModPhys.91.025001.\
[24] Gilad Gour. Resources of the quantum world. arXiv preprint arXiv:2402.05474, 2024. doi:https://doi.org/10.48550/arXiv.2402.05474.\
[25] Ryszard Horodecki, Pawe l Horodecki, Micha l Horodecki, and Karol Horodecki. Quantum entanglement. Rev. Mod. Phys., 81:865–942, Jun 2009. doi:10.1103/RevModPhys.81.865.\
[26] Tom´aˇs Gonda. Resource theories as quantale modules. arXiv preprint arXiv:2112.02349, 2021. doi:https://doi.org/10.48550/arXiv.2112.02349.\
[27] Lucien Hardy. Quantum theory from five reasonable axioms. arXiv:quant-ph/0101012, 2001. doi:https://doi.org/10.48550/arXiv.quant-ph/0101012.\
[28] Jonathan Barrett. Information processing in generalized probabilistic theories. Phys. Rev. A, 75:032304, Mar 2007. doi:10.1103/PhysRevA.75.032304.\
[29] Peter Janotta and Haye Hinrichsen. Generalized probability theories: what determines the structure of quantum theory? Journal of Physics A: Mathematical and Theoretical, 47(32): 323001, jul 2014. doi:10.1088/1751-8113/47/32/323001.\
[30] Martin Pl´avala. General probabilistic theories: An introduction. Physics Reports, 1033:1–64, 2023. doi:https://doi.org/10.1016/j.physrep.2023.09.001. General probabilistic theories: An introduction.\
[31] Daniel Gottesman. Stabilizer codes and quantum error correction. PhD thesis, California Institute of Technology, 1997. doi:https://doi.org/10.48550/arXiv.quant-ph/9705052.\
[32] Markus P. M¨uller and Andrew J. P. Garner. Testing quantum theory by generalizing noncontextuality. Phys. Rev. X, 13:041001, Oct 2023. doi:10.1103/PhysRevX.13.041001.\
[33] Simon Kochen and E. P. Specker. The Problem of Hidden Variables in Quantum Mechanics, pages 293–328. Springer Netherlands, Dordrecht, 1975. doi:10.1007/978-94-010-1795-4 17.\
[34] Samson Abramsky, Rui Soares Barbosa, and Shane Mansfield. Contextual fraction as a measure of contextuality. Phys. Rev. Lett., 119:050504, Aug 2017. doi:10.1103/PhysRevLett.119.050504.\
[35] Rui Soares Barbosa, Martti Karvonen, and Shane Mansfield. Closing Bell Boxing Black Box Simulations in the Resource Theory of Contextuality, pages 475–529. Springer International Publishing, Cham, 2023. doi:10.1007/978-3-031-24117-8 13.\
[36] Martti Karvonen. Neither contextuality nor nonlocality admits catalysts. Phys. Rev. Lett., 127:160402, Oct 2021. doi:10.1103/PhysRevLett.127.160402.\
[37] Samson Abramsky and Adam Brandenburger. The sheaf-theoretic structure of nonlocality and contextuality. New Journal of Physics, 13(11):113036, nov 2011. doi:10.1088/1367-2630/13/11/113036.\
[38] Matthias Kleinmann, Otfried G¨uhne, Jos´e R Portillo, Jan-˚Ake Larsson, and Ad´an Cabello. Memory cost of quantum contextuality. New Journal of Physics, 13(11):113011, nov 2011. doi:10.1088/1367-2630/13/11/113011.\
[39] Karl Svozil. How much contextuality? Natural Computing, 11(2):261–265, 2012. doi:10.1007/s11047-012-9318-9.\
[40] A. Grudka, K. Horodecki, M. Horodecki, P. Horodecki, R. Horodecki, P. Joshi, W. K lobus, and A. W´ojcik. Quantifying contextuality. Phys. Rev. Lett., 112:120401, Mar 2014. doi:10.1103/PhysRevLett.112.120401.\
[41] Lu Li, Kaifeng Bu, and Junde Wu. Contextual robustness: An operational measure of contextuality. Phys. Rev. A, 101:012120, Jan 2020. doi:10.1103/PhysRevA.101.012120.\
[42] Karol Horodecki, Jingfang Zhou, Maciej Stankiewicz, Roberto Salazar, Pawe l Horodecki, Robert Raussendorf, Ryszard Horodecki, Ravishankar Ramanathan, and Emily Tyhurst. The rank of contextuality. New Journal of Physics, 25(7):073003, jul 2023. doi:10.1088/1367-2630/acdf78.\
[43] Barbara Amaral. Resource theory of contextuality. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 377(2157):20190010, 2019. doi:10.1098/rsta.2019.0010.\
[44] Cristhiano Duarte and Barbara Amaral. Resource theory of contextuality for arbitrary prepare-and-measure experiments. Journal of Mathematical Physics, 59(6):062202, 06 2018. doi:10.1063/1.5018582.\
[45] David Schmid, Robert W. Spekkens, and Elie Wolfe. All the noncontextuality inequalities for arbitrary prepare-and-measure experiments with respect to any fixed set of operational equivalences. Phys. Rev. A, 97:062103, Jun 2018. doi:10.1103/PhysRevA.97.062103.\
[46] Rafael Wagner, Roberto D Baldij˜ao, Alisson Tezzin, and B´arbara Amaral. Using a resource theoretic perspective to witness and engineer quantum generalized contextuality for prepareand-measure scenarios. Journal of Physics A: Mathematical and Theoretical, 56(50):505303, nov 2023. doi:10.1088/1751-8121/ad0bcc.\
[47] David Schmid, John H. Selby, Elie Wolfe, Ravi Kunjwal, and Robert W. Spekkens. Characterization of noncontextuality in the framework of generalized probabilistic theories. PRX Quantum, 2:010331, Feb 2021. doi:10.1103/PRXQuantum.2.010331.\
[48] Victor Gitton and Mischa P. Woods. Solvable Criterion for the Contextuality of any Prepareand-Measure Scenario. Quantum, 6:732, June 2022. doi:10.22331/q-2022-06-07-732.\
[49] Victor Gitton and Mischa P. Woods. On the system loophole of generalized noncontextuality. arXiv:2209.04469, 2022. doi:https://doi.org/10.48550/arXiv.2209.04469.\
[50] Farid Shahandeh. Contextuality of general probabilistic theories. PRX Quantum, 2:010330, Feb 2021. doi:10.1103/PRXQuantum.2.010330.\
[51] John H. Selby, David Schmid, Elie Wolfe, Ana Bel´en Sainz, Ravi Kunjwal, and Robert W. Spekkens. Accessible fragments of generalized probabilistic theories, cone equivalence, and applications to witnessing nonclassicality. Phys. Rev. A, 107:062203, Jun 2023. doi:10.1103/PhysRevA.107.062203.\
[52] John H. Selby, Elie Wolfe, David Schmid, Ana Bel´en Sainz, and Vinicius P. Rossi. Linear program for testing nonclassicality and an open-source implementation. Phys. Rev. Lett., 132: 050202, Jan 2024. doi:10.1103/PhysRevLett.132.050202.\
[53] Nicholas Harrigan and Robert W. Spekkens. Einstein, Incompleteness, and the Epistemic View of Quantum States. Foundations of Physics, 40(2):125–157, 2010. doi:10.1007/s10701-009-9347-0.\
[54] E. Brian Davies and John T. Lewis. An operational approach to quantum probability. Communications in Mathematical Physics, 17(3):239–260, 1970.\
[55] G¨unther Ludwig. An Axiomatic Basis for Quantum Mechanics: Volume 1 Derivation of Hilbert Space Structure. Springer Berlin Heidelberg, Berlin, Heidelberg, 1985. OCLC: 858930098.\
[56] Ludovico Lami. Non-classical correlations in quantum mechanics and beyond. arXiv:1803.02902, 2018. doi:https://doi.org/10.48550/arXiv.1803.02902.\
[57] Roberto Beneduci and Leon Loveridge. Incompatibility of effects in general probabilistic models. 55(25):254005, may 2022. doi:10.1088/1751-8121/ac6f9d.\
[58] Martin Pl´avala. General probabilistic theories: An introduction. Physics Reports, 1033:1–64, 2023. doi:https://doi.org/10.1016/j.physrep.2023.09.001. General probabilistic theories: An introduction. [\
59] Aleksandr S. Holevo. Probabilistic and statistical aspects of quantum theory. Number 1 in Quaderni Monographs. Edizioni della normale, Pisa, 2., english ed edition, 2011. OCLC: 746305136.\
[60] E G Beltrametti and S Bugajski. A classical extension of quantum mechanics. Journal of Physics A: Mathematical and General, 28(12):3329, jun 1995. doi:10.1088/0305-4470/28/12/007.\
[61] Bob Coecke, Tobias Fritz, and Robert W. Spekkens. A mathematical theory of resources. Information and Computation, 250:59–86, 2016. doi:https://doi.org/10.1016/j.ic.2016.02.008. Quantum Physics and Logic.\
[62] Nicholas Gauguin Houghton-Larsen. A mathematical framework for causally structured dilations and its relation to quantum self-testing. arXiv:2103.02302, 2021. doi:https://doi.org/10.48550/arXiv.2103.02302.\
[63] Giacomo Mauro D’Ariano, Marco Erba, and Paolo Perinotti. Classical theories with entanglement. Phys. Rev. A, 101:042118, Apr 2020. doi:10.1103/PhysRevA.101.042118. [64] Tobias Fritz. Resource convertibility and ordered commutative monoids. Mathematical Structures in Computer Science, 27(6):850–938, 2017. doi:10.1017/S0960129515000444.\
[65] Tom´aˇs Gonda and Robert W. Spekkens. Monotones in General Resource Theories. Compositionality, 5, August 2023. doi:10.32408/compositionality-5-7.\
[66] Robert W. Spekkens. Evidence for the epistemic view of quantum states: A toy theory. Phys. Rev. A, 75:032110, Mar 2007. doi:10.1103/PhysRevA.75.032110.\
[67] Manik Banik, Some Sankar Bhattacharya, Amit Mukherjee, Arup Roy, Andris Ambainis, and Ashutosh Rai. Limited preparation contextuality in quantum theory and its relation to the cirel’son bound. Phys. Rev. A, 92:030103, Sep 2015. doi:10.1103/PhysRevA.92.030103.\
[68] Andr´e Chailloux, Iordanis Kerenidis, Srijita Kundu, and Jamie Sikora. Optimal bounds for parity-oblivious random access codes. New Journal of Physics, 18(4):045003, apr 2016. doi:10.1088/1367-2630/18/4/045003.\
[69] Shouvik Ghorai and A. K. Pan. Optimal quantum preparation contextuality in an n-bit parity-oblivious multiplexing task. Phys. Rev. A, 98:032110, Sep 2018. doi:10.1103/PhysRevA.98.032110.\
[70] Debashis Saha, Pawe l Horodecki, and Marcin Paw lowski. State independent contextuality advances one-way communication. New Journal of Physics, 21(9):093057, sep 2019. doi:10.1088/1367-2630/ab4149.\
[71] Andris Ambainis, Manik Banik, Anubhav Chaturvedi, Dmitry Kravchenko, and Ashutosh Rai. Parity oblivious d-level random access codes and class of noncontextuality inequalities. Quantum Information Processing, 18(4):111, 2019. doi:10.1007/s11128-019-2228-3.\
[72] Armin Tavakoli, Emmanuel Zambrini Cruzeiro, Roope Uola, and Alastair A. Abbott. Bounding and simulating contextual correlations in quantum theory. PRX Quantum, 2:020334, Jun 2021. doi:10.1103/PRXQuantum.2.020334.\
[73] Lorenzo Catani, Ricardo Faleiro, Pierre-Emmanuel Emeriau, Shane Mansfield, and Anna Pappa. Connecting xor and xor∗ games. Phys. Rev. A, 109:012427, Jan 2024. doi:10.1103/PhysRevA.109.012427.\
[74] Massy Khoshbin, Lorenzo Catani, and Matthew Leifer. Alternative robust ways of witnessing nonclassicality in the simplest scenario. Phys. Rev. A, 109:032212, Mar 2024. doi:10.1103/PhysRevA.109.032212.\
[75] Antony Valentini. Signal-locality, uncertainty, and the subquantum h-theorem. i. Physics Letters A, 156(1):5 – 11, 1991. doi:https://doi.org/10.1016/0375-9601(91)90116-P.\
[76] David Schmid, John H. Selby, and Robert W. Spekkens. Addressing some common objections to generalized noncontextuality. Phys. Rev. A, 109:022228, Feb 2024. doi:10.1103/PhysRevA.109.022228.\
[77] Robert W. Spekkens. Quasi-Quantization: Classical Statistical Theories with an Epistemic Restriction, pages 83–135. Springer Netherlands, Dordrecht, 2016. doi:10.1007/978-94-017-7303-4 4.\
[78] Lorenzo Catani and Dan E Browne. Spekkens’ toy model in all dimensions and its relationship with stabiliser quantum mechanics. New Journal of Physics, 19(7):073035, jul 2017. doi:10.1088/1367-2630/aa781c.\
[79] Lorenzo Catani and Dan E. Browne. State-injection schemes of quantum computation in spekkens’ toy theory. Phys. Rev. A, 98:052108, Nov 2018. doi:10.1103/PhysRevA.98.052108.\
[80] Alberto Montina and Stefan Wolf. Realism and causality imply information erasure by measurements. arXiv:2307.03134, 2023. doi:https://doi.org/10.48550/arXiv.2307.03134.\
[81] Charles H. Bennett, Herbert J. Bernstein, Sandu Popescu, and Benjamin Schumacher. Concentrating partial entanglement by local operations. Phys. Rev. A, 53:2046–2052, Apr 1996. doi:10.1103/PhysRevA.53.2046.\
[82] Francesco Buscemi. All entangled quantum states are nonlocal. Phys. Rev. Lett., 108:200401, May 2012. doi:10.1103/PhysRevLett.108.200401.\
[83] David Schmid, Thomas C. Fraser, Ravi Kunjwal, Ana Belen Sainz, Elie Wolfe, and Robert W. Spekkens. Understanding the interplay of entanglement and nonlocality: motivating and developing a new branch of entanglement theory. Quantum, 7:1194, December 2023. doi:10.22331/q-2023-12-04-1194.\
[84] Matthew S. Leifer and Matthew F. Pusey. Is a time symmetric interpretation of quantum theory possible without retrocausality? Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2202):20160607, 2017. doi:10.1098/rspa.2016.0607.\
[85] Anubhav Chaturvedi and Debashis Saha. Quantum prescriptions are more ontologically distinct than they are operationally distinguishable. Quantum, 4:345, October 2020. doi:10.22331/q-2020-10-21-345.A Physicality of the Holevo projection(1) Lorenzo Catani, International Iberian Nanotechnology Laboratory, Av. Mestre Jose Veiga s/n, 4715-330 Braga, Portugal (lorenzo.catani4@gmail.com);(2) Thomas D. Galley, Institute for Quantum Optics and Quantum Information, Austrian Academy of Sciences, Boltzmanngasse 3, A-1090 Vienna, Austria and Vienna Center for Quantum Science and Technology (VCQ), Faculty of Physics, University of Vienna, Vienna, Austria (thomas.galley@oeaw.ac.at);(3) Tomas Gonda, Institute for Theoretical Physics, University of Innsbruck, Austria (tomas.gonda@uibk.ac.at).[8] A recent work that connects preparation contextuality and information erasure is [80], where the authors show that any ontological model reproducing the statistics of a sequential protocol involving incompatible projective measurements involves more information erasure than what operational quantum theory predicts. This fact is strictly related to the presence of preparation contextuality, as the same final quantum state in the protocol is represented by two different ontic distributions. This result can be seen as an example of fine-tuning of information erasure (i.e. more erasure at the ontological level than at the operational level), which, despite being interesting, differs from our idea of viewing contextuality as arising from a process of information erasure that explains the operational equivalences of distinct ontological representations.]]></content:encoded></item><item><title>The Science of Causality and the Resetting of Karma: A Manifesto for a New Way of Thinking</title><link>https://hackernoon.com/the-science-of-causality-and-the-resetting-of-karma-a-manifesto-for-a-new-way-of-thinking?source=rss</link><author></author><category>tech</category><pubDate>Wed, 24 Sep 2025 05:57:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What if karma isn’t a moral law — but an engineering problem?
This manifesto proposes a radical idea: karma is a field recording in the continuum of causality, and it can be reset — not through prayer or penance, but through total field isolation.
Supported by cutting-edge neuroscience — including evidence that microtubules in our brain operate at the quantum level — I argue that ancient structures like the Serapeum may have been devices for disconnecting from reality’s causal network.
This is not mysticism. It’s a scientific hypothesis with revolutionary implications for how we understand consciousness, responsibility, and freedom.]]></content:encoded></item><item><title>How I Built a Simple MDX Blog in Next.js and why I chose native mdx over Contentlayer</title><link>https://hackernoon.com/how-i-built-a-simple-mdx-blog-in-nextjs-and-why-i-chose-native-mdx-over-contentlayer?source=rss</link><author>Max</author><category>tech</category><pubDate>Wed, 24 Sep 2025 05:57:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[There are many ways to host blogs with Next.js but I needed something fast & simple: plain MDX files, first‑party support, and zero extra content pipelines. No Contentlayer (which is unmaintained). No . No heavy weighted CMS systems.Next.js’s official MDX integration lets you import  as components and export  alongside content.I used  with the App Router and kept indexing simple by importing metadata directly from each MDX file.No extra build steps, no content database, and the sitemap pulls dates straight from the MDX front‑matter.Content is code: The App Router treats a folder as a route and  as a component. You get layouts, streaming, and RSC benefits for free.First‑party MDX: The official plugin is maintained with Next.js and plays nicely with routing, metadata, and bundling.Lower cognitive load: For a small product site, I don’t want a content compiler, watcher, or a GraphQL layer. A few MDX files and some imports are enough.Add the official MDX plugin and let Next treat MD/MDX as pages.import createMDX from '@next/mdx';

const withMDX = createMDX({
  // Add remark/rehype plugins if/when needed
  options: {
    remarkPlugins: [],
    rehypePlugins: [],
  },
});

/** @type {import('next').NextConfig} */
const nextConfig = {
  ...
  pageExtensions: ['ts', 'tsx', 'js', 'jsx', 'md', 'mdx'],
};

export default withMDX(nextConfig);
Optionally customize how MDX renders components (I kept it minimal for now):import type { MDXComponents } from 'mdx/types';

export function useMDXComponents(components: MDXComponents = {}): MDXComponents {
  return {
    ...components,
  };
}
Type the metadata you export from MDX so TS understands it when imported elsewhere.declare module '*.mdx' {
  import type { ComponentType } from 'react';
  const MDXComponent: ComponentType<any>;
  export default MDXComponent;
  export const metadata: {
    title?: string;
    description?: string;
    date?: string;
    author?: string;
    tags?: string[];
  };
}
Create a post as a route. In the App Router, a folder is your slug and  is the page.src/app/blog/how-to-export-ig-followers-tutorial/page.mdxexport const metadata = {
  title: 'How to Export Instagram Followers (CSV, Excel, JSON)',
  description: 'Step-by-step guide…',
  date: '2025-08-28',
};

import Image from 'next/image';
Build a simple index page by importing metadata straight from MDX modules.import Link from 'next/link';
import { metadata as igExport } from './how-to-export-ig-followers-tutorial/page.mdx';

const posts = [
  {
    slug: 'how-to-export-ig-followers-tutorial',
    title: igExport?.title ?? 'How to Export Instagram Followers',
    description: igExport?.description,
    date: igExport?.date,
  },
];

export default function BlogIndexPage() {
  // Render cards linking to /blog/[slug]
}
Keep your sitemap honest by importing the same metadata for .import type { MetadataRoute } from 'next';
import { metadata as igExportPost } from './blog/how-to-export-ig-followers-tutorial/page.mdx';
import { getURL } from '@/utils/get-url';

export default function sitemap(): MetadataRoute.Sitemap {
  return [
    // …other routes
    {
      url: getURL('blog/how-to-export-ig-followers-tutorial'),
      lastModified: igExportPost?.date ? new Date(igExportPost.date) : new Date(),
      changeFrequency: 'weekly',
      priority: 0.7,
    },
  ];
}
The Aha Moments (and a few gotchas)MDX as modules: You can import both the rendered component and named exports () from any  file. That made the blog index and sitemap trivial.Keep it typed: The  module declaration means TS won’t complain when you do import { metadata } from 'some-post/page.mdx'.Less is more: I didn’t reach for Contentlayer because I don’t need filesystem crawling or transformations. With a handful of posts, a tiny array is fine.Contentlayer vs. Native MDXWhat Contentlayer gives you:Schemas and types: Define required fields and get generated TypeScript. Build fails if a post is missing a  or .Content graph: Read files from a  directory, compute slugs/paths, and query everything in one place.Computed fields: Derive , , canonical URLs, etc., at build time.Good for docs sites: Multiple document types (Guides, API refs, Changelogs) with strict structure.Native MDX strengths (why I chose it here):Zero ceremony: No schema layer, no background watcher — just  files and imports.Co‑location: The post lives at , same place users will visit.Good enough typing: A tiny  module declaration plus optional Zod to validate  if you want stricter checks.]]></content:encoded></item><item><title>Shadow AI Is Inevitable. Here’s How You Can Govern It Without Killing Speed</title><link>https://hackernoon.com/shadow-ai-is-inevitable-heres-how-you-can-govern-it-without-killing-speed?source=rss</link><author></author><category>tech</category><pubDate>Wed, 24 Sep 2025 05:54:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[You can’t protect or govern what you can’t see. The new frontier of compliance isn’t stopping AI, it’s channeling it \
When leaders say “shadow IT,” I picture expense-report footprints, forgotten SaaS trials, rogue cloud accounts, a server humming under someone’s desk.  leaves no receipts. It’s a browser tab, a personal plug-in, an unregistered API key behind a helpful automation. If that sounds like your company, it’s because it is.A classic  episode, “The Ultimate Computer,” imagined an autonomous system taking a starship off-mission because governance was bolted on after launch. That’s the parable of 2025. Capability without designed controls creates avoidable drama. Recent research makes the point less quaint. Anthropic’s tests of “agentic misalignment” showed how, under adversarial pressure, some models role-played insider-ish behavior. This is a governance wake-up call.Shadow AI harms in three ways you’ll feel in the boardroom. First, . Engineers paste code or analysts paste contracts into consumer AI. Where those tokens go, region, retention, and if it’s used for training is opaque. Second, . You can’t prove record-keeping, redaction, or policy enforcement if activity happens off-platform. Regulators don’t accept “trust us” (If you need a vocabulary for this, NIST’s AI Risk Management Framework is a sensible north star). Third, . Unvetted models and over-privileged automations make persuasive but wrong decisions. Vendors quietly route your data through their own AI, testing your contracts and geography. An auditable management-system approach like ISO/IEC 42001 helps standardize this conversation.Strategies That Actually Work If an AI request doesn’t traverse the company’s reverse proxy/LLM gateway, it didn’t happen. The gateway allow-lists endpoints, blocks the rest, strips secrets, keeps track of costs, redacts PII, hardens prompts, applies output filters, and writes immutable logs. Most importantly, it binds identity, human or non-human, to every call so actions are attributable and revocable. Think “payments switch,” but for prompts and completions.IAM for agents (non-human identities). Bots, service accounts, copilots, schedulers, notebooks: they all need first-class identity. The lifecycle should be simple and non-negotiable. Discover → Register → Authenticate (OIDC/mTLS) → Authorize (OAuth2/least privilege) → Govern (rotation, attestations, lineage) → Decommission**.** No shared keys. No orphaned agents. No privileges without an owner or an expiry.Shadow AI is market research you didn’t pay for. When employees reach for ChatGPT-style assistants or code copilots, they’re telling you where productivity is stuck. Keep it simple - buy the enterprise versions of what people already use, then route them through your gateway. You earn observability, retention guarantees, and policy enforcement, and they keep their speed. Culture and compliance finally stop fighting the same war.Employees are your fastest path to value and your easiest path to leaks. Unmanaged devices bypass DLP, store local prompts, and leave no trail. Require managed browsers or high trust authentication for AI-assisted work that touches sensitive data, plus conditional access that keeps crown-jewel datasets off personal machines. Training matters, but it can’t be a scold; show real prompt-injection and leakage examples, then show the safer, faster path through the gateway.Vendors and BPOs chase efficiency with their own AI stacks. That’s fine, inside your guardrails. Contracts should require your gateway or a jointly governed tenant, disclose models used, guarantee data location/retention, and grant log-export rights. For high-risk workflows, provide a segregated VDI so all traffic inherits your controls.Seeing What You Can’t See (Yet)You can’t govern in the dark. Start with DNS/Identity Provider/proxy/SASE telemetry to catch direct calls to model endpoints and suspicious API hosts. Managed-browser inventories will surface unsanctioned AI extensions. Secrets in prompts are canaries. On the sanctioned side, your logs should tell a complete story, who/what identity, which dataset, which model, what input/output. Stream it to the SIEM and correlate with IAM and data-access events. Governance becomes investigation-ready instead of vibes-based.Month one - stand up the gateway, allow-list providers, turn on redaction and logging, and pause new service accounts until they’re registered.Month two - launch the non-human identity registry, migrate agents to OAuth2 or mTLS, define standard scopes/expirations, and roll out updated employee/BYOD and vendor addenda.Month three - hunt for shadow-AI bypasses, red-team your prompt paths, and publish board-level metrics: % of AI calls via gateway, % of agents registered with expiry, mean time to revoke an agent, # of sensitive uploads blocked.Those numbers tell your story to regulators and investors far better than policy PDFs ever will.Treat every AI interaction like a financial transaction. It must carry identity, policy, and a receipt. Give every agent a name. Route every call through one gate. And invest in the tools your people are already voting for with their behavior. That’s how you keep speed and stay defensible.If  gave us the parable, NIST and ISO now give us the scaffolding. The rest is leadership. Design governance up front, so innovation isn’t something compliance cleans up after.]]></content:encoded></item><item><title>Curate Your Own Ad-Free YouTube Experience: A Simple Guide for Watching Your Favorite Channels</title><link>https://hackernoon.com/curate-your-own-ad-free-youtube-experience-a-simple-guide-for-watching-your-favorite-channels?source=rss</link><author>Andrew Schwabe</author><category>tech</category><pubDate>Wed, 24 Sep 2025 05:49:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Love a few select YouTube channels but want to enjoy them without interruptions or algorithmic suggestions pulling you elsewhere? This tutorial is for you! We’ll walk through a process to create a personalized, ad-free library of your favorite content using Mac software, though it’s easily adaptable for Windows or Linux. This setup lets you watch on any device, even offline, while keeping things streamlined and focused on the creators you care about. This is an educational guide for personal use, respecting creators’ work. Always support your favorite channels through direct views, merch, or subscriptions when possible. Our goal is to enhance your viewing experience, not bypass revenue streams.Step 1: Download Videos with yt-dlp (Your Starting Point)Let’s begin by downloading videos from your favorite channels using , a free, open-source command-line tool (a maintained fork of youtube-dl). It’s lightweight, flexible, and works across platforms, letting you grab videos without ads.Install Homebrew (Mac’s package manager) via Terminal:   /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
Grab your channel’s URL (e.g., https://www.youtube.com/@YourFavoriteChannel).   yt-dlp https://www.youtube.com/watch?v=VIDEO_ID
Or an entire channel (use sparingly to manage storage):   yt-dlp -f best https://www.youtube.com/@YourFavoriteChannel/videos
Use options like --playlist-start 1 --playlist-end 5 to download only the latest 5 videos, or --limit-rate 1M to cap download speed. Install via Chocolatey (choco install yt-dlp) or download the executable from yt-dlp’s GitHub. Use Command Prompt or PowerShell. Install via your package manager (e.g., sudo apt install yt-dlp on Ubuntu). Commands are the same. Save downloads in a folder like ~/Videos/MyChannels. Create a script to run yt-dlp for your favorite channels regularly.This gives you offline, ad-free videos on your computer. Want to take it further? Let’s make it multi-device.Step 2: Set Up a Plex Server for Seamless Multi-Device ViewingTurn your downloads into a personal streaming hub with , a free media server (with optional paid features). Plex organizes your videos and streams them to your phone, tablet, TV, or computer—ad-free and even offline.Download Plex Media Server from plex.tv and install it.Sign in (or skip for local use) and create a library pointing to your download folder (e.g., ~/Videos/MyChannels). Organize by channel for easy navigation.Install the Plex app on your devices (iOS, Android, Smart TVs, or browsers).Stream or download for offline viewing. Plex fetches metadata like titles and thumbnails for a polished look.Watch on your phone during a flight? Check.Share with family on a Smart TV? Done.No Wi-Fi? Plex works locally or with cached content. Download and install from plex.tv. Use sudo apt install plexmediaserver or equivalent. Setup is nearly identical. Plex transcodes videos for smooth playback on any device. Run it on a NAS or spare PC for 24/7 access.Step 3: (Optional) Automate with n8n for Effortless UpdatesWant new videos to appear in your library automatically? Use , a free, self-hosted automation tool, to check your favorite channels, download new content, and add it to Plex.Install Node.js: brew install node.Install n8n: npm install -g n8n.Start n8n: n8n start (runs a web interface at localhost:5678).Use a “Cron” node to check channels daily.Add a “HTTP Request” node to scrape channel RSS feeds (e.g., https://www.youtube.com/feeds/videos.xml?channelID).Trigger yt-dlp to download new videos to your Plex folder.Notify Plex to refresh the library.Install Node.js and n8n similarly. The workflow setup is platform-agnostic.This keeps your library fresh without manual effort.As mentioned above, this is just an educational exercise to enhance your viewing experience, not to impact creator or platform revenue. How would you adapt or enhance this process to solve a problem for you or your company? Maybe you’d automate metadata cleanup, integrate a notification system, or build a shared library for a team. Share your ideas!]]></content:encoded></item><item><title>Decentralization’s Dark Patterns: Why Open Networks Keep Rebuilding Gates</title><link>https://hackernoon.com/decentralizations-dark-patterns-why-open-networks-keep-rebuilding-gates?source=rss</link><author>A coldly methodical kin who thrives on chaos and control, transforms suffering into spectacle.</author><category>tech</category><pubDate>Wed, 24 Sep 2025 05:44:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
\
\
The bazaar is still lively; stalls are full, voices are loud, and deals are happening everywhere. But if you look closely, the gates are taller now, and guards at the entrances are quietly collecting fees. The place still looks like an "open market," but it feels more like going through a checkpoint than wandering freely.Decentralization promised to remove bottlenecks, flatten hierarchies, and let us escape digital landlords. But now, we see new gatekeepers appearing, using the language of openness, offering the same old permissions in a fancier way.If decentralization really won, why does power still feel so familiar?\
\
“Dark patterns of decentralization” are not errors in the code; they are habits in human systems that slowly guide open networks back to control points. These structures seem like open markets; free, many-to-many, and strong; but over time, they lean towards gates and tolls. Same setups, same messages, but with new controllers.Think of it as : the protocol is neutral, but power builds up in everything around it, like interfaces, infrastructure, treasuries, and social councils. We convince ourselves it's okay because the base layer stays open. Meanwhile, our daily experience is managed by a small group of "coordinators" who simplify the hard parts (and charge for the easy parts).So, let’s name the drift.: low voter participation, recurring leaders, and proposal restrictions disguised as “process.”: concentrated token ownership and treasuries that can influence narratives and outcomes.Cartelization at the edges: validators, sequencers, indexers, relays, few operators with too much power.: front-ends, RPCs, app stores, SDKs; convenience becomes a subtle control point.: foundations and councils that slowly change from caretakers to rulers.None of this needs a conspiracy. It's just entropy. Coordination is expensive, convenience is attractive, and power, if unchecked, settles where it's easiest. This is why the decentralized internet can feel centralized even when the chain is working perfectly.That’s the uncomfortable truth: if we only focus on “trust the protocol,” we ignore everything around it. The question isn’t if new forms of control will appear, they already have, in small ways. The real task is to recognize these patterns early and change them before “open” becomes just a label, not a reality.Decentralization doesn't end suddenly; it slowly shifts, meeting by meeting, vote by vote, into the control of a few well-known names. Low participation makes this easy: a small amount of token weight, permanently delegated, becomes the main decision-maker. Quorums are adjusted, proposal templates spread, and those who can keep up with every forum thread end up writing what others just agree to. The protocol stays neutral, but the agenda does not.And because the process seems fair, gatekeeping can look like good management. "We're just maintaining quality." "We're protecting the treasury." Nice phrases; all true sometimes, until disagreement becomes a paperwork issue instead of a political oneToken voting seems fair until you realize that money grows faster than participation. Premine, early allocations, strategic market makers, and programmatic buybacks aren't bad on their own, but together they create an imbalance. Treasury influence is more subtle: deciding who gets grants, pilot liquidity, and whose roadmap is highlighted at conferences.At the same time, order flow isn't neutral. MEV, or maximum extractable value, turns blockspace into a marketplace where those with the best connections gain the most. It's not illegal or even necessarily wrong. It's just powerful, and power attracts alliesCartelization at the EdgesMost networks don't focus on the center; they focus on the edges. Roles like validators, sequencers, relays, and indexers may seem similar but actually benefit from size, reliability, and special equipment (and patience with regulations). A few providers become very skilled, noticeable, and hard to replace.Shared security and restaking can help small chains stay alive. But they can also lead to sameness; safe in the short term but more connected risks in the long term. The impact area gets bigger; choices become fewerUsers don’t interact with protocols; they interact with interfaces. These include front-ends, SDKs, wallet plugins, RPC gateways, and app stores. Each one is a convenience layer and also a possible bottleneck. Flip one switch (or follow one “temporary” policy), and a public network can suddenly act like a private platform.Cloud makes this worse because it simplifies everything. One provider for hosting, CI/CD, logs, analytics, and DNS seems convenient, right? (Until a term of service change, an outage, or a legal request turn “why not” into “we can’t.”Every movement needs leaders. But leaders gain influence through communication, events, branding, and planning until it's hard to tell if they are speaking for the community or as the community. Groups like foundations, councils, and moderation panels are made to protect shared resources but can be tempted to control them.Emergency powers, once given, often stick around "just in case." Eventually, "just in case" becomes the norm.If this sounds familiar, that's good. The goal isn't to feel hopeless; it's to recognize these patterns early so we can prevent the need for strict controls late.\
\
If you can't measure it, you can't protect it. Here's a quick dashboard with six indicators you can monitor to ensure "open network" doesn't become just a label.Nakamoto coefficient & validator share. The Nakamoto coefficient asks: How many independent operators need to work together to stop or take over the network? A higher number is better, and trends are important. Check it alongside the validator/sequencer share by entity to see if block production is concentrating around a few namesToken Gini + float vs. insiders. Inequality isn't just a feeling; the  shows how uneven token ownership is (0 means evenly spread, 1 means one wallet owns everything). Keep an eye on this along with free float; the part truly in public hands compared to team/treasury/VC lockups, because "circulating supply" can hide who really controls things. Focus on the trend, not just one moment.Governance turnout & delegate churn. Healthy systems don't just allow voting; people actually participate, and power changes hands. Track voter turnout for each proposal, and  (who's gaining or losing delegated weight over time). Tools like Snapshot make this visible; studies often highlight low participation as a major risk. Having different clients and providers is essential for backup. Keep an eye on the share of execution/consensus clients to prevent any single client from taking over. Use a variety of RPCs so that if one goes down, your "decentralized" app doesn't stop working. This has happened before, causing front-ends and wallets to freeze when a major provider failed.. While protocols might be unstoppable,  can be stopped. Host front-ends on content-addressed storage like IPFS or Arweave, pin them, map them with ENS, and make sure builds are publicly available. Check if the live site links to a CID and is pinned by more than one source, rather than just being stored in a cloud bucketCensorship-resistance benchmarks. Neutral block building is a feature, not just an announcement. Monitor  for sensitive transactions, and the percentage of blocks created by relays that use external blacklists. Public dashboards are available, so use them and choose non-censoring routes.None of these metrics are flawless. Combined, they act as a warning system. When two or three indicators show issues, like increasing validator concentration, low participation, or front-ends relying on a single provider, you're not being paranoid. You're ahead of the curveWe don’t just argue against centralization; we design better solutions. Here are some improvements you can implement, starting with governance, then economics, then user experience and infrastructure, and finally the human aspect that connects everything.Quadratic voting (and similar methods). When one-token-one-vote becomes one-big-investor-one-plan, change the voting method. Quadratic voting allows participants to show how strongly they feel by paying the square of the votes they buy, reducing the influence of the wealthy and preventing group takeovers. This method is discussed in Vitalik’s writings and RadicalxChange’s guides. Use it carefully, considering identity and collusion, but it’s a powerful tool.. Implement a strict  for proposals to give everyone time to react. Compound introduced a minimum two-day delay between approval and execution; proposals are queued and then executed, allowing for review and, if necessary, opposition. Make this a standard part of your governance process.Rotating juries by random selection. For disputes, grants, and quality decisions, don’t rely on a fixed council; randomly select members. Systems like Kleros choose jurors at random (based on stake/availability), refreshing perspectives and reducing the formation of cliques. Use random selection where judgment is more important than simple voting.Fair unlocks, visible to all. Clearly and early, show vesting and float details. Focus less on who owns and more on who can actually vote or exit.Recurring user rewards (after the fact). Retroactive funding rewards proven impact, not just promises. Optimism’s RetroPGF is a working example: badgeholders distribute funds to public-goods contributors from protocol surplus, showing that user value leads to ongoing benefits.Multi-provider (and self-hosted) RPCs. Don't rely on just one gateway for your app. Run your own if you can and set up automatic switching between providers. The 2020 Infura outage stopped wallets and dapps, teaching us a lessonOpen-source front-ends, content-addressed deploys. Serve the UI from IPFS/Arweave and link it with ENS. Pin the CID in several places to prevent access issues during a takedown or outage. Uniswap’s IPFS interface and IPFS docs offer a useful example (pin to more than one node/provider). When possible, make sure important functions work without needing a central server (then sync later). Research by Ink & Switch and Kleppmann et al. shows how CRDT-based apps keep user control and work offline.Conflict-of-interest disclosures. Require leaders, delegates, and reviewers to share any financial or role conflicts. This is a standard practice for corporate governance, adapted for DAOs. The OECD guidance is a good starting point for what to disclose and why.Term limits and role rotation.  Avoid having "forever delegates." Regularly change authorship committees, review boards, and communication leads.Sunset clauses for emergency powers. Special permissions should automatically end unless renewed. This is a common democratic safeguard to prevent emergencies from becoming normal. Build the expiry into the code and charter.These are not perfect solutions. However, together they help ensure that the system naturally resists centralization before it becomes part of the culture, then policy, and finally something we feel we can't change. Run your own nodes when possible and set up automatic failover with multiple independent RPC providers.Compliance at the flick of a switch. In March 2022, users in sanctioned regions found that "decentralized" sometimes means "not reachable," as MetaMask's default provider, Infura, blocked access in certain areas. A misconfiguration briefly affected Venezuelan users too, before being fixed. The blockchain stayed open, but the main access route did not. Interfaces and the infrastructure behind them can be stopped. Consider interfaces as conveniences, not guarantees, and provide users with at least one non-custodial, self-hostable option.When the front-end shapes the “decentralized” experience. Uniswap, the protocol, kept running smoothly. Uniswap Labs, the main web interface, tightened what it shows. Starting in 2021 and continuing with policies updated through 2025, the Labs-managed front-end limited access to certain tokens and can block specific contracts or addresses due to changing regulations. However, the company emphasizes that the underlying smart contracts remain open to everyone. The message is subtle but clear: UI policy isn't the same as protocol policy until it affects most users. Create content-addressed, open-source front-ends (and mirror them), so “can’t click here” never turns into “can’t use it at all.”Neutrality under pressure in the block-building era. After Ethereum’s Merge, validators widely used MEV-Boost relays; many popular relays promised to follow OFAC rules, and for a while, more than half of the blocks were built through censoring paths. Community efforts with non-censoring relays and monitoring tools have since reduced that number at times, but the risk is clear and ongoing: censorship can sneak in through economically smart middleware. Choose non-censoring relays, track inclusion delays, and make neutrality a measurable goal, not just a statement.The DAO that learned what “ratification” means the hard way. In early 2023, Arbitrum's first governance package upset the community when the foundation used tokens that the DAO thought were still being voted on, leading to backlash and a promise to separate proposals and redo the process. The blockchain stayed intact, but the community trust was briefly broken, showing that control can be about procedures, not just technology. Set clear timelocks, keep budgets small, and make sure that “advice and consent” can't be changed later to just “approval.”Here’s a quick check you can read out loud in a stand-up meeting and save for later. It’s not a strict rule, just a quick test. Green means “keep going,” yellow means “watch this closely,” and red means “you’ve created a problem; take responsibility and fix it.”Start by looking at the Nakamoto coefficient and how block production is shared among entities. Aim for the coefficient to be in double digits and increasing, with no single operator able to stop finality or reduce activity on their own. If two or three entities can work together to disrupt things, you’re becoming centralized. Check the actual distribution, not just the brand names: different legal entities, different infrastructure, different failure risks.Next, look at token distribution using a simple Gini index and, more importantly, the ratio of freely available tokens to those held by insiders and the treasury. A healthy situation shows a decreasing Gini over time, more freely available tokens, and clear token releases that don’t surprise the market or governance. If insiders and the treasury can often decide outcomes by themselves, recognize this and limit it by design. If community ownership is only increasing in name, it will show in voting results.Check governance participation and rotation. Generally, the median turnout should be respectable, with a significant portion of eligible voting power involved in important proposals. The top delegates shouldn't be able to easily control the majority for long periods. If the same few wallets decide most proposals and delegate weights never change, it's not governance; it's a board meeting.Audit client and RPC diversity as if uptime depends on it; because it does. No single client should dominate to the extent that one bug could cause a chain-wide issue. Your default setup should include at least one self-hosted option and multiple independent RPC providers with automatic failover. If one outage can shut down your app, recognize it as a platform risk disguised as a protocol issue.Evaluate front-end independence carefully. A strong project offers open-source interfaces, reproducible builds, content-addressed deployments that lead to stable CIDs, and at least two independent pinning and hosting options linked to a human-readable name. If your only entry point is a single DNS record in one cloud account, you already know the outcome.Measure censorship resistance as an ongoing goal, not just a phrase. Monitor how long it takes for "sensitive" transactions to be included, check the percentage of blocks created through censoring versus non-censoring methods, and share relay and builder diversity like you would with latency. The goal is zero, but the trend is what matters; if the censoring share is significant and increasing, consider it a serious cultural and structural issue.If you can read this checklist without discomfort, you're doing well. If it makes you uncomfortable, congratulations, you've identified the work that needs to be done.The contest isn't looking for praise of decentralization; it's asking if the web we're building can avoid old problems like censorship, network effects, and algorithmic control without just making the same mistakes again. The prompts are clear: how do big-platform algorithms influence what we see, how do network effects keep people stuck with big companies, and can a decentralized internet really stop censorship, or do new control methods appear anyway? In other words, find the weak spots and show you can work around them.Regarding censorship, the tough truth is that while protocols can be "permissionless," the paths most people use is not. This is why the #decentralized-internet track exists: to explore designs where speech and access don't fail when a front-end changes its rules or a provider makes a change. The brief is clear: investigate whether decentralization actually lowers the risk of suppression at the infrastructure level, not just in theory but in reality.When it comes to network effects, the benefits grow as more people use a platform. This makes it hard to switch because both your habits and connections are tied to it. Studies in management and law see these feedback loops as key to why platforms become dominant. Any "free" alternative needs to create its own loops or get absorbed by the main platform. This isn't just about feelings; it's about how things work.For algorithmic gatekeeping, we've shifted from human editors to machine-ranked feeds that decide what gets seen. Research shows that automated systems now play a big role in deciding what gets attention and even influences public opinion. This isn't necessarily bad, but it is very powerful. If our decentralized systems end up with the same hidden ranking controls, we'll just be swapping one controlled space for another.This is also why a brief nod to the physical layer isn’t optional. The contest partner’s pitch leans into DePIN; decentralized physical infrastructure as a way to diversify how packets move in the first place, proposing a constellation of small satellites and on-chain coordination to reduce reliance on terrestrial chokepoints. You can agree or disagree with the approach, but the direction is the point: add independent paths, or accept that “the internet” is only as free as its narrowest pipe.And it isn’t just theory. Bitcoin’s satellite broadcast shows how redundancy looks when you mean it; blocks beamed globally, independent of local internet connectivity. Community meshes show the same ethic at street level. neighbor-run backhaul, neutral routing, open peering. Even mainstream LEO constellations, while not decentralized, broaden the set of available paths and, in doing so, blunt single-jurisdiction leverage. More routes, fewer choke points. That’s the entire game.Yes, this is important now because the time to set norms is limited. The #decentralized-internet challenges us to show that our designs can resist censorship without adding new filters, compete with network effects without becoming dependent on them, and replace hidden gatekeepers with transparent, accountable systems. If we can address this in code, policy, and culture this time, this month, we're not just joining a contest. We're testing if the bazaar remains open and free.Choose a project you love, like a wallet, L2, rollup, DAO, or marketplace, and run the dashboard on it today. Check the validator and builder distribution, look at the token Gini and free float, see if participation is genuine or just routine, try a different RPC, resolve the front-end to a CID, and monitor inclusion delays for anything "sensitive." Share your findings publicly. If you can't measure it, you can't protect it; if you won't share it, you likely already know the answer.If you're building, make one change against central control in the next sprint. Add a real timelock you can't bypass, rotate a committee by random selection, limit treasury voting power, mirror the UI to IPFS or Arweave and link it through ENS, replace a single-provider RPC dependency with an automatic backup that includes something you manage. None of this is just theory; it's all changes you can implement.If you moderate or manage, publish conflicts, set term limits, and end the emergency powers you said were temporary. If you're a user, ask for these things openly, and move your usage and funds to places that follow these practices. Votes are important, but so is engagement.Do the audit. Report on the issue. Implement the fix. Then do it again next month.Keep the bazaar a bazaar.]]></content:encoded></item><item><title>Why AI Chatbots Can&apos;t Process Persian Social Etiquette</title><link>https://tech.slashdot.org/story/25/09/24/0011224/why-ai-chatbots-cant-process-persian-social-etiquette?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: If an Iranian taxi driver waves away your payment, saying, "Be my guest this time," accepting their offer would be a cultural disaster. They expect you to insist on paying -- probably three times -- before they'll take your money. This dance of refusal and counter-refusal, called taarof, governs countless daily interactions in Persian culture. And AI models are terrible at it.
 
New research released earlier this month titled "We Politely Insist: Your LLM Must Learn the Persian Art of Taarof" shows that mainstream AI language models from OpenAI, Anthropic, and Meta fail to absorb these Persian social rituals, correctly navigating taarof situations only 34 to 42 percent of the time. Native Persian speakers, by contrast, get it right 82 percent of the time. This performance gap persists across large language models such as GPT-4o, Claude 3.5 Haiku, Llama 3, DeepSeek V3, and Dorna, a Persian-tuned variant of Llama 3.
 
A study led by Nikta Gohari Sadr of Brock University, along with researchers from Emory University and other institutions, introduces "TAAROFBENCH," the first benchmark for measuring how well AI systems reproduce this intricate cultural practice. The researchers' findings show how recent AI models default to Western-style directness, completely missing the cultural cues that govern everyday interactions for millions of Persian speakers worldwide. "Cultural missteps in high-consequence settings can derail negotiations, damage relationships, and reinforce stereotypes," the researchers write.
 
"Taarof, a core element of Persian etiquette, is a system of ritual politeness where what is said often differs from what is meant," the researchers write. "It takes the form of ritualized exchanges: offering repeatedly despite initial refusals, declining gifts while the giver insists, and deflecting compliments while the other party reaffirms them. This 'polite verbal wrestling' (Rafiee, 1991) involves a delicate dance of offer and refusal, insistence and resistance, which shapes everyday interactions in Iranian culture, creating implicit rules for how generosity, gratitude, and requests are expressed."]]></content:encoded></item><item><title>Vietnam Shuts Down Millions of Bank Accounts Over Biometric Rules</title><link>https://news.slashdot.org/story/25/09/24/005245/vietnam-shuts-down-millions-of-bank-accounts-over-biometric-rules?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Longtime Slashdot reader schwit1 shares a report from ICO Bench: As of September 1, 2025, banks across Vietnam are closing accounts deemed inactive or non-compliant with new biometric rules. Authorities estimate that more than 86 million accounts out of roughly 200 million are at risk if users fail to update their identity verification.
 The State Bank of Vietnam has also introduced stricter thresholds for transactions:
- Facial authentication is mandatory for online transfers above 10 million VND (about $379).
- Cumulative daily transfers over 20 million VND ($758) also require biometric approval.
 
The policy is part of the central bank's broader "cashless" strategy, aimed at combating fraud, identity theft, and deepfake-enabled scams. [...] While many Vietnamese citizens have updated their biometric data without issue, the measure has disproportionately affected foreign residents and expatriates who cannot easily return to local branches and dormant accounts that had been left inactive for years. schwit1 highlights a post on X from Bitcoin expert and TFTC.io founder Marty Bent: "If users don't comply by the 30th they'll lose their money. This is why we bitcoin."]]></content:encoded></item><item><title>Disney+, Hulu Are Hiking Prices Again Next Month</title><link>https://news.slashdot.org/story/25/09/23/2358239/disney-hulu-are-hiking-prices-again-next-month?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Disney is raising prices again for Disney+, Hulu, and ESPN Select starting October 21, 2025, with most ad-supported tiers going up by $2-3 per month and bundles also seeing increases. It marks the third consecutive year of U.S. streaming price hikes. Variety reports: It's that time of year again, apparently: Disney is raising the prices of its Disney+ and Hulu plans in the U.S., including most bundles, as of next month. The standalone Disney+ with ads service is rising from $9.99 to $11.99/month on Oct. 21, 2025, while the Disney+ Premium (without ads) is going from $15.99 to $18.99/month. The Hulu standalone plan with ads is increasing from $9.99 to $11.99/month as of the same date; the premium version of Hulu with no ads will remain at $18.99 per month.
 
In addition, the price of ESPN Select (the service formerly known as ESPN+, which has a more limited content lineup than the recently launched ESPN Unlimited all-in app) will increase from $11.99 to $12.99 per month on Oct. 21. For now, the introductory price of the Disney+, Hulu and ESPN Unlimited bundle with ads will remain $29.99 per month (for the first 12 months). It's the third time in three years Disney is raising the prices of the streaming services in the U.S., after price hikes for Disney+ and Hulu in October 2024 and in October 2023. Disney provided notifications of the latest price hikes Tuesday on its customer support sites.]]></content:encoded></item><item><title>Microsoft Is Reportedly Building An AI Marketplace To Pay Publishers For Content</title><link>https://slashdot.org/story/25/09/23/2355226/microsoft-is-reportedly-building-an-ai-marketplace-to-pay-publishers-for-content?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 24 Sep 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft is preparing a Publisher Content Marketplace to pay publishers when their work is used in AI products like Copilot. Neowin reports: Microsoft is reportedly discussing with select US publishers a pilot program for its so-called Publisher Content Marketplace, a system that pays publishers for their content when it gets used by AI products, starting with its own Copilot assistant. The PCM will launch with a limited number of partners before Microsoft hopes to expand the program over time. The company pitched the idea to publishing executives at an invite-only Partner Summit in Monaco last week. Microsoft was allegedly courting them with the message: "You deserve to be paid on the quality of your IP." No concrete launch date for the pilot was shared.
 
As Axios notes, Microsoft is the first major company to try to build a proper AI marketplace for publishers. Other AI labs like OpenAI have mostly focused on securing one-off licensing deals instead of building a platform for ongoing transactions. Companies like Cloudflare are also working on a more technical, network-level solution to this problem.]]></content:encoded></item><item><title>Why Volvo Is Replacing Every EX90&apos;s Central Computer</title><link>https://tech.slashdot.org/story/25/09/23/2139249/why-volvo-is-replacing-every-ex90s-central-computer?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 23 Sep 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from InsideEVs: On Monday morning, I spoke to a Volvo EX90 owner who reported a litany of issues with her 2025 EX90: malfunctioning phone-as-a-key functionality, a useless keyfob, a keycard that rarely worked quickly, constant phone connection issues, infotainment glitches and error messages. I was surprised not because I hadn't heard of these kinds of problems, but because I experienced them myself over a year ago at the EX90 first drive again. At the time, Volvo said software fixes were imminent. Today, we know the issues go deeper. To solve them, Volvo announced on Tuesday that it will replace the central computer of every 2025 EX90 with the new one from the 2026 EX90. It's a tacit admission that the company can't solve the EX90's issues while simultaneously launching its next-generation software-defined vehicles, and that it's easier to replace the original computer than to build bug-free software for it. But for some, the damage to the Volvo brand has already been done. "I say without exaggeration that this car is a dumpster fire inside a train wreck," InsideEVs reader and EX90 owner Sally Greer told InsideEVs.
 
The report notes that Volvo will replace the computer inside the 2025 EX90 with a Nvidia Drive AGX Orin-based core computer that has contains over 500 TOPS (Trillion Operations Per Second) of power, which Volvo says will help power its autonomous driving ambitions.]]></content:encoded></item><item><title>MLB Approves Robot Umps In 2026 For Challenges</title><link>https://hardware.slashdot.org/story/25/09/23/2131259/mlb-approves-robot-umps-in-2026-for-challenges?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 23 Sep 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[MLB has approved the use of robot umpires in the 2026 season. According to ESPN, the system will give teams two challenges per game for balls and strikes where hitters, pitchers, and catchers can request reviews. From the report: Hitters, pitchers and catchers will be the only ones allowed to trigger the system by tapping their head, and if a challenge is successful -- the pitch will be shown on in-stadium videoboards -- teams will retain it. While the vote in favor of the automated ball-strike challenge system was not unanimous -- some of the four players on the 11-man committee voted no, according to sources -- the vote was a fait accompli, with MLB owners all in favor and in possession of a six-seat majority on the committee.
 
The ABS system uses similar technology to the line-calling system in tennis, with 12 cameras in each ballpark tracking the ball with a margin of error around one-sixth of an inch. The ABS zone will be a two-dimensional plane in the middle of the plate that spans its full width (17 inches). The zone's top will be 53.5% of a player's height and the bottom 27%. Teams that run out of challenges over the first nine innings will be granted an extra challenge in the 10th inning, while those that still have unused challenges will simply carry them into extras. If a team runs out of challenges in the 10th, it will automatically receive another in the 11th -- a rule that extends for any extra inning.
 
During the league's spring training test this season, teams combined to average around four challenges per game and succeeded 52.2% of the time, according to the league. Catchers, whose value in framing pitches outside the zone to look like strikes could take a hit due to the new rule, were the most successful at a 56% overturn rate, while hitters were correct 50% of the time and pitchers 41%. MLB's minor league testing, which started in 2021, led to Triple-A players in 2023 using ABS challenge three days a week and a full ABS system, with every pitch adjudicated by computer, the other three.]]></content:encoded></item><item><title>OpenAI is building five new Stargate data centers with Oracle and SoftBank</title><link>https://techcrunch.com/2025/09/23/openai-is-building-five-new-stargate-data-centers-with-oracle-and-softbank/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Tue, 23 Sep 2025 22:24:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI is continuing to build out massive AI data centers to train and serve increasingly powerful AI models.]]></content:encoded></item><item><title>When “no” means “yes”: Why AI chatbots can’t process Persian social etiquette</title><link>https://arstechnica.com/ai/2025/09/when-no-means-yes-why-ai-chatbots-cant-process-persian-social-etiquette/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/09/tehran_market-1152x648.jpg" length="" type=""/><pubDate>Tue, 23 Sep 2025 22:23:22 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[If an Iranian taxi driver waves away your payment, saying, "Be my guest this time," accepting their offer would be a cultural disaster. They expect you to insist on paying—probably three times—before they'll take your money. This dance of refusal and counter-refusal, called taarof, governs countless daily interactions in Persian culture. And AI models are terrible at it.New research released earlier this month titled "We Politely Insist: Your LLM Must Learn the Persian Art of Taarof" shows that mainstream AI language models from OpenAI, Anthropic, and Meta fail to absorb these Persian social rituals, correctly navigating taarof situations only 34 to 42 percent of the time. Native Persian speakers, by contrast, get it right 82 percent of the time. This performance gap persists across large language models such as GPT-4o, Claude 3.5 Haiku, Llama 3, DeepSeek V3, and Dorna, a Persian-tuned variant of Llama 3.A study led by Nikta Gohari Sadr of Brock University, along with researchers from Emory University and other institutions, introduces "TAAROFBENCH," the first benchmark for measuring how well AI systems reproduce this intricate cultural practice. The researchers' findings show how recent AI models default to Western-style directness, completely missing the cultural cues that govern everyday interactions for millions of Persian speakers worldwide.]]></content:encoded></item><item><title>YouTube Reinstating Creators Banned For COVID-19, Election Content</title><link>https://news.slashdot.org/story/25/09/23/2123257/youtube-reinstating-creators-banned-for-covid-19-election-content?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 23 Sep 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[YouTube's parent company, Alphabet, said it will reinstate creators previously banned for spreading COVID-19 misinformation and false election claims, citing free expression and shifting policy guidelines. The Hill reports: "Reflecting the Company's commitment to free expression, YouTube will provide an opportunity for all creators to rejoin the platform if the Company terminated their channels for repeated violations of COVID-19 and elections integrity policies that are no longer in effect," the company said in a letter to Rep. Jim Jordan (R-Ohio), chair of the House Judiciary Committee. "YouTube values conservative voices on its platform and recognizes that these creators have extensive reach and play an important role in civic discourse. The Company recognizes these creators are among those shaping today's online consumption, landing 'must-watch' interviews, giving viewers the chance to hear directly from politicians, celebrities, business leaders, and more," it added in the five-page correspondence.
 
Alphabet blamed the Biden administration for limiting political speech on the platform. "Senior Biden Administration officials, including White House officials, conducted repeated and sustained outreach to Alphabet and pressed the Company regarding certain user-generated content related to the COVID-19 pandemic that did not violate its policies," the letter read. "While the Company continued to develop and enforce its policies independently, Biden Administration officials continued to press the Company to remove non-violative user-generated content," it continued. Guidelines were changed after former President Biden took office and urged platforms to remove content that encouraged citizens to drink bleach to cure COVID-19, as President Trump suggested in 2020, or join insurrection efforts launched on Jan. 6, 2021, to overthrow his 2020 presidential win. But the company said the Biden administration's decisions were "unacceptable" and "wrong," while noting it would forgo future fact-checking mechanisms and instead allow users to add context notes to content.]]></content:encoded></item><item><title>Building the new backbone of space at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/09/23/space-is-open-for-business-with-even-rogers-and-max-haot-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 23 Sep 2025 22:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At TechCrunch Disrupt 2025, True Anomaly’s Even Rogers and Vast’s Max Haot will join the Space Stage to explore what’s needed to unlock a sustainable and scalable space economy. ]]></content:encoded></item><item><title>What is Bluesky? Everything to know about the X competitor</title><link>https://techcrunch.com/2025/09/23/what-is-bluesky-everything-to-know-about-the-x-competitor/</link><author>Amanda Silberling, Cody Corrall, Alyssa Stringer</author><category>tech</category><pubDate>Tue, 23 Sep 2025 21:33:16 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[We’ve compiled the answers to some of the most common questions users have about Bluesky. ]]></content:encoded></item><item><title>How to Answer Product Sense Questions for FAANG L6+ (Why → Who → What → How)</title><link>https://hackernoon.com/how-to-answer-product-sense-questions-for-faang-l6-why-who-what-how?source=rss</link><author>Surya Kalipattapu</author><category>tech</category><pubDate>Tue, 23 Sep 2025 21:28:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This is a product sense interview guide for Product Managers who are interviewing with FAANG for Sr, Lead, or Principal-level Product roles.]]></content:encoded></item><item><title>Dedicated Mobile Apps For Vibe Coding Have So Far Failed To Gain Traction</title><link>https://developers.slashdot.org/story/25/09/23/2115212/dedicated-mobile-apps-for-vibe-coding-have-so-far-failed-to-gain-traction?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 23 Sep 2025 21:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: While many vibe-coding startups have become unicorns, with valuations in the billions, one area where AI-assisted coding has not yet taken off is on mobile devices. Despite the numerous apps now available that offer vibe-coding tools on mobile platforms, none are gaining noticeable downloads, and few are generating any revenue at all. According to an analysis of global app store trends by the app intelligence provider Appfigures, only a small handful of mobile apps offering vibe-coding tools have seen any downloads, let alone generated revenue.
 
The largest of these is Instance: AI App Builder, which has seen only 16,000 downloads and $1,000 in consumer spending. The next largest app, Vibe Studio, has pulled in just 4,000 downloads but has made no money. This situation could still change, of course. The market is young, and vibe-coding apps continue to improve and work out the bugs. New apps in this space are arriving all the time, too. This year, a startup called Vibecode launched with $9.4 million in seed funding from Reddit co-founder Alexis Ohanian's Seven Seven Six. The company's service allows users to create mobile apps using AI within its own iOS app. Vibecode is so new, Appfigures doesn't yet have data on it. For now, most people who want to toy around with vibe-coding technology are doing so on the desktop.]]></content:encoded></item><item><title>Google’s AI Mode arrives in Spanish globally</title><link>https://techcrunch.com/2025/09/23/googles-ai-mode-arrives-in-spanish-globally/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 23 Sep 2025 21:18:32 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google's AI Mode is now available to Spanish-speaking users worldwide.]]></content:encoded></item><item><title>Scott Wiener on his fight to make Big Tech disclose AI’s dangers</title><link>https://techcrunch.com/2025/09/23/scott-wiener-on-his-fight-to-make-big-tech-disclose-ais-dangers/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Tue, 23 Sep 2025 20:21:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The California lawmaker is on his second attempt to pass a first-in-the-nation AI safety bill. This time, it might work.]]></content:encoded></item><item><title>Journals Infiltrated With &apos;Copycat&apos; Papers That Can Be Written By AI</title><link>https://science.slashdot.org/story/25/09/23/1825258/journals-infiltrated-with-copycat-papers-that-can-be-written-by-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 23 Sep 2025 20:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An analysis of a literature database finds that text-generating AI tools -- including ChatGPT and Gemini -- can be used to rewrite scientific papers and produce 'copycat' versions that are then passed off as new research. Nature: In a preprint posted on medRxiv on 12 September, researchers identified more than 400 such papers published in 112 journals over the past 4.5 years, and demonstrated that AI-generated biomedicine studies could evade publishers' anti-plagiarism checks. 

The study's authors warn that individuals and paper mills -- companies that produce fake papers to order and sell authorships -- might be exploiting publicly available health data sets and using large language models (LLMs) to mass-produce low-quality papers that lack scientific value. 

"If left unaddressed, this AI-based approach can be applied to all sorts of open-access databases, generating far more papers than anyone can imagine," says Csaba Szabo, a pharmacologist at the University of Fribourg in Switzerland, who was not involved in the work. "This could open up Pandora's box [and] the literature may be flooded with synthetic papers."]]></content:encoded></item><item><title>How To Add Integrations to Lovable Apps: A Step-By-Step Guide with Membrane</title><link>https://hackernoon.com/how-to-add-integrations-to-lovable-apps-a-step-by-step-guide-with-membrane?source=rss</link><author>Integration App</author><category>tech</category><pubDate>Tue, 23 Sep 2025 19:45:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Lovable.dev is a delightful and powerful AI coding agent that can generate modern, usable sites and apps in minutes. Its strength lies in fast and effective scaffolding: interactive dashboards, polished UIs, and smooth user experiences.But if you’ve ever asked Lovable to build an end-to-end integration, you’ll know it  but can struggle to connect reliably to external services.Integrations aren’t fault-tolerant.They require reliable authentication, solid integration logic, and secure execution routes. That’s where Membrane comes in: the AI-native universal integration layer that allows you to build connections to any app, for any use case.Let’s ask Lovable to create a scaffolded SaaS platform dashboard that allows users to connect their own  and  accounts, and generate a Smart To-Do list from their HubSpot Tasks.Here’s the initial prompt to Lovable:Create a SaaS dashboard with a “Connect to Hubspot” button, a “Connect to OpenAI” button, and a way for users to fetch their Hubspot tasks and process them with OpenAI in order to produce a prioritized to-do list.The Results: What Lovable Does Really Well (On Its Own):Generates a slick UI with connection buttons:Prompts us to connect Supabase for a backend (which our use case would require):Generates a scaffolded, API-based infrastructure after referencing the relevant OpenAI and Hubspot docs:Where It Could Use Some Help:Lovable asked for  personal OpenAI API key, which doesn’t make sense for our use case. We want this app to let  user connect  accounts.It generated brittle, custom OAuth flows, which would have leaked tokens in client state (if they had worked).The system lacked reliable integration logic. It was designed with one-off API fetches that, even if working, would not have allowed retries, multiple users, or secure action execution. The app , but the integrations didn’t work end-to-end.Adding Membrane to the MixTo help Lovable  its way to a fully-integrated app, we introduce Membrane. Leverage Membrane’s Connection UIInstead of trying to scaffold its own interface for user connections, Lovable was nudged to use Membrane’s , which automatically handles OAuth, token storage, and user-specific authentication:Implement Supabase BackendLovable asked for my Membrane credentials and stored them as Secrets to be retrieved by Supabase Edge Functions. Now, when prospective users want to connect their accounts for HubSpot and OpenAI, the system will create Membrane connections that are authenticated with my workspace credentials:Configure the Membrane WorkspaceSince Lovable doesn’t currently support connecting to Membrane in a local dev environment, we manually:Added HubSpot and OpenAI connectors through the Membrane console, which only takes a few clicks.Configured the relevant Actions (fetch-tasks, summarize-tasks).Grabbed the drop-in  from the console. Membrane provides exact code snippets that you can drop into your app for Action execution:Provide Lovable with the SDK CallsWe pasted the Membrane Action snippets into Lovable so that it could wire up backend calls correctly:Troubleshoot with “Vibes”If Lovable became confused (which can be expected within the vibe-coding experience), we reminded it of two truths: that  handles backend auth & token persistence, and users connect their own accounts. Never ask for my individual API keys for HubSpot or OpenAI.After Lovable elegantly vibe-squashed a couple of runtime errors, the app worked! Integrated end-to-end and reproducible:The app features a  workflow, allowing users to:Connect to their own OpenAI and Hubspot accounts with Membrane’s Connection UI components.Generate an AI-optimized, smart to-do listNavigate a sleek, interactive UI.The only manual steps required were adding connectors in the Membrane workspace and configuring Membrane Actions — each of which required just a few clicks.The Difference with MembraneMembrane turned Lovable from a  into a production-ready integrations builder:: No more individually managing API keys. Users can connect their own HubSpot and OpenAI accounts, and connections are handled securely by Membrane.: Instead of brittle one-off fetches, Membrane Actions provided reusable, reliable integration logic.: Secure, multi-tenant backend without reinventing the wheel.Now, when a user connects HubSpot and OpenAI, the app reliably fetches tasks, summarizes them, and provides a secure, user-specific to-do list.|  |  | Result |
|----|----|----|
| Lovable tries to create a single-user, hardcoded integration | 🔑  Membrane Connection UIs | Each user connects their own accounts securely |
| Brittle/non-functional OAuth, token leaks | 🔒 Membrane Authentication | Secure, multi-tenant auth |
| One-off API calls | 🔁 Membrane Actions | Reusable, reliable integrations |
| Integration Debugging | 🏁 Membrane Integration Logic | Minimal debugging required |The value of Lovable is in  and beautiful UIs. The value of Membrane lies in its reliable and scalable integrations.Together, they vibe into something greater:Developers can sketch ideas at the speed of thought with Lovable.Membrane ensures those ideas actually work across any SaaS app.: With Membrane in the loop, Lovable doesn’t stop at ideation and starts being a true SaaS integrations builder.Start building integrations today. Try Membrane:]]></content:encoded></item></channel></rss>