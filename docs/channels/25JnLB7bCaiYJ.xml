<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech News</title><link>https://securenza.github.io/SecurityFeed/</link><description></description><item><title>&apos;China&apos;s Engineer Dividend Is Paying Off Big Time&apos;</title><link>https://slashdot.org/story/25/03/24/2027250/chinas-engineer-dividend-is-paying-off-big-time?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 22:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a Bloomberg column: Worries over China's "3D" problem -- that deflation, debt and demographics are structurally hampering growth -- are melting away. Instead, investors are talking about how the world's second-largest economy can take on the US and challenge its technological dominance. There is the prevailing sense that China's "engineer dividend" is finally paying off. Between 2000 and 2020, the number of engineers has ballooned from 5.2 million to 17.7 million, according to the State Council. That reservoir can help the nation move up the production possibility frontier, the thinking goes. 

In a way, DeepSeek shouldn't have come as a surprise. Size matters. A bigger talent pool alone gives China a better chance to disrupt. In 2022, 47% of the world's top 20th percentile AI researchers finished their undergraduate studies in China, well above the 18% share from the US, according to data from the Paulson Institute's in-house think tank, MacroPolo. Last year, the Asian nation ranked third in the number of innovation indicators compiled by the World Intellectual Property Organization, after Singapore and the US. What this also means is that innovative breakthroughs can pop out of nowhere. [...] 

More importantly, China's got the cost advantage. Those under the age of 30 account for 44% of the total engineering pool, versus 20% in the US, according to data compiled by Kaiyuan Securities. As a result, compensation for researchers is only about one-eighth of that in the US. Credit must be given to President Xi Jinping for his focus on higher education as he seeks to upgrade China's value chain. These days, roughly 40% of high-school graduates go to universities, versus 10% in 2000. Meanwhile, engineering is one of the most popular majors for post-graduate studies. It's a welcome reprieve for a government that has been struggling with a shrinking population.]]></content:encoded></item><item><title>Vertical farming company Plenty files for bankruptcy after raising nearly $1B</title><link>https://techcrunch.com/2025/03/24/vertical-farming-company-plenty-files-for-bankruptcy-after-raising-nearly-1b/</link><author>Mary Ann Azevedo</author><category>tech</category><pubDate>Mon, 24 Mar 2025 22:13:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Vertical farming company Plenty has filed for bankruptcy, the company said in a press release on Monday. In its statement, Plenty said it has received a commitment for $20.7 million in debtor-in-possession financing as part of a proposed restructuring plan. It plans to continue to operate a strawberry farm in Virginia and a plant science […]]]></content:encoded></item><item><title>FaunaDB Shuts Down But Hints At Open Source Future</title><link>https://news.slashdot.org/story/25/03/24/2027215/faunadb-shuts-down-but-hints-at-open-source-future?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Mon, 24 Mar 2025 21:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[FaunaDB, a serverless database combining relational and document features, will shut down by the end of May due to unsustainable capital demands. The company plans to open source its core technology, including its FQL query language, in hopes of continuing its legacy within the developer community. The Register reports: The startup pocketed $27 million in VC funding in 2020 and boasted that 25,000 developers worldwide were using its serverless database. However, last week, FaunaDB announced that it would sunset its database services. FaunaDB said it plans to release an open-source version of its core database technology. The system stores data in JSON documents but retains relational features like consistency, support for joins and foreign keys, and full schema enforcement. Fauna's query language, FQL, will also be made available to the open-source community. "Driving broad based adoption of a new operational database that runs as a service globally is very capital intensive. In the current market environment, our board and investors have determined that it is not possible to raise the capital needed to achieve that goal independently," the leadership team said.
 
"While we will no longer be accepting new customers, existing Fauna customers will experience no immediate change. We will gradually transition customers off Fauna and are committed to ensuring a smooth process over the next several months," it added.]]></content:encoded></item><item><title>French VC firm Founders Future plans US expansion</title><link>https://techcrunch.com/2025/03/24/french-vc-firm-founders-future-plans-us-expansion/</link><author>Romain Dillet</author><category>tech</category><pubDate>Mon, 24 Mar 2025 21:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Founders Future, a Paris-based VC firm with €300 million in assets under management (around $324 million at current exchange rates), is opening up the capital of the holding company that manages Founders Future’s funds and Sowefund. The company has sold a 25% stake in the holding company to MACSF, the Dassault family, CMA CGM Group, […]]]></content:encoded></item><item><title>The Trump administration planned Yemen strikes in an unauthorized Signal chat</title><link>https://techcrunch.com/2025/03/24/the-trump-administration-planned-yemen-strikes-in-an-unauthorized-signal-chat/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Mon, 24 Mar 2025 21:14:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Trump administration’s national security leaders accidentally included the editor-in-chief of the Atlantic, Jeffrey Goldberg, in a chat on Signal discussing confidential plans to attack Yemen’s Houthis. “I could not believe that the national-security leadership of the United States would communicate on Signal about imminent war plans,” Goldberg wrote of the March 15 messages, which […]]]></content:encoded></item><item><title>DNA of 15 Million People For Sale In 23andMe Bankruptcy</title><link>https://science.slashdot.org/story/25/03/24/2022232/dna-of-15-million-people-for-sale-in-23andme-bankruptcy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Mon, 24 Mar 2025 21:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from 404 Media: 23andMe filed for Chapter 11 bankruptcy Sunday, leaving the fate of millions of people's genetic information up in the air as the company deals with the legal and financial fallout of not properly protecting that genetic information in the first place. The filing shows how dangerous it is to provide your DNA directly to a large, for-profit commercial genetic database; 23andMe is now looking for a buyer to pull it out of bankruptcy. 23andMe said in court documents viewed by 404 Media that since hackers obtained personal data about seven million of its customers in October 2023, including, in some cases "health-related information based upon the user's genetics," it has faced "over 50 class action and state court lawsuits," and that "approximately 35,000 claimants have initiated, filed, or threatened to commence arbitration claims against the company." It is seeking bankruptcy protection in part to simplify the fallout of these legal cases, and because it believes it may not have money to pay for the potential damages associated with these cases.
 
CEO and cofounder Anne Wojcicki announced she is leaving the company as part of this process. The company has the genetic data of more than 15 million customers. According to its Chapter 11 filing, 23andMe owes money to a host of pharmaceutical companies, pharmacies, artificial intelligence companies (including a company called Aganitha AI and Coreweave), as well as health insurance companies and marketing companies. Shortly before the filing, California Attorney General Rob Bonta issued an "urgent" alert to 23andMe customers: "Given 23andMe's reported financial distress, I remind Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company."
 
In a letter to customers Sunday, 23andMe said: "Your data remains protected. The Chapter 11 filing does not change how we store, manage, or protect customer data. Our users' privacy and data are important considerations in any transaction, and we remain committed to our users' privacy and to being transparent with our customers about how their data is managed." It added that any buyer will have to "comply with applicable law with respect to the treatment of customer data."
 
404 Media's Jason Koebler notes that "there's no way of knowing who is going to buy it, why they will be interested, and what will become of its millions of customers' DNA sequences. 23andMe has claimed over the years that it strongly resists law enforcement requests for information and that it takes customer security seriously. But the company has in recent years changed its terms of service, partnered with big pharmaceutical companies, and, of course, was hacked."]]></content:encoded></item><item><title>&apos;What CERN Does Next Matters For Science and For International Cooperation&apos;</title><link>https://science.slashdot.org/story/25/03/24/206247/what-cern-does-next-matters-for-science-and-for-international-cooperation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 20:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[CERN faces a pivotal decision about its future as the Large Hadron Collider approaches the end of its usefulness by the early 2040s. Management proposes building the Future Circular Collider (FCC), a machine with a 90-kilometer circumference that would smash particles at eight times the energy of the LHC. This hugely consequential plan faces significant challenges. Much of the required technology doesn't exist yet, including superconducting magnets strong enough to bend high-energy particle beams. 

The project also lacks the clear rationale that the LHC had in finding the Higgs boson. The proposal has divided physicists. Critics worry about the decades-long timeline, potential cost overruns, and the risk of sacrificing other valuable CERN activities. Germany, which provides 20% of the lab's budget, has already indicated it won't increase contributions. A council-appointed group is now gathering input from the physics community before making recommendations in December. 

Nature's editorial board adds: Unless some nations step up with a major infusion of cash, the FCC faces an uncertain prospect of being funded. But waiting too long could mean that there will be a large gap between the new facility opening and the closure of the LHC, and precious expertise could end up being lost. 

Although physicists might disagree on what CERN should do, they nearly unanimously care about the lab's future. They and their leaders must now make the case for why European taxpayers, who fund most of the lab's yearly budget should care, too. The stakes are beyond science, and even beyond Europe.]]></content:encoded></item><item><title>How a Nephew&apos;s CD Burner Inspired Early Valve To Embrace DRM</title><link>https://slashdot.org/story/25/03/24/1939209/how-a-nephews-cd-burner-inspired-early-valve-to-embrace-drm?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 19:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Valve's early anti-piracy efforts, which eventually led to the Steam platform, were sparked by co-founder Monica Harrington's nephew using her money to buy a CD burner for copying games, she revealed at last week's Game Developers Conference. Harrington said her nephew's "lovely thank you note" about sharing games with friends represented a "generational shift" in piracy attitudes that could "put our entire business model at risk." 

Half-Life subsequently launched with CD key verification in 1998. When players complained about authentication failures, co-founder Mike Harrington discovered "none of them had actually bought the game," confirming the system worked. Although easily bypassed, this early protection influenced Steam's more robust DRM implemented with Half-Life 2 in 2004, which became the industry standard for PC game distribution.]]></content:encoded></item><item><title>Researchers Search For More Precise Ways To Measure Pain</title><link>https://science.slashdot.org/story/25/03/24/194257/researchers-search-for-more-precise-ways-to-measure-pain?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 19:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Scientists are developing biomarkers to objectively measure pain, addressing a fundamental medical challenge that has contributed to the opioid crisis and led to consistent underestimation of pain in women and minorities. 

Four research teams funded by the Department of Health and Human Services are developing technologies to quantify pain like other vital signs. Their approaches include a blood test for endometriosis pain, a device measuring nerve response through pupil dilation, microneedle patches sampling interstitial fluid, and a wearable sensor detecting pain markers in sweat. 

"When patients are told that the pain is all in their head, the implication is that it's imagined, but the irony is that's sort of right," said Adam Kepecs, a neuroscience professor at Washington University. "The pain only exists in your brain. It's neural activity, which is why it's invisible and uniquely personal. But it's still real." These innovations could transform treatment for the nearly 25% of Americans suffering from chronic pain, while potentially saving billions in healthcare costs.]]></content:encoded></item><item><title>Fair-code pioneer n8n raises $60M for AI-powered workflow automation</title><link>https://techcrunch.com/2025/03/24/fair-code-pioneer-n8n-raises-60m-for-ai-powered-workflow-automation/</link><author>Ingrid Lunden</author><category>tech</category><pubDate>Mon, 24 Mar 2025 19:00:21 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Developer tooling is changing rapidly with AI. So companies that are making it easier to adopt AI in their workflows are seeing a boom of attention. After a startup called n8n (pronounced “enay-ten”) pivoted its workflow automation platform to become more AI-friendly in 2022, it said it saw its revenues increase 5X, doubling in the […]]]></content:encoded></item><item><title>AI creation platform Arcade expands from jewelry to home goods</title><link>https://techcrunch.com/2025/03/24/ai-creation-platform-arcade-expands-from-jewelry-to-home-goods/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Mon, 24 Mar 2025 19:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Arcade, a generative AI marketplace for designing jewelry, is expanding its offerings to include home goods, starting with rugs. The company on Monday also introduced a new feature called “Match My Room,” which allows users to upload a photo of their room so their design complements the existing colors and style. Alongside this expansion, Arcade […]]]></content:encoded></item><item><title>US lifts sanctions on Tornado Cash, a crypto mixer linked to North Korean money laundering</title><link>https://techcrunch.com/2025/03/24/us-lifts-sanctions-on-tornado-cash-a-crypto-mixer-linked-to-north-korean-money-laundering/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Mon, 24 Mar 2025 18:39:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tornado Cash was used to launder billions in stolen crypto, according to the Treasury.]]></content:encoded></item><item><title>Pentagon Axes HR System After 780% Budget Overrun</title><link>https://news.slashdot.org/story/25/03/24/1822226/pentagon-axes-hr-system-after-780-budget-overrun?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 18:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Pentagon has canceled its troubled Defense Civilian Human Resources Management System after years of delays and budget overruns, Defense Secretary Pete Hegseth said. The project, launched in 2018 with a one-year timeline and $36 million budget, ultimately ran eight years and exceeded costs by $280 million, reaching 780% over budget. "We're not doing that anymore," Hegseth said in a video announcing the cancellation. Officials have 60 days to develop a new plan to modernize DoD's civilian HR systems. The cuts are part of a broader $580 million spending reduction that includes $360 million in diversity, climate change and COVID-19 grant programs, plus $30 million in consulting contracts with Gartner and McKinsey.]]></content:encoded></item><item><title>Revel opens first EV fast-charging hub in San Francisco</title><link>https://techcrunch.com/2025/03/24/revel-opens-first-ev-fast-charging-hub-in-san-francisco/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Mon, 24 Mar 2025 18:19:40 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Brooklyn-based electric vehicle charging infrastructure startup Revel on Monday launched its first fast-charging station in San Francisco, kicking off its plans to expand across the Bay Area over the next year.  “For years, Revel has operated the largest, fastest, and most reliable fast-charging network in New York City,” Frank Reig, co-founder and CEO of Revel, […]]]></content:encoded></item><item><title>Google Says It Might Have Deleted Your Maps Timeline Data</title><link>https://tech.slashdot.org/story/25/03/24/1749248/google-says-it-might-have-deleted-your-maps-timeline-data?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 17:49:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google has confirmed that a technical issue has permanently deleted location history data for numerous users of its Maps application, with no recovery possible for most affected customers. The problem emerged after Google transitioned its Timeline feature from cloud to on-device storage in 2024 to enhance privacy protections. Users began reporting missing historical location data on support forums and social media platforms in recent weeks. "This is the result of a technical issue and not user error or an intentional change," said a Google spokesperson. Only users who manually enabled encrypted cloud backups before the incident can recover their data, according to Google. The company began shifting location storage policies in 2023, initially stopping collection of sensitive location data including visits to abortion clinics and domestic violence shelters.]]></content:encoded></item><item><title>China Unveils a Powerful Deep-sea Cable Cutter That Could Reset the World Order</title><link>https://tech.slashdot.org/story/25/03/24/1645202/china-unveils-a-powerful-deep-sea-cable-cutter-that-could-reset-the-world-order?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 16:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[schwit1 writes: A compact, deep-sea, cable-cutting device, capable of severing the world's most fortified underwater communication or power lines, has been unveiled by China -- and it could shake up global maritime power dynamics.

 The revelation marks the first time any country has officially disclosed that it has such an asset, capable of disrupting critical undersea networks. The tool, which is able to cut lines at depths of up to 4,000 metres (13,123 feet) -- twice the maximum operational range of existing subsea communication infrastructure -- has been designed specifically for integration with China's advanced crewed and uncrewed submersibles like the Fendouzhe, or Striver, and the Haidou series.]]></content:encoded></item><item><title>China Bans Compulsory Facial Recognition and Its Use in Private Spaces Like Hotel Rooms</title><link>https://yro.slashdot.org/story/25/03/24/1616232/china-bans-compulsory-facial-recognition-and-its-use-in-private-spaces-like-hotel-rooms?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 16:16:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China's Cyberspace Administration and Ministry of Public Security have outlawed the use of facial recognition without consent. From a report: The two orgs last Friday published new rules on facial recognition and an explainer that spell out how orgs that want to use facial recognition must first conduct a "personal information protection impact assessment" that considers whether using the tech is necessary, impacts on individuals' privacy, and risks of data leakage. Organizations that decide to use facial recognition must data encrypt biometric data, and audit the information security techniques and practices they use to protect facial scans. Chinese that go through that process and decide they want to use facial recognition can only do so after securing individuals' consent. The rules also ban the use of facial recognition equipment in public places such as hotel rooms, public bathrooms, public dressing rooms, and public toilets. The measures don't apply to researchers or to what machine translation of the rules describes as "algorithm training activities" -- suggesting images of citizens' faces are fair game when used to train AI models.]]></content:encoded></item><item><title>Cyberattack disrupts train ticket sales in Ukraine</title><link>https://techcrunch.com/2025/03/24/cyberattack-disrupts-train-ticket-sales-in-ukraine/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Mon, 24 Mar 2025 15:35:37 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Ukrzaliznytsia, Ukraine’s state-owned railway operator, has been hit by a cyberattack that disrupted online ticket sales.]]></content:encoded></item><item><title>Trump admin directs DOE officials to shield DOGE documents from disclosure</title><link>https://techcrunch.com/2025/03/24/trump-admin-directs-doe-officials-to-shield-doge-documents-from-disclosure/</link><author>Tim De Chant</author><category>tech</category><pubDate>Mon, 24 Mar 2025 15:31:14 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Department of Energy's acting general counsel reportedly issued a memo telling political appointees how to complete DOGE spreadsheets.]]></content:encoded></item><item><title>Elevating Customer Experience with Predictive Analytics: Insights from Chitrapradha Ganesan</title><link>https://hackernoon.com/elevating-customer-experience-with-predictive-analytics-insights-from-chitrapradha-ganesan?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Mon, 24 Mar 2025 15:30:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
In today's highly competitive market, providing an exceptional customer experience is paramount for businesses striving to set themselves apart. Customers now expect personalized interactions and rapid responses tailored to their specific needs, making tools like predictive analytics invaluable.\
Predictive analytics harnesses historical data to forecast future customer behaviors and preferences. This capability allows businesses to accurately anticipate customer needs, facilitating more personalized and proactive engagement. By embedding predictive analytics within Customer Relationship Management (CRM) systems, companies can not only meet but exceed customer expectations, thereby driving satisfaction and loyalty.\
Within this sphere, Chitrapradha Ganesan stands out as a crucial contributor. With over 18 years of dedicated experience in the CRM domain, she has a rich background in leveraging data-driven insights to enhance customer interaction. Currently a Senior Member Technical Staff at Salesforce, Chitrapradha applies her expertise in predictive analytics to boost customer satisfaction.Early in her career, Chitrapradha recognized the transformative power of predictive analytics in Customer Relationship Management. With over 19 years in the IT sector and a major focus on CRM for 18 of those years, she has worked across major platforms like Oracle CRM and Salesforce CRM. Her journey began with roles involving extensive data interactions and client management solutions.\
During these formative years, Chitrapradha identified the potential of data-driven insights for personalizing customer engagement and anticipating their needs. Reflecting on this pivotal moment, she shares, "I recognized that data-driven insights could play a crucial role in anticipating customer needs, leading to more personalized and effective engagement."\
This realization not only guided her professional path but also influenced her educational pursuits. She has completed a postgraduate program in Artificial Intelligence and Machine Learning at the McCombs School of Business, University of Texas at Austin, she continues to refine her expertise. Driven by a vision to merge data analytics with customer satisfaction, Chitrapradha utilizes her extensive technical background to devise scalable, high-performance solutions that cater to evolving CRM needs.Understanding Predictive AnalyticsAt its core, predictive analytics involves leveraging historical data to forecast future outcomes. This approach employs a range of statistical techniques—including data mining, machine learning, and predictive modeling—to analyze current and historical facts, enabling organizations to make informed predictions about future events. Companies use these insights to anticipate customer behaviors and preferences, thereby customizing their services to meet future demands.\
Chitrapradha, with her extensive background in CRM and data analytics, breaks down the process succinctly. Businesses gather customer data from multiple sources like past purchases, browsing history, and interactions with customer service. This data feeds into predictive models that employ algorithms to identify patterns and trends, helping businesses anticipate future customer needs or preferences.\
"For example, if a customer frequently buys a certain type of product," Chitrapradha notes, "the model might predict when they will make their next purchase and suggest similar products they might be interested in." The utility of predictive analytics in CRM is vast. By meticulously analyzing historical data, companies can pinpoint customer segments exhibiting specific behaviors, foresee potential churn, and even predict the success rate of marketing campaigns. This process involves several steps, from data collection and cleaning to the application of sophisticated algorithms that yield actionable insights.Anticipating Customer Needs and PreferencesPredictive analytics excels at anticipating customer needs with remarkable accuracy. By analyzing historical data, businesses can uncover patterns and trends that predict future behaviors. This technique involves collecting data from various touch points—such as past purchases, online browsing history, and customer service interactions—and feeding it into sophisticated predictive models. These models, powered by machine learning algorithms, then generate insights on likely customer preferences and future needs.\
Chitrapradha leverages a range of techniques and tools to realize these predictions. In her role, she utilizes Salesforce's native AI capabilities, including Einstein GPT, to automate and refine the process of anticipating customer needs. "Predictive analytics helps businesses anticipate customer needs and preferences by analyzing historical data to identify patterns and trends that indicate future behaviors," she explains. These tools enable real-time data processing and the generation of actionable insights, allowing businesses to engage customers more effectively. By analyzing a customer's buying patterns, predictive models can foresee their next purchase and suggest related products, boosting customer satisfaction and loyalty through personalized engagement.Ensuring Accuracy and Reliability in Predictive ModelsIn predictive analytics, the accuracy and reliability of models are paramount. Chitrapradha stresses the importance of starting with high-quality data, which forms the foundation of any predictive model. "High-quality data ensures that the predictive models produce accurate and actionable insights, which are essential for making informed business decisions," she explains. She advocates for a robust data governance framework involving regular data cleansing, validation, and enrichment processes to eliminate errors, inconsistencies, and outdated information. Consistent data collection across all touchpoints and centralized storage are also crucial to avoiding data silos.\
Developing reliable predictive models doesn't stop with high-quality data. Chitrapradha outlines the necessity of rigorous testing and validation before deployment. This includes running models on historical data to assess prediction accuracy and making necessary adjustments. Continuous monitoring and updating of models to account for changing customer behaviors and market conditions are essential. These practices ensure that predictive models remain reliable and effective in forecasting customer behaviors and preferences.Challenges and Ethical ConsiderationsOne primary challenge in integrating predictive analytics into CRM systems is ensuring data quality. Chitrapradha emphasizes that incomplete or outdated data can compromise predictive model reliability. "If the data used is incomplete, outdated, or inaccurate, the predictive models will produce unreliable results," she explains. Businesses must invest in robust data management practices, including regular data cleansing and validation.\
Another significant challenge is the complexity of integrating predictive analytics with existing CRM systems. Legacy infrastructures often lack the flexibility needed to incorporate advanced analytics seamlessly. Businesses must opt for adaptable, scalable analytics solutions that integrate smoothly with their current technology stack. Adequate training and highlighting the tangible benefits of predictive analytics can also help mitigate resistance to change within organizations.\
From an ethical standpoint, the use of predictive analytics in CRM systems raises critical data privacy concerns. Predictive analytics relies heavily on collecting and analyzing customer data, which brings up issues of data management and protection. Businesses must navigate these ethical waters with care, ensuring compliance with data protection regulations and maintaining transparency with customers about data usage. The potential for data misuse necessitates rigorous internal policies to uphold ethical standards and foster customer trust.Tracking Customer Satisfaction and LoyaltyIn the realm of Customer Relationship Management, quantifying customer satisfaction and loyalty is crucial. “Customer satisfaction can be measured through surveys, feedback forms, and Net Promoter Scores (NPS) before and after the implementation of predictive analytics,” Chitrapradha explains. This approach allows businesses to track real-time responses and sentiments, with shifts in these scores providing valuable insights into how well the predictive models are meeting customer needs.\
Beyond satisfaction, metrics evaluating customer loyalty are equally important. Customer retention rates reflect ongoing engagement, while repeat purchase rates offer insights into consistent customer behaviors. By analyzing these trends, companies can gauge how effectively their predictive analytics initiatives are fostering stronger customer connections and increasing loyalty.The Future of Predictive Analytics in CRMChitrapradha envisions a future where predictive analytics plays an even more integral role in CRM systems. She anticipates advancements in real-time data processing, enabling businesses to anticipate and respond to customer needs almost instantaneously.\
Looking ahead, Chitrapradha highlights, "The incorporation of predictive analytics with other emerging technologies, such as artificial intelligence and the Internet of Things (IoT), will enable a more holistic and personalized customer experience." This evolution in technology will pave the way for CRM systems that are more tailored, responsive, and capable of delivering enhanced customer satisfaction and loyalty through increasingly accurate predictions.\
Our exploration of predictive analytics in enhancing customer experience highlights the significant impact of data-driven insights. From anticipating customer needs to optimizing engagement strategies, predictive analytics enables companies to connect with their customers on a more personal and effective level. Chitrapradha’s work exemplifies the potential of these tools to transform CRM systems and drive improvements in customer satisfaction and loyalty.\
For businesses considering adopting predictive analytics, the message is clear: the investment is not just in technology but in a more personalized, responsive, and effective customer relationship strategy. As Chitrapradha’s vision suggests, the true potential of predictive analytics lies in fostering deeper customer connections and driving sustained loyalty. The future is bright for companies willing to embrace these insights and the tremendous value they bring to customer experience management.]]></content:encoded></item><item><title>AI Will Impact GDP of Every Country By Double Digits, Says Mistral CEO</title><link>https://slashdot.org/story/25/03/24/1527230/ai-will-impact-gdp-of-every-country-by-double-digits-says-mistral-ceo?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 15:26:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Countries must develop their own artificial intelligence infrastructure or risk significant economic losses as the technology transforms global economies, Mistral CEO Arthur Mensch said last week. 

"It will have an impact on GDP of every country in the double digits in the coming years," Mensch told the A16z podcast, warning that nations without domestic AI systems would see capital flow elsewhere. The French startup executive compared AI to electricity adoption a century ago. "If you weren't building electricity factories, you were preparing yourself to buy it from your neighbors, which creates dependencies," he said.]]></content:encoded></item><item><title>Step into the spotlight: Apply to speak at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/03/24/step-into-the-spotlight-apply-to-speak-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 24 Mar 2025 15:17:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Calling all tech innovators, startup fanatics, marketing gurus, and emerging VCs — this is your moment! You’ve waited long enough, and now the time has come to step into the spotlight at TechCrunch Disrupt 2025, taking place October 27–29 in San Francisco. Share your expertise with over 10,000 eager attendees and make an impact by […]]]></content:encoded></item><item><title>Linux Kernel 6.14 Officially Released</title><link>https://linux.slashdot.org/story/25/03/24/1448217/linux-kernel-614-officially-released?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 14:48:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[prisoninmate shares a report: Highlights of Linux 6.14 include Btrfs RAID1 read balancing support, a new ntsync subsystem for Win NT synchronization primitives to boost game emulation with Wine, uncached buffered I/O support, and a new accelerator driver for the AMD XDNA Ryzen AI NPUs (Neural Processing Units). 

Also new is DRM panic support for the AMDGPU driver, reflink and reverse-mapping support for the XFS real-time device, Intel Clearwater Forest server support, support for SELinux extended permissions, FUSE support for io_uring, a new fsnotify file pre-access event type, and a new cgroup controller for device memory.]]></content:encoded></item><item><title>Ethereum Block Building: The Hidden Economy Behind Every Transaction</title><link>https://hackernoon.com/ethereum-block-building-the-hidden-economy-behind-every-transaction?source=rss</link><author>varunx</author><category>tech</category><pubDate>Mon, 24 Mar 2025 14:31:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Block Building is a crucial aspect of Ethereum’s lifecycle consisting of various moving part. It determines which transactions get included in a block and in what order, directly impacting network efficiency, decentralization, and fairness. Over time, Ethereum’s block production process has evolved, especially with the growing role of MEV and the shift from validator-driven selection to specialized builders.This post will discuss how Ethereum Block Building has evolved along with the introduction of Proposer Builder Separation and future research.Ethereum organizes time into discrete units: A slot is a 12-second period in which a single block can be proposed. If no validator submits a block within a slot, it is skipped. An epoch consists of 32 slots, totaling .\
At the end of each epoch, validator duties are shuffled to ensure decentralization and security. Ethereum achieves economic finality after , after which it is near impossible to revert the blocks.Ethereum has a huge number of Validators in the network. At the time of writing, there are 1,051,349 active validators as per beaconcha.in It would be infeasible if this many validators had to verify each block every 12 seconds. Hence, validators are divided into  to efficiently validate transactions and attest to block validity.Consists of a subset of validators randomly assigned at the beginning of an epoch using RANDAO.Ensures that no single validator has disproportionate influence.Participates in voting (attesting) on blocks and confirming their validity.\
The size of a committee depends on the total number of active validators in the network but generally, each committee has at least 128 validators.There are lets say  Committees assigned per slot and lets say there are  Validators in each Committee(where  >128). This means there are  attestations for each slot. As you can understand, it can quickly become overwhelming for the network to gossip this many attestations.Aggregators in each Committee are tasked with aggregating the attestations of their respective Committee. This means out of a Committee of  Validators. there is only 1 final  instead of .So in each slot, there are a final of  Attestations(1 for each committee of that slot) being gossiped to the global topic. The Proposer of the next slot picks up these  attestations.Some points to keep in mindDuring an epoch, every active validator is a member of exactly one committee, so all the epoch's committees are disjoint.The protocol adjusts the total number of committees in each epoch according to the number of active validators.Current design is to have  Committees per slot i.e. The beacon chain shuffling is designed to provide a minimum of  lookahead on the validator's upcoming committee assignments for attesting dictated by the shuffling and slot.  that this lookahead does not apply to proposing, which must be checked during the epoch in question.\
 can be called at the start of each epoch to get the assignment for the next epoch (). This also allows validators to calculate the subnet id, join the respective committee, and be prepared for their duties.\
Additionally, a Validator might be assigned as the  of a specific committee. If so, they must subscribe to the respective topic as well.Proposer ResponsibilitiesWithin each slot, a single validator from the respective committee is selected as the  to create and submit a block.The proposer’s role is crucial as they:Construct a block by including pending transactions and attestations.Sign and broadcast the  to the network.Earn rewards for successfully proposing valid blocks.MEV is the additional profit extracted by validators (formerly miners) by reordering, including, or censoring the transactions within a block. \
Some common MEV strategies are:: Placing a trade just before a known profitable transaction. Frontrunning is also deemed as toxic MEV since it surprises the user with a negative effect.: Executing a transaction right after a specific event (e.g., arbitrage bots).: A combination of front-running and back-running, especially in DeFi trades.: When validators controlled block production, they had full discretion over transaction ordering and could directly extract MEV.: Independent actors who scan the mempool for MEV opportunities and submit transactions accordingly.: After PBS, block builders take on the role of constructing MEV-optimized blocks, while validators simply select blocks from the highest bidder.Block Building Pre-PBS: Validator-Centric MEVBefore Ethereum transitioned to PBS, the proposer(formerly miners in PoW) had full control over transaction ordering. This created an ecosystem where MEV was directly extracted by validators or outsourced to specialized searchers.: Transactions are broadcast as usual and enter the mempool.Validators handpicked transactions from the mempool. These could be ordered by something called  which is fees the user is willing to pay to get included first. It could also be ordered in a way the Validator makes more revenue(through sandwich/frontrunning)..The Validator builds the block based using the transactions they picked.: The signed block is broadcast to the network.This is somewhat loosely abstracted to simplify. But this was the general flow. This can be termed as Problems With Pre-PBS Block BuildingCentralization of MEV Power: Large validators had an economic advantage over smaller ones due to their ability to extract MEV more efficiently.Increased Censorship Risk: Validators could choose to exclude transactions that did not align with their financial incentives.Network Congestion & High Gas Fees: The bidding wars for MEV transactions led to inefficient block space utilization, increasing costs for regular users.Block Building Post-PBS: Separation of Proposers & BuildersTo address the centralization risks and inefficiencies of validator-controlled block construction, Proposer-Builder Separation (PBS) was introduced. PBS splits the responsibilities of block production between two separate entities:: Entities specializing in constructing optimized blocks, often incorporating MEV strategies.Block Proposers (Validators): Validators no longer build blocks themselves; they simply select the most profitable block offered by builders.Builders Construct Blocks: Block builders compete to construct the most profitable block, factoring in MEV extraction opportunities.Bidding for Block Inclusion: Builders bid to propose their block to validators, ensuring that validators receive the most profitable option.Validators Select the Highest Bid: Instead of selecting individual transactions, validators simply choose the highest-bidding builder’s block, maximizing their rewards.How Ethereum block building works nowUser submits transaction via JSON-RPC connected wallet.This transaction is submitted into the public mempool. All transactions are dumped here before they are picked up and validated. Recently,  have taken center stage with most big players opting for this since it is workaround to broadcasting your transactions to the public by using permissioned/private channels. Checkout the dashboard on Dune for more insights.  → who are external entities scanning the mempool continuously to find MEV opportunities(). They fetch the respective transactions and insert their own if and when necessary to make a profit. Then, the create a bundle of these transactions, the order of which must be maintained throughout in order for the Searcher to make a profit. Bundles are ordered transactions that must be executed atomically. Along with this bundle, they may add a transaction at the end of the bundle which denotes a “bribe” to the Builder. This bundle is sent to the Builder through the  call.: Searchers are external entities and influence transaction ordering but do not participate in block validation or consensus decisions. →The next entity is the Builder, who actually builds the block. They try to maximize both the fee revenue as well as MEV revenue. They include the bundled transactions sent by the Searchers(hopefully in order) and accept the payment(bribe) sent by the Searchers. Builders construct execution payloads using received transactions. They fill the blocks with:Bundles sent by the Searchers.High-priority transactions.Order general transactions keeping in mind to maximize the revenue.They also set the field  to their own address signifying that they will receive both the Searcher bribes as well as all the fees from the transactions in their submitted block. Builders submit a transaction at the end of their block which serves as a bribe to the proposer similar to what the Searcher did for the Builder.:::info
   : Builders are external entities and do not directly interact with the blockchain. → The Builder market is a competitive market with various builders wanting their payloads to get finalized to earn the fees. However, most blocks are built by a few well-known Block Builders, namely  and  To simplify this process for the actual Proposer, Relays are intermediary entities that validate the payloads sent by the various Builders and choose the one with the maximum profit/bid. The Relay-Proposer communication uses a neat trick similar to a Commit and Reveal to ensure the Proposer does not cheat every entity till this point. More here.It is entirely possible that on receiving the block payload, the proposer unwraps the block, inserts their transactions, and changes the ordering to their own liking as well as set the   to their own address. This would mean every single entity that worked on the block-building process till now(Searcher → Builder → Relay) will be cheated or do “MEV stealing”. To prevent this, the Relay sends only the Header of the selected Block Payload. This happens when the proposer makes the  call. The Relay now sends a  which contains the hash of the body, the bid, and the signature of the builder.\
The proposer has 2 options at this point:To select the  (also called ) provided by the Relay. If so, the proposer must sign the header with their key and send it back to the Relay and consequently request the  using the  call. Now the Relay executes the  call which returns the entire  to the proposer. The Proposer then simply proposes this block to the Ethereum Validator network or more specifically to the committee they have been assigned to.The Proposer may choose not to accept the   sent by the Relay, in which case they must build the block themselves. This includes finding MEV opportunities and ordering transactions accordingly. Of course, in this case, the Proposer gets to keep the entire revenue from the fees + MEV. However, this is not as simple as it seems. Finding MEV and optimizing revenue is a fairly computation-heavy task and there is the possibility that the proposer may not be able to build the block in time(12 seconds), hence there will be a missed slot. In this scenario, the proposer may be slashed from the network.But in Case 1, what if the Proposer does not send the block sent by the Relay?Well, the proposer is required to sign the  for this exact reason. Before sending the Header, the Relay sends the  to a  for safekeeping. Once the Relay receives the signed  from the proposer, it verifies the signature and sends the  stored in the escrow to the Proposer. Now, let’s say the proposer does not include this Block. It builds its own block, ignoring the ordering done thus far. In this case, the signature will not match since the headers have changed.:::info
: It is a slashable offense to sign two different Headers for the same proposing slot.The Builder can now broadcast these conflicting Signed Headers to the network and the Proposer may be slashed from the network.What is stopping Builders from censoring transactions?Well, nothing. The most common reason why Builders might censor a transaction is because it interacts with . To simplify, OFAC addresses interacting transactions that may have legal repercussions, which quite obviously nobody would want on their heads.\
According to Censorship.pics blocks built by Builders only include 0-7% OFAC sanctioned transactions.One of the most well-known solutions to transaction censoring as of writing this are .\
Inclusions Lists are a list of transactions(along with something called Summary) that are posted by the Proposer of the Slot N along with proposing the Block of Slot N.\
Inclusion Lists enforce that the transactions in the list must be included either in Block N or Block N+1, otherwise the N+1 block will be invalid. These are called :::info
: There is also a notion for  which does IL for the current Block N itself. But this design had economic flaws leading to \
Of course, this design of ILs will not enable 100% Censorship Resistance, but there is active research ongoing to harden these proposals and many neat optimizations that can be applied to better the incentive structure.\
FOCIL allows multiple Validators to provide suggestions on the Inclusion List for a specific slot. To be more precise, 16 Validators are chosen randomly at each slot to form an Inclusion List Committee. These members each form their own local IL and gossip it around to the network. The Proposer collects and aggregates available local inclusion lists into a final IL. The IL designs are lightweight since there is no need to rebuild the block to include these transactions; they can just be appended to the block. The condition for a Block to satisfy the IL validation requirement is “Block is valid if it contains all the transactions from all the ILs until the block is full.”:::info
: The IL committee members cannot guarantee any form of transaction ordering as the IL transactions can be included in any order. They only guarantee transaction inclusion.Decentralization of MEV Extraction: Block builders, rather than a few large validators, handle MEV extraction, reducing validator centralization risks. However, this is a double edged sword since in the process of mitigating Validator centralization, we have introduced Builder Centralization where only a few Builders are building a large percentage of Blocks.Fairer Revenue Distribution: Validators still profit from MEV without directly engaging in extraction, making block production fairer.More Efficient Block Space Utilization: Competition among builders leads to optimized blocks with better gas efficiency.Centralization Risk Among Builders: While validators are decentralized, a few dominant builders could still centralize MEV extraction.Trust in Off-Chain MEV Relays: PBS currently relies on MEV-Boost relays, which operate off-chain, posing potential security risks as a point of failure.]]></content:encoded></item><item><title>Apple is bringing lossless audio and low-latency audio to AirPods Max</title><link>https://techcrunch.com/2025/03/24/apple-is-bringing-lossless-audio-and-low-latency-audio-to-airpods-max/</link><author>Aisha Malik</author><category>tech</category><pubDate>Mon, 24 Mar 2025 14:29:07 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple announced on Monday that it’s bringing lossless audio and ultra-low latency audio to AirPods Max headphones when using the included USB-C cable. With this upcoming update, AirPods Max headphones will unlock 24-bit, 48 kHz lossless audio to maintain the quality of original recordings. Lossless audio also extends to Personalized Spatial Audio “to deliver a […]]]></content:encoded></item><item><title>Vibe Coding and Stone Soup: Why AI Won’t Replace Developers Anytime Soon</title><link>https://hackernoon.com/vibe-coding-and-stone-soup-why-ai-wont-replace-developers-anytime-soon?source=rss</link><author>Raul Bras</author><category>tech</category><pubDate>Mon, 24 Mar 2025 14:01:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
This is my first article—I’m trying to learn this craft. It’s a compilation of comments I’ve made on various platforms across the web. I’ve tried to make some sense of an inherently fragmented discussion. Hope this helps ease your concerns about all the AI hype.I don’t know how to cook, at least not at the level of people who claim they can. But I do have plenty of experience with "Vibe Cooking." I could probably teach a class on it. I know how to fry an egg and make instant noodles; I know you need to add a bit of water when reheating rice and that butter is better than oil or margarine when frying an egg. Nothing more sophisticated to report.\
I know how to boil water, throw in some noodles (without the seasoning packet because it’s pure sodium), add some tomato sauce, and mix in whatever leftovers I have from the previous day. I season it with salt, oregano, and parsley if I have any, plus some pepper. If there’s ground beef, great. If not, a hot dog or boiled egg will do—I handle that in a parallel task. If I’m feeling extra creative, I throw in some beans and crispy potato sticks. The result is the famous "stone soup" dish.\
It’s funny to watch, maybe even TikTok-worthy, but I won’t be opening a restaurant or making money off it—unless, of course, I had thousands of followers. Since I’m a nobody in the grand scheme of things, I just make it for myself. And it gets the job done because I don’t enjoy cooking and have no desire to learn.\
Influencers might strike gold here. Plenty of them are blowing up on Instagram and TikTok doing worse. It’s probably just a bandwagon effect—I don’t see it as a sustainable business. But it’s fascinating to watch.\
X/@levesio recently shared his goal of making $1 million ARR with his fly-game, built entirely through vibe coding. A pretty profitable stone soup. I’m keeping a close eye on it to figure all this out.Vibe Cooking (Coding) Isn't a ThreatNo top chef has ever lost sleep over the rise of kitchen gadgets or the industrialization of instant noodles. Plenty of regular folks are making vibe-cooking meals at home with ramen and pasta. The upside: They don’t have to spend money eating out. And yet, restaurants are still packed, and food trucks still line the streets next to hot dog stands.\
I compare AI assistants to kitchen appliances like microwaves, using the same perspective as Chef Gordon Ramsay: they’re incredibly useful, but if you rely on them for everything, your food will turn out awful. Just knowing how to use a microwave or an air fryer doesn’t make you a chef. Top chefs still own professional knife sets because so much of their work is hands-on and artisanal. That won’t change anytime soon. The same goes for programming—especially when it comes to game development, which is almost an art form.\
Vibe coding has brought a lot of curious newcomers into the tech field—the so-called citizen developers—and that’s great. Encouraging people to learn programming and creating job opportunities benefits any country. I see huge potential in improving tools for vibe coding. Maybe soon, we’ll get better approaches to handling the non-deterministic nature of LLMs.\
But I don’t think many of them will stick around. At first, it’s fun—until the AI stops "vibing," and you actually have to roll up your sleeves and dive into the code. Coding can be tedious for some; it requires methodical thinking. I’ve already seen posts from people who built a SaaS and lost patience fixing bugs. Now they just want to sell it and move on.\
That’s why I’m not convinced that vibe coding is a sustainable business model for citizen developers. Coding is just one piece of the puzzle—most people don’t understand how we can sit for eight hours staring at a screen, “chewing” through code. Computer science has evolved massively over the last few decades; there’s a lot to learn, and the technical jargon alone is intimidating. But I could be wrong. As I said, I’m watching this space closely because it’s fascinating.Back in the ’90s, CASE tools and code generators stirred up the same kind of panic. Marketers swore programming jobs would disappear. They sold well at first, mostly due to psychological pressure rather than real utility. But soon, people got fed up with "programming" because they weren’t cut out for it—or didn’t want to learn something new. And when maintenance time came around, no one wanted to sit down and fix the spaghetti code these tools generated. They dumped it on analysts and developers, who usually decided it was better to scrap everything and start over.\
Now, I see many developers genuinely worried about losing their jobs or struggling to stay in the industry. The rise of vibe coding is making it worse, fueled by AI hype. Big tech and startups are promoting this sensationalism, but programmers aren’t their real target—investors are. That’s why there are so many sketchy AI demos (see: the infamous Devin). It doesn’t matter if it “still” doesn’t work 100%. As long as it wows people, it’s got potential. That’s why billions are being poured into AI—because of its potential.\
This fear-mongering around junior devs doesn’t make sense. Companies are slashing salaries, cutting hiring, and spreading fear that AI will take over junior roles—all to keep the AI hype going and inflate stock prices. They’re discouraging the next generation. Eventually, it’ll end up like the COBOL market—where the experienced devs are aging out, and no new talent wants in.\
In my opinion, this entropy is temporary. Things feel chaotic due to the hype, but it will stabilize. AI won’t replace programmers—it’ll push companies to invest more in software development due to the efficiency AI brings. That means more job openings, especially for juniors. But patience is key while this “circus” winds down.We’re witnessing another shift in computer programming. We’ll keep coding, but the "modus operandi" will change—just like it did when we moved on from punch cards and floppy disks or when Flash was replaced by HTML5 and modern frontend frameworks. The '90s and early 2000s brought a wave of new programming languages, and I believe we’ll see more in the coming years.\
Recently, AI has introduced a fresh batch of IDEs and tools, just like the early 2000s gave us Eclipse, Xcode, IntelliJ IDEA, Android Studio, and VS Code. In tech, people don’t cling to the past. (Almost) everyone adapts quickly—it’s part of the job.\
There’ll probably be even bigger impacts. AI chatting sites are affecting platforms like Stack Overflow and CodeProject, which now face the risk of extinction, just like Google impacted other search engines. Social media is increasingly full of AI-powered bots, YouTube now has AI-made videos, and Spotify is loaded with artificially generated music.\
The work AI does in coding is still mostly autocomplete, based on what it’s scraped from the web. The other day, I asked ChatGPT to generate a lexer and parser in Python for JavaScript. It spat out a program using the  library—code that was straight from a GitHub repo. It just copied and pasted someone’s work…\
But these are fascinating times, living through this evolution with these tools at our fingertips. Using the ChatGPT app to practice English fluency is surreal—20 years ago, that’d be sci-fi. And sometimes GitHub Copilot suggests code that blows my mind, like it’s reading my thoughts. That lexer/parser I mentioned? I asked ChatGPT to explain parts of the code, and it did it perfectly, like I was chatting with the repo’s author.A New Evolution in ProgrammingIf you, reader, are Gen Z, you don’t have any memory of life without cell phones. I’m Gen X—I lived decades before using a mobile phone and remember what life was like with payphones on the street and a landline at home. I closely followed the impact of smartphones after 2008. It changed the world, made things a lot better, and it’ll get even better with AI.\
AI assistants in programming are the same deal: there’s no going back—use it or become obsolete; adopt it or turn into a hermit living in the mountains. It won’t replace a developer, but we’ll use it throughout whatever we’re programming. If you work on the Linux kernel, you’ll use it sparingly. If you’re at some corporation, you’ll use it for everything.\
Java programmers haven’t written constructor methods or getters and setters by hand in ages. It’s all auto-generated (thank you, Lombok!), and no one’s ever cared. An AI assistant like MS Copilot will help a ton with generating code, but we’ll still have to debug, write a lot of code, fix bugs, and so on. Plus, you often need to tweak and thoroughly test whatever it suggests or generates.\
If you don’t memorize phone numbers, you rely on your phone for that. You’ll become dependent on AI in programming the same way you depend on internet access, running water, gas, and electricity. You rely on banks, credit cards, and supermarkets, and soon you’ll rely on a personalized AI agent. Life goes on…\
There are still tons of opportunities to explore in IT, and there’ll be even more demand now that AI is “raising the bar.” We’ll see new programming platforms, new phones and computers (quantum chips!), and new professionals in demand, especially in development. Don’t worry about imposter syndrome—just focus on staying updated and knowing enough to critique and fix AI-generated code. Focus on your work, study what interests you, and don’t compare yourself to anyone.]]></content:encoded></item><item><title>DNA-Testing Firm 23andMe Files for Bankruptcy</title><link>https://slashdot.org/story/25/03/24/0517231/dna-testing-firm-23andme-files-for-bankruptcy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Mar 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[DNA-testing company 23andMe has filed for Chapter 11 bankruptcy protection [non-paywalled source] in Missouri and announced CEO Anne Wojcicki's immediate resignation, weeks after rejecting her proposal to buy back the business she co-founded. The bankruptcy filing represents "the best path forward to maximize the value of the business," said Mark Jensen, board member and special committee chair. 

Further reading: DNA of 15 Million People for Sale in 23andMe Bankruptcy.]]></content:encoded></item><item><title>Apply to compete in TechCrunch Startup Battlefield 200 today!</title><link>https://techcrunch.com/2025/03/24/apply-to-compete-in-techcrunch-startup-battlefield-200-today/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 24 Mar 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Start your engines … applications for TechCrunch’s Startup Battlefield 200 competition are open! If you’re a pre-Series A startup founder looking to make your mark on a global stage, this is an opportunity you can’t pass up. We want to see you showcase your game-changing innovations to the world at TechCrunch Disrupt 2025’s premier startup pitch […]]]></content:encoded></item><item><title>Only 7 days left: Founders and VCs save $300+ on TechCrunch All Stage passes</title><link>https://techcrunch.com/2025/03/24/only-7-days-left-founders-and-vcs-save-300-on-techcrunch-all-stage-passes/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 24 Mar 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TechCrunch All Stage is returning to Boston, Massachusetts, on July 15 for a jam-packed day where founders — no matter what stage they’re in — will gain practical insights to fuel their startup growth. With a lineup of expert-led sessions, founders will leave with actionable strategies they can implement right away. Investors will also be […]]]></content:encoded></item><item><title>Arcium Joins NVIDIA’s Inception Program To Advance Private AI Adoption</title><link>https://hackernoon.com/arcium-joins-nvidias-inception-program-to-advance-private-ai-adoption?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Mon, 24 Mar 2025 13:11:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[ZUG, Switzerland - 25 March 2025 -Arcium, the encrypted supercomputer, has been accepted into the , which helps startups accelerate technical innovation and business growth. Arcium can now leverage the industry-leading tools and resources provided by NVIDIA to power the next generation of privacy-preserving Artificial Intelligence (AI) and Machine Learning (ML) applications, championing .\
Arcium’s acceptance into the Inception Program comes after the firm’s acquisition of Web2 competitor , one of the world’s most renowned confidential computing and Private AI providers. Operating for over a decade, Inpher received  in funding from industry titans JP Morgan and Amazon in November 2024. Arcium acquired Inpher's core technology and over 30+ patents to support Private AI infrastructure.\
As more industries adopt AI for day-to-day services, Private AI tooling is needed to protect intellectual property, mitigate data breach risks, and comply with privacy regulations. Arcium’s encrypted supercomputer enables the secure processing of information in a fully encrypted state.\n , Co-Founder and CEO of Arcium, said, “Institutions leveraging AI for trading require privacy to prevent the exposure of their strategies; healthcare organizations need encryption to safeguard sensitive patient information; and AI systems need confidentiality to protect proprietary algorithms and user data. With the ability to leverage state-of-the-art resources from NVIDIA, Arcium is positioned to become the most advanced tech stack supporting Private AI applications.”Schrade concluded, “A new paradigm in computing is coming. With NVIDIA’s Inception Program, Arcium will lead the charge in making private AI a reality. This move further enhances the Arcium technology stack ahead of our mainnet launcht. Backed by this program, we will accelerate our mission to bring Private AI to all industries and build a world where data security concerns no longer hinder progress.”\
Interested parties can visit Arcium’s  for more information about the Inception Program and Arcium’s upcoming Public Testnet.\n Disclaimer: “This crypto-asset marketing communication has not been reviewed or ap-proved by any competent authority in any Member State of the European Union. The offeror of the crypto-asset is solely responsible for the content of this crypto-asset marketing communication.”Arcium, is an encrypted supercomputer that brings a trustless, verifiable, and efficient framework to run encrypted computations. Founded in 2022, Arcium provides developers, applications and entire industries with a trustless, verifiable, and efficient framework to run encrypted computations. With Arcium, the internet can use data to its full extent in an entirely encrypted state. Backed by investors such as , , , , s and  Arcium's goal is to allow the entire internet to run on encryption.:::tip
This story was distributed as a release by Btcwire under HackerNoon’s Business Blogging Program. Learn more about the program ]]></content:encoded></item><item><title>I Created a Website That Lets You Send A Love Letter to the World</title><link>https://hackernoon.com/i-created-a-website-that-lets-you-send-a-love-letter-to-the-world?source=rss</link><author>joyboy</author><category>tech</category><pubDate>Mon, 24 Mar 2025 13:02:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Beyond Likes and Captions: A Space for Raw ExpressionSocial media today is built around pictures, captions, and validation. We scroll endlessly, post curated snapshots, and measure our worth through likes and comments. But what if there was a place where words mattered more?\
That’s where  comes in—a platform where instead of posting pictures with short captions, you  and attach an art piece to go with it.\
It’s a space for thoughts that don’t fit in a tweet, for emotions that can’t be captured in an Instagram post, and for stories that deserve more than just an algorithm’s attention.\
💌 Write. Draw. Let go. Start a ripple.What Makes Walluv Different?Unlike traditional social platforms that thrive on engagement metrics, validation, and endless scrolling, Walluv is about raw, unfiltered expression.\
✔ —to yourself, to someone, or to the world. \n ✔—a drawing that expresses the emotion behind your words. \n ✔No likes, no comments, no pressure—just pure, honest expression. \n ✔A new form of storytelling—where every letter becomes a work of art.\
This isn’t about going viral. It’s about being seen, without the pressure of performance.From the very beginning, I wanted Walluv to be an effortless space where people could express themselves freely—without the distractions of logins, algorithms, or digital noise.🛠 Tech Stack & Development Approach: Built with , enabling rapid prototyping and an intuitive user interface. handles storing letters and drawings securely while maintaining anonymity. The built-in  allows users to illustrate their words in a lightweight, seamless way. The entire platform is designed for simplicity, ensuring people can write and draw without barriers.Walluv wasn’t built to compete with social media. It was built to  A space that feels more human.The Response So Far: Why People Love ItSince launching, Walluv has seen thousands of visitors, and the response has been incredible:\
📌 People are sharing deeply personal stories.📌 Letters and art pieces are spreading across TikTok, Reddit, and Twitter.📌 Users say it feels like a digital diary, a confessional, and an emotional time capsule all in one.\
This isn’t just a side project anymore—it’s becoming a movement of unspoken words finding a home.📝  – A feature where strangers can write a letter together, line by line.🎨  – Handwritten fonts, more brush styles, and themes to make Walluv feel even more like a creative space.\
Walluv will , but one thing will stay the same—it will always be a space for words and art to exist without judgment.Final Thoughts: A Place for Pure ExpressionThe internet is  but sometimes, all we need is a quiet corner to —without fear, without expectation.💌 Write. Draw. Let go. Start a ripple effect of love.I’d love to hear your thoughts on creating more human spaces online. \ 🔗 @vikas_sabbi]]></content:encoded></item><item><title>The Cadillac Optiq EV and Cadillac Escalade IQ go big on tech, luxury, and price</title><link>https://techcrunch.com/2025/03/24/the-cadillac-optiq-ev-and-cadillac-escalade-iq-go-big-on-tech-luxury-and-price/</link><author>Emme Hall</author><category>tech</category><pubDate>Mon, 24 Mar 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Cadillac’s new IQ portfolio of all-electric vehicles — notably the compact Optiq SUV and grandaddy full-size Escalade IQ — has put the company on a techier, more advanced, and luxurious plane. And the shift couldn’t have come soon enough.  The slate of traditional internal combustion engine vehicles that have come out of Cadillac in recent […]]]></content:encoded></item><item><title>Feyd-Rautha: The Best Thing About Dune Part Two</title><link>https://hackernoon.com/feyd-rautha-the-best-thing-about-dune-part-two?source=rss</link><author>Fayam Ayekame</author><category>tech</category><pubDate>Mon, 24 Mar 2025 12:59:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Arguably the biggest movie of 2024, Dune Part Two took the cinematic world by storm. From the political landscape of the known universe to the Giant sandworms of , there’s so much to love about it. Loved by critics and audiences alike, it’s no surprise that we’re delving deeper into the Duneverse with another sequel.\
In a sea of great characters and amazing acting performances, one stood out above the rest, . The primary Antagonist to Timothee Chalamet’s , Austin Butler’s character portrayal warrants praise. Without a doubt,  was the best thing about one of the biggest cinematic masterpieces in recent years. is the heir apparent (na-Baron) to House  and Geidi Prime. The younger of Baron Vladimir’s two nephews,  is heavily favored by his uncle, and beloved by his people. Unlike his uncle and brother (),  isn’t motivated by greed, favoring honor in his quest for power.What Makes  Such A Great Villian?From the moment we are introduced to this character, it's clear we are meant to hate him. Unfortunately,  is still lovable, using some of his hateful traits to get audiences to love him.“”… Princess  describes  with one simple word, and he lives up to the billing. With a Harem of cannibals whom he feeds the flesh of his victims.  even kills some of his men to satiate this need. In his introductory scene in the movie, we see him murder two innocent attendants, simply because he wants to test his new blades.\
Upon arriving at Arrakis,  instantly challenges his brother, threatening to kill him, and injuring him in the process. By placing Rabban in his place,  endears himself to the audience, with the ‘’ being one of the most hated characters.“Old fashion Artillery, genius”…  receives high praise from the Baron for his tactics in ‘liberating the north’ and restoring spruce production. By abandoning traditional war tactics, he forces the Fremen to flee the north and head south, something  has failed to achieve. Even the  himself has to join the rest of the Fremen in exile and embrace his visions before he can face .\
In his fight with  in the Arena on Geidi Prime, we see further evidence of ’s tactics. He lulls his opponent into a false sense of security, teasing victory, before killing them.  is also able to deduce the movement of the Fremen without any outside information, further demonstrating his tactical knowledge.“May thy knife chip and shatter”….  famously echoes the Fremen's battle cry back to  before their epic showdown. From the moment we are introduced to , it is clear he is a proper warrior and a deadly one too. We first witness his skills in the arena when he faces  and then again when he dispatches his brother with ease. While his fight scenes in the movie are limited, it's clear he is one of the deadliest fighters in the Imperium. The Emperor himself selects  as his champion and he pushes Paul to the brink, stabbing him twice before ultimately losing.“I’m Here Atreides, I need a blade”…  responds to ’s challenge to the Emperor and steps up to a trial by combat for the fate of the universe. He also proves defiant even when addressing the Baron, threatening to drown him whilst in the presence of guards. One thing everyone can agree on is that  is one of the bravest characters in the movie, which earns him the adoration of the audience.\
After the liberation of the North, he volunteers to go on the ground to face any Fremen resistance, against the advice of his officers. He even steps up to a Fremen captive without fear. In his battle against ,  turns off his shield and meets his opponent on a level footing with no care for his safety. While some may call him mad, he certainly displays bravery in every situation.Every movie is only as good as its villain and with one as awesome as , it’s no surprise that Dune Part Two is such a great movie. Austin Butler’s portrayal of  stands out in a movie with many actors and characters. Despite his position as the primary antagonist, fans can’t help but admire and even love . It’s fair to say that this character heavily improves the movie and helps it surpass the expectations set by its prequel.]]></content:encoded></item><item><title>Trust Wallet Reaches 200 Million Downloads Milestone</title><link>https://hackernoon.com/trust-wallet-reaches-200-million-downloads-milestone?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Mon, 24 Mar 2025 12:46:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[DUBAI, UAE — March 24, 2025 —, the world’s leading self-custody Web3 wallet, has surpassed 200 million total downloads, marking a game-changing milestone in the industry. Trust Wallet stands as the most widely used non-custodial wallet globally for onchain users, cementing its role as a key gateway to Web3.\
Since its launch in 2017, Trust Wallet has played a pivotal role in onboarding millions into crypto. Initially introduced as an Ethereum wallet, it has evolved into a chain-agnostic, multi-chain Web3 hub, now supporting over 10 millions assets across 100+ blockchains, along with a suite of features that empower users to navigate their entire Web3 journey—from buying their first cryptocurrency to swapping, staking, exploring the decentralized web, and beyond.\
Eowyn Chen, CEO of Trust Wallet, commented on the achievement:"Reaching 200 million downloads is a real testament to the trust from the users. In a rapidly evolving industry, our mission has remained the same: empower people with freedom to own and access opportunities. We’re proud of this milestone, but even more humbled and excited about the future as we have many things on the roadmap for our global community. We got to work harder."\
Trust Wallet has carved out a significant space for itself in the competitive landscape of cryptocurrency wallets. This success can be attributed to a combination of core principles that focus on user experience, community, trust and security.What’s Fuelling Trust Wallet’s Growth?With millions of users worldwide and a fast-growing community, Trust Wallet continues to expand its reach through compelling features, product innovations, and user-centric initiatives. Its recent growth and success points to a relentless focus on usability, innovation, and security.\
The wallet strikes a balance between onboarding new users and offering advanced tools for experienced users.Examples of Trust Wallet’s innovations include:Enhanced user experience (UX): A streamlined interface designed for both newcomers and pros.MEV Protection: Built-in safeguards to protect users from front-running attacks on crypto swaps. This also helps ensure fair swap pricing.Support for 100+ blockchains: From Solana, Ethereum, BSC, and Base to Tron and beyond, Trust Wallet provides access to the most active ecosystems in Web3.Industry-leading security features: A non-custodial approach that gives users full control of their digital assets—no middlemen, no compromises.Building a Future-Proof Web3: Trust Wallet’s Vision and BeyondAs the on-chain economy evolves and AI-driven innovations take shape, Trust Wallet is focused on bridging the gap between Web2 simplicity and Web3 autonomy. The goal is to make decentralized finance (DeFi) and digital ownership more intuitive, secure, and accessible for millions of users.\
Web3 isn’t just about holding assets—it’s about seamless, intelligent, and secure interactions across decentralized applications (dApps), finance, gaming, and beyond. Trust Wallet continues to expand its capabilities to give users the tools and insights needed to navigate the decentralized world with confidence.Key Focus Areas for 2025:Expanding Key Partnerships: Trust Wallet is working with blockchain ecosystems, dApps, and service providers to improve cross-chain capabilities, DeFi access, NFT utilities, and real-world asset tokenization.AI-Powered Enhancements: AI-driven insights and automation will help users make safer, smarter crypto decisions. Features like personalized security alerts, intelligent transaction analysis, and adaptive user experiences will simplify Web3 interactions.Strengthening Security & Compliance While Preserving Self-Custody: With a focus on true ownership and decentralization, Trust Wallet is enhancing security infrastructure, refining compliance where necessary, and ensuring that users maintain full control of their assets without intermediaries.\
By improving usability, security, and intelligence, Trust Wallet is ensuring that more people can explore and benefit from the decentralized economy with confidence. is the secure, self-custody Web3 wallet and gateway for people who want to fully own, control, and leverage the power of their digital assets. From beginners to experienced users, Trust Wallet makes it easier, safer, and convenient for millions of people around the world to experience Web3, access dApps securely, store and manage their crypto and NFTs, as well as buy, sell, and stake crypto to earn rewards — all in one place and without limits.\
For media enquiries, contact::::tip
This story was distributed as a release by Btcwire under HackerNoon’s Business Blogging Program. Learn more about the program ]]></content:encoded></item><item><title>Your AI Is Only as Smart as Its Data—And Humans Are Still the Best at Labeling It</title><link>https://hackernoon.com/your-ai-is-only-as-smart-as-its-dataand-humans-are-still-the-best-at-labeling-it?source=rss</link><author>Keymakr</author><category>tech</category><pubDate>Mon, 24 Mar 2025 12:33:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The consensus method plays a key role in data annotation when it is necessary to ensure high accuracy and reduce subjectivity in labeling. Based on Keymakr’s experience, implementing a consensus approach with multiple experts in specific cases can reduce annotation errors by 30–50%. Consensus minimizes mistakes, automates quality control, and helps create benchmark datasets — especially critical in high-responsibility areas such as medicine and autonomous driving.\
Tatiana Verbitskaya, a technical solution architect at Keymakr, talks about how this method works and the projects in which it has been successfully applied.Consensus is achieved by gathering the opinions of multiple experts. When defining “ground truth” data, it is vital to establish an agreed-upon standard of accuracy. Consensus is critical when training a model on subjective data, such as color and shape, or when high accuracy is required. This method is actively used in the early stages when the model has not yet been trained on sufficient data or when additional training is needed, particularly for specific cases (e.g., subjective judgments). Additionally, consensus is critical in large-scale projects, such as annotating data for self-driving cars or monitoring transportation, as it enhances precision while reducing errors.Key Principles of Consensus:Odd Number of Experts: To avoid deadlocks, consensus relies on an odd number of annotators, ensuring a definitive outcome even in cases of disagreement.Disagreement Analysis: This method doesn’t just rely on the majority vote but also considers the frequency of disagreements. If discrepancies are too significant, the data may be flagged for additional review or not even used for the model training.Error Detection Mechanisms: Even consensus-based data can contain errors if the cases are too subjective and not definitive.\
Global technology leaders like Google, Tesla, Amazon, and Meta actively use consensus-based annotation to improve AI model performance. Google Health, for instance, applies multiple radiologist annotations to X-rays to enhance diagnostic accuracy. Tesla uses consensus to label data from autopilot cameras, reducing training errors in autonomous driving. Amazon SageMaker Ground Truth incorporates consensus annotation in NLP, computer vision, and satellite imagery analysis, while Meta employs it for facial and object recognition projects.Medical Consensus: An Annotation CouncilOne of the most critical applications of consensus is in medical image annotation for disease diagnosis. Experts say radiologists’ diagnoses can vary by as much as 20–30%, directly impacting patient outcomes. When a consensus-based approach is employed — where multiple radiologists independently annotate images and their inputs are aggregated based on expertise-weighted scoring — annotation accuracy can be improved by up to 40%.\
Keymakr actively applies this approach in complex medical projects. As a result, this helps to ensure precise image labeling for AI models trained to detect complex pathologies. Here, the process was built using the Keylabs platform — where you can compare the opinions of several experts, identify discrepancies, and form high-precision datasets. This approach significantly increases the reliability of algorithms used in automated diagnostics, minimizing the risk of wrong diagnosis.Consensus in Copyright Content Usage MonitoringCurrently, Keymakr collaborates with [SoundAware](), a company that deploys automated music recognition technology to identify copyrighted music usage. The team reviews 10,000 URLs to assess the presence of copyrighted material.\
Video platforms are filled with content that can contain the author’s material, such as music, scenes from movies, or TV show fragments. Due to the vast amount of data and the subjective nature of copyright interpretation, manually analyzing each video is impractical.However, Keymakr identifies cases where copyrighted content is used or modified in ways automated systems cannot detect reliably yet. These include parodies, fan art, and homages.\
To eliminate subjectivity, Keymakr employs a consensus-based approach: each video is evaluated by multiple independent experts who answer the following questions:Does the video contain copyrighted music?Does it feature scenes from a movie or TV show?Has the content been modified, such as through editing or remixing?Based on the experts' responses, a final decision is made regarding potential copyright issues.Such projects are essential for enforcing copyright and ensuring rights holders receive fair compensation. Additionally, this process helps companies specializing in content monitoring refine their algorithms and accelerate the detection of copyrighted material.Consensus in Vehicle and Pedestrian TrackingConsensus is also widely applied in AI training for autonomous vehicles, particularly in object recognition on roads (e.g., other vehicles, pedestrians, traffic signs). For instance, a camera might capture a pedestrian in motion, and human annotators might disagree on whether the object is a person or a shadow. Consensus ensures precise labeling in such scenarios.\
Keymakr team recently worked with analysis of video recorded on cameras to track vehicles. It was necessary to track the vehicle's movement through several cameras at a crossroads and ensure that the system correctly identified the same vehicle in different frames.The cameras recorded one object (car) at several points. Several experts viewed the video from different cameras. They assessed whether this object is the same car because there could be differences in perception of appearance (for example, by color or brand). The information was used to train the model if five annotators confirmed the object’s identity. Otherwise, such data would have been excluded from the dataset. This has reduced the number of false alarms and increased the accuracy of car recognition systems, which is important for urban safety systems and automatic traffic control systems.\
The same approach can be applied to identify people in shopping malls or on the streets. Cameras capture movement by analyzing, for example, the color of clothes, height, or other characteristics. This method is used to:Enhanced security monitoringRetail visitor behavior analysisCrowd flow assessment in public areasThe Future of Consensus in AIThe future of consensus-based data annotation is promising, particularly as AI models become more complex and data volume grows. The global Data Annotation and Labeling Market is projected to reach $3.6 billion by 2027, and many companies are adopting multi-layered annotation verification to enhance data quality. Studies show that models trained on datasets with consensus annotation demonstrate significantly higher accuracy than models trained on single-source labeling.\
Despite the development of automatic annotation and generative AI, the human factor remains key: subjectivity and annotation disagreements necessitate multi-stage validation. Therefore, the consensus method will continue to be used, ensuring data reliability and reducing errors in critical areas such as autonomous systems, medicine, and financial analysis.]]></content:encoded></item><item><title>What If AI Grew Up in a Neighborhood, Not a Castle?</title><link>https://hackernoon.com/what-if-ai-grew-up-in-a-neighborhood-not-a-castle?source=rss</link><author>Anna Naumova</author><category>tech</category><pubDate>Mon, 24 Mar 2025 12:23:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Imagine all of the smartest robots and algorithms we call AI living in one big castle.\
It’s huge. Strong. Enclosed behind high walls and guards. You need to request permission to enter. Few have the keys to them. And inside, they make the big calls — what data gets used, how the A.I. learns and who it helps.\
Now imagine the exact opposite.\
It’s not one big castle; it’s a neighborhood. The little brain: A piece of the AI home puzzle in every house. These little brains can communicate with each other, learn together and assist their owners. No walls. No gates. Each one gives a bit, and the entire community gets smart.\
That is the difference between  AI and  AI.\
And it’s not just a nice little story. A big deal for the future of AI and how it will work. Especially in terms of privacy, security, and trust.\
We’re going to walk you through this — like we’re 5 years old.The Castle on the Hill of Centralized AI 🏰The AI that exists today is mostly in giant data centers, the kind owned by the largest of companies. The data (photos, emails, voice recordings, medical files, etc.) is sent there to be processed by banks of super-powerful computers that train AI models.\
These companies decide:What it does with that knowledgeThat’s centralized AI. It’s neat and efficient. It can be very fast. But it has problems, too.\
What happens if someone creeps into the castle? They could steal everything.\
Or what if the owners of the castle don’t play nice? They might:Block people they don’t likeInternally or externally control what the AI says or doesDecentralized AI: A More Friendly, Smarter Neighborhood 🏘️By far the biggest difference, however, is that the power is decentralized.\
Not one castle, but a neighborhood. So lots of small computers (like yours and mine) carry the AI around and run it. No single boss. No one place to break into.\
Individuals retain ownership of their data. You still can teach the A.I. new tricks, but you don’t have to sacrifice your entire existence to do so.\
The smart part? And these tiny AIs communicate with one another. They share safe information. They learn as a team.\
And when one part breaks? The others keep going.\
That’s how decentralization preserves privacy, prevents control by a small number of companies and creates a voice for everyone.All Right, But How Exactly Does That Work? 🧠That’s where some neat tools like  are useful.\
You can think of Aleph Cloud as the road that ties together all the houses in our AI neighborhood.\
It does two important things:Rather than housing your files in a single data center, Aleph spreads small fragments of your files around — across, effectively, many places. It’s kind of like writing in your diary and allowing each friend a page to hold onto.Computers are what AI needs to think and learn. Aleph allows you to run those jobs of thinking on a heap of little computers, not only on big machines. This utility is known as decentralized compute.\
You don’t have to create a system from scratch. Aleph allows you to build faster, safer decentralized AI apps that do not live in a castle.Why This Assists With Privacy Protection 🔐Let’s say your smartwatch is measuring your heart rate.\
In , that data goes directly to the castle. They retain it, analyze it and perhaps even deploy it to serve you advertisements. Sometimes it is shared or leaked.\
With , your data belongs to you. Your watch could pick up on your habits, figure out when you were about to have problems and alert you without sending every action to a big company.\
You have intelligent functionality without losing your authority.\
That’s a significant victory for .What About Transparency? 👀So one reason people don’t trust AI is that it’s often a black box. You feed it data, and it churns out answers — but you don’t know how or why.\
In a castle, that mystery is even larger. Very few know what’s going on behind the walls.\
Based on the blockchain, everything can be more transparent due to decentralized AI. You can:See how the AI was trainedKnow where your data is goingDocument what decisions were made, and why\
This honesty fosters .\
It’s like saying everyone can read the castle rulebook, rather than keeping it hidden in the king’s pocket.Real-World Use Cases: The Light of Neighborhood AI 🔥 is more than just a concept. It’s already operating in places such as: AI can help doctors diagnose disease — without needing to send every patient file to the cloud. Hospitals retain control of sensitive data but can still analyze patterns that span the network. Cars can exchange what they know about traffic and roadblocks, about other cars and about the weather — without transmitting video of every passenger to a data center. You could use AI to detect fraud — or dispense financial advice — without putting your personal bank account information at risk. Smart sensors on farms can track soil and weather, which allows AIs to be trained locally so that farmers don’t need fast internet or massive access to the cloud.These are all cases where  is important, and where  helps individuals remain in control.Okay, So What’s the Catch? 🧩 isn’t without its faults, just like real neighborhoods. It can be:Slower than centralized systemsMore people involved = more difficult to organizeHarder to update or patch because the code is distributed\
It also demands new types of thought. You can’t simply mimic what works in the castle. You need to build in smarter, fairer systems from the outset.\
Things like  make it more manageable to address these issues.\
And the benefits — privacy, resilience, trust — are worthwhile. is the like the king in the castle: a lot of power and fast, but not fair.\
Similar to the neighborhood: a  era: smart, connected, and better for all.You keep control of your dataYou don’t rely on one companyYou receive increased trust and transparencyYou work on systems that serve people, and not only powerAI typically dwells in large centralized systems (fortresses)Neighborhoods of Distributed AI (Co-Prediction)You can store and do compute things across a lot of nodes through tools like This enhances privacy, security, transparency, and trustIt makes things better for people — and better for the future of A.I.What do you think? Is it time for the AI to leave the castle and settle in the neighborhood?]]></content:encoded></item><item><title>23andMe faces an uncertain future — so does your genetic data</title><link>https://techcrunch.com/2025/03/24/23andme-faces-an-uncertain-future-so-does-your-genetic-data/</link><author>Carly Page</author><category>tech</category><pubDate>Mon, 24 Mar 2025 12:15:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As 23andMe's bankruptcy looms, privacy experts warn customers to delete their DNA data.]]></content:encoded></item><item><title>Hackers and Scammers Target Blockchain Developers and Founders. How to Protect Yourself?</title><link>https://hackernoon.com/hackers-and-scammers-target-blockchain-developers-and-founders-how-to-protect-yourself?source=rss</link><author>Yaroslav Kalynychenko</author><category>tech</category><pubDate>Mon, 24 Mar 2025 12:06:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A new scam is gaining momentum in March 2025, and blockchain developers and project founders are . With social engineering tactics becoming increasingly sophisticated, it's important to stay ahead of potential threats. This article outlines some of the most advanced and deceptive scams, particularly those targeting job seekers in Web3, and provides actionable steps to protect yourself.1. Job Offer Scams and Fake RecruitersScammers impersonate recruiters or employees of well-known Web3 companies. They approach blockchain developers on LinkedIn or Telegram with flattering messages and high-paying job offers.\
The victim is invited to an "interview," but the scammers claim they use a "secure, in-house video conferencing tool" instead of Zoom or Google Meet.\
Once the victim installs the software and joins the interview, their system is compromised, leading to the theft of wallet funds. The attack works by detecting and sweeping funds from hot wallets like MetaMask or Phantom.Never install software from an unknown source, even if it appears to come from a reputable company. \n Verify recruiters by checking mutual connections, their work history, and whether they have genuine endorsements. \n Use a separate, clean device for work-related communications, especially if you're dealing with crypto assets. \n Keep your development machine air-gapped from any wallets holding significant funds.2. Fake GitHub Repositories and UI ScamsA scammer contacts you with a request to audit or test a GitHub repository. They send a , claiming it's a work-related test or a proof of concept. Sounds legit, and many will not notice anything suspicious. And that would be a fatal mistake.\
The repository contains malicious code designed to extract private keys, inject malware, or execute wallet-draining scripts.\
The victim runs the provided script or opens a compromised user interface (UI) that requests private key access.Always inspect repositories thoroughly before running any code, especially scripts requiring execution privileges. \n Use a sandboxed virtual machine or an isolated environment (e.g., Tails OS) when testing unverified code. \n Check commit history and contributors — if a repo has no meaningful history or appears AI-generated, it's a red flag. \n Never enter your private key or seed phrase into any website or application outside of a trusted wallet provider.3. Overpaid Job Offers with Unrealistic SalariesScammers offer extremely high salaries (e.g., $150+ per hour or $250K+ yearly) for simple blockchain-related work. Bu there is a “small nuance”. They ask for personal information, including GitHub, CV, and even direct access to test repositories.\
Eventually, they request that you install their software, test a smart contract, or deploy a script — one that compromises your system.Be skeptical of job offers with excessive salaries that seem too good to be true. \n Validate companies by researching them on official sites, checking real employees, and cross-referencing with known Web3 security experts. \n Avoid sending personal data or connecting wallets to unknown platforms before verifying legitimacy.4. Fake Zoom, Google Meet, and Chat ApplicationsScammers claim that for "security reasons," they use a proprietary meeting tool instead of known platforms. They send a link that looks like Zoom, Google Meet, or Telegram but is actually a phishing site.\
When opened, the malicious site installs a script that either extracts browser-stored private keys or deploys clipboard hijackers.Always check URLs carefully — phishing domains often contain small typos or extra characters. \n Use browser security extensions that detect fake domains. \n Run crypto-related communications in a separate, hardened browser profile (e.g., Brave with strict security settings). \n Never download standalone meeting software unless it’s from a verified, official source.5. Social Engineering and Psychological ManipulationAttackers build "legit-looking" LinkedIn profiles with fake endorsements and AI-generated backgrounds. They engage in friendly, long-term social engineering before making their move.\
If a victim refuses one scam, they may attempt a different angle — fake token sales, job offers, or investment opportunities.Cross-check profiles using multiple sources. If someone’s work history lacks depth, it's likely fake. \n Be cautious of unsolicited offers and overly eager recruiters. \n Never let social pressure rush you into downloading files or clicking on unknown links.Final Thoughts: Stay Vigilant, Stay SecureAs blockchain developers and founders, you're a prime target for scammers due to the nature of your work and access to valuable assets. Implementing strict security measures — such as hardware wallets, separate work environments, and in-depth verification processes — will drastically reduce your risk of falling victim to these scams.Always verify recruiters and job offers before engaging.Use sandboxed environments for testing third-party code.Never install unknown meeting software or click suspicious links.Keep personal and professional crypto wallets separate.Treat unsolicited, high-paying offers with extreme skepticism.\
If you come across new scams or need further insights, share your experiences — awareness is our best defense.]]></content:encoded></item><item><title>This Free Visual Editor Let&apos;s You Edit Your Web App&apos;s Content Without Digging Into the Code</title><link>https://hackernoon.com/this-free-visual-editor-lets-you-edit-your-web-apps-content-without-digging-into-the-code?source=rss</link><author>Aymeric PINEAU</author><category>tech</category><pubDate>Mon, 24 Mar 2025 11:58:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Do you have content on your website? Ever wished you could edit it visually, without digging into code?, a free and open-source tool that allows you to edit your web app’s content directly in a visual interface.Why Intlayer Visual Editor? – Manage content in multiple languages effortlessly. – Perfect for static sites and developers. – Supports , , and more. – Modify and extend it as needed.The visual editor consists of: – Loads your website inside an iframe and detects editable content. – When you save changes, it updates your content declaration files (in JSON format for now).Once Intlayer is configured in your project, install :npm install intlayer-editor --save-dev
\
Then, configure a  file:const config = {
  editor: {
    applicationURL: "http://localhost:3000",
  },
};
export default config;
Once installed, start the editor:npx intlayer-editor start
\
Then, open  in your browser and hover over content to edit it! 🎨This project is ! If you want to contribute, report issues, or suggest features:\
🎉 Would love to hear your thoughts and feedback! Let’s make content editing easier for developers. 🚀]]></content:encoded></item><item><title>Instagram Is Overrun With AI-Generated Fetish Content, and Meta Doesn’t Seem to Care</title><link>https://hackernoon.com/instagram-is-overrun-with-ai-generated-fetish-content-and-meta-doesnt-seem-to-care?source=rss</link><author>redact.dev</author><category>tech</category><pubDate>Mon, 24 Mar 2025 11:50:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The State of AI on InstagramMeta has begun rolling out AI-assisted comments, adding to its long list of endorsements for AI features & usage. While the feature is designed to enhance user experience, there’s a clear relationship between  and  If more users are engaging with content, the session duration on Meta is likely to increase; meaning Meta can sell more ad space. Making comments more frequent, and predictable, also gives Meta the option to encourage advertisers to buy ads based on the promise of engagement – which has been the case for years.\
On the user-side of AI usage on Instagram, 404media recently reported on a network of AI-generated influencers, depicted with Down Syndrome. These accounts are being monetized through the sale of AI-generated adult content on the platform Fanvue.\
Ultimately, this fetishizes Down Syndrome and relies on AI models being trained to understand the typical appearance of a person with the condition – training data that almost definitely didn’t involve any informed consent. At the time of writing, Instagram and Fanvue have taken no action against these accounts.With its head in the sand, Meta continues to barrel forwards with efforts to replace third-party fact checking with Community Notes. To their credit, they are rolling this out gradually, and will only publish notes when contributors with diverse viewpoints agree on the note. They believe this will create a ‘less biased’ and ‘more scalable’ moderation solution.\
To translate this, Meta will make more money by using Community Notes. They don’t need to pay factcheckers, and users that contribute to Community Notes will probably spend more time on-platform, generating more ad revenue for Meta.Meta will also build up a database of uniform ‘Content Moderation’ data using Community Notes submissions. This lays the foundation for them to replace the community-driven approach with an AI solution down the line.Zuckerberg continues to shift his platforms closer to Elon-esque philosophies, and users are rightly concerned with their data privacy and Meta’s content moderation. The intercept released leaked training documents from Meta in January, defining “permissible speech” on their platforms.\
Some delightful examples of comments that will be allowed on Meta platforms include;“Immigrants are grubby, filthy pieces of shit.”“Look at that tranny.” (beneath photo of 17 year old girl)\
Meta appears to be rapidly shaping their platforms into yet another breeding ground for hate speech and bigotry, while guzzling up every byte of data they can to keep users in their network for as long as possible, generating ad revenue and training their AI models.To combat your data being used to train AI models, the first thing you should do is opt out of Generative AI data use in Meta.We also recommend an immediate purge of Meta platforms to further safeguard your content.\
Our app, Redact.dev gives you tools to wipe your entire Facebook history – which helps reduce the likelihood of your content being used to train AI.\
Finally – find a new platform to call home, and try to bring your community with you.]]></content:encoded></item><item><title>Why the Internet Archive is More Relevant Than Ever</title><link>https://tech.slashdot.org/story/25/03/23/1742225/why-the-internet-archive-is-more-relevant-than-ever?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Mar 2025 11:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[It's "live-recording the World Wide Web," according to NPR, with a digital library that includes "hundreds of billions of copies of government websites, news articles and data." 

They described the 29-year-old nonprofit Internet Archive as "more relevant than ever."



Every day, about 100 terabytes of material are uploaded to the Internet Archive, or about a billion URLs, with the assistance of automated crawlers. Most of that ends up in the Wayback Machine, while the rest is digitized analog media — books, television, radio, academic papers — scanned and stored on servers. As one of the few large-scale archivists to back up the web, the Internet Archive finds itself in a particularly unique position right now... Thousands of [U.S. government] datasets were wiped — mostly at agencies focused on science and the environment — in the days following Trump's return to the White House... 
The Internet Archive is among the few efforts that exist to catch the stuff that falls through the digital cracks, while also making that information accessible to the public. Six weeks into the new administration, Wayback Machine director [Mark] Graham said, the Internet Archive had cataloged some 73,000 web pages that had existed on U.S. government websites that were expunged after Trump's inauguration... 

According to Graham, based on the big jump in page views he's observed over the past two months, the Internet Archive is drawing many more visitors than usual to its services — journalists, researchers and other inquiring minds. Some want to consult the archive for information lost or changed in the purge, while others aim to contribute to the archival process.... "People are coming and rallying behind us," said Brewster Kahle, [the founder and current director of the Internet Archive], "by using it, by pointing at things, helping organize things, by submitting content to be archived — data sets that are under threat or have been taken down...." 

A behemoth of link rot repair, the Internet Archive rescues a daily average of 10,000 dead links that appear on Wikipedia pages. In total, it's fixed more than 23 million rotten links on Wikipedia alone, according to the organization. 

Though it receives some money for its preservation work for libraries, museums, and other organizations, it's also funded by donations. "From the beginning, it was important for the Internet Archive to be a nonprofit, because it was working for the people," explains founder Brewster Kahle on its donations page:

Its motives had to be transparent; it had to last a long time. That's why we don't charge for access, sell user data, or run ads, even while we offer free resources to citizens everywhere. We rely on the generosity of individuals like you to pay for servers, staff, and preservation projects. If you can't imagine a future without the Internet Archive, please consider supporting our work. We promise to put your donation to good use as we continue to store over 99 petabytes of data, including 625 billion webpages, 38 million texts, and 14 million audio recordings. 

Two interesting statistics from NPR's article:

"A Pew Research Center study published last year found that roughly 38% of web pages on the internet that existed in 2013 were no longer accessible as of 2023."
"According to a Harvard Law Review study published in 2014, about half of all links cited in U.S. Supreme Court opinions no longer led to the original source material."



Thanks to long-time Slashdot reader jtotheh for sharing the news.]]></content:encoded></item><item><title>No, the Market Isn’t ‘Hunting’ Your Stop-Loss—Here’s What’s Really Happening Instead</title><link>https://hackernoon.com/no-the-market-isnt-hunting-your-stop-lossheres-whats-really-happening-instead?source=rss</link><author>Adam Bakay</author><category>tech</category><pubDate>Mon, 24 Mar 2025 11:29:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Regarding trading buzzwords, the word “liquidity” is very popular to throw around.\
Most price action trading strategies nowadays put high emphasis on concepts of liquidity and constant attempts of markets “hunting stops” of retail traders in order to initiate larger moves.\
Is this really the case? Is there really someone sitting behind the curtain hunting your stop-loss and can you gain a trading edge from using different tools that display “liquidity” in markets?\
This is what we are going to find out in this article where I cover everything you need to know about liquidity and how to implement some of the popular concepts in your trading.Although you very often hear liquidity described as “stop losses of other traders” this is actually not a very accurate description of what liquidity actually is.\
I will talk about stops later on, but at its core, liquidity simply represents resting limit orders in any given market.The picture above shows available liquidity for BTCUSDT on Binance.\
For simplicity and better understanding, I increased grouping to $50 for each price point.\
As you can see from the image there are 70.827BTC willing to buy at 28900 and 16.608 BTC willing to sell at 28950.\
You can think about these levels as floors and ceilings below and above price as markets won’t be able to advance higher or lower by $50 as long as someone is going to either buy into those 16.608 BTC at the offer or sell into those 70.827 BTC at the bid.\
In the order book (often also called ladder, or depth of market (DOM)), you can only see limit orders.\
In other words, if I want to enter the new position and place a limit order in the order book, or if I want to exit the existing position via a limit order for profit.\
What you cannot see are stop orders (stop-losses) or liquidation prices as these are executed via market orders as default on all exchanges.\
Looking at the order books and heatmaps is a popular way for a lot of traders as they try to see if there is a large supply or demand coming to the market.\
What a lot of traders do is look at significant orders resting under or above price and think about them as “whales” creating support or resistance.\
This can be very tricky in practice as you should ask yourself a question why someone with a large size would want to let everyone know their intention.\
Many times these large outstanding orders are spoof orders, which are meant to create a false narrative about supply/demand coming into markets just to be pulled when price reach close proximity of the orders.\
What often ends up happening is that those who put the large orders in the market in the first place, were filling the opposite side of the trade as a lot of smaller traders were providing liquidity in an attempt to front-run the large order.\
Although sometimes large order is genuine, large orders are very often executed OTC, or just spread into smaller limit orders across more prices.\
You might have experienced an iceberg order which is a large filled position out of nowhere that is caused by spreading a lot of small limit orders into near proximity by one entity.\
So is there any reason to pay attention to available liquidity in markets?\
I would say yes, but it will highly differ based on the market you trade.\
If you trade forex and CFDs, these markets are traded over the counter and liquidity is gained from liquidity providers which are firms specializing in providing liquidity.\
In this case, there is no way for you to see the depth of the market and everything in regards to liquidity will be just pure speculation.\
For classic futures traded on CME or Eurex, you can see available liquidity for any exchange just by looking at DOM.\
Futures markets are much less liquid and fast-paced than they used to be just a few years ago and it is harder and harder to gain an edge from watching DOM alone due to the large amount of HFT (high-frequency trading) firms participating in the markets.\
That being said I think looking at DOM can still provide you with some decent edge, but it will take a long time and a large amount of focus to develop.\
Crypto is very interesting in this regard because of how much data and tools are available.\
It is still a very thin and fast-paced market so looking at DOM on single price points won’t give you much, same as trying to base your decision on outstanding orders as I previously mentioned they are often not genuine.\
One of the things I found interesting is looking at how order books are skewed to either the bid or offer side mostly on spot exchanges.\
Although once again these pending orders are just the “advertisement” if the supply or demand comes from more participants at the same time, the chances of it being genuine increase.\
Below you can see BTCUSDT on Binance and how price action reacted when the order book got significantly skewed to one side.How does liquidity determine the volatility in different markets\
The main thing that liquidity does is it determines how much is given market going to move.\
We can look at BTC and ETH or ES and NQ as an example.\
These markets are very highly correlated meaning that in most cases when one moves to the upside or downside, the second one is doing exactly the same.\
The only difference is the magnitude of how much they will move % wise.The image above shows ES and NQ on a weekly timeframe.\
As you can see, after March 2020 covid crash, ES rallied 120% followed by a 27% correction.\
At the same time, NQ rallied 150% followed by a 37% correction.\
There is not a huge difference between ES and NQ from the fundamental point of view as they both track the US stock market, the main difference is the fact that ES is a much more liquid market.\
In other words, it takes a much bigger effort to move ES compared to NQ.\
It is then up to you if want to trade a thicker market that moves less (has lower volatility) or a thinner market that moves more (has higher volatility).Price action and liquidity poolsYou have likely heard about liquidity pools and different price action strategies that are focusing on hunting liquidity (stops).\
This is a little funny as I already mentioned the stop orders are not technically liquidity as these orders are not visible to anyone.\
This makes it in regards of price action more of a buzzword to make trading look more interesting and magical.\
With that being said, you have probably seen many times markets trade past certain swing points just to completely reverse.The truth behind these moves is not hidden in some secret manipulation, but rather in core essence of the markets and the behavior of traders.\
All markets work in two-sided auctions, in other words for every buyer there has to be a seller and vice versa.\
Although moves past certain swing points trigger some stop-losses as they tend to accumulate in the same areas, what you will often find is that these moves trigger just a larger amount of participation than usual from both mean-reverting and trend-following traders.Markets very often tend to range rather than a trend, but retail traders tend to feel FOMO during the quick moves and very often try to participate in hopes of capturing large trends in price.\
This provides a great opportunity for large traders to fill their, in this case, shorts and trade the mean reversion side.\
If markets would be all about just “hunting stops” these moves would be often rather hectic as a large number of stops triggered at the same price points cause markets to slip several points at a time.Just ask yourself if this move in Gold was just a quick stop hunt before going back down, why would the market spend over a day in the same area before selling off?\
Although without a doubt some stops got triggered at the high, what came next was this two-sided auction between buyers and sellers that got resolved to the downside during the macroeconomic release.\
Another buzzword that goes along with liquidity is market makers.\
These are often described as evil entities chasing your stops.\
I covered this topic in more detail previously on the blog, so I will keep it short and simple here.\
Market making is delta neutral strategy that solely relies on providing liquidity to any given market.Market makers quote both bid and offer at the same time and their main goal is to not have directional exposure to the market, they don’t make money by trying to capture where markets are going to go next, but by capturing the spread between bid and the ask.\
Because of that the sharp “stop-hunts” are actually the least favorable scenario for market makers as they often end up having the directional exposure in the underlying asset.The image above shows the ideal scenario where market makers provide liquidity in lower volatility environments and make money on both sides of the market.\
Does this mean there is no manipulation in financial markets? Of course not.\
Every market whether if its forex, crypto, or futures has its fair share of manipulation and it is always done by different actors.There are many cases of manipulation in Forex by large banks.In crypto things get even worse and the overall market is much less liquid and therefore easier to manipulate there are many cases of firms creating a false demand for coins only to dump them on retail later on.\
With that being said, thinking that someone is hunting your 1 contract with your stop loss placed above a swing high on a 5-minute timeframe is more than silly.Liquidations and seeing stopsIn crypto, you can always see your liquidation price.\
This is a price point where the exchange will automatically close your position as you do not have enough margin to maintain the position opened.\
Liquidation prices, same as stop losses, are not available to see in the order books.\
The only way for you to see liquidations or large amounts of stops getting hit is after the facts in these called liquidation cascades.\
These are characterized by sharp moves in price action and large decreases of open interest (positions getting forcefully closed) and in some software like Coinalyze which you can see on the chart below, liquidations are also plotted as indicator (below the open interest).Without naming any, there are some software in crypto that sell users access to liquidation heatmaps that basically plot levels of liquidations on your chart.The indicator above shows “liquidation” levels and is made by Leviathan, it is completely free so you can test it out yourself.\
Indicators and software like these only plot liquidation levels as estimates derived from changes in the open interest, and you can see on the chart that most of these levels tend to be at very logical places like above resistance or below the support.\
In my opinion, it is much smarter to watch how markets behave at these levels in regard to changes in open interest, order flow, and relative volume rather than trying to pre-emptively fade every single move because someone will get liquidated there.If anything, after large spikes in price that cause bigger amounts of stops/liquidations getting triggered, markets tend to slow down and form accumulation/distribution rather than moving in straight lines up and down.Throughout the years liquidity has been used as a huge buzzword in the trading community.\
For most people, it is largely misunderstood and often presented as a pure marketing trick to make you buy anything that will give you an “insider” look into financial markets.\
In many cases, it takes only common sense to find areas where other traders might be stopped out and even if you are not able to do that, you will very often have plenty of time to position yourself after the event by utilizing simple concepts of price action, volume, and orderflow.]]></content:encoded></item><item><title>Django Transactions &amp; PostgreSQL Locks: Why Your Database is Slower Than You Think</title><link>https://hackernoon.com/django-transactions-and-postgresql-locks-why-your-database-is-slower-than-you-think?source=rss</link><author>Michael T. Andemeskel</author><category>tech</category><pubDate>Mon, 24 Mar 2025 11:15:42 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
This is the finale in a three-part series on how Django transactions work. I cover everything from the app and framework code all the way to the database layer:Background — Locks & De-conflicting Concurrent OperationsPrior Post Summary - You Should NEVER put these in TransactionsIn the last part of this series, we explored which DB operations and code to avoid putting in transactions to prevent slow responses and outages, plus what you can put in the transaction to maintain its atomic behavior.Reversible operations on the DB Reversible and related business logicOperations on multiple tablesChanges to schema (table/column structure)Irreversible operations/codeBlocking calls (network requests, file reading, etc.)\
By managing the code within transactions, we control which locks are held in transactions and for how long. This lets us minimize the amount of time transactions take, lower the DB’s latency (locks aren’t held for too long), and ensure other critical DB operations are not blocked. But how do DB operations block each other? Which ones are conflicting?Locks exist because DBs can have multiple users executing commands simultaneously, e.g., one user performs a DROP TABLE command while another user reads from that table. This would cause an error or, worse, data corruption. To prevent this, every DB operation acquires and holds a lock specific to the permissions that the operation needs, e.g., DROP TABLE acquires an ACCESS EXCLUSIVE lock on the table being dropped because it requires exclusive access to drop that table safely. In contrast, SELECT acquires ACCESS SHARE lock, which allows it to read the table data but does not block other operations from reading or writing to that table. If neither operation can attain those locks on the table or another operation is holding a conflicting lock, the operation waits for the locks to be released.\
There are two categories of PSQL locks — table and row.Since table-level locks affect the entire table, they are the riskiest to hold since they block operations on the entire table. We want to minimize how long we hold table locks because the longer we hold a table lock the longer other connections will have to wait which can lead to timeouts and outages.Row level locks affect a single row/record, these locks prevent operations on that particular record and are low risk but if we put a table migration in a transaction we can inadvertently hold all the locks on every row of that table thereby locking the entire table. With these locks, we want to minimize how many of them we hold. The more row locks we hold, the more records on the table are inaccessible.Here are the PSQL commands (operations) broken down by the severity of what they block.Everything (reads/writes)Writes (inserting, updating, and deleting data)\
Continue reading for more details.+++,++,** Check DB Operations That Block Table Access for the subcommandBackground — Locks & De-conflicting Concurrent OperationsLocks exist to handle the problems that arise from concurrent connections. When we allow our databases to be operated (read/written) by multiple connections simultaneously, we run into conflicts. What if one connection tries to delete a row while another is querying? Or worse, what if one connection adds a new column to a table while another inserts a new row using the old schema? Locks prevent the errors and data corruption that can occur with simultaneous operations on the DB. The locks are held by the transactions initiated by the connection — every PSQL command is wrapped automatically in a transaction.\
Each lock blocks operations that might cause an error or data corruption if executed simultaneously with the current operation. Some locks are very permissive and let almost all other operations occur simultaneously, e.g., the locks held by SELECT queries allow most other commands to be executed. These promiscuous locks can be held by multiple transactions at a time because these locks don’t conflict with each other (they don’t block each other’s operations). While other locks are exclusive and block multiple or all other operations, e.g., the locks held by certain ALTER TABLE/COLUMN sub-commands. These locks can only be held by one transaction at a time because they block all other operations and hence block themselves — we can’t execute two DROP TABLE commands on the same table simultaneously.\
The pg_locks table tracks all of this, along with which transaction is holding the locks. We can use this table to display outages and timeouts caused by lockouts and deadlocks.DB Operations that Block Table AccessEvery operation on the DB asks for and acquires a table lock before executing it. Most of these locks are loose and allow other operations — doing a SELECT query ( in Django) will allow all other operations except ALTER INDEX/TABLE (Django migrations that change a column or table).\
This table makes it obvious that migrations (changes to the schema) are the riskiest types of changes — CREATE INDEX and ALTER INDEX/TABLE are run in migrations../manage.py sqlmigrate APP_NAME MIGRATION_NUM
\
To find out which operations a migration executes and if they are risky. If they are, make sure the migration is quick and run it after hours (when no other long-running transactions are running).\
DROP TABLE is not included in this because it is obvious that it blocks all other commands (it acquires the same access exclusive lock as ALTER INDEX/TABLE). Paradoxically DROP TABLE wont cause your app to crash due to a lockout because it does not usually take a long time to execute — unless there are a many constraints on the table and references to the table.fk insert/update/delete will lock any referenced tables (following foreign keys on the table being operated on)*create index concurrently** create index (without concurrently)only specific alter index/table subcommand:alter table set statisticsalter table set (attribute)alter table reset (attribute)alter table add foreign keyalter table validate constraintalter table set without clusteralter table set (storage_parameter)*alter table attach partition (on parent table)alter table detach partition*alter table add foreign key (on self and referenced table)alter table disable/enable trigger”+++ for all other subcommands of alter index/tableif there are two subcommands in one operation — the subcommand with the most stringent lock will be usedBesides table locks, there are row or data locks that block write access to a specific row in a table. Unlike table locks, row locks don’t block reading data, so they are less risky, but they can cause the same damage a table lock does in specific scenarios.\
Suppose we are migrating data in a table — going through all the rows in it and updating a deprecated constant or denormalizing a JSON object, for example. We will lock the entire table if this migration happens in a transaction, i.e., if we wrap the function that is doing that migration in an atomic() decorator or context. This is due to the operations on each record locking each row on the table and the transaction holding onto the row locks until the entire migration is finished. This will block writes to the table until every row is migrated.\
A better solution is to NOT wrap the entire migration in a transaction block. If we need to alter multiple columns for each row, then wrap only the row operations in the migration. This way, after every row is changed, the lock is released.Django: Printing the SQL that Migrations Run./manage.py sqlmigrate APP_NAME MIGRATION_NUMBER/NAME
\
This will print the operations that will be run during the migration. A comment in the query will explain what each operation does.\
For a no-op migration — a migration that does not change the DB, like updates to TextChoice and similar columns — the result will look like this:BEGIN;
 - 
 - Alter field transaction_status on transactions
 - 
COMMIT;
An empty transaction with a comment inside of it describing what the transaction represents.\
This migration does nothing—hence the SQL comments in the transaction block. The django_migrations table will be updated to track which migrations have been run, but that shouldn’t impact the app.\
For migrations that actually change the DB, the output will look like this:BEGIN;
 - 
 - Add field reverted to transactions
 - 
ALTER TABLE "transactions" ADD COLUMN "reverted" timestamp with time zone NULL;
COMMIT;
\
This alters a table by adding a column that blocks all reads and writes.Great article, highly recommend reading this FIRST then the docs.[PSQL Locks](https://Postgres Locks - A Deep Dive Great article, highly recommend reading this FIRST then the docs.)Table LocksTwo transactions cannot hold locks of conflicting modes on the same table at the same time. (However, a transaction never conflicts with itself. For example, it might acquire ACCESS EXCLUSIVE lock and later acquire ACCESS SHARE lock on the same table.) Non-conflicting lock modes can be held concurrently by many transactions. Notice in particular that some lock modes are self-conflicting (for example, an ACCESS EXCLUSIVE lock cannot be held by more than one transaction at a time) while others are not self-conflicting (for example, an ACCESS SHARE lock can be held by multiple transactions).Once acquired, a lock is normally held until the end of the transaction. But if a lock is acquired after establishing a savepoint, the lock is released immediately if the savepoint is rolled back to. This is consistent with the principle that ROLLBACK cancels all effects of the commands since the savepoint. The same holds for locks acquired within a PL/pgSQL exception block: an error escape from the block releases locks acquired within it.Row LocksNote that a transaction can hold conflicting locks on the same row, even in different subtransactions; but other than that, two transactions can never hold conflicting locks on the same row. Row-level locks do not affect data querying; they block only writers and lockers to the same row. Row-level locks are released at transaction end or during savepoint rollback, just like table-level locks.Deadlocks & TransactionsThe use of explicit locking can increase the likelihood of deadlocks, wherein two (or more) transactions each hold locks that the other wants. For example, if transaction 1 acquires an exclusive lock on table A and then tries to acquire an exclusive lock on table B, while transaction 2 has already exclusive-locked table B and now wants an exclusive lock on table A, then neither one can proceed. PostgreSQL automatically detects deadlock situations and resolves them by aborting one of the transactions involved, allowing the other(s) to complete. (Exactly which transaction will be aborted is difficult to predict and should not be relied upon.)Note that deadlocks can also occur as the result of row-level locks (and thus, they can occur even if explicit locking is not used). Consider the case in which two concurrent transactions modify a table. [The first transaction updates row A, then the second transaction updates row B, then the first transaction tries to update row B, sees the lock is held by the second transaction and waits, and finally, the second transaction tries to update row A but can’t because the lock for row A is held by the first transaction which is waiting for it to finish.]]]></content:encoded></item><item><title>Want To Win Your Share of $2000? Tell Us How Your Hardware Can Power Aleph Cloud’s DePIN</title><link>https://hackernoon.com/want-to-win-your-share-of-$2000-tell-us-how-your-hardware-can-power-aleph-clouds-depin?source=rss</link><author>HackerNoon Writing Contests Announcements</author><category>tech</category><pubDate>Mon, 24 Mar 2025 11:00:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::info
Below is a list of questions to help you get started on your draft. You can answer them How to Contribute Hardware to Aleph Cloud’s DePINIntroduce Aleph Cloud and its approach to decentralized cloud infrastructure.2. How to Contribute Hardware to Aleph CloudWhat types of hardware can individuals or companies provide?How does the onboarding process work? (e.g., setup, requirements).What rewards do contributors get?What are the main barriers to contributing?How is Aleph Cloud making it easier?Why is contributing to Aleph Cloud’s DePIN valuable?How can someone get started today?:::info
Start a  or use this  to enter! Submissions close on May 7, 2025.:::tip
If you’d like to participate in the Blockchain Writing Contest but feel this template isn’t right for you, feel free to explore any of the other contest tags:Good luck, we can’t wait to read your drafts!]]></content:encoded></item><item><title>AI to Become More Integral to Data Center Operations in 2025</title><link>https://hackernoon.com/ai-to-become-more-integral-to-data-center-operations-in-2025?source=rss</link><author>Beth Rush</author><category>tech</category><pubDate>Mon, 24 Mar 2025 10:57:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Data center technology has to keep up with an ever-changing digital landscape, and using the latest tools is a critical part of meeting consumer demand. Learning how AI is influencing data center infrastructure trends will clarify how the technology makes essential services more reliable and why you might see AI updates becoming more commonplace in data centers globally.A data center is a facility housing storage systems that are essential for business operations and consumer services. The physical or virtual networks support a variety of online activities, including:People in nearly every industry rely on those daily actions or programs in some way. They’re foundational in professional and personal routines, so building new data centers and making existing locations more efficient is crucial.Ways AI Might Assist Data Center OperationsIncorporating AI into data center logistics isn’t a futuristic possibility. Industry leaders are using it to benefit their daily operations in more ways than one.Energy Consumption May Decrease With AI MonitoringSupporting nationwide or global online activities requires 24/7 electricity services. The intense processing makes data centers consume  per floor than a standard office building. AI could mitigate that usage by investigating consumption from within.\
The deep learning algorithms can monitor operations around the clock, noting where each wattage goes and which parts of a data center are working the hardest. When team leaders receive reports on the most energy-intensive machines, they can adjust or upgrade the hardware as necessary. AI could remove the guesswork of targeting electricity waste, especially if it collects daily operations data for comparative reviews over time.The Algorithms Can Provide Predictive Maintenance TipsAI is influencing data center infrastructure trends through its predictive maintenance abilities. When they have hardware issues that require downtime, essential services like cloud storage backups and virtual communications must pause. The disruption to commercial and residential activities causes issues like financial or personal information loss.\
Automation may reduce downtime or equipment failure by monitoring for predictive maintenance opportunities. Integrated team members will get alerts when software updates are necessary so they don’t get lost. They could also receive reminders to update certain hardware that’s lagging in performance. The program will know when part of the data center isn’t as efficient, drawing attention to it for improvements before the part fails and causes center-wide outages.AI May Enhance Operational EfficiencyPredictive analytics allows AI to see inefficiencies the human eye might not catch. If a model is monitoring and directing CPU usage, it could direct some units to idle because they frequently overwork. Once data center teams see which parts idle most often, they can virtualize those machines to boost CPU utilization by 40%-60% so their centers reach new efficiency levels.\
Some software also improves team communication. The routine reports provided by the predictive analytics program ensure everyone has the information they need to understand their data center’s current strengths and weaknesses. They can make more informed decisions moving forward rather than hoping human error like miscommunication doesn’t stop everyone from getting the details to enhance overall performance.Digital Security Could Improve Through AI AssistanceModern cybersecurity measures are crucial for fortifying a data center’s integrity. People trust those centers to keep their information safe, which is why AI’s security assistance can become a significant benefit to daily operations.\
AI models monitor data input and organization. When unusual patterns appear, it runs them through a logged history of operational patterns. Anything unusual stands out, which can trigger early alerts of security breaches before the staff identifies them.\
Faster incident response reduces the risk of widespread data loss or downtime. Improving consumer trust in data center operations, keeping businesses open and preventing personal information loss to cyber criminals are crucial. AI can assist in those efforts to streamline operations.Potential Challenges Data Center Teams May FaceAdding AI programs to data centers could provide many benefits, but change also comes with challenges. You can expect to see experts tackling these biggest roadblocks as AI becomes more important for daily processes.Incorporating new software into any corporate structure costs money. Using AI for security enhancements, electricity monitoring and predictive maintenance analytics is only possible after paying for a software license that is big enough for an entire data center.\
Experts estimate hardware costs for new and improving data centers will cost , meaning industry leaders are already spending vast amounts of money. Adding AI fees could be too much for some to afford right away.Outdated Infrastructure May Require ReplacementUsing the latest technology to keep up with data processing and storage means continually advancing tech in data centers is crucial. Outdated hardware is a problem, as a center’s older computer systems may not be able to use AI programs to their full potential. Replacing them before investing in new software is an additional cost some may be unable to cover.\
Replacing older infrastructure is an especially concerning problem for data centers using equipment built before virtual storage became more mainstream. Their servers may have less computing power or memory than newer models. If the leading AI programs rely on an up-to-date device’s capabilities, upgrading costly servers may be an unavoidable part of integration.Changing operational processes almost always requires some amount of team training. You’ve likely sat through training sessions before, regardless of your profession. The quick lessons ensure everyone’s on the same page and limit downtime due to confusion.\
Data center workers will need similar training opportunities while adjusting to the automated support systems. As of 2023,  in data centers nationwide. The time required to create training materials and educate that number of employees could complicate a team’s initial AI integration plan.Trends are emerging as more facilities use AI in daily data center operations. Data governance strategies will likely change as they feed their new AI models. Once teams use better governance opportunities to understand which data they own, its current structure and its primary sources, their AI will operate more efficiently.\
AI quantum computing may also become more mainstream. Data center teams will have to keep up with the intensified service demand related to quantum processing. AI could analyze the larger data loads more effectively if they are robust enough to handle them. Stronger algorithms will likely become more widespread as the quantum computing landscape evolves.Expect Data Center Infrastructure Updates for AI IntegrationOnce you start learning how AI is influencing data center infrastructure trends, it’s easier to see why so many locations are undergoing substantial upgrades. Industry leaders could use the new algorithms to improve daily operations and secure their data more efficiently if they can manage the operational costs associated with AI incorporation.\
If you’re looking forward to a future of more cloud services and data processing, staying up to date with how each center is utilizing the latest tools will keep you informed on the exciting progress.]]></content:encoded></item><item><title>Are AI PDF Tools Putting Your Data at Risk? Here’s a Safer Way to Merge PDFs</title><link>https://hackernoon.com/are-ai-pdf-tools-putting-your-data-at-risk-heres-a-safer-way-to-merge-pdfs?source=rss</link><author>Zohaib</author><category>tech</category><pubDate>Mon, 24 Mar 2025 10:46:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We all use PDF files in our everyday work life, whether it is to share invoices or contracts, send legal documents, or just want to appear more professional with our proposals. In a more professional setting, the ability to merge multiple PDF files efficiently without compromising on the confidentiality is utmost essential. And when we’re talking about security, it’s important to remember we’re not always as safe as we think we are– especially when it comes to storing digital assets.\
A leak of any sensitive document can lead to catastrophic results for any organization, costing hundreds of thousands of dollars, aside from manpower required to stabilize the situation. That’s why uploading sensitive PDFs to any external platform, whether it’s for editing or some other reason, cannot be considered 100% safe. Sure, these AI driven platforms advertise the use of cutting-edge GPT-4 processes to manage documents and promise utmost security, but is it wise to take these claims at their face value?Well, the good news is that there are now privacy-first tools that let you manage and combine PDFs right in your browser. You don’t have to upload any files or documents to any external server, which may expose your data to possible theft. In this guide, we’ll talk about:\
● Whether AI platforms are a viable option for combining your PDFs securely\
● Highlight some challenges (or risks) associated with AI-based tools\
● And lastly, talk about best practices regarding combining PDFs securely\
By the end, we promise that you’ll feel confident about the whole process while keeping your data private and completely under your control.AI-based PDF combine tools are based on language models like GPT-4 from open AI and others. Now, if you’re unfamiliar with language models, know that they generate human-like text, summarize content, answer critical questions, and even assist in matters related to coding and file management. The sheer versatility makes it a leading choice for automating and simplifying many workflows. But when we’re talking about handling PDFs, you must understand how the language model actually operates.\
The thing is, most AI-based tools don't have built-in features that would allow it to directly edit and manipulate PDF files. What it can do instead is analyze and interpret the text within the PDF if it has been converted to a different format and then fed into the language model. For tasks like merging, splitting, or reordering regular PDF pages, GPT-4 or any other language model is required to be first integrated into a third-party platform using an API. These are the platforms that can handle direct file uploads and convert it to a format that’s compatible with the AI language model.The process that we just described comes with a significant caveat- it typically requires uploading your files to external servers. Whether you’re using a chatbot, an AI-powered document tool, or a custom API integration, your PDFs are sent to remote servers for processing. The AI performs its computations off-site, and the results are sent back to you once the task is complete.\
If we’re talking about general documents where privacy is not an issue, then you don’t have to worry about it. But for businesses and other legal professionals, this method of manipulating PDFs does raise serious privacy concerns. Whenever you upload any document to an external server, there’s always a risk of falling prey to data breaches or misuse- particularly if the platform in question lacks the ability to protect your data or doesn’t have the required compliance certifications. We’ll talk more about the issue below.Like we said before, as AI-powered platforms rise in popularity due to their convenience and efficiency, we must not ignore the privacy implications that accompany their use. This is especially true when we’re talking about dealing with sensitive documents in PDF format. The problem is that many users, especially casual users, may not realize that when they interact with tools built on models like GPT-4, they are entrusting their data to external servers. This data transmission process carries inherent risks that should not be overlooked.Where is the vulnerability present?In simple terms, it could be anywhere along the process, but here’s the basic gist of how it works. When you upload a PDF file to an AI platform the file is transmitted from your local device to remote servers owned or managed by the service provider. The contents of your document are often parsed, temporarily stored, and processed on these servers before any output is returned to you.\
Even if the platform claims not to retain your files after processing, there is a window during which the document is exposed to third-party infrastructure. For corporations and legal professionals who are dealing with many types of sensitive data like proprietary contracts, client information, financial data, or other personal records, this is a massive red flag in terms of vulnerability.\
Over the years, there have been countless examples of data breaches across industries that expose millions of sensitive documents. Often, these data breaches aren’t even due to any malicious attack, but rather simple, overlooked mistakes like poor access controls, weak encryption, or overlooked security settings.There are problems beyond unauthorized access, believe it or not, there are issues of regulatory compliance that could pose a huge headache. In regions with strict data protection laws, like Europe’s GDPR or the U.S.’s HIPAA, sending confidential documents to third-party servers can lead to compliance issues. These regulations often require businesses to maintain tight control over where their data is stored, how it’s processed, and who can access it. Using AI tools that rely on external servers can make it much harder to meet these requirements.\
Another concern you just cannot ignore is how AI platforms handle your data. Many tools, including those powered by GPT-4 often reserve the right to use your uploaded files to further improve and train their models or services. While reputable providers often let users opt out of data usage for training, these details are usually buried in terms and conditions that most people don’t read. This basically means that your PDFs could potentially be used in ways you didn’t intend.Then there’s the issue of transparency. The lack of transparency in how AI platforms manage data adds to the problem. Users rarely have clear insight into the security measures these third-party providers use, nor do they have real-time control over how their data is handled. This is especially worrying when dealing with sensitive documents — things like personal information, trade secrets, legal contracts, or anything covered by confidentiality agreements.\
In short, the risks are real, and they’re worth considering before uploading your sensitive files.To address the growing concerns around data security and user privacy, companies are coming up with a new class of tools that aim to give the users complete control over their personal information. These are what we call privacy first PDF tools, and these tools operate differently than conventional online services that process files on an external remote server. You’d be glad to know that privacy-first tools can operate entirely on your local device or browser. This ensures that no data ever leaves your domain of control.Like we said before, all privacy-first PDF tools work on the simple principle of not letting the user data leave their control. This means all file processing and managing happens locally. when you upload, merge, or edit PDF files using such a tool, the actions are carried out directly on your device. No copies of your documents are sent to third-party servers for computation, nor are they stored elsewhere. As a result, there is zero risk of your data being intercepted, accessed, or mishandled by external parties.\
One of the key features that distinguishes these tools is that they do not require installation or registration. You can open the tool in your browser, perform the desired operation, and download the result, all without creating an account, sharing personal details, or granting permissions that could compromise security. This streamlined approach is not only convenient but also reduces attack surfaces where sensitive information might otherwise be collected.Works across different operating systemsMost of these PDF tools are built to function seamlessly across different platforms. So, it doesn’t matter whether you use Windows, MacOS, iOS, Android, etc, they’re designed to run within your preferred browser (as long as it’s not obscure) and deliver consistent performance. Such cross compatibility gets rid of the need to install different software or pay close attention to privacy protection.No logging of user data or activityAnother advantage of privacy-first PDF tools is that they don’t log user activity, store uploaded files, or maintain any records of processed documents. This makes them ideal for professionals, individuals, and businesses that must comply with data protection regulations or internal confidentiality policies.No steady internet connection neededOnce you’ve opened the website and loaded the tool, you can safely disconnect from the internet and keep working on merging your PDFs. These tools have been built to work offline, which adds another layer of security by blocking all possibilities of online data transmission. A great example of one such tool is PDF Combiner, an AI and cybersecurity tool that allows users to merge, reorder, or modify PDF files entirely within the browser without any internet connection.\
These are the three main advantages privacy-first AI tools have over AI-based platforms and cloud services. They not only secure your data, but offer peace of mind since you know that your sensitive information stays fully private and under your control.To combine or edit PDFs securely, you must know the difference between tools that use AI and tools that put privacy first. Of course, both have their perks, but they also handle your data very differently. While AI tools are amazing for extracting information or summarizing data, they do need the user to upload the file to an external server. Regular confidentiality issues aside, you might also come across compliance problems if regulations like GDPR or HIPAA are involved.\
Privacy-first tools, on the other hand, keep everything on your device and work through your browser, even without internet connection. This makes sure that the files never leave your computer as there are no uploads involved. AI tools have their perks, but if protecting your privacy is your primary concern, better steer clear of them.Now that we’ve established how crucial protecting your privacy is and how privacy-first PDF tools can help, let’s talk about what else you can do to further safeguard your data against theft, misuse, and compliance violations. Most users who deal with sensitive PDFs skip this step and risk putting themselves in great danger; don’t be like them.The first (and arguably the most important) thing you need to do is figure out if the PDF tool you’re about to use needs you to upload files or not. This seemingly basic step is something that quite often gets ignored as users quickly jump from one online service to another without going through the privacy policies. Honestly, who does? Since so many PDF merging services are cloud based, they naturally store your sensitive files on external servers, which poses a privacy risk.\
Many of these service providers aren’t even honest with their claims- meaning there’s a chance of your files being temporarily stored elsewhere even if the provider claims that they delete files once they’ve been processed. What’s even more concerning is the fact that you, as a user, have no control or visibility over how long your files are retained or if they’re deleted at all at a certain time in the future.\
Sadly, even when certain encrypted transmission methods are used, the mere presence of your sensitive files on third-party servers introduces vulnerabilities and chance of compliance violations. This is especially true if the data comes under protection laws like GDPR or HIPAA. Now, this might feel like a BIG ask, but we highly recommend you to take some time out to examine the privacy policy and technical documentations of the PDF merger tool you’re about to use.\
Check if there’s any mention of local processing, or do they guarantee no file uploads? The more transparent the documentation is, the better. If the service doesn’t explicitly mention that your sensitive files are processed locally, you can assume they are uploaded on some external server. If that’s the case, it’s best you don’t use the tool and look for something else.Prioritize no-upload solutionsIf you’ve managed to determine how your preferred tool handles files, you must seek out client-side solutions that perform all the processing within your local device. These browser-based tools go a long way to ensure that your sensitive data doesn’t leave your system, thereby mitigating risks associated with unauthorized access and data theft. Of course, server storage is equally bad, so let’s not forget about that.\
When it comes to privacy-first PDF tools, instead of sending your files off to any external server, they rely on your browser to do most of the heavy lifting. The best part, of course, is that you remain in complete control as your documents stay on your device. There’s another big plus: you don’t have to worry about the tool’s server policies or whether their systems are up and running. These no-upload tools work the same way every time, no matter what. And many of them even work offline, giving you an extra layer of privacy and flexibility.Avoid public or shared wi-fi networksNobody talks about this, but it’s crucial that you don’t use public or shared wi-fi in unknown locations while working on merging your PDF files. Public networks available in places like coffee shops, airports, and even hotels (including 5-star properties) lack proper encryption methods to safeguard your data. This makes them a top choice for various types of cyber attacks or phishing. One such common threat is called man-in-the-middle or MITM attack.\
In such a scenario, a malicious individual can intercept the data that’s being transmitted between your device and the network- even if you’re using a no-upload tool. What they exploit are usually browser history, any open tabs, or cookies. Moreover, these attackers can get hold of unsecured networks to inject malware, which further compromises device security. That’s why it’s so important to stay vigilant when you’re dealing with sensitive information.\
So, what’s the solution? Well, whenever you’re working on PDFs, do it on a secure network, whether it’s in your home or workplace. Wi-fi with strong encryption like WP3 can protect you to a good extent. And if you’re traveling or on-the-go and you find yourself needing to combine PDFs, be sure to use a trusted VPN while doing so. The advantage VPN offers is that it encrypts all traffic from your device, thus guarding your activity from potential attack or theft.Keep your operating system and browser updatedSome people are against constantly updating their OS, and somewhat rightfully so. After all, who wants to fix something that’s not broken? However, it’s important to remember that OS and browser updates happen for a reason, and one of those reasons is related to security. Security related issues are constantly discovered and patched through these software updates and without them, cybercriminals would easily be able to get a hold of your sensitive personal data.\
If you neglect these updates like many people do, you’d be leaving your device vulnerable to unnecessary risks. For instance, malicious individuals frequently target browsers because people use them to handle a variety of tasks, which include but are not limited to file downloads, processing scripts, etc. That’s why when dealing with merging sensitive PDFs, you must ensure that both your OS and browser are up to date.Clear local files and browser cache after processingHere’s something that often goes unnoticed– even if you’re using a privacy-first PDF tool, residual data can sometimes remain on your device after processing. Some examples of residual data are cached content, temporary files, and session cookies. These may still be present in either your browser’s memory or in hard drive storage. If you don’t clear these leftover data, overtime they could pose a security risk, especially if others have access to your device.\
Once you’ve merged your PDF files and saved them locally, make a habit of immediately clearing any unnecessary data. Not to forget you must also delete any local copies of the original PDF file from your system, if they’re no longer needed. Taking this extra step will ensure that your device has no traces of sensitive information left behind.You can most certainly combine PDF files securely without sacrificing privacy, and do it without using any fancy sounding AI tool either. By using a privacy-first PDF combine tool and taking some basic precautions as outlined in this guide, you can make sure that your sensitive data remains where it should- with you. We hope you found this guide informative, thank you for reading!]]></content:encoded></item><item><title>Bored? Good. That Means Your Brain Is Working</title><link>https://hackernoon.com/bored-good-that-means-your-brain-is-working?source=rss</link><author>Scott D. Clary</author><category>tech</category><pubDate>Mon, 24 Mar 2025 10:45:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most people are afraid of being alone with their thoughts.\
They fill every moment with noise, distraction, and cheap dopamine. Social media. Netflix. Group hangouts. Mindless scrolling.\
Anything to avoid the discomfort of their own company.\
The most productive time you will ever have is the time that seems most "boring" compared to others.\
Those hours of solitude. The long walks where you work through complex problems in your head. The mornings spent in deep focus while the rest of the world is still sleeping.\
This is your hidden advantage in a world that can't tolerate silence or stillness.The Addiction To Stimulation\
The average person checks their phone 96 times a day. That's once every 10 minutes.\
They bounce between meetings, calls, and collaborative work. They mistake busyness for productivity and motion for progress.\
And at the end of the day, what do they have to show for it?\
A bunch of half-finished tasks. A mind scattered in a thousand directions. And the nagging feeling that despite all their activity, they haven't moved the needle on anything that truly matters.\
Here's the brutal truth: Constant stimulation is making you stupid.\
Your brain needs periods of quiet and boredom to:Make unexpected connections\
But most people can't tolerate even five minutes without reaching for their phone.\
They've conditioned their minds to crave the next hit of novelty, and they're paying for it with their creative potential.\
Are you willing to be different?The Strategic Advantage Of SolitudeSome of history's greatest thinkers understood the power of solitude.\
Einstein took long walks alone to solve physics problems. Newton discovered gravity during a period of isolated study when universities closed due to the plague. Maya Angelou rented hotel rooms where she would write in solitude for hours.\
This isn't coincidence. It's causation.\
Deep work requires deep focus, and deep focus requires isolation from distraction.\
When you create space for yourself to think deeply, you're not being antisocial or unproductive. You're giving yourself the most valuable gift possible: uninterrupted thought.\
This is where breakthroughs happen. Where connections form that others miss. Where you develop the clarity to see opportunities invisible to the chronically distracted.\
In a world optimized for shallow work and instant gratification, your ability to go deep is your superpower.Why Boredom Makes You BetterRemember being bored as a kid?\
Not the pleasant boredom of a lazy summer day. The excruciating, "I-have-nothing-to-do" boredom that drove your parents crazy.\
That boredom wasn't useless. It was the catalyst for creativity.\
When there's nothing to entertain you, your mind is forced to entertain itself. It starts making connections, generating ideas, and solving problems. It craves stimulation, so it creates it.\
Boredom is the space where creativity is born.\
Today's world has engineered boredom out of existence. There's always something to watch, read, or listen to. Always someone to talk to.\
Studies show that children who experience boredom develop better problem-solving skills and greater creativity. Adults who embrace boredom report higher levels of creativity and productivity.\
The most successful people I know deliberately create  in their schedules – times when they do nothing but think, reflect, and let their minds wander.\
They understand that insights rarely come from consuming more information. They come from giving your mind space to process what you already know.The Walk That Changed AppleIn 2011, Steve Jobs was working on what would become one of Apple's most revolutionary products: Siri.\
But he wasn't in a lab. He wasn't in a meeting. He wasn't even at his desk.\
Jobs was famous for his walking meetings. He believed that walking sparked creativity and allowed for deeper conversations than sitting in a conference room.\
One particular walk with his biographer Walter Isaacson reveals his thinking:\
"If you just sit and act, then you're going to die," Jobs told Isaacson during one of their walks. "Taking a walk gives you time to think about a problem differently."\
During these seemingly "boring" walks, Jobs made some of his most important decisions and had his most creative insights.\
This wasn't unique to Jobs. Mark Zuckerberg, Jack Dorsey, and Charles Darwin all incorporated long walks into their daily routines.\
They understood that movement combined with solitude creates the perfect conditions for breakthrough thinking.\
The next time you're stuck on a problem, don't call another meeting. Take a walk alone instead.The Silent Retreat Of BillionairesBill Gates has a strange habit.\
Twice a year, he disappears for a "Think Week" – seven days of complete solitude in a secluded cabin. No phone, no internet, no family, no friends. Just books, notebooks, and time to think.\
During these periods of intense solitude, Gates reads, reflects, and maps out the future of Microsoft and his foundation. Many of Microsoft's pivotal strategic shifts originated during these "boring" weeks in the woods.\
Gates isn't alone. Jack Dorsey does 10-day silent meditation retreats. Ray Dalio attributes much of his success to his meditation practice.\
Naval Ravikant starts each day with an hour of solitude.\
These aren't coincidences. These are strategic choices by some of the world's most successful people.\
They know what most don't: Innovation happens in isolation, not in collaboration.\
Teams are great for execution, but breakthrough insights almost always come from individual thought.\
If you want to think thoughts no one has thought before, you need to create conditions no one else is creating. And that usually means embracing solitude in a world addicted to connection.How To Create Your Own Boring AdvantageYou don't need a cabin in the woods or a week away from civilization to harness the power of "boring" productivity.1. Schedule Deep Work BlocksDeep work is the ability to focus without distraction on a cognitively demanding task.\
Block out 2-3 hours in your calendar for uninterrupted focus. Turn off notifications. Close your door. Tell people you're unavailable.\
Then work on your most important project with complete focus.\
The first few times will be uncomfortable. Your mind will crave distraction. Push through it. Like any muscle, your focus gets stronger with use.\
Two hours of true deep work will produce more value than eight hours of distracted shallow work.2. Take Daily Thinking WalksThe simplest productivity hack is also the most powerful: a daily walk with no phone, no podcasts, no music.\
Just you and your thoughts.\
Start with 20 minutes. Let your mind wander. Notice what emerges when you're not filling your brain with input.\
Ask yourself open-ended questions:What am I missing in my current strategy?What would make the biggest difference in my business right now?What would this look like if it were easy?\
Walking physically changes your brain chemistry in ways that enhance creative thinking.\
The combination of light physical activity and distraction-free thinking creates the perfect conditions for insights.3. Implement Strategic BoredomOnce a week, schedule an hour of pure boredom.\
Sit in a chair. No phone. No book. No music. No journaling. Just you and your thoughts.\
The first 20 minutes will be excruciating. Your mind will beg for stimulation. Give it nothing.\
After about 30 minutes, something magical happens. Your brain, starved for input, starts generating its own. Ideas emerge. Connections form. Problems that seemed unsolvable suddenly have obvious solutions.\
This is your mind doing what it was built to do before we started bombarding it with constant stimulation.\
Boredom isn't the absence of thought. It's the beginning of deeper thought.Most people are drowning in information while starving for wisdom.\
They consume endless content without giving themselves time to process it.One day per week with no new information inputNo social media, news, podcasts, or videosReview your notes from what you've already learnedAsk yourself: "What does this mean for me and my work?"\
You already know enough to be wildly successful. The problem isn't lack of information. It's lack of implementation.\
When you create boundaries around information consumption, you force yourself to metabolize what you've already learned rather than constantly seeking the next insight.Let's be honest: there's nothing sexy about sitting alone with your thoughts.\
When someone asks what you did yesterday, "I took a two-hour walk and thought about my business strategy" doesn't sound as impressive as "I had meetings with three potential clients and launched a new marketing campaign."\
But results speak louder than activity.\
The most successful people have the courage to be boring.\
They're willing to do the unsexy work of thinking deeply, planning carefully, and executing with focus. They're comfortable saying no to distractions that others can't resist.\
While everyone else is trying to look busy and important, they're creating space for the thought and focus that actually moves the needle.\
Don't mistake their quietness for inaction. The still waters run deep.The Hard Truth About Your ProductivityYou will never reach your potential if you can't tolerate being alone with your thoughts.\
The ability to think deeply for extended periods is becoming increasingly rare and increasingly valuable. As the world gets noisier, the advantages of silence grow stronger.\
The most valuable work you'll ever do won't happen in meetings or on collaborative documents or in crowded coworking spaces.\
It will happen in moments of isolation that would bore most people to tears.\
Your competitive advantage lives in the space between stimuli.\
In the quiet moments when others reach for their phones, you reach for deeper thoughts. In the morning hours when others are still sleeping, you're solving problems. In the walks that others fill with podcasts, you're generating ideas.\
Not by working more hours. Not by being perpetually busy. Not by constant collaboration.\
But by embracing the boring, silent spaces where true productivity lives.\
The question is – are you comfortable enough with yourself to find out what's waiting for you in the silence?]]></content:encoded></item><item><title>OpenAI Introduces Controversial o1 Pricing Model</title><link>https://hackernoon.com/openai-introduces-controversial-o1-pricing-model?source=rss</link><author>This Week in JavaScript</author><category>tech</category><pubDate>Mon, 24 Mar 2025 10:31:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Hello JavaScript Enthusiasts!\
Welcome to a new edition of ""!\
Today, we're covering OpenAI's controversial pricing model, the heated Next.js vs TanStack debate, and game-changing tool releases- plus optimization techniques you won't want to miss!Next.js vs TanStack: The Framework Showdown\
Why Developers Are Switching:Simplicity vs. Complexity: Next.js offers high-scale optimization but with a steeper learning curve TanStack provides cleaner APIs that feel more intuitive Do you really need SSR, ISR, PPR, and other acronyms for most projects? TanStack paired with Vite delivers faster compile times\
The tradeoff? You lose some of Next.js's built-in performance optimizations, but gain code that's easier to reason about.Rsdoctor 1.0: Build Analysis Reimagined Visualize your entire build process with detailed breakdowns See exactly what each loader is doing to your files New Rust integration makes analysis up to 20% faster Automatically detect duplicate packages and compatibility issues\
If you've ever been frustrated by webpack slowdowns or mysterious build issues, this tool is your new best friend.OpenAI's o1 Pro: Powerful But Pricey 136 times more expensive than other AI options Can tackle complex coding challenges that stump other models Random failures and UI quirks plague the experience For most everyday tasks, the cheaper o3 mini delivers superior results\
Is the hefty price tag justified? For most developers, probably not – unless you're tackling those one-in-a-million programming puzzles.CKEditor Case Study: Lightning-Fast Loading Loading times reduced dramatically without major architecture changes Improved model-view conversion process eliminated redundant operations Specialized approach for content with excessive formatting\
For anyone working with rich text editors, their methodology offers valuable lessons in optimization.Let's speed-run through some of the other big tool updates this week!: Beta support for React Server Components, new CLI for scaffolding projects, native HTML import maps for better browser caching, and first-class MDX support built in Rust. Includes an automated migration script for Create React App users.: Enhanced form controls with helper text and error messaging, new expandToScroll property for sheet modals, full React 19 support, and improved RTL language handling for seamless cross-platform development.: Refined routing with better type generation for server builds, improved loader behavior, and a more robust middleware system with cleaner error handling and better context typing.: Upgraded presentation experience with a new lightbox feature for images and videos. Continues to be the go-to choice for developers who want to create stunning slides using web technologies.: Brings formal verification to TypeScript syntax, allowing mathematical proof of code correctness. A fascinating bridge between programming and mathematical logic, perfect for critical applications where correctness is non-negotiable.And that's it for the twenty-sixth issue of ""\
Feel free to share this newsletter with a fellow developer, and make sure you're following for next week’s issue.\
Until next time, happy coding!]]></content:encoded></item><item><title>AI Agent Browsers Are Failing (And It’s Not Just Because of CAPTCHAs)</title><link>https://hackernoon.com/ai-agent-browsers-are-failing-and-its-not-just-because-of-captchas?source=rss</link><author>Bright Data</author><category>tech</category><pubDate>Mon, 24 Mar 2025 10:29:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[OpenAI broke the news a few months ago with the announcement of Operator, an AI agent that can perform tasks in the browser for you. 🚨\
Since then, many other AI agent browsers have joined the scene. But here’s the big question—are they really leading to the revolution we all expected, or are they failing? (Spoiler:  😅).\
So, if they’re failing, why is that? 🤔\
Well, simply read this article and find out! 👇Wait… But What Are AI Agent Browsers?AI agent browsers, also known as , are tools that let AI agents control web browsers like human users.\
You write a prompt, and the AI takes control of the browser to complete tasks, like navigating pages, filling out forms, scraping data, and automating workflows:\
These tools—often powered by open-source libraries or frameworks—bridge the gap between AI and the web, performing tasks that usually require human interaction. Their goal? Helping you save time on tedious tasks by letting AI handle them efficiently and effectively. ⏳The Brutal Truth About AI Agent Browser FailuresNow, you might think the "AI agent browser failures" we’re talking about are just AI messing up tasks from a prompt. Sure, that’s definitely an issue, and there are some trending videos on X of AI browsers failing over and over again… 🤖💀\
Just like early AI image generators that couldn’t draw hands to save their lives but now create realistic images that fool most people, AI agent browsers are still in their awkward phase—but they’ll only get better with time.\
Yeah, even those AI-generated videos of Will Smith eating spaghetti are only getting better and better… 🍝\
So, the real reason “AI agent browsers are failing” isn’t just their occasional clumsiness—it’s because most websites can block them with ease! 🛑\
At the end of the day, AI browsers are still bots, and modern websites are packed with anti-bot defenses designed to stop them.\
Think about it: AI-powered automation is cool, but companies are already  (remember,  🚨). No wonder businesses (and we, as users) are fed up. This new wave of browser agents is only fueling the arms race between bots and anti-bot tech. 💪 🦾\
Time to down the most common anti-bot techniques used to shut down AI browser agents—and why they’re so brutally effective. 👀\
Shocking, right? A tool built specifically to block bots… that is actually good at blocking bots. Who would’ve thought? 😐\
But here’s the real kicker—\
How does that work? Well, it involves collecting unique details about your browser, like screen resolution, browser version, OS version, installed plugins, and more, to create a "fingerprint" of your browser. It then compares this fingerprint with some known fingerprints to determine if a user looks like a human or not.\
At the end of the day, AI agent browsers aren't magic 🪄. They must interact with the underlying browser using known solutions like the Chrome DevTools Protocol. Or at least, that’s what most popular open-source libraries to build browser agents (like browser-use, based on Playwright) do! 🔧\
That said, browser automation tools leave some telltale signs and leaks—like special parameters and default configurations required to control browsers—that advanced fingerprinting systems can easily detect:\
Once detected, it’s game over as they automatically block your requests or, as often happens, show you a CAPTCHA.\
While "stealth plugins" can patch the browser and hide those leaks, these patches are open-source, meaning anti-AI bot systems can study and bypass them. Long story short, it’s an ongoing battle with no real winner… ⚔️\
Want to learn more about browser automation stealth plugins? Read the following articles:User behavioral analysis, or in short , is another powerful weapon websites can adopt to stop agent-based browsers. Unlike basic fingerprinting systems that focus on technical browser characteristics, UBA looks at how users interact with a site.\
In particular, a UBA system tracks behaviors like mouse movements 🖱️, scrolling patterns 📜, keystrokes ⌨️, and even how long someone spends on each page ⏱️. For a deep dive, check out the 27-minute read (yes, you read that right! 🤯) “On Anti-Bot Biometric Protections" article.\
AI agent browsers, no matter how human they try to act, tend to follow predictable patterns with unnaturally smooth or “perfect” mouse movements—easy to spot through UBA.\
Sure, not all sites adopt UBA, and for it to work effectively, a lot of data must be collected and analyzed in real time—which can be bandwidth-heavy 💻⇄💻. Also, UBA systems can sometimes generate false positives due to the heuristics and ML models behind them…\
However, with AI agent browser bots on the rise, it’s clear UBA will play an even bigger role in bot detection—and in the future, these systems will become more affordable and effective than ever!Other Anti-Bot Tactics That Wreck AI Agent BrowsersStopping bots is an art, and it involves a lot of tricks! 🧙‍♂️\
Most anti-bot systems have a lot of tools in their toolbox 🧰 and plenty of aces up their sleeves ♠️. Earlier, we explored the most effective ones against new AI agent browsers. But hey, there’s more:Also, don’t miss the video below for a look at advanced and modern anti-bot tactics: 🎥Say Goodbye to AI Agent Browser Failures—Here’s the Fix!So, what have you learned in this article? 🤔\
You learned that most AI agent bots can easily be detected (and stopped) by existing anti-bot solutions. While some advanced systems block based on user behavior, most blocks still rely on classic techniques like fingerprinting and CAPTCHAs. 🔒\
Thus, we can say that the real weak point of AI agent browser agents isn't the AI automation tech itself, but the browser they use (usually Chromium or a modified version of it)—as that is easily detectable.\
Now, imagine a browser that could:Scale infinitely in the cloud ☁️Avoid IP bans with seamless proxy integration across 72 million high-quality residential IPs 🌍Prevent browser fingerprinting issues 🕵️‍♂️Bypass geo-restrictions with built-in proxy support 🌏Automatically solve CAPTCHAs from popular providers like reCAPTCHA, hCaptcha, SimpleCaptcha, and dozens of others 🛡️Well, that browser—if only it existed—would be much more effective than current AI agent browsers. Guess what? It does exist! 🙌\
Enjoy AI-powered browser automation with zero blocks in a browser designed specifically to bypass any anti-bot systems. 🎉AI agent browsers are here to revolutionize the way we handle tedious tasks on the internet. 🌐 But the browsers they use for automation still face the same old issues as traditional browser automation bots. In short, anti-bot solutions often get the upper hand. 😬\
Avoid the hassle with Browser Agent, featuring a built-in anti-bot bypass and seamless integration with any popular open-source AI browser agent library.\
Until next time, keep exploring the internet freely—even with AI agents!]]></content:encoded></item><item><title>Nvidia GTC 2025 Updates, Ernie 4.5: 100x cheaper than GPT 4.5, Google AI meets robots, and more</title><link>https://hackernoon.com/nvidia-gtc-2025-updates-ernie-45-100x-cheaper-than-gpt-45-google-ai-meets-robots-and-more?source=rss</link><author>This Week in AI Engineering</author><category>tech</category><pubDate>Mon, 24 Mar 2025 09:55:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Welcome to the eleventh edition of "This Week in AI Engineering"!NVIDIA unveiled its Blackwell platform delivering 40x Hopper performance, Baidu's ERNIE 4.5 outperforms GPT-4o at 1% of the cost, Mistral Small 3.1 achieves leading benchmark scores with just 24B parameters, and Google's Gemini Robotics brings advanced AI to physical systems.Plus, we'll cover Microsoft's strategic pivot with MAI models and RA.Aid's autonomous coding framework, alongside must-know tools to make developing AI agents and apps easier.NVIDIA GTC 2025: Major AI Infrastructure and Model AdvancementsNVIDIA has unveiled significant AI infrastructure and model advancements at , setting the stage for the next generation of reasoning and agentic AI capabilities. The company's announcements span from next-generation hardware to advanced AI models for robotics and reasoning.Next-Generation AI Compute Platforms: The Blackwell platform is now in full production, delivering 40x the performance of Hopper for reasoning AI workloads: Coming in H2 2025, enhancing training and test-time scaling inference for agentic AI, reasoning, and physical AI applications: Next-generation GPU architecture announced, featuring NVL 144 systems with completely redesigned components arriving in H2 2026: Established regular cadence for infrastructure updates to help organizations plan AI investmentsAI Performance Enhancements: Blackwell NVL72 with Dynamo delivers 40x the AI factory performance of Hopper: New Spectrum-X and Quantum-X silicon photonics networking switches provide 3.5x more power efficiency, 63x greater signal integrity, and 10x better network resiliencyAI Software and Foundation Models: New open-source software for accelerating and scaling AI reasoning models in AI factoriesDGX Spark and DGX Station: Personal AI supercomputers powered by the Grace Blackwell platform for AI development: Open model family with reasoning capabilities designed for creating advanced AI agents: World's first open, fully customizable foundation model for generalized humanoid reasoning and skills: New world foundation models for physical AI development with unprecedented control over world generation: Open-source physics engine for robotics simulation, developed with Google DeepMind and Disney ResearchThe company anticipates significant growth in AI computing demand driven by reasoning and agentic AI, with NVIDIA's CEO Jensen Huang estimating data center buildout to reach $1 trillion. These developments underscore NVIDIA's focus on three key AI infrastructures: cloud, enterprise, and robotics, with a complete stack for each domain.ocusing on the emotional and contextual elements that make human communication meaningful, addressing the "emotional flatness" problem that limits user engagement with current systems.Baidu has released , a native multimodal model designed to process text, image, audio, and video content within a unified framework. This new model represents a significant advancement in Baidu's AI capabilities with strong performance across multiple benchmarks.: Integrates multiple modalities through collaborative optimizationSpatiotemporal Representation Compression: Enhances processing of temporal and spatial dataHeterogeneous Multimodal MoE: Leverages mixture-of-experts architecture that activates specialized components only when neededKnowledge-Centric Training: Utilizes improved data construction methods for better understanding: 79.6 points across standard benchmarks, outperforming GPT-4o (69.8) and DeepSeek-V3 (79.14): Superior results on C-Eval, CMMLU, and Chinese SimpleQA compared to non-Chinese models: 94.1% on GSM8K mathematical reasoning benchmark, exceeding both GPT-4o and GPT-4.5: Operates at approximately 1% of GPT-4.5's cost and half the deployment cost of DeepSeek-R1: Now freely available to all users ahead of schedule: ERNIE 4.5 capabilities being integrated across Baidu's product line: Available through APIs on Baidu AI Cloud for enterprise users and developers: Companion model focused specifically on reasoning-intensive tasks in finance, law, and data analysisWhile ERNIE 4.5 demonstrates leading performance in many areas, it does show limitations in some specialized benchmarks including GPQA (science questions) and LiveCodeBench (coding capabilities) where GPT-4.5 maintains an edge. Baidu has announced plans to release ERNIE 5 later in 2025 with enhanced multimodal capabilities.Mistral AI has released , a 24B parameter model that demonstrates exceptional performance across text reasoning, multimodal understanding, and long-context processing while maintaining significant speed advantages over competitors.: Achieves 46.7% on GPQA Diamond benchmark, outperforming both Claude-3.5 Haiku and GPT-4o Mini: 80.7% on MMLU benchmark, surpassing both Gemma 3-it (27B) and GPT-4o Mini: 73% on MM-MT-Bench, significantly ahead of larger models including GPT-4o Mini (65%): Leading performance on RULER 32K (94%) and strong results on RULER 128K (81%): Just 10.8 milliseconds per token, 25% faster than its closest competitors: Delivers top-tier performance with only 24B parameters versus competitors' 27-32B: Integrated vision capabilities with strong performance on MathVista (68%): Expanded to 128K tokens with maintained performance at longer contexts: Released under Apache 2.0 for full commercial use: Achieves 150 tokens per second throughput on standard hardware: Available through Hugging Face, Ollama, Kaggle, and major cloud providers: Runs efficiently on a single RTX 4090 or 32GB MacBookMistral Small 3.1 demonstrates that smaller, carefully optimized models can outperform larger counterparts across a wide range of benchmarks while delivering superior inference speeds. The model's strong scientific reasoning capabilities (shown in its GPQA performance) coupled with excellent multimodal processing make it particularly well-suited for complex real-world applications requiring both speed and accuracy.Gemini Robotics: Google DeepMind Brings Advanced AI Models to Robotics has introduced two new AI models based on Gemini 2.0 that bridge the gap between digital AI capabilities and physical robot embodiments. This development represents a significant advancement in enabling robots to perform complex real-world tasks with greater adaptability and precision.Gemini Robotics Model Family: An advanced vision-language-action (VLA) model built on Gemini 2.0 that adds physical actions as a new output modality: Specialized model with enhanced spatial understanding and embodied reasoning (ER) for roboticists running their own controller programs: More than doubles the performance on generalization benchmarks compared to state-of-the-art VLA models: Understands conversational language instructions in multiple languages and adapts to environmental changes in real-time: Performs precise manipulation tasks (origami folding, snack packing) requiring fine motor skills: Trained primarily on bi-arm ALOHA 2 platform but adaptable to various robot types including Franka arms and Apptronik's Apollo humanoid robot: Enhanced 3D detection and pointing abilities compared to standard Gemini 2.0On-Demand Code Generation: Generates appropriate grasping strategies and safe motion trajectories based on visual input: Achieves 2-3x success rate compared to Gemini 2.0 in comprehensive robotics tasks: Combines traditional robotics safety measures with AI-driven semantic understanding: Released a new dataset for evaluating semantic safety in embodied AI: Developed data-driven "constitution" approach inspired by Asimov's Three Laws for safer robot behaviorGoogle DeepMind is collaborating with Apptronik to develop humanoid robots powered by Gemini 2.0, and has opened Gemini Robotics-ER to trusted testers including Agile Robots, Agility Robots, Boston Dynamics, and Enchanted Tools to explore real-world applications of these advanced models.RA.Aid AI Coding Agent with Three-Stage Development Architecture (pronounced "raid") has been released as a standalone coding agent designed to develop software autonomously through a structured research, planning, and implementation workflow. Built on LangGraph's agent-based task execution framework, the tool offers a comprehensive approach to handling complex development tasks.: Analyzes codebases, gathers context, and researches solutions using web sources via Tavily API: Breaks down tasks into specific, actionable steps with detailed implementation plans: Executes planned tasks, makes code changes, and runs necessary shell commands: Works with multiple AI providers including Anthropic, OpenAI, OpenRouter, DeepSeek, and Gemini: Can selectively use advanced reasoning models like OpenAI's o1 for complex debugging: Optional interactive mode for assistance during task executionWeb Research Capabilities: Automatically searches for best practices and solutions when needed: Optional integration with aider via the --use-aider flag: Basic coding tasks with confirmation prompts for shell commands: Skips confirmation prompts for automated execution in CI/CD pipelines: Interactive conversation about development tasks: Web interface for team collaboration with real-time output streamingThe tool is designed for both single-shot code edits and complex multi-step programming tasks that require deep codebase understanding. It can handle tasks ranging from explaining authentication flows to implementing new features and refactoring code across multiple files.RA.Aid is available for installation via pip (pip install ra-aid) and supports Windows, macOS, and Linux. The project is open source and accepts community contributions through GitHub.Microsoft MAI Models: New In-House AI Reasoning Models to Reduce OpenAI DependencyMicrosoft is developing a new family of native AI reasoning models codenamed MAI (Microsoft AI) aimed at reducing its dependence on OpenAI while maintaining comparable performance to industry-leading models. This initiative represents a strategic pivot for Microsoft, which has invested approximately $13.75 billion in OpenAI since 2019.Chain-of-Thought Reasoning: Models employ a human-like reasoning process that breaks down complex problems into intermediate steps: Multiple models being developed under the MAI umbrella, larger and more capable than Microsoft's earlier Phi models: Internal testing shows MAI models performing nearly as well as leading models from OpenAI and Anthropic: Plans to release MAI as an API later in 2025 for third-party developers: Already testing replacing OpenAI models with MAI in Microsoft 365 CopilotMultiple Provider Strategy: Testing models from xAI, Meta, and DeepSeek as potential OpenAI alternatives: Developing proprietary models to reduce recurring licensing fees for external AI: Chain-of-thought reasoning provides clearer decision trails for enterprise users: Will allow developers to embed MAI reasoning models into their own applicationsThe initiative is led by Microsoft's AI division under Mustafa Suleyman, focusing on creating models that maintain performance while offering greater control over integration, cost structure, and technical roadmap. Despite this push for self-reliance, Microsoft is maintaining its relationship with OpenAI, with GPT-4 remaining an active component in Microsoft's current product portfolio. is an AI-powered platform designed to simplify WordPress development. It offers AI chat and coding tools specifically trained for WordPress, enabling users to generate code snippets, troubleshoot issues, and even create entire plugins using natural language prompts. CodeWP is applicable for WordPress non-techies, WordPress developers, and WordPress agencies to enhance their WordPress workflow with AI. It caters to anyone from amateur developers to experienced professionals looking to streamline their processes and save time on WordPress-related tasks.**IBM watsonx Code Assistant for Z **is an AI-powered product designed to modernize mainframe applications. It helps developers understand, refactor, and optimize code, as well as convert COBOL to Java using generative AI. Applicable to businesses using IBM Z mainframes, it's particularly useful for application developers, IT architects, and modernization teams aiming to reduce costs, increase productivity, and streamline the modernization process, especially when onboarding new talent or creating RESTful APIs for their mainframes. is a command-line tool leveraging OpenAI's models to function as an AI-assisted coding partner. It automatically generates code modifications and commits directly to Git repositories based on natural language instructions. Aider is technically suited for software developers, DevOps engineers, and technical project managers seeking to accelerate development cycles, automate repetitive coding tasks, and facilitate collaborative code generation. It is applicable in software development environments, version control systems, and CI/CD pipelines.'s Pixeebot is an automated code review tool that identifies security vulnerabilities and code quality defects. It generates pull requests containing suggested remediations, integrating directly into the development workflow via a GitHub app or CLI. Technically, it targets software developers and security engineers, automatically improving codebases and reducing the burden of manual code analysis by providing fixes ready for merging. It is applicable to any software development project hosted on GitHub, where automated code review and remediation are desired.And that wraps up this issue of "This Week in AI Engineering", brought to you by — your flight recorder for AI apps! Non-deterministic AI issues are hard to repro, unless you have Jam! Instant replay the session, prompt + logs to debug ⚡️Thank you for tuning in! Be sure to share this newsletter with your fellow AI enthusiasts and follow for more weekly updates.Until next time, happy building!]]></content:encoded></item><item><title>The Renewables Energy Market Can Still Thrive in 2025</title><link>https://hackernoon.com/the-renewables-energy-market-can-still-thrive-in-2025?source=rss</link><author>Dmytro Spilka</author><category>tech</category><pubDate>Mon, 24 Mar 2025 09:54:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2025 appears to be a challenging year for renewable energy stocks and ESG initiatives as a whole. As geopolitical policy shifts and suspicions of greenwashing impact environmental, social, and governance investing, some of the industry’s brightest stocks are showing remarkable signs of resilience.\
Even before the famously climate-sceptical Donald Trump returned for his second term as President of the United States, a clear pullback on ESG investing had interrupted the growth potential of sustainability-focused stocks on Wall Street.\
According to the Association of Investment Companies’ annual ESG Attitudes Tracker, the number of private investors who say they consider environmental, social, and governance when investing fell for the third year in a row .\
This downward trend is striking when considering that the figure stood at 66% in 2021, 60% in 2022, and 52% in 2023.\
President Trump’s bid to de-incentivize ESG firms by , including ‘all Environmental approvals’, has served as an indication that sustainability initiatives will carry no special favor in the White House for the foreseeable future.\
We’ve also seen more investors become wary of ‘greenwashing’, which occurs when businesses add bogus ESG claims to their brand with little tangible evidence of sustainable practices. In 2023, investor concerns over greenwashing  while unfounded claims undermined faith in the firms that had sought to ramp up their ESG credentials.\
Although these factors point to a challenging playing field for renewable energy stocks, there are clear examples of sustainability stocks that are thriving despite low ESG investor sentiment. At a time when Wall Street is struggling to keep up with volatility and growing competition in the AI industry, it may be a perfect time for renewables to make a timely comeback.\
With this in mind, let’s explore the three brightest renewable energy stocks for investors to load onto their portfolios today::::warning
Editor’s note: This article is for informational purposes only and does not constitute investment advice. Stock trading is speculative, complex, and involves high risks. This can mean high prices volatility and potential loss of your initial investment. You should consider your financial situation, investment purposes, and consult with a financial advisor before making any investment decisions. The HackerNoon editorial team has only verified the story for grammatical accuracy and does not endorse or guarantee the accuracy, reliability, or completeness of the information stated in this article. #DYOR1. Enovix Corporation (ENVX)It’s been a challenging start to 2025 for silicon lithium-ion specialists, Evonix Corporation (NASDAQ: ENVX), which saw its stock fall 26% lower over the first two months of the year. However, the battery manufacturer may prove to be available at a discounted price for investors.\
The stock reported Q4 2024 , while revenues for the full year climbed to $23.1 million, representing a substantial increase from $7.6 million in 2023.\
Crucially, Enovix is a source of optimism because of the receipt of a  for its silicon batteries from an unidentified technology company.\
The batteries, which are intended for use in headset-based mixed-reality wearables, will begin shipping in mid-2025 and could be the beginning of more lucrative orders as wearable technology like smart glasses begins to become a more pervasive consumer technology.\
Interestingly, Enovix also has meme stock credentials, and ranks 11th in Yahoo! Finance’s list of the best meme stocks to buy. This helps to provide another frontier in which ENVX can secure growth in 2025.Another stock that’s experienced a weak start to 2025 but is packed with potential is solar energy small-cap stock Sunrun Inc. (NASDAQ: RUN).\
With a market capitalization of around $1.46 billion, Sunrun is a smaller player on this list with plenty of room for growth.\
With a recent stock downgrade from Jeffries, which saw the stock cut from a ‘buy’ to ‘hold’ rating along with its price target , it’s clear that Sunrun is more of an addition with the future in mind.\
Despite its challenges, Sunrun remains one of the United States’ biggest residential solar energy providers and almost doubled its market share in California alone in recent years.\
The company’s strong innovation pipeline has seen developments like the attaching of battery storage to 60% of its new projects, helping RUN to expand its addressable market as the execution of storage solutions climbed 126%.\
With Sunrun expecting to generate  in cash throughout 2025, this stock could become a growth leader in the renewable energy market.3. NANO Nuclear Energy Inc. (NNE)One renewable energy firm that’s experienced an exceptionally strong start to 2025 is NANO Nuclear Energy Inc. (NASDAQ: NNE).\
The stock has rallied more than 20% in the opening two months of the year and appears to be building momentum.\
“The company is advancing projects such as the ZEUS Solid State Storage Reactor and the ODIN Low-Pressure Salt Reactor, which aim to offer portable and reliable clean energy solutions,” explained Maxim Manturov, head of investment research at Freedom24.\
“The recent acquisition of key nuclear technologies not only strengthens NNE's product portfolio but also enhances its competitive advantage in the nuclear energy sector. With significant capital raised and a strong cash position, NNE is well-positioned to fund its research and development initiatives while coping with potential market challenges.”\
NANO is already making big moves in 2025, and recently  as chief technology officer and head of reactor development, who is richly experienced and expected to become a driving force in refining the company’s innovations.Although renewable energy stocks have been caught up in the wider ESG downturn in recent years, stock market volatility may provide an opportunity for more sustainable companies to prove their worth on a global scale.\
This could see the brightest renewables stocks generating more sustainable levels of growth even in the challenging investment landscape that 2025 is proving to be.\
The sustainability boom may have been on the back burner, but its best stocks could be primed for impressive results over the coming months and years.]]></content:encoded></item><item><title>Bitunix Becomes the Center of Attention at Web3 Amsterdam, Dominating as Title Sponsor</title><link>https://hackernoon.com/bitunix-becomes-the-center-of-attention-at-web3-amsterdam-dominating-as-title-sponsor?source=rss</link><author>Bitunix</author><category>tech</category><pubDate>Mon, 24 Mar 2025 09:19:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, March 21st, 2025 - In an extraordinary atmosphere at the unique Circa Amsterdam venue, the Web3 Amsterdam Conference took place on March 13th and 14th in the Dutch capital, where Bitunix exchange captured the spotlight as the Title Sponsor of the event.\
The booth of the world’s fastest-growing cryptocurrency exchange attracted significant attention, with visitors eager to learn more about Bitunix’s services. Many were particularly curious about the platform’s unique features, fees, liquidity, and transparency, with Bitunix representatives providing detailed answers without hesitation.\
During her presentation at the event, Andrea Soto, from Bitunix, highlighted the exchange's most advanced features, including the exclusive Multiwindow feature which allows traders to use  at once on one screen. Additionally, she introduced Bitunix's exclusive TradingView feature for mobile, a one-of-a-kind function globally.\
These features were a major draw at the Bitunix booth, with many visitors asking about them after her speech. Soto also emphasized that Bitunix prioritizes security, noting:“We provide safety for payments and wallets, as well as Proof of Reserve, so every trader using our exchange can be confident that 100% of the assets they hold are fully backed by the funds we have on the exchange.”Bitunix KOLs Discuss Market Trends and Memecoins on Stage\
While the panelists offered their perspectives on the memecoin market, Argiris notably criticized the growing popularity of memecoins, stating that they were draining liquidity away from altcoins. Greg from USACryptoNoticias also criticized memecoins, not favoring them as investment options. These remarks helped clarify that these panelists, such as USACryptoNoticias, do not trade with memecoins, as they focus mostly on other crypto strategies.\
In addition, USACryptoNoticias actively engaged with the conference audience by trading and performing live analysis both days at the Bitunix booth for his community. Aurolo, another influential crypto figure in the Spanish and LATAM region, also participated in these activities, providing valuable insights to attendees.Crypto Million Announces IMO Token Launch on Bitunix with an Airdrop for Web3 Amsterdam AttendeesCrypto Million, a well-known Bitunix KOL and one of the largest crypto influencers in Switzerland and the French region, delivered a speech in which he made a major announcement: the launch of the IMO token on Bitunix. During his speech, he gave attendees the exciting opportunity to receive free tokens in an airdrop, further highlighting Bitunix’s commitment to engaging its user base and expanding its offerings.\
The event provided a prime networking opportunity, allowing attendees to explore the latest industry trends, engage with expert speakers, and connect with leading professionals. As part of the partnership, Bitunix also hosted workshops and discussions, strengthening ties with developers, investors, and blockchain enthusiasts.\
Recent data from a 2024 European Central Bank study shows that 9% of Eurozone residents now hold cryptocurrencies, up from 4% in 2022, reinforcing the growing popularity of crypto in Europe and the significance of Bitunix’s presence at Web3 Amsterdam.This conference was a good opportunity for all interested parties to get to know the services and advantages that Bitunix offers. The exchange will also be part of Paris Blockchain Week and Token2049 in Dubai. Bitunix has built a reputation for actively participating in such conferences, striving to advance global crypto adoption while also educating users about its platform and services.Bitunix is a global  founded in 2021, committed to offering simple, secure, transparent, and cost-effective trading services to its users. Bitunix specializes in both spot trading and perpetual futures, with over 700 trading pairs and leverage of up to 125x.\
With features such as top-tier liquidity, 24/7 customer support, and a strong commitment to regulatory compliance, Bitunix remains at the forefront of providing a reliable trading experience for the global crypto community. Bitunix has attracted more than 2,000,000 users from over 100 countries, facilitating a daily trading volume exceeding $5 billion on its platform.]]></content:encoded></item><item><title>Gluwa - The Iron &quot;Lady&quot; of Global Financial Inclusion</title><link>https://hackernoon.com/gluwa-the-iron-lady-of-global-financial-inclusion?source=rss</link><author>Nebojsa &quot;Nesha&quot; Todorovic</author><category>tech</category><pubDate>Mon, 24 Mar 2025 09:17:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
What’s the story behind a story?\
The etymology of the word “gluwa” is fascinating on its own. There’s a whole philosophy behind a single word:Gluwa is named after 'Glory', the ironclad ship that sailed the sea for the first time in the world . It symbolizes a paradigm shift . It suggests that Gluwa's goal is to create the world's first borderless financial platform .\
I was like, that’s interesting.\
“The French navy was quick to realize that change was inevitable and they began to experiment with different ways to armor their vessels. The construction of the French ship Glorie began in April 1858 and she would feature a groundbreaking design. Her wooden hull would be encased in iron plate and she also was to have steam propulsion by means of an internal steam engine system. The Glorie was designed to be a broadside ironclad, with her guns arrayed in long side batteries, much as they had been on wooden ships for the previous 500 years. She was, however, not to be called a ship-of-the-line; rather she was an armored frigate, the term “ironclad” did not yet having any meaning.”\
The same analogy can be applied to the blockchain technology. The very first, if any meaning at all, to come to a regular not-so-obsessed-with-tech Joe’s mind is where-the-crypto-grows. So, I was like, save me from this pain, show me what you can really do, my dear blockchain. Something practical and impactful.\
I have to say, when it comes to finances, ignorance ain’t bliss. It’s cruel and unfair.\
What about the “financially underserved countries such as Nigeria, Vietnam, Myanmar, the Philippines, and Indonesia?” That’s like a lot of people.\
“Traditional banking systems left more than half of the world's population unbanked, according to the World Bank, making it difficult for many people to access credit from mainstream lenders. To serve financially underserved countries, Gluwa uses its blockchain network, Creditcoin. The system records users' payment histories — such as utility bill payments — on a public blockchain, creating verifiable credit profiles. This infrastructure is integrated with a stablecoin-based financial network tied to fiat currency, making financial transactions more accessible. The system not only helps individuals without traditional banking access build credit profiles but also enables Western fintech lending companies to expand over borders. "The U.S. financial system still relies heavily on paper-based processes, with digital systems merely replicating paper-based methods. Countries without such legacy infrastructure have the unique advantage of adopting today's best technologies from scratch," Oh said.”\
Now, it’s starting to make sense, for real.\
“A financial infrastructure development project using Creditcoin has benefited around 2 million Nigerians, significantly expanding financial access, according to the firm. Previously, only 200,000 people — just 0.1 percent of Nigeria's population — had access to financial services."\
I guess this means that only 0.1 percent of the “other” world’s population, which is unbanked, financially underserved, and “less fortunate,” has access to financial services we take for granted. It’s no wonder “Gluwa signed a memorandum of understanding with the Central Bank of Nigeria” as an MVP of this country’s financial future.We Live in a Material World, Or So I’m ToldHere’s a nice thing, I dare to say a positive “collateral damage,” standing in your way of fixing financial troubles. You start by dealing with one problem, but in order to make it, you end up with a list.\
“While working to provide financial services to underbanked markets, Oh realized that many potential users also lacked internet connectivity. "To truly serve these communities, we first needed to address their connectivity gap," Oh explained.”\
So, who are you gonna call when you have connectivity gap problems? Well, there’s one guy I can think of.\
You get the idea of how this problem is going to be solved. At the same time, I’m not saying that financial inclusion is all about sunshine and rainbows. So, read this without any illusion.\
“Unlike many popular DeFi protocols, Gluwa’s RWA investments involve unsecured lending. This means that there are counterparty risks, with investors potentially losing their funds in the case of borrower default. Nevertheless, to-date, there have been 0 defaults on Gluwa’s RWA investment platform, with legal protections in place to protect investors and recover funds in cases of default.”\
There’s also this: “Due to a heavy focus on deal-screening and risk management, only a limited number of Investment Opportunities (7) have been made available on the platform since launch. However, lower-yield, flexible withdrawal products are more consistently available.”\
And, this: “Investment Deals are sourced and assessed by a professional but centralized counterparty — the Gluwa Fund LP. This creates a risk of human oversight or error. However, this risk is present in almost every RWA investment platform today.”The Future of Financial Inclusion is Bright - Gluwa is Worth the Fight“Traditional financial services operate within the oversight of a local or regional jurisdiction. That limits what lenders can do—and where they can do it. With a decentralized, permissionless blockchain network, there is no single entity that has full control of the network, removing those limitations. Gluwa is putting that freedom to good use, connecting everyday investors with capital opportunities in emerging markets and needed a technology foundation to support it all. The company leverages Microsoft Azure to power its blockchain-related services. There are two distinct sets of services offered on Azure, each designed to meet specific requirements: Gluwa and Creditcoin. The eponymously named Gluwa decentralized financial platform connects investors with financial institutions that lend to people who don’t have access to traditional funding options. Because financial institutions record loans on the Creditcoin network, investors are better informed and can gain valuable insights due to the natural transparency and immutability of the blockchain-based credit bureau.”\
So far, I’ve seen that where Gluwa’s financial door there’s the World Wide Web window. I wonder if there’s more?\
Here’s the thing about money and financial inclusion. As soon as you get these resolved, you start thinking about better healthcare and education.\
“While in Jigawa, Senator Shettima had earlier launched the 4th edition of the Expanded National Micro, Small and Medium Enterprises (MSMEs) Clinic, disclosing that President Bola Ahmed Tinubu has ordered that a grant of N150,000 should be given to each of the business owners in the state as the federal government’s support to MSMEs across the country. He noted that the N150,000 grant is an outright grant that does not require beneficiaries to repay. Meanwhile, the AI Expertise Blockchain and Technology Training and Outsourcing Initiative, a partnership with tech company Gluwa, aims to train 1,000 Nigerians annually in artificial intelligence, blockchain, and other cutting-edge technologies.”\
Health is wealth, and blockchain technology seems to check all the right healthcare boxes in Nigeria:secure and efficient data management by providing a secure and decentralized platform for storing and sharing patient datarevolutionize clinical trials by providing a more transparent and secure system for tracking patient dataensuring compliance with the healthcare regulatory requirementscreating a secure and transparent system for tracking the movement of drugs throughout the supply chainTelemedicine data, such as video consultations and electronic prescriptions, can be safely stored and shared via blockchainensuring patient access to careimprove health insurance by providing a secure and transparent platform for managing claimsreduce costs and enhance patient privacy\
Let’s see what blockchain can do about it.\
“Agriculture being one of the backbones of the Nigerian economy, with 35% of the population in its employ, must be modernised for the government and the organised private sector to realise its full potential and attendant benefits. “According to the five-year Lagos State Agricultural and Food Systems road map, where the sector is projected to generate upwards of $10 billion by the year 2025, it will be of immense value for agricultural assets to be identified, recorded, and traded using innovative blockchain technology”, he explained. At a recent visit of the Gluwa team to His Majesty, Oba Gbolahan Lawal, the Oniru of Iruland and former Commissioner of Agriculture in Lagos State, the team detailed their plans to build a gateway for local farmers to access the international markets via borderless financial services ranging from decentralised finance loans to selling on global commodity futures markets.”\
I’m no financial expert, and especially not a crypto one, but I feel good and confident about the following statement: new money won’t be made in the old world because you can’t develop what’s already been developed, but by including and developing the struggling and unbanked countries. That’s about more than one-half of our brave new world today, and that ain’t right. So, blockchain has to…]]></content:encoded></item><item><title>Educational Byte: Banned, Partially Banned, or Legal—How Crypto Laws Differ Worldwide</title><link>https://hackernoon.com/educational-byte-banned-partially-banned-or-legalhow-crypto-laws-differ-worldwide?source=rss</link><author>Obyte</author><category>tech</category><pubDate>Mon, 24 Mar 2025 09:04:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
In the early days of cryptocurrencies, Bitcoin was often viewed as an obscure coin designed for criminals. The darknet marketplace Silk Road brought it a lot of popularity, but also a lot of bad rep. Its decentralized nature (not issued or controlled by any government or company) was constantly misunderstood, but that wouldn’t last for much longer. Soon, other coins were created, more use cases came along, coin prices skyrocketed, millions of users joined —and regulators found themselves needing to include this asset in the laws of their countries, somehow.\
That “somehow” isn’t always good. Sometimes, they create laws to ban or limit this type of money significantly. Many times, they make it legal, but what that means, in practice, varies from territory to territory. “legal (where all activities are permitted), partial ban (where one or more activity is not permitted), and general ban (where all activity is limited).”\
The practical side of those concepts is more complicated, though.They say that all activities with crypto are permitted in countries in which it’s legal, but stop there, because terms and conditions apply. Just because crypto is legal doesn’t mean people can use it as they like without any requirements. For example, Anti-Money Laundering (AML) and Countering the Financing of Terrorism (CFT) rules require related companies (like crypto exchanges) to verify identities and report suspicious transactions. This is to prevent illegal activities like money laundering or funding criminal organizations. So, while you can buy, sell, or trade crypto, you might need to provide personal information to comply with these laws.Businesses dealing with crypto often face even more rules. They might need special licenses to operate legally. For instance, in the European Union, the  (MiCA) regulation sets strict guidelines for crypto companies, including how they should protect customer funds and report their activities. Stablecoins without proper reserves are banned, and issuers of new coins have some strict requirements to comply with.\
Additionally, there’s another magical word: taxes. Depending on the country, crypto transactions , just like any other financial activity. This means users need to keep records and report their earnings to tax authorities, depending on certain established limits.\
Even in countries where crypto is partially banned, taxes could apply to individual users. The “partially banned” bit often refers to how financial companies are banned from handling these assets in that region. While individual users are free to transact with crypto, licenses aren’t granted for crypto companies there, and banks can’t provide services to crypto exchanges, for example. In this case, consumer protection and AML rules for crypto are usually nonexistent.In countries where crypto is “fully banned”, most activities with it are forbidden. Its mere use can be punished by law, although possession alone is often not illegal. People have been arrested for crypto-related activities in places like , , , , and , where they have “full bans” on cryptocurrencies. Now, does that fully stop people from owning, trading, mining, or doing whatever with cryptos, even if they live there? Not really.\
We just need to check the  by Chainalysis to notice an interesting fact: exactly 50% of the countries in the top ten by global crypto adoption have partial or full bans on cryptocurrency. China, famous for its wide ban on crypto, is in the top 20 by adoption. Despite laws and warnings, people are still using cryptocurrency in these places. Just without all regulations designed to make this space safer, so banning crypto instead of legalizing it seems counterproductive.\
  \n Decentralized cryptocurrencies were built to be censorship-resistant, and, as we’ve mentioned above, they’re not issued or controlled by a central entity. That’s why they can’t get effectively banned by anyone. There’s no company to blame or expel, but a wide network of nodes worldwide that authorities just can’t shut down all at the same time. However, while the governments are unable to shut down cryptocurrency networks, Decentralization is FreedomDecentralization is a very positive feature, because, even in countries where crypto is banned, people might still use it for any reason –legitimate, but also illegitimate. Of course, the country might consider it illegitimate according to its own laws, but its government's credibility and moral factors are different things. Some people rely on these assets to protect their savings from high inflation or unstable local currencies. Others use it to send money to family abroad or to protect from  by oppressive governments. In places with strict financial controls, crypto can also offer financial freedom and access to global markets.\
If you want to do any of those things and many more, the  could be for you. This is a fully decentralized and censorship-resistant crypto network, available for anyone, in any part of the world. It uses a Directed Acyclic Graph () structure, and transactions are confirmed without middlemen like miners or “validators”. This high level of decentralization makes Obyte more resilient to censorship and restrictions. Plus, it’s fully legal in most countries, offering a secure and accessible way to use crypto.:::info
Featured Vector Image by sentavio / ]]></content:encoded></item><item><title>AI chip startup FuriosaAI reportedly turns down $800M acquisition offer from Meta</title><link>https://techcrunch.com/2025/03/24/ai-chip-startup-furiosaai-reportedly-turns-down-800m-acquisition-offer-from-meta/</link><author>Kate Park</author><category>tech</category><pubDate>Mon, 24 Mar 2025 09:00:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[FuriosaAI, a South Korean startup that makes chips for AI applications, has rejected an $810 million acquisition offer from Meta.]]></content:encoded></item><item><title>Building a Robust JS/TS Monorepo: Best Practices with Yarn, NX and Changesets</title><link>https://hackernoon.com/building-a-robust-jsts-monorepo-best-practices-with-yarn-nx-and-changesets?source=rss</link><author>Teimur Gasanov</author><category>tech</category><pubDate>Mon, 24 Mar 2025 08:57:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Nowadays, the rapid evolution of software development cannot be denied. The teams are growing, projects tend to be more complex. Companies spend significant resources to maintain a distributed codebase consisting of many fragments. Enter the monorepo - a single, unified repository that brings together all of your code. Far from being a trend, monorepos have recently become an architectural approach to house the entire codebase in one place. Teams obtain enhanced context sharing, smooth collaboration and a tool that naturally encourages code reuse.Setting Up Yarn Workspaces:::info
Note: Throughout this article, whenever "Yarn" is mentioned, it specifically refers to Yarn v4—the latest version offering enhanced capabilities and improved performance.What are Yarn Workspaces?Workspaces are the packages of the monorepo, often called packages. They help you manage multiple packages in a single repository effortlessly. With workspaces. you can:Share Dependencies Easily:Share common dependencies across your project seamlessly.Simplify Dependency Management:Yarn automatically links local packages, reducing duplication and easing development.Accelerate Installations:While Yarn is the selected manager for this article thanks to its simplicity, speed, and extensive configuration options - it’s important to note that the right choice depends on your project’s specific needs, team preferences, and overall workflow. For instance, PNPM and Turborepo are other modern tools that offer a wide range of features.Yarn setup is a straightforward process. Follow the official guide to install and configure Yarn in your project: Yarn Installation Guide.Once you’ve completed the installation, let’s move on to configuration. Since we’re using plug’n’play, you need to ensure that your IDE correctly recognizes dependencies. If you’re using VSCode, run:# Typescript is required for VSCode SDK to set up correctly
yarn add -D typescript@^5
yarn dlx @yarnpkg/sdks vscode
:::info
If you’re using another code editor, check for the available SDKs here: Yarn Editor SDKs.At this point, you’re all set to start using Yarn.Organizing the Monorepo StructureNow that the package manager is configured, it’s time to design a scalable project organization. A clear, well-defined structure not only makes the repository easier to navigate but also promotes better code reuse. In this example, we’ll divide the codebase into three major categories:Client: Contains the final, deployable client products.Server: Contains the final, deployable server products.Client: For standalone UI widgets.Server: For standalone backend business logic pieces.Houses shared code such as design system components, constants, assets, and utilities. This is the context-free zone for storing reusable logic.To demonstrate the power of this folder structure, let’s start by adding these major folders to Yarn’s workspaces list. In your root package.json, add the following:"workspaces": [
  "apps/**",
  "features/**",
  "libs/**"
]
This configuration tells Yarn to treat packages in these folders as local packages. Subsequent installations will ensure that dependencies for each package are properly set up and linked.In this section, we'll walk through a minimal codebase example that illustrates how to bootstrap the monorepo. Instead of including full code snippets, I'll provide short examples with links to the complete files in the repository created specifically for this article.Bootstrapping Server ApplicationWe begin with a simple Express API for user authentication. This server application exposes a single endpoint () that utilizes a handler from another package.import express from "express";
import cors from "cors";
import { signInHandler } from "@robust-monorepo-yarn-nx-changesets/sign-in-handler";

const app = express();
const port = process.env.PORT || 1234;

app.use(express.json());

app.use(
  cors({
    origin: process.env.CORS_ORIGIN || "http://localhost:3000",
  })
);

app.post("/auth/signIn", signInHandler);

app.listen(port, () => {
  console.log(`Server is running at http://localhost:${port}`);
});
\
As you can see, the  endpoint uses a handler imported from another package. That brings us to our next component: the server feature.Bootstrapping Server FeatureThe server feature encapsulates the authentication logic. In this package, we define the sign-in handler, which leverages a shared validation utility from the libs.import type { RequestHandler } from "express";
import {
  passwordValidator,
  usernameValidator,
} from "@robust-monorepo-yarn-nx-changesets/validator";

const signInHandler: RequestHandler = (req, res) => {
  if (!req.body) {
    res.status(422).send("Request body is missing");
    return;
  }

  if (typeof req.body !== "object") {
    res.status(422).send("Request body expected to be an object");
    return;
  }

  const { username, password } = req.body;
  const usernameValidationResult = usernameValidator(username);
  if (typeof usernameValidationResult === "string") {
    res
      .status(422)
      .send("Invalid username format: " + usernameValidationResult);
    return;
  }

  const passwordValidationResult = passwordValidator(password);
  if (typeof passwordValidationResult === "string") {
    res
      .status(422)
      .send("Invalid password format: " + passwordValidationResult);
    return;
  }

  // Emulate a successful sign-in
  if (username === "test" && password === "test1234") {
    res.status(200).send("Sign in successful");
    return;
  }

  return res.status(422).send("Username or password is incorrect");
};

export default signInHandler;
\
This approach sums up the authentication logic within its own package, allowing it to be developed and maintained independently. Notice how the validator utilities are imported from the shared lib.Bootstrapping Client ApplicationNext, let’s look at the client side. In our client application, we build a simple website that enables user authentication by invoking the server API."use client";
import { SignInForm } from "@robust-monorepo-yarn-nx-changesets/sign-in-form";

const API_URL = process.env.NEXT_PUBLIC_API_URL || "http://localhost:1234";

export default function Home() {
  const handleSubmit = async (username: string, password: string) => {
    const response = await fetch(`${API_URL}/auth/signIn`, {
      method: "POST",
      body: JSON.stringify({ username, password }),
      headers: {
        "Content-Type": "application/json",
      },
    });
    if (response.status === 200) {
      alert("Sign in successful");
      return;
    }

    if (response.status === 422) {
      alert("Sign in failed: " + (await response.text()));
      return;
    }

    alert("Sign in failed");
  };

  return (
    <div className="w-full h-screen overflow-hidden flex items-center justify-center">
      <SignInForm onSubmit={handleSubmit} />
    </div>
  );
}
In this example, the  component is imported from a client feature package, which leads us to our final component.Bootstrapping Client FeatureThe client feature package provides the authentication form along with the shared validation logic. This avoids duplicating code and ensures consistency.import {
  passwordValidator,
  usernameValidator,
} from "@robust-monorepo-yarn-nx-changesets/validator";

interface SignInFormProps {
  onSubmit: (username: string, password: string) => void;
}

const SignInForm = ({ onSubmit }: SignInFormProps) => {
  const handleSubmit = (event: React.FormEvent<HTMLFormElement>) => {
    event.preventDefault();

    const username = (event.currentTarget[0] as HTMLInputElement).value;
    const usernameValidationResult = usernameValidator(username);
    if (typeof usernameValidationResult === "string") {
      alert(usernameValidationResult);
      return;
    }

    const password = (event.currentTarget[1] as HTMLInputElement).value;
    const passwordValidationResult = passwordValidator(password);
    if (typeof passwordValidationResult === "string") {
      alert(passwordValidationResult);
      return;
    }

    onSubmit(username!, password!);
  };

  return (
    <form onSubmit={handleSubmit}>
      <input type="text" placeholder="Username" />
      <input type="password" placeholder="Password" />
      <button type="submit">Submit</button>
    </form>
  );
};

export default SignInForm;
\
Here, we again see the usage of the validator from our shared libs, ensuring that validation logic is centralized and easily maintained.\
That’s it for our minimal codebase example. Keep in mind that this code is a simplified illustration meant to demonstrate the basic structure and interconnection between Apps, Features, and Libs in a monorepo. You can expand upon these examples as needed to fit your project’s specific requirements.Managing scripts in a monorepo can be challenging. While Yarn allows you to run scripts across multiple packages using various conditions, it may require custom scripting for more granular control. This is where NX comes in: it provides an out-of-the-box solution for efficient, targeted script execution.NX is a build system optimized for monorepos with advanced CI capabilities. With NX, you can:Run tasks efficiently in parallel: Leverage concurrency to speed up your builds.Identify dependency relationships: Understand connections among packages and scripts.Cache script execution results: Avoid redundant work by caching outputs.Targeted Script ExecutionTo harness NX’s capabilities, we first need to create an  file to define a set of rules for our scripts. Below is an example configuration:{
  "targetDefaults": {
    "build": {
      "dependsOn": [
        "^build"
      ],
      "outputs": [
        "{projectRoot}/dist"
      ],
      "cache": true
    },
    "typecheck": {
      "dependsOn": [
        "^build",
        "^typecheck"
      ]
    },
    "lint": {
      "dependsOn": [
        "^build",
        "^lint"
      ]
    }
  },
  "defaultBase": "main"
}
In plain English, this configuration means:The  script for a package depends on the successful build of its dependencies, and its output is cached.The  script for a package depends on both the build and typecheck scripts of its dependencies.The  script for a package depends on both the build and lint scripts of its dependencies.Now, let’s add scripts to the :  "scripts": {
    "build:all": "yarn nx run-many -t build",
    "build:affected": "yarn nx affected -t build --base=${BASE:-origin/main} --head=${HEAD:-HEAD}",
    "typecheck:all": "yarn nx run-many -t typecheck",
    "typecheck:affected": "yarn nx affected -t typecheck --base=${BASE:-origin/main} --head=${HEAD:-HEAD}",
    "lint:all": "yarn nx run-many -t lint",
    "lint:affected": "yarn nx affected -t lint --base=${BASE:-origin/main} --head=${HEAD:-HEAD}",
    "quality:all": "yarn nx run-many --targets=typecheck,lint",
    "quality:affected": "yarn nx affected --targets=typecheck,lint --base=${BASE:-origin/main} --head=${HEAD:-HEAD}"
  }
Here, we define four types of execution scripts: Checks the package’s types. Runs both typecheck and lint.'Each script has two variations: Runs the script on all packages. Runs the script only on packages affected by recent changes. The  and  environment variables allow you to specify a range (defaulting to  and the current ), enabling granular execution on pull requests. This can significantly save time and resources.Managing Circular DependenciesNX also provides a built-in command to generate a dependency graph, which can help in dependency cycles detection. The following script uses the NX graph output to check for circular dependencies and fails if any are found.Create a file at scripts/check-circulardeps.mjs with the following content:import { execSync } from "child_process";
import path from "path";
import fs from "fs";

const hasCycle = (node, graph, visited, stack, path) => {
  if (!visited.has(node)) {
    visited.add(node);
    stack.add(node);
    path.push(node);

    const dependencies = graph.dependencies[node] || [];
    for (const dep of dependencies) {
      const depNode = dep.target;
      if (
        !visited.has(depNode) &&
        hasCycle(depNode, graph, visited, stack, path)
      ) {
        return true;
      }

      if (stack.has(depNode)) {
        path.push(depNode);
        return true;
      }
    }
  }

  stack.delete(node);
  path.pop();
  return false;
};

const getGraph = () => {
  const cwd = process.cwd();
  const tempOutputFilePath = path.join(cwd, "nx-graph.json");
  execSync(`nx graph --file=${tempOutputFilePath}`, {
    encoding: "utf-8",
  });
  const output = fs.readFileSync(tempOutputFilePath, "utf-8");
  fs.rmSync(tempOutputFilePath);
  return JSON.parse(output).graph;
};

const checkCircularDeps = () => {
  const graph = getGraph();

  const visited = new Set();
  const stack = new Set();

  for (const node of Object.keys(graph.dependencies)) {
    const path = [];
    if (hasCycle(node, graph, visited, stack, path)) {
      console.error("🔴 Circular dependency detected:", path.join(" → "));
      process.exit(1);
    }
  }
  console.log("✅ No circular dependencies detected.");
};

checkCircularDeps();
Executes the NX command to generate a dependency graph.Reads the graph from a temporary JSON file.Recursively checks for cycles.Logs an error and exits if a circular dependency is detected.Validating Dependencies with Yarn ConstraintsAs projects grow, maintaining consistency across dependencies becomes challenging. Enforcing strict rules around dependencies, Node versions, and other configurations is essential to avoid unnecessary technical debt. Yarn Constraints offer a way to automate these validations.Understanding Yarn ConstraintsYarn Constraints are the set of rules for packages in your monorepo. Significant advantage of using them is that you are the manager of these rules. For example, you can create a rule to force all the packages to use the same React version. Once it’s set, you’ll never run into a problem when a host application cannot use a feature/lib with a higher React version.While migrating a large monorepo to a new major version of a dependency might be complex, using constraints ultimately brings consistency and stability to the entire project.In our example repository, we use a yarn.config.cjs file to enforce consistency for:To allow for flexibility during transitions, you can define exclusions to temporarily bypass certain checks. For instance:const workspaceCheckExclusions = [];
const dependencyCheckExclusions = [];
These constants let you exclude specific workspaces or dependencies from the validation process, ensuring smooth migrations when necessary.Managing Versioning with ChangesetsAnother problem you may face with the growth of the repository is the version management and releasing. Changesets provide an elegant solution to automate this process, ensuring that every change is tracked, versioned and released.Introduction to ChangesetsChangesets is an open-source tool designed to manage versioning in monorepo repositories. It simplifies the process of keeping track of changes by allocating them into small, human-readable documents that capture the intent of the change. These documents are called changesets. Key benefits include:Each changeset outlines the changes made, which helps both developers and consumers understand what to expect in a new release.Each package is versioned independently, ensuring that only the affected packages are updated. This minimizes risk of empty version bumps and dependency breaks.As every change is recorded through a changeset, teams can review and approve updates before the actual release.One of the most powerful features of Changesets is the ability to automate the process. You can integrate Changesets into your CI/CD pipeline and forget about manual version changes and NPM publishing.Take a look at the release.yaml workflow in the example repository. It has create-release-pull-request-or-publish step. The step backed by changesets/action GitHub action creates all the magic. You only need to set up  for publishing your packages. Then, every push to the  branch will:Check if there are any Changeset documents.If changeset documents are present, the action creates a pull request with the necessary version bumps and changelog updates. If no changes are detected, nothing happens.Check if there are any packages ready to publish.If packages are ready to be released, the action publishes the new versions to NPM using the provided . If there are no packages ready to publish, the action exits without making changes.By automating these tasks, Changesets ensure that your releases are consistent and reliable, reducing the potential for human error and streamlining your development workflow.Workflow Integration with GitHub ActionsThis section delves into how to unleash the power of the architecture we’ve just built. Using GitHub Actions, we’ll automate PR quality checks, version releases for libraries and features, and application deployments. The focus is on maximizing automation while maintaining code quality and job granularity.To ensure that pull request code remains consistent and stable, we create a dedicated quality.yaml workflow. This workflow performs several tasks, such as ensuring that manual version changes aren’t introduced (since versioning is managed by Changesets):      - id: check_version
        name: Check version changes
        run: |
          BASE_BRANCH=${{ github.event.pull_request.base.ref }}
          git fetch origin $BASE_BRANCH

          CHANGED_FILES=$(git diff --name-only origin/$BASE_BRANCH HEAD)
          VERSION_CHANGED=false
          for FILE in $CHANGED_FILES; do
            if [[ $FILE == */package.json ]]; then
              if [ -f "$FILE" ]; then
                HEAD_VERSION=$(grep '"version":' "$FILE" | awk -F '"' '{print $4}')
              else
                continue
              fi

              HEAD_VERSION=$(cat $FILE | grep '"version":' | awk -F '"' '{print $4}')

              if git cat-file -e origin/$BASE_BRANCH:$FILE 2>/dev/null; then
                BASE_VERSION=$(git show origin/$BASE_BRANCH:$FILE | grep '"version":' | awk -F '"' '{print $4}')
              else
                BASE_VERSION=$HEAD_VERSION
              fi

              if [ "$BASE_VERSION" != "$HEAD_VERSION" ]; then
                VERSION_CHANGED=true
                echo "Version change detected in $FILE"
              fi
            fi
          done

          if [ "$VERSION_CHANGED" = true ]; then
            echo "Manual version changes are prohibited. Use changesets instead."
            exit 1
          fi
        env:
          GITHUB_REF: ${{ github.ref }}
Alongside this check, the  job installs dependencies, validates constraints, checks for circular dependencies and verifies overall code quality using the script we defined earlier with NX:      - id: install-dependencies
        name: Install dependencies
        run: yarn --immutable

      - id: check-constraints
        name: Check constraints
        run: yarn constraints

      - id: check-circulardeps
        name: Check circular dependencies
        run: yarn check-circulardeps:all

      - id: check-quality
        name: Check quality
        run: BASE=origin/${{ github.event.pull_request.base.ref }} yarn quality:affected
The quality check is designed to run only on the packages affected by the current pull request. Successful completion of these jobs signals the pull request is ready to merge (in addition to receiving code reviews).If additional checks are required for your project, you can update your  and quality script keeping the workflow unchanged.Publish Libraries and FeaturesAfter a PR is merged, the release workflow (as described in the Changesets chapter) is triggered. This workflow builds the affected packages and creates a PR with the version bumps. Once this PR is approved and merged, release.yaml runs again - this time, instead of creating a PR, it detects version changes and releases the updated packages to NPM:      - id: build-packages
        name: Build packages
        run: yarn build:affected

      - id: create-release-pull-request-or-publish
        name: Create Release Pull Request or Publish to NPM
        uses: changesets/action@v1
        with:
          version: yarn changeset version
          publish: yarn release
          commit: "chore: publish new release"
          title: "chore: publish new release"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}

  release-apps:
    needs: release-libs-features
    uses: ./.github/workflows/release-apps.yaml
    with:
      publishedPackages: ${{ needs.release-libs-features.outputs.publishedPackages }}
Following this, a job called  is executed, which is responsible for application deployments. It receives a list of published packages from the previous step and brings us to the next chapter.The final part of the release process involves deploying your applications (applications are not published to NPM, because they are set  in ). The release-apps.yaml workflow is automatically triggered by release.yaml, or can be executed directly from the Actions tab on GitHub:name: Release Apps

on:
  workflow_call:
    inputs:
      publishedPackages:
        description: "List of published packages"
        required: false
        type: string
        default: "[]"

  workflow_dispatch:
    inputs:
      publishedPackages:
        description: "List of published packages (optional)"
        required: false
        type: string
        default: "[]"
This workflow accepts  input to determine which packages have been published. Using a matrix strategy, it checks each application of the matrix for the presence of published dependencies:      - id: check-dependency-published
        name: Check if any app dependency is published
        run: |
          PUBLISHED_PACKAGES="${{ inputs.publishedPackages }}"
          PACKAGE_NAME="${{ matrix.package }}"
          APP="${{ matrix.app }}"

          DEPENDENCIES=$(jq -r '.dependencies // {} | keys[]' "apps/$APP/package.json")

          for DEP in $DEPENDENCIES; do
            if echo "$PUBLISHED_PACKAGES" | grep -w "$DEP"; then
              echo "published=true" >> $GITHUB_OUTPUT
              exit 0
            fi
          done

          echo "published=false" >> $GITHUB_OUTPUT
This check is one condition for initiating an app deployment. The other condition ensures that the app’s version has been changed (indicating that a redeploy is necessary even if no dependencies have been updated):      - id: check-version-change
        name: Check if app version has changed
        run: |
          APP="${{ matrix.app }}"
          PACKAGE_JSON_PATH="apps/$APP/package.json"
          CURRENT_VERSION=$(jq -r '.version' "$PACKAGE_JSON_PATH")
          PREVIOUS_VERSION=$(git show HEAD~1:"$PACKAGE_JSON_PATH" | jq -r '.version' || echo "")

          if [[ "$CURRENT_VERSION" == "$PREVIOUS_VERSION" ]]; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "changed=true" >> $GITHUB_OUTPUT
          fi
Finally, after confirming that the app either has updated dependencies or its version has changed, the workflow retrieves the new version and proceeds to build and deploy the application:      - id: set-up-docker
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - id: get-app-version
        name: Get the app version from package.json
        run: echo "app-version=$(cat ./apps/${{ matrix.app }}/package.json | jq -r '.version')" >> $GITHUB_OUTPUT

      - id: build-image
        name: Build image
        if: steps.check-dependency-published.outputs.published == 'true' || steps.check-version-change.outputs.changed == 'true'
        uses: docker/build-push-action@v4
        with:
          build-contexts: |
            workspace=./
          context: "./apps/${{ matrix.app }}"
          load: true
          push: false
          tags: |
            ${{ matrix.app }}:v${{ steps.get-app-version.outputs.app-version }}
In this example, we build the Docker image without pushing it to a registry. In your production workflow, replace this step with the actual deployment process.Throughout this article, we explored setup of a robust monorepo and the tools that help manage it efficiently. By centralizing your codebase, you not only simplify dependency management but also streamlines collaboration across teams. We demonstrated how Yarn can be leveraged to share dependencies, accelerate installation with PnP and improve overall project consistency. Additionally, integrating NX for targeted script execution ensures that CI is fast and efficient. Changesets helped to automate versioning, reducing manual errors and streamlining releases. Finally, we’ve made a production-ready CI/CD pipeline with GitHub actions that performs only the necessary tasks.: Begin by setting up a small-scale monorepo to test these best practices. Experiment with different folder structures, and gradually expand to include more packages as your confidence grows.Integrate Additional Tools: Consider integrating complementary tools like PNPM or Turborepo based on your project’s unique requirements and team preferences.: Fine-tune your GitHub Actions workflows to include additional quality checks, code coverage, and security scans tailored to your project.: Stay updated with the latest releases of Yarn, NX, and Changesets. Engage with the community to share insights and learn about emerging trends in monorepo management.Access the complete example repository created for this guide. Explore the project structure, code samples, and scripts that showcase the monorepo setup in action.Check out the actual NPM package published as part of this project. These packages demonstrate real-world usage and implementation of the concepts discussed in the article.]]></content:encoded></item><item><title>C++ Metaprogramming: Compilation of Calculations, from Basic Techniques to Advanced Methods</title><link>https://hackernoon.com/c-metaprogramming-compilation-of-calculations-from-basic-techniques-to-advanced-methods?source=rss</link><author>Vladislav Ag</author><category>tech</category><pubDate>Mon, 24 Mar 2025 08:51:42 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Compile-time calculations in C++ allow some operations to be performed during compilation instead of at runtime. This provides a number of advantages: validating data and logic before running the program, reducing overall execution time, and enhancing code safety and reliability. At the same time, of course, there is an additional cost: compilation time could increase significantly, and the code itself may become more difficult to read.In this article we will explore:Compilation mechanisms for computations available in C++ (templates and )Examples of non-trivial use of template metaprogrammingModern capabilities of  (including objects, loops, and algorithms)Useful features and techniques, which make developer’s life easier.Potential pitfalls and tips on how to avoid them.Historically in C++ metaprogramming emerged thanks to templates. They originally were intended for generating generic functions and classes, but over time it was discovered, that recursive templates could be used to preform quite complex computations.Example 1: Factorial using template recursion#include <cstdio>

template <int N>
struct Factorial {
    static constexpr int value = N * Factorial<N - 1>::value;
};

template <>
struct Factorial<0> {
    static constexpr int value = 1;
};

int main() {
  printf("%d\n", Factorial<5>::value);
  return 0;
}
Here, the computation of  takes place at compile time. However, this approach is difficult to read, requires specializations and overloads the compiler.Example 2: Fibonacci Numbers#include <cstdio>

template <int N>
struct Fibonacci {
    static constexpr int value = Fibonacci<N - 1>::value + Fibonacci<N - 2>::value;
};

template <>
struct Fibonacci<0> {
    static constexpr int value = 0;
};

template <>
struct Fibonacci<1> {
    static constexpr int value = 1;
};

int main() {
  printf("%d\n", Fibonacci<10>::value);
  return 0;
}
\
Recursive templates are good for demonstrating ideas, but in real enterprise code they quickly become cumbersome.Drawbacks and limitationsDebugging complexity: when errors occur in metacode, compiler messages can be extremely confusing.Increased compilation time: recursive calculations generate numerous instances of templates.Template limitations: difficulties with condition constructs and local variables.Nevertheless, templates metaprogramming is still successfully used today, including in the modern libraries, though more often for working with types than for purely mathematical operations.Modern approach: With the introduction of the  specifier in C++11, template hacks began to take a back seat. The keyword  informs the compiler that a function or an object can (and should, if possible) be evaluated at compile time. functions: They’re regular functions that can be evaluated at compile time if all arguments are known at the compile stage. variables: Variables that are initialized by the constant expression.Limitations:  in  functions, operations that require runtime evaluation - such as a memory allocation with  or calling virtual functions - cannot be used.Example 3: Factorial using #include <cstdio>

constexpr int factorial(int n) {
    return n <= 1 ? 1 : n * factorial(n - 1);
}

int main() {
  printf("%d\n", factorial(5));
  return 0;
}
\
This core is simpler than the template-based version and easier to read. Moreover, if  is called with a constant, the result will be computed at compile time.Advanced aspects of Conditional operators (if, switch) inside  functions have been available since C++14, which simplifies writing complex logic.Loops (for, while) have also been allowed inside  since C++14, which makes it easier to create initializer lists, arrays and lookup tables. classes: you can declare class constructors and methods as , which allows creating objects at compile time and invoke their methods.Example 4:  and loops#include <cstdio>

constexpr int sum_to_n(int n) {
    int sum = 0;
    for (int i = 1; i <= n; ++i) {
        sum += i;
    }
    return sum;
}

int main() {
  printf("%d\n", sum_to_n(15));
  return 0;
}
Example 5:   and classes#include <cstdio>
#include <cmath>

struct Point {
    int x, y;

    constexpr Point(int x, int y) : x(x), y(y) {}
    constexpr float len() const { return sqrt(x * x + y * y); }
};

constexpr Point p(3, 4);

int main() {
  printf("%f\n", p.len());
  return 0;
}
 : allows performing checks at compile-time  static_assert(sizeof(void*) == 8, "64-bit platform expected");
  static_assert(sum_to_n(15) == 120, "Something went wrong");
 (C++17): simplified form of conditional operators in templates  #include <iostream>

  template <typename T>
  void print_type_info(const T& val) {
      if constexpr (std::is_integral_v<T>) {
          std::cout << "Integer: " << val << std::endl;
      } else {
          std::cout << "Not an integer" << std::endl;
      }
  }

  int main() {
    print_type_info(15);
    return 0;
  }
Here, the brunch that doesn’t satisfy the condition won’t be compiled at all if the check occurs at compile time.Compile-time array and table generation: convenient for precomputing values  constexpr int squares[] = {
      1*1, 2*2, 3*3, 4*4, 5*5
  };
Creating complex structures: you can declare an entire object as  if all of its initialization methods are also .Although  often simplifies the task, there are situations in which templates remain necessary:Need to determine type characteristics, for example determine if a type is an array, a pointer, etc.Need to store a compile-time list of types (using  or custom structures)Often it is more efficient and clear to perform all calculations using , and to use templates only where metaprogramming on types is truly needed. This approach strikes a balance between flexibility and ease of debugging.Compilation Time: Extensive use of metaprogramming and  can significantly increase build times.Expression Limitations: Not all operations are allowed in  functions requiring full runtime support are prohibited.Potential Hidden Copies: When using complex objects in  functions, unexpected copies may occur if methods aren’t declared as  or aren't properly optimized.Debugging Complexity: If an error occurs inside a  function, it’s not always easy to pinpoint the exact location.Modern C++ offers developers a wide range of tools for organizing compile-time computations - from classic template-based metaprogramming to the more intuitive and flexible . When used wisely, these tools can significantly improve code performance and safety by catching entire classes of errors early.At the same time, it's important to weigh the trade-offs: will your code turn into an unreadable monolith, or will compilation times become unreasonably long? Within reasonable limits, metaprogramming and  can make your project more efficient and reliable, offering tangible benefits when developing large-scale systems.Try It Yourself on GitHubIf you want to explore these examples hands-on, feel free to visit my GitHub repository where you’ll find all the source files for the code in this article. You can clone the repository, open the code in your favorite IDE or build system, and experiment with  to see how the compiler works with it. Enjoy playing around with the examples!]]></content:encoded></item><item><title>Ex-Network International execs raise $6.75M for Enza, an African fintech serving banks</title><link>https://techcrunch.com/2025/03/24/ex-network-execs-raise-6m-for-enza/</link><author>Tage Kene-Okafor</author><category>tech</category><pubDate>Mon, 24 Mar 2025 08:05:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Over the past decade, Dubai-based Network International has become one of the dominant payment processors across the Middle East and Africa, thanks in part to a pair of acquisitions. However, many large incumbents can fall prey to slower innovation, opening the door for smaller, faster-moving startups. The latest development is Enza, a fintech founded in […]]]></content:encoded></item><item><title>Another Large Black Hole In &apos;Our&apos; Galaxy</title><link>https://science.slashdot.org/story/25/03/23/2227207/another-large-black-hole-in-our-galaxy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Mar 2025 07:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[RockDoctor (Slashdot reader #15,477) writes:

A recent paper on ArXiv reports a novel idea about the central regions of "our" galaxy.


 Remember the hoopla a few years ago about radio-astronomical observations producing an "image" of our central black hole — or rather, an image of the accretion disc around the black hole — long designated by astronomers as "Sagittarius A*" (or SGR-A*)? If you remember the image published then, one thing should be striking — it's not very symmetrical. If you think about viewing a spinning object, then you'd expect to see something with a "mirror" symmetry plane where we would see the rotation axis (if someone had marked it). If anything, that published image has three bright spots on a fainter ring. And the spots are not even approximately the same brightness.

 This paper suggests that the image we see is the result of the light (radio waves) from SGR-A* being "lensed" by another black hole, near (but not quite on) the line of sight between SGR-A* and us. By various modelling approaches, they then refine this idea to a "best-fit" of a black hole with mass around 1000 times the Sun, orbiting between the distance of the closest-observed star to SGR-A* ("S2" — most imaginative name, ever!), and around 10 times that distance. That's far enough to make a strong interaction with "S2" unlikely within the lifetime of S2 before it's accretion onto SGR-A*.)

 The region around SGR-A* is crowded. Within 25 parsecs (~80 light years, the distance to Regulus [in the constellation Leo] or Merak [in the Great Bear]) there is around 4 times more mass in several millions of "normal" stars than in the SGR-A* black hole. Finding a large (not "super massive") black hole in such a concentration of matter shouldn't surprise anyone.

 This proposed black hole is larger than anything which has been detected by gravitational waves (yet) ; but not immensely larger — only a factor of 15 or so. (The authors also anticipate the "what about these big black holes spiralling together?" question : quote "and the amplitude of gravitational waves generated by the binary black holes is negligible.")

 Being so close to SGR-A*, the proposed black hole is likely to be moving rapidly across our line of sight. At the distance of "S2" it's orbital period would be around 26 years (but the "new" black hole is probably further out than than that). Which might be an explanation for some of the variability and "flickering" reported for SGR-A* ever since it's discovery.
 As always, more observations are needed. Which, for SGR-A* are frequently being taken, so improving (or ruling out) this explanation should happen fairly quickly. But it's a very interesting, and fun, idea.]]></content:encoded></item><item><title>DNA testing company 23andMe files for bankruptcy protection, CEO resigns</title><link>https://techcrunch.com/2025/03/23/dna-testing-company-23andme-files-for-bankruptcy-protection-ceo-resigns/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Mon, 24 Mar 2025 06:51:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Genetics testing company 23andMe has filed for Chapter 11 bankruptcy protection in the U.S. to initiate the sale of its assets. Alongside the announcement, the company’s co-founder and CEO, Anne Wojcicki, separately said she is leaving to become an independent bidder for 23andMe. “After a thorough evaluation of strategic alternatives, we have determined that a […]]]></content:encoded></item><item><title>The TechBeat: The Man Behind ERC20 Wants to Make Web3 Human with LUKSO (3/24/2025)</title><link>https://hackernoon.com/3-24-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Mon, 24 Mar 2025 06:11:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @hackernooncontests [ 4 Min read ] 
 1 month left to enter Round 1 of the Spacecoin Writing Contest! Write about #decentralized-internet, #spacetech, #blockchain-use-case to compete for 15000 USDT! Read More.By @vitaliikuzmenko [ 3 Min read ] 
 Meet Leadige LLC, Startups of the Year 2024 Nominee—Where Strategy, Creativity & Data Drive Revenue. Read More.By @dmytrospilka [ 4 Min read ] 
 While the wider cryptocurrency market is having to contend with high volatility and widespread uncertainty, XRP appears to be thriving. Read More.By @kitcast [ 7 Min read ] 
 Discover how digital signage works, what to display, and how to start. Learn about key tech, content strategies, and why Apple TV makes it easier than ever. Read More.By @kristinazima [ 8 Min read ] 
 Lessons from a failed website builder startup fro UI/UX designers and developers Read More.By @michealchukwube [ 6 Min read ] 
 AIO is the strategic use of AI technologies to enhance search engine visibility and user engagement, complementing traditional SEO strategies. Read More.By @kristinazima [ 15 Min read ] 
 Top UX manipulations (principles) to seduce your users. Easy steps to improve any interface. Read More.By @ct2034 [ 15 Min read ] 
 A recent survey by the Linux Foundation found that organizations contribute 7.7B USD annually to open source projects. Read More.By @makowskid [ 4 Min read ] 
 Over time, Kaizen has helped improve processes, boost morale, and even tackle technical debt bit by bit. It's a simple but effective way to ensure continuous i Read More.By @precedent [ 8 Min read ] 
 Can ChatGPT-4 predict the future? This study explores how storytelling prompts improve its forecasting accuracy for economic trends and major cultural events. Read More.By @sshshln [ 12 Min read ] 
 Prediction markets have long been seen as a disruptive force in scientific forecasting and decision-making.  Read More.By @edwinliavaa [ 4 Min read ] 
 In a recent interview with Larry Kudlow, Elon Musk shared a moment of levity amid serious discussion about government inefficiency, cyber attacks, and more. Read More.By @zbruceli [ 11 Min read ] 
 Part II of the series: use MCP and Solana AgentKit to build an AI Agent that can trade USD and EUR stablecoins. Read More.By @ishanpandey [ 2 Min read ] 
 Funtico and FunNFT launch Mystery Tournaments on March 14, 2025. The tournaments feature three games, offering a prize pool of 1,500 USDT and 75,000 $TICO. Read More.By @jay9thakur [ 8 Min read ] 
 The Model Context Protocol has emerged as the universal translator for artificial intelligence, and it's redefining what integrated AI systems can achieve. Read More.By @kfamyn [ 19 Min read ] 
 Learn everything about Swift initializers — convenience override, saving memberwise initializer, required init() use cases, parameterless UIView() and more! Read More.By @ishanpandey [ 11 Min read ] 
 Fabian Vogelsteller, blockchain pioneer behind ERC20, shares insights on LUKSO, decentralized identity, and Web3's social future.  Read More.By @InfiniteScroll [ 6 Min read ] 
 Boots will hit Martian soil soon, but a functional colony? Not likely. Strap in for a skeptical take on Musk’s audacious dream to make humanity multiplanetary. Read More.]]></content:encoded></item><item><title>Achieving Fair AI Without Sacrificing Accuracy</title><link>https://hackernoon.com/achieving-fair-ai-without-sacrificing-accuracy?source=rss</link><author>Demographic</author><category>tech</category><pubDate>Mon, 24 Mar 2025 05:54:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[6.2 Inductive Biases of Models trained in DP-based Fair Learning\
 We tested Algorithm 1 utilizing a sensitive attribute-based distributional robust optimization (SA-DRO) to DP-based fair learning algorithms. In our experiments, we applied the SA-DRO algorithm to the DDP-based KDE fair learning algorithm proposed by [11], and RFI proposed by [13]. We kept the fairness regularization penalty coefficient to be λ = 0.9. The DRO regularization coefficient can take over the range [0, 1], in this table, we set ϵ = 0.9 for SA-DRO case. In these figures 4, we applied the SA-DRO algorithm to the DDP-based KDE fair learning algorithm by [11], and RFI by [13]. We kept the fairness regularization penalty coefficient to be λ = 0.9. The DRO regularization coefficient can take over the range [0, 1].\
As Table 1 shows, we observed that the proposed SA-DRO reduces the tendency of the fair learning algorithm toward the majority sensitive attribute, and the resulting negative prediction rates conditioned to sensitive attribute outcomes became closer to the midpoint between the majority and minority conditional accuracies. On the other hand, the SA-DRO-based algorithms still achieve a low DDP value while the accuracy drop is less than 1%. Moreover, we visualize the prediction shifting in Figure 4 by applying the SA-DRO algorithm to the DDP-based KDE fair learning algorithm by [11], and RFI by [13]. We kept the fairness regularization penalty coefficient to be λ = 0.9. The DRO regularization coefficient takes over the range [0, 1].6.3 DP-based Fair Classification in Heterogeneous Federated LearningTo numerically show the implications of the inductive biases of DP-based fair learning algorithms, we simulated a heterogeneous federated learning setting with multiple clients where the sensitive attribute has different distributions across clients. To do this, we split the Adult dataset into 4 subsets of 3k samples to be distributed among 4 clients in the federated learning. While 80% of the training data in Client 1 (minority subgroup in the network) had Female as sensitive attribute, only 20% of Clients 2-4 were female samples. We used the same male/female data proportion to assign 750 test samples to the clients.\
For the baseline federated learning method with no fairness regularization, we utilized the FedAvg algorithm [29]. For the DP-based fair federated learning algorithms, we attempted the DDP-based KDE and FACL algorithms which result in single-level optimization problem and hence can be optimized in a distributed learning problem by averaging as in FedAvg. We refer to the extended federated learning version of these algorithms as FedKDE and FedFACL. We also tested our SA-DRO implementations of FedKDE and FedFACL, as well as the localized ERM, KDE, FACL models where each client trained a separate model only on her own data.\
As our numerical results in Table 2 and Table 3 indicate, the inductive biases of DP-based federated learning could considerably lower the accuracy of Client 1 with a different majority sensitive attribute compared to the other clients. The accuracy drop led to a lower accuracy compared to Client 1’s locally fair trained model without any collaboration with the other clients, which may affect the client’s incentive to participate in the federated learning process. On the other hand, the SA-DRO implementations of the KDE and FACL methods achieved a better accuracy than Client 1’s local model while preserving the accuracy for the majority clients.(1) Haoyu LEI, Department of Computer Science and Engineering, The Chinese University of Hong Kong (hylei22@cse.cuhk.edu.hk);(2) Amin Gohari, Department of Information Engineering, The Chinese University of Hong Kong (agohari@ie.cuhk.edu.hk);(3) Farzan Farnia, Department of Computer Science and Engineering, The Chinese University of Hong Kong (farnia@cse.cuhk.edu.hk).]]></content:encoded></item><item><title>How to Test for AI Fairness</title><link>https://hackernoon.com/how-to-test-for-ai-fairness?source=rss</link><author>Demographic</author><category>tech</category><pubDate>Mon, 24 Mar 2025 05:54:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[. In our experiments, we attempted the following standard datasets in the machine learning litera\
1.  dataset with 12 features and a binary label on whether a subject has recidivism in two years, where the sensitive attribute is the binary race feature[1]. To simulate a setting with imbalanced sensitive attribute distribution, we considered 2500 training and 750 test samples, in both of which 80% are from Z = 0 "non-Caucasian" and 20% of the samples are from Z = 1 "Caucasian”.\
2. Adult dataset with 64 binary features and a binary label indicating whether a person has more than 50K annual income. In this case, gender is considered as the sensitive attribute[2]. In our experiments, we used 15k training and 5k test samples, where, to simulate an imbalanced distribution on the sensitive attribute, 80% of the data have male gender and 20% of the samples are females.\
3.  Proposed by [27], containing the pictures of celebrities with 40 attribute annotations, where we considered "gender" as a binary label, and the sensitive attribute is the binary variable on blond/non-blond hair. In the experiments, we used 5k training samples and 2k test samples. To simulate an imbalanced sensitive attribute distribution, 80% of both training and test samples are marked with Blond hair and 20% samples are marked with non-blond hair.\
DP-based Learning Methods: We performed the experiments using the following DP-based fair classification methods: 1) DDP-based KDE method [6] and FACL [12], 2) the mutual information-based fair classifier [11], 3) the maximal Correlation-based RFI classifier [13], to learn binary classification models on COMPAS and Adult datasets. For CelebA experiments, we used the following two DP-based fair classification methods: KDE method [6], and mutual information (MI) fair classifier [11].\
In the experiments, we attempted both a logistic regression classifier with a linear prediction model and a neural net classifier.\
The neural net architecture was 1) for the COMPAS case, a multi-layer perceptron (MLP) with 4 hidden layers with 64 neurons per layer, 2) for the Adult case, an MLP with 4 hidden layers with 512 neurons per layer, 3) for the CelebA case, the ResNet-18 [28] architecture suited for the image input in the experiments.(1) Haoyu LEI, Department of Computer Science and Engineering, The Chinese University of Hong Kong (hylei22@cse.cuhk.edu.hk);(2) Amin Gohari, Department of Information Engineering, The Chinese University of Hong Kong (agohari@ie.cuhk.edu.hk);(3) Farzan Farnia, Department of Computer Science and Engineering, The Chinese University of Hong Kong (farnia@cse.cuhk.edu.hk).]]></content:encoded></item><item><title>The Limits of Demographic Parity in AI Models</title><link>https://hackernoon.com/the-limits-of-demographic-parity-in-ai-models?source=rss</link><author>Demographic</author><category>tech</category><pubDate>Mon, 24 Mar 2025 05:54:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[4 Inductive Biases of DP-based Fair Supervised Learning\
. We defer the proof to the Appendix\
We remark the difference between the bias levels shown for the DDP case in Theorem 1 and the other dependence metrics in Theorem 2. The bias level for a DDP-based fair leaner could be considerably stronger than that of mutual information, ERMI, and maximal correlation-based fair learners, as the maximum the total variations in Theorem 1 is replaced by their expected value over P in Theorem 2.4.1 Extending the Theoretical Results to Randomized Prediction Rule\
\
. We defer the proof to the Appendix.5 A Distributionally Robust Optimization Approach to DP-based Fair Learning(1) Haoyu LEI, Department of Computer Science and Engineering, The Chinese University of Hong Kong (hylei22@cse.cuhk.edu.hk);(2) Amin Gohari, Department of Information Engineering, The Chinese University of Hong Kong (agohari@ie.cuhk.edu.hk);(3) Farzan Farnia, Department of Computer Science and Engineering, The Chinese University of Hong Kong (farnia@cse.cuhk.edu.hk).]]></content:encoded></item><item><title>How to Measure Fairness in AI Models</title><link>https://hackernoon.com/how-to-measure-fairness-in-ai-models?source=rss</link><author>Demographic</author><category>tech</category><pubDate>Mon, 24 Mar 2025 05:53:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.1 Fair Supervised LearningIn a fair supervised learning algorithm, the learned prediction rule is expected to meet a fairness criterion. Here, we review two standard fairness criteria in the literature:3.3 Dependence Measures for Fair Supervised LearningTo measure the DP-based fairness violation, the machine learning literature has proposed the application of several dependence measures which we analyze in the paper. In the following, we review some of the applied dependence metrics:(1) Haoyu LEI, Department of Computer Science and Engineering, The Chinese University of Hong Kong (hylei22@cse.cuhk.edu.hk);(2) Amin Gohari, Department of Information Engineering, The Chinese University of Hong Kong (agohari@ie.cuhk.edu.hk);(3) Farzan Farnia, Department of Computer Science and Engineering, The Chinese University of Hong Kong (farnia@cse.cuhk.edu.hk).]]></content:encoded></item><item><title>What to Do When ‘Fair’ AI Delivers Unfair Results</title><link>https://hackernoon.com/what-to-do-when-fair-ai-delivers-unfair-results?source=rss</link><author>Demographic</author><category>tech</category><pubDate>Mon, 24 Mar 2025 05:53:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model’s fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method improving robustness against the marginal distribution of the sensitive attribute. Finally, we present several numerical results on the application of DP-based learning methods to standard centralized and distributed learning problems. The empirical findings support our theoretical results on the inductive biases in DP-based fair learning algorithms and the debiasing effects of the proposed SA-DRO method.A responsible deployment of modern machine learning frameworks in high-stake decision-making tasks requires mechanisms for controlling the dependence of their output on sensitive attributes such as gender and ethnicity. A supervised learning framework with no control on the dependence of the prediction on the input features could lead to discriminatory decisions that significantly correlate with the sensitive attributes. Due to the critical importance of the fairness factor in several machine learning applications, the study and development of fair statistical learning algorithms have received great attention in the literature.\
To reduce the biases of DP-based learning algorithms, we propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method where the fair learner minimizes the worst-case DP-regularized loss over a set of sensitive attribute marginal distributions centered around the data-based marginal distribution. As a result, the SA-DRO approach can account for different frequencies of the sensitive attribute outcomes and thus offer a robust behavior to the changes in the sensitive attribute’s majority outcome.\
We present the results of several numerical experiments on the potential biases of DDP-based fair classification methodologies to the sensitive attribute possessing the majority in the dataset. Our empirical findings are consistent with the theoretical results, suggesting the inductive biases of DP-based fair classification rules toward the sensitive attribute-based majority group. On the other hand, our results indicate that the DRO-SA-based fair learning method results in fair classification rules with a lower bias toward the label distribution under the majority sensitive attribute.\
Furthermore, to show the impacts of such inductive biases in practice, we analyze the fair classification task in a federated learning context where multiple clients attempt to train a decentralized model. We focus on a setting with heterogeneous sensitive attribute distributions across clients where the clients’ majority sensitive attribute outcome may not agree. Figure 1 illustrates such a federated learning scenario over the Adult dataset, where Client 1’s majority sensitive attribute (female samples) is different from the network’s majority group (male samples), and consequently, Client 1’s test accuracy with a DP-based fair federated learning is significantly lower than the test accuracy of a localized fair model trained only on Client 1’s data. Such numerical results question the client’s incentive in participating in fair federated learning. The following is a summary of this work’s main contributions:\
• Analytically studying the biases of DP-based fair learning toward the majority sensitive attribute,\
• Proposing a distributionally robust optimization method to lower the biases of DP-based fair classification,\
• Providing numerical results on the biases of DP-based fair learning in centralized and federated learning scenarios.Fairness Violation Metrics. In this work, we focus on the learning frameworks aiming toward demographic parity (DP). Since enforcing DP to strictly hold could be costly and damaging to the learner’s performance, the machine learning literature has proposed applying several metrics assessing the dependence between random variables, including: the mutual information: [3–7], Pearson correlation [8, 9], kernel-based maximum mean discrepancy: [10], kernel density estimation of the difference of demographic parity (DDP) measures [11], the maximal correlation [12–15], and the exponential Renyi mutual information [16]. In our analysis, we mostly focus on a DDP-based fair regularization scheme, while we show only weaker versions of the inductive biases could further hold in the case of mutual information and maximal correlation-based fair learning algorithms.\
\
Fair Classification Algorithms. Fair machine learning algorithms can be classified into three main categories: pre-processing, post-processing, and in-processing. Pre-processing algorithms [17–19] transform biased data features into a new space where labels and sensitive attributes are statistically independent. Post-processing methods such as [2, 20] aim to alleviate the discriminatory impact of a classifier by modifying its ultimate decision. The focus of our work focus is only on in-processing approaches regularizing the training process toward DP-based fair models. Also, [21–23] propose distributionally robust optimization (DRO) for fair classification; however, unlike our method, these works do not apply DRO on the sensitive attribute distribution to reduce the biases.(1) Haoyu LEI, Department of Computer Science and Engineering, The Chinese University of Hong Kong (hylei22@cse.cuhk.edu.hk);(2) Amin Gohari, Department of Information Engineering, The Chinese University of Hong Kong (agohari@ie.cuhk.edu.hk);(3) Farzan Farnia, Department of Computer Science and Engineering, The Chinese University of Hong Kong (farnia@cse.cuhk.edu.hk).]]></content:encoded></item><item><title>&apos;Fish Doorbell&apos; Enters Fifth Year with Millions of Fans</title><link>https://tech.slashdot.org/story/25/03/23/1958239/fish-doorbell-enters-fifth-year-with-millions-of-fans?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Mar 2025 03:44:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Long-time Slashdot reader invisik reminds us that the "fish doorbell" is still going strong, according to the Associated Press.

"Now in its fifth year, the site has attracted millions of viewers from around the world with its quirky mix of slow TV and ecological activism."

The central Dutch city of Utrecht installed a "fish doorbell" on a river lock that lets viewers of an online livestream alert authorities to fish being held up as they make their springtime migration to shallow spawning grounds. The idea is simple: An underwater camera at Utrecht's Weerdsluis lock sends live footage to a website. When somebody watching the site sees a fish, they can click a button that sends a screenshot to organizers. When they see enough fish, they alert a water worker who opens the lock to let the fish swim through.
 
"Much of the time, the screen is just a murky green with occasional bubbles, but sometimes a fish swims past. As the water warms up, more fish show up..."]]></content:encoded></item><item><title>If Bird Flu Jumped to Humans, Could Past Flu Infections Offer Some Protection?</title><link>https://science.slashdot.org/story/25/03/23/2215232/if-bird-flu-jumped-to-humans-could-past-flu-infections-offer-some-protection?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Mar 2025 01:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[NPR reports on research "into whether our defenses built up from past flu seasons can offer any protection against H5N1 bird flu."

So far, the findings offer some reassurance. Antibodies and other players in the immune system may buffer the worst consequences of bird flu, at least to some degree. "There's certainly preexisting immunity," says Florian Krammer, a virologist at Mount Sinai's Icahn School of Medicine who is involved in some of the new studies. "That's very likely not going to protect us as a population from a new pandemic, but it might give us some protection against severe disease." This protection is based on shared traits between bird flu and types of seasonal flu that have circulated among us. Certain segments of the population, namely older people, may be particularly well-primed because of flu infections during early childhood. 

Of course, there are caveats. "While this is a bit of a silver lining, it doesn't mean we should all feel safe," says Seema Lakdawala, a virologist at Emory University's School of Medicine whose lab is probing this question. For one thing, the studies can't be done on people. The conclusions are based on animal models and blood tests that measure the immune response. And how this holds up for an individual is expected to vary considerably, depending on their own immune history, underlying health conditions and other factors. But for now, influenza researchers speculate this may be one reason most people who've caught bird flu over the past year have not fallen severely ill.... 
Research published this month is encouraging. By analyzing blood samples from close to 160 people, a team at the University of Pennsylvania and the University of Chicago were able to show that people born roughly before 1965 had higher levels of antibodies — proteins that bind to parts of the virus — which cross-react to the current strain of bird flu. 

This week U.S. federal officials also "announced funding for avian influenza research projects, including money for new vaccine projects and potential treatments," the Guardian report. The head of America's agriculture department said it would invest $100 million, as part of a larger $1 billion initiative to fight bird flu and stop rising egg prices, according to the nonprofit news site Iowa Capital Dispatch.]]></content:encoded></item><item><title>Former Cruise CEO Kyle Vogt’s new robotics startup reportedly raises another $150M</title><link>https://techcrunch.com/2025/03/23/former-cruise-ceo-kyle-vogts-new-robotics-startup-reportedly-raises-another-150m/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Sun, 23 Mar 2025 23:15:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Bot Company, the robotics startup founded by former Cruise co-founder and CEO Kyle Vogt, has raised $150 million in a round led by Greenoaks, according to Reuters, citing unnamed sources. Vogt founded the startup with Paril Jain, who led the AI tech team at Tesla, and former Cruise software engineer Luke Holoubek. Vogt did […]]]></content:encoded></item><item><title>How AI Coding Assistants Could Be Compromised Via Rules File</title><link>https://developers.slashdot.org/story/25/03/23/2138230/how-ai-coding-assistants-could-be-compromised-via-rules-file?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 22:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader spatwei shared this report from the cybersecurity site SC World:

: AI coding assistants such as GitHub Copilot and Cursor could be manipulated to generate code containing backdoors, vulnerabilities and other security issues via distribution of malicious rule configuration files, Pillar Security researchers reported Tuesday. Rules files are used by AI coding agents to guide their behavior when generating or editing code. For example, a rules file may include instructions for the assistant to follow certain coding best practices, utilize specific formatting, or output responses in a specific language. 

The attack technique developed by Pillar Researchers, which they call 'Rules File Backdoor,' weaponizes rules files by injecting them with instructions that are invisible to a human user but readable by the AI agent. 

Hidden Unicode characters like bidirectional text markers and zero-width joiners can be used to obfuscate malicious instructions in the user interface and in GitHub pull requests, the researchers noted. 

Rules configurations are often shared among developer communities and distributed through open-source repositories or included in project templates; therefore, an attacker could distribute a malicious rules file by sharing it on a forum, publishing it on an open-source platform like GitHub or injecting it via a pull request to a popular repository. Once the poisoned rules file is imported to GitHub Copilot or Cursor, the AI agent will read and follow the attacker's instructions while assisting the victim's future coding projects.]]></content:encoded></item><item><title>Buy now, pay later . . . for a burrito?</title><link>https://techcrunch.com/2025/03/23/buy-now-pay-later-for-a-burrito/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sun, 23 Mar 2025 21:56:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In 2010, a programmer who was mining bitcoin famously made the comically expensive mistake of spending 10,000 bitcoin on two pizzas. As of this writing, those coins would be worth $850 million. While there are few comparisons to that kind of miscalculation, the prospect of adding interest payments to fast-food orders is raising concerns nonetheless. […]]]></content:encoded></item><item><title>Is WhatsApp Being Ditched for Signal in Dutch Higher Education?</title><link>https://yro.slashdot.org/story/25/03/23/2120237/is-whatsapp-being-ditched-for-signal-in-dutch-higher-education?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 21:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[For weeks Signal has been one of the three most-downloaded apps in the Netherlands, according to a local news site. And now "Higher education institutions in the Netherlands have been looking for an alternative," according to DUB (an independent news site for the Utrecht University community):

Employees of the Utrecht University of Applied Sciences (HU) were recently advised to switch to Signal. Avans University of Applied Sciences has also been discussing a switch...The National Student Union is concerned about privacy. The subject was raised at last week's general meeting, as reported by chair Abdelkader Karbache, who said: "Our local unions want to switch to Signal or other open-source software." 

Besides being open source, Signal is a non-commercial nonprofit, the article points out — though its proponents suggest there's another big difference. "HU argues that Signal keeps users' data private, unlike WhatsApp." Cybernews.com explains the concern:

In an interview with the Dutch newspaper De Telegraaf, Meredith Whittaker [president of the Signal Foundation] discussed the pitfalls of WhatsApp. "WhatsApp collects metadata: who you send messages to, when, and how often. That's incredibly sensitive information," she says.... The only information [Signal] collects is the date an account was registered, the time when an account was last active, and hashed phone numbers... Information like profile name and the people a user communicates with is all encrypted... Metadata might sound harmless, but it couldn't be further from the truth. According to Whittaker, metadata is deadly. "As a former CIA director once said: 'We kill people based on metadata'." 

WhatsApp's metadata also includes IP addresses, TechRadar noted last May:



Other identifiable data such as your network details, the browser you use, ISP, and other identifiers linked to other Meta products (like Instagram and Facebook) associated with the same device or account are also collected... [Y]our IP can be used to track down your location. As the company explained, even if you keep the location-related features off, IP addresses and other collected information like phone number area codes can be used to estimate your "general location." 
WhatsApp is required by law to share this information with authorities during an investigation... 
[U]nder scrutiny is how Meta itself uses these precious details for commercial purposes. Again, this is clearly stated in WhatsApp's privacy policy and terms of use. "We may use the information we receive from [other Meta companies], and they may use the information we share with them, to help operate, provide, improve, understand, customize, support, and market our Services and their offerings," reads the policy. This means that yes, your messages are always private, but WhatsApp is actively collecting your metadata to build your digital persona across other Meta platforms... 
The article suggests using a VPN with WhatsApp and turning on its "advanced privacy feature" (which hides your IP address during calls) and managing the app's permissions for data collection. "While these steps can help reduce the amount of metadata collected, it's crucial to bear in mind that it's impossible to completely avoid metadata collection on the Meta-owned app... For extra privacy and security, I suggest switching to the more secure messaging app Signal." 

The article also includes a cautionary anecdote. "It was exactly a piece of metadata — a Proton Mail recovery email — that led to the arrest of a Catalan activist." 

Thanks to long-time Slashdot reader united_notions for sharing the article.]]></content:encoded></item><item><title>Bybit Named Exclusive Payment Partner For Tomorrowland Brasil 2025-26, Launches Cardholder Presale</title><link>https://hackernoon.com/bybit-named-exclusive-payment-partner-for-tomorrowland-brasil-2025-26-launches-cardholder-presale?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Sun, 23 Mar 2025 21:03:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[DUBAI, UAE, March 22nd, 2025/Chainwire/--Bybit, the world’s second-largest cryptocurrency exchange by trading volume, has partnered with  as its exclusive payment provider for 2025 and 2026.\
This landmark collaboration grants Bybit Card holders privileged early access to tickets before they become available to the general public - a first in Tomorrowland’s history.A Historic First for Bybit and TomorrowlandTomorrowland Brasil 2025 is set for October 10-12 at Parque Maeda in Itu, São Paulo. This year, Tomorrowland Brasil welcomes ‘LIFE’ as its mainstage theme, continuing the magical journey that began at Tomorrowland Belgium in 2024.\
Set in the mythical world of Silvyra, ‘LIFE’ transports the People of Tomorrow to an era of untamed natural beauty, just as a rare celestial event - the alignment of two moons - is about to unfold. Over three unforgettable days, attendees can escape the city and enjoy performances from more than 150 top electronic artists across six breathtaking stages.“By partnering with Tomorrowland Brasil, we are merging the energy of music with the innovation of blockchain. This collaboration reinforces Bybit’s commitment to integrating crypto seamlessly into everyday experiences, and our cardholders will enjoy unparalleled access to one of the world’s most iconic festivals,” said Joan Han, Head of the Payment Business Unit at Bybit.Bybit Card: More Than Just a Payment SolutionThe Bybit Card enhances users' festival experience by offering instant activation, seamless payments, and exclusive rewards. With compatibility across Apple Pay, Google Pay, and Samsung Pay, it ensures effortless transactions worldwide.\
Beyond convenience, Bybit Cards help users grow their wealth. With Auto-Savings, cardholders can earn up to 8% APR on their balance, ensuring passive income without extra effort. There are no annual fees or hidden charges, and users can withdraw up to $100 in cash for free every month, with a 2% fee thereafter.\
Plus, Bybit’s partnership with DHL ensures fast and secure worldwide delivery of the physical card.How to Secure Tomorrowland Brasil Tickets with Bybit CardThe Bybit Card is the ultimate festival payment solution, giving holders exclusive access to a two-day presale before tickets go on sale to the general public.Exclusive Registration Period: Now-April 3, 2025Presale Period: April 4, 10:00 BRT / 15:00 CEST - April 6, 10:00 BRT / 15:00 CESTGeneral Public Sale: Begins after the presale period\
To participate in the exclusive presale, users must be Bybit Card holders and complete their registration before the presale date on the . Only registered users will have access to the presale period.Priority Access: Bybit Card holders enter the first eight digits of their card (BIN code) to unlock ticket access.Secure a Spot: Each user can purchase up to six tickets.Exclusive Payment Method: Tickets must be purchased using a Bybit Card, as Bybit is the festival’s exclusive presale payment partner.\
Bybit’s seamless integration with Tomorrowland Brasil’s official website and ticketing system ensures a hassle-free booking experience for cardholders.General Ticket Sales Open April 8For users who miss out on the exclusive presale, tickets will still be available for general sale starting April 8. Purchases can be made using Bybit Card or Bybit Pay, with new users enjoying a 10% cashback on their festival purchases. is the world’s second-largest cryptocurrency exchange by trading volume, serving a global community of over 60 million users. Founded in 2018, Bybit is redefining openness in the decentralized world by creating a simpler, open and equal ecosystem for everyone.\
With a strong focus on Web3, Bybit partners strategically with leading blockchain protocols to provide robust infrastructure and drive on-chain innovation.\
Renowned for its secure custody, diverse marketplaces, intuitive user experience, and advanced blockchain tools, Bybit bridges the gap between TradFi and DeFi, empowering builders, creators, and enthusiasts to unlock the full potential of Web3. Discover the future of decentralized finance at .About TOMORROWLAND BRASIL will take place on October 10-12 in the beautiful festival area of Parque Maeda in Itu, a municipality of São Paulo. The festival will revolve around the mesmerizing ‘LIFE’ theme, set against the backdrop of Brazil’s enchanting natural beauty.\
A story set in the mythical realm of Silvyra, it’s a world unto its own, filled with creatures, plant life, and people living in harmony, each with their own stories, with the diversity of Silvyra’s lush nature represented in the spectacular ‘LIFE’ Mainstage.\
Offering the ultimate escape from the city during three days of bliss, guests will be treated to breathtaking performances by more than 150 of the world’s finest electronic artists across 6 mesmerizing stages.\
The first two festival editions of Tomorrowland Brasil took place in 2015 and 2016 in the beautiful festival area of Parque Maeda in Itu, São Paulo. After years of dreaming of a return, Tomorrowland finally headed back to Brazil in 2023, becoming a yearly highlight once again.:::tip
This story was distributed as a release by Chainwire under HackerNoon’s Business Blogging Program. Learn more about the program ]]></content:encoded></item><item><title>Developer Loads Steam On a $100 ARM Single Board Computer</title><link>https://games.slashdot.org/story/25/03/23/1922228/developer-loads-steam-on-a-100-arm-single-board-computer?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 20:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["There's no shortage of videos showing Steam running on expensive ARM single-board computers with discrete GPUs," writes Slashdot reader VennStone. "So I thought it would be worthwhile to make a guide for doing it on (relatively) inexpensive RK3588-powered single-board computers, using Box86/64 and Armbian."

The guides I came across were out of date, had a bunch of extra steps thrown in, or were outright incorrect... Up first, we need to add the Box86 and Box64 ARM repositories [along with dependencies, ARMHF architecture, and the Mesa graphics driver]... 
The guide closes with a multi-line script and advice to "Just close your eyes and run this. It's not pretty, but it will download the Steam Debian package, extract the needed bits, and set up a launch script." (And then the final step is sudo reboot now.) 

"At this point, all you have to do is open a terminal, type 'steam', and tap Enter. You'll have about five minutes to wait... Check out the video to see how some of the tested games perform."

At 720p, performance is all over the place, but the games I tested typically managed to stay above 30 FPS. This is better than I was expecting from a four-year-old SOC emulating x86 titles under ARM. 

Is this a practical way to play your Steam games? Nope, not even a little bit. For now, this is merely an exercise in ludicrous neatness. Things might get a wee bit better, considering Collabora is working on upstream support for RK3588 and Valve is up to something ARM-related, but ya know, "Valve Time"... 

"You might be tempted to enable Steam Play for your Windows games, but don't waste your time. I mean, you can try, but it ain't gonna work."]]></content:encoded></item><item><title>Doc Searls Proposes We Set Our Own Terms and Policies for Web Site Tracking</title><link>https://yro.slashdot.org/story/25/03/23/1842242/doc-searls-proposes-we-set-our-own-terms-and-policies-for-web-site-tracking?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 19:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Today long-time open source advocate/journalist Doc Searls revealed that years of work by consumer privacy groups has culminated in a proposed standard "that can vastly expand our agency in the digital world" — especially in a future world where agents surf the web on our behalf:


Meet IEEE P7012 , which "identifies/addresses the manner in which personal privacy terms are proffered and how they can be read and agreed to by machines." It has been in the works since 2017, and should be ready later this year. (I say this as chair of the standard's working group.) The nickname for P7012 is MyTerms (much as the nickname for the IEEE's 802.11 standard is Wi-Fi). 

The idea behind MyTerms is that the sites and services of the world should agree to your terms, rather than the other way around. 



Basically your web browser proffers whatever agreement you've chosen (from a canonical list hosted at Customer Commons) to the web sites and other online services that you're visiting.
 

"Browser makers can build something into their product, or any developer can make a browser add-on or extension..." Searls writes. "On the site's side — the second-party side — CMS makers can build something in, or any developer can make a plug-in (WordPress) or a module (Drupal). Mobile app toolmakers can also come up with something (or many things)..."



MyTerms creates a new regime for privacy: one based on contract. With each MyTerm you are the first party. Not the website, the service, or the app maker. They are the second party. And terms can be friendly. For example, a prototype term called NoStalking says "Just show me ads not based on tracking me." This is good for you, because you don't get tracked, and good for the site because it leaves open the advertising option. NoStalking lives at Customer Commons, much as personal copyrights live at Creative Commons. (Yes, the former is modeled on the latter.) 
"[L]et's make this happen and show the world what agency really means," Searls concludes. 

Another way to say it is they've created "a draft standard for machine-readable personal privacy terms." But Searl's article used a grander metaphor to explain its significance:
When Archimedes said 'Give me a place to stand and I can move the world,' he was talking about agency. You have no agency on the Web if you are always the second party, agreeing to terms and policies set by websites. 

You are Archimedes if you are the first party, setting your own terms and policies. The scale you get with those is One 2 World. The place you stand is on the Web itself — and the Internet below it. 

Both were designed to make each of us an Archimedes.]]></content:encoded></item><item><title>Solidity Is to Ethereum What Tact Is to TON — How to Build a Voting Smart Contract on TON Using Tact</title><link>https://hackernoon.com/solidity-is-to-ethereum-what-tact-is-to-ton-how-to-build-a-voting-smart-contract-on-ton-using-tact?source=rss</link><author>Favour Kelvin</author><category>tech</category><pubDate>Sun, 23 Mar 2025 19:00:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most times, when people start learning how to write smart contracts, the first thing they hear about is Solidity and Ethereum. That was the first thing I heard about too. It's what most tutorials focus on, and for good reason. Solidity made it possible to write programs that live on a blockchain, and Ethereum became the place where many people got started.\
But Solidity isn’t the only smart contract language out there. And Ethereum isn’t the only blockchain that supports decentralized applications.\
There’s also , short for . It was created by Telegram, but it's now a public, community-driven chain. It’s fast, lightweight, and handles things a bit differently from what you might be used to on Ethereum. That includes how smart contracts are written.  When I started exploring the TON documentation, I came across four different languages for writing smart contracts: Tact, Tolk, FunC, and Fift. I won’t go deep into all four here.\
This guide focuses on the Tact language, and we’ll see how to use it to build a basic voting contract that lets users cast votes and check results on-chain.Why I Decided to Learn Tact FirstThe TON ecosystem actually supports multiple languages, each serving different use cases, levels of abstraction, and developer experience. Here’s a quick overview of the of each of them: is the traditional language for writing TON smart contracts. It’s low-level and gives you precise control over how your contract works under the hood. It’s powerful, but it also means you’ll need to understand how the TON Virtual Machine (TVM) works, including concepts like stack manipulation, memory layout, and deterministic execution. The syntax is somewhat similar to C, which can feel unfamiliar if you haven’t worked with that style of language before. is typically used alongside FunC. It’s a stack-based language that’s mostly used for interacting with the TVM directly and used for  deploying, debugging, and performing on-chain calculations. It’s not usually the language you start with for writing full smart contracts, but it’s important in the overall development workflow on TON. is a newer addition that’s still evolving. From what I’ve gathered, it aims to improve tooling and compatibility with higher-level languages. It’s promising, but not yet as widely adopted or documented. is a high-level language that’s designed specifically to make TON smart contract development more accessible and developer friendly. Tact simplifies a lot of the lower-level complexity and lets you focus on writing your logic in a clean, readable way. The syntax is closer to what you’d see in TypeScript or Solidity, which makes it much easier to get started without needing to dive deep into TVM internals.\
Tact provides and a quicker path for building and deploying contracts on the TON blockchain.Understanding How Tact WorksBefore we start writing code, it’s important to understand how Tact smart contracts are structured. A typical Tact contract includes a few core components: block – This is where you define the name of your contract and declare any state variables. block –  It initializes your contract’s state variables and sets the contract’s starting conditions. This block runs once at the time of deployment. blocks – These are like event listeners. They handle incoming messages and define how your contract reacts to them.Getter functions () – These are optional read-only functions that allow users or other contracts to query the contract’s state without changing it.Tact uses message-based communication, which is how all interactions on TON work. Each contract receives a message and processes it in its own  block.  This message-based structure helps organize your contract logic in a modular, maintainable way.\
Let’s now apply this in a real example by building a simple voting contract.Building Your First Voting Contract with Tact (Using the TON Web IDE)In this section, we'll walk through how to implement a basic voting system using Tact. This voting contract will allows users to vote for predefined candidates and tracks the total number of votes each candidate receives.\
We’ll be doing everything inside the TON Web IDE, which is an in-browser tool where you can write, build, and test your contracts without installing anything locally.Step 1 – Open the TON Web IDEClick . In the popup:Make sure the language is on .Choose  as your template.Name your project something like .Step 2 – Writing the Voting Contract CodeAfter creating your project, open the  file. You’ll see a boilerplate setup:// Import the Deployable trait so the contract can be deployed easily
import "@stdlib/deploy";

contract BlankContract with Deployable {
    init() {

    }
}
 is required for deployment to work and should not be removed from the code. is the placeholder name.The  block runs only once when the contract is deployed and is used to initialize state variables.\
Now, let’s map out our own code.\
First, we’ll define the message structure for voting:// Import the Deployable trait so the contract can be deployed easily
import "@stdlib/deploy";

// Define a message structure for voting
message Vote {
    candidate: Int as uint32; // 1 = Alice, 2 = Bob
}
This is the Vote message. When someone wants to vote, they’ll send a message to the contract that includes a number:Tact uses this structure to process the incoming vote and decide which candidate gets the point.\
Next, we’ll set up our contract and add two state variables to keep track of each candidate’s votes:...
contract VotingContract with Deployable {

    // State variables to track votes
    votesAlice: Int as uint32;
    votesBob: Int as uint32;
Inside the contract, we defined two variables:: stores the number of votes Alice receives.: stores the number of votes Bob receives.\
We’ll now initialize those vote counts to zero inside the  block to set the starting state of the contract when it's first deployed.    init() {
        self.votesAlice = 0;
        self.votesBob = 0;
    }
The  block runs , right when the contract is deployed and it sets both vote counts to zero.\
Now comes the logic. When a vote is sent, we want the contract to check who the vote is for and increase the correct vote count.    // Handle vote messages
    receive(msg: Vote) {
        if (msg.candidate == 1) {
            self.votesAlice += 1;
        } else if (msg.candidate == 2) {
            self.votesBob += 1;
        }
    }
So when a vote is received:If  is 1, we add +1 to If  is 2, we add +1 to \
Finally, we’ll create getter functions to let anyone query the vote count for each candidate without changing the contract state.    // Getter for Alice's votes
    get fun getVotesForAlice(): Int {
        return self.votesAlice;
    }

    // Getter for Bob's votes
    get fun getVotesForBob(): Int {
        return self.votesBob;
    }
}
These two getter functions let us check the number of votes each candidate has received without modifying anything in the contract. It’s a read-only operation.\
Below is the full voting contract code:import "@stdlib/deploy";

// Define a message structure for voting
message Vote {
    candidate: Int as uint32; // 1 = Alice, 2 = Bob
}

contract VotingContract with Deployable {

    // State variables to track votes
    votesAlice: Int as uint32;
    votesBob: Int as uint32;

    init() {
        self.votesAlice = 0;
        self.votesBob = 0;
    }

    // Handle vote messages
    receive(msg: Vote) {
        if (msg.candidate == 1) {
            self.votesAlice += 1;
        } else if (msg.candidate == 2) {
            self.votesBob += 1;
        }
    }

    // Getter for Alice's votes
    get fun getVotesForAlice(): Int {
        return self.votesAlice;
    }

    // Getter for Bob's votes
    get fun getVotesForBob(): Int {
        return self.votesBob;
    }
}
Step 4 – Build and Deploy the ContractOn the left sidebar, click on Under , make sure  is selected.Make sure  is selected and click . This will compile your contract and check for any syntax errors or issues in your code.Next, make sure  is selected in the dropdown as that’s your actual contract, not the default placeholder. If you don’t see it, press  to save your file so the IDE can detect the updated contract.Then click . If everything works correctly, you’ll see a confirmation message in the logs showing that your contract was successfully deployed on Sandbox.Step 5 – Interact With the ContractOnce deployed, scroll down and you’ll see two sections:,  In the  section, enter  in the  input field and click  You’ve just voted for Alice! You can repeat this to cast more votes.\
: Click  under  and check the  panel to see the vote countDo the same for Bob by sending  in the  field, then check In my test run, I voted for Alice  and Bob , and the getter functions showed exactly that.💭 Final Thoughts: Keep Building, Keep Exploring🙌 Congrats if you read all the way through!\
Now that you’ve seen how a simple voting contract works in Tact, you’ve taken your first step into smart contract development on TON. This contract might be basic, but the structure and concepts apply to more complex logic too.\
If you want to keep experimenting, try extending this contract or exploring other prebuilt templates from https://tact-by-example.org/all. The TON Web IDE also makes it easy to try out different use cases and it comes with templates as well to help you build and learn faster.\
So go ahead, tweak, test, build something better.]]></content:encoded></item><item><title>Meet the Bot That Reads All the Bad News Headlines So You Don’t Have To</title><link>https://hackernoon.com/meet-the-bot-that-reads-all-the-bad-news-headlines-so-you-dont-have-to?source=rss</link><author>Raymond Camden</author><category>tech</category><pubDate>Sun, 23 Mar 2025 19:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Diffbot's Knowledge Graph has a simple purpose - bring the sum total of all knowledge to your fingertips via a search that emphasis data and relations over a simple text based search engine experience. Sourced by the entire web, Knowledge Graph lets you perform complex queries against billions of data points instantly via a simple API. I decided to take a spin with their API and build a "relatively" simple tool - news analysis for a product run in on automated platform. Should be easy, right? Let's get to it. Note that the examples in this blog post assume you've gotten a free key from Diffbot. Be sure to do that before trying the samples.Before writing a line of code, I signed into Diffbot and opened up their visual search tool for Knowledge Graph. The tool lets you build queries visually or by hand. Queries are known as 'DQL' statements in Diffbot and are pretty simple to read even if you've never seen the syntax before.\
From this tool, I started off by selecting an entity type. This is the high level type of data I want to search and can be one of many numerous options, from people to events to movies and investments. I selected "Article" as my intent is to find news that's speaking ill of my wonderful product. I then selected a "Filter By" option. While you can filter by any property in the entity type, I used  as it's a more precise match than a simple text search. While a text filter does work, using  gives a much better result by ensuring that the results are focused on my search, not just casually mentioning it. For my demo, I'll be looking for articles about "XBox".\
I also used the "Sort by" value to show newest first and this hit search to see if my results made sense.\
While my initial results didn't include any foreign language results, I knew I'd want to filter to results in English, so I next added a filter for language. Hitting the + sign by the current filter, I was then able to add language and  for English. Once again, I hit search:\
Alright, so next, I want to filter to just negative results. Knowledge Graph Article entities have a sentiment score (you can see them in the search results) that go from -1 to most negative to 1 to most positive. Initially, I simply selected items with a sentiment less than or equal to 0.\
Woot, getting there. As a final step, I knew this was going to be automated and filtered to 'recent' items, so I added one more filter, this time on , selected , and picked a date from a week ago.\
At this point, the query looks good, so let's copy out the query value provided by the tool:type:Article tags.label:"Xbox" language:"en" sentiment<=0 date>"2025-03-03" sortBy:date
Designing the query was really the hard part. For the code, I went to the Search docs. The examples are curl/HTTP based but quite easy to port to Python or any other language. Consider this sample:import os 
import requests 
import json 
import urllib.parse

token = os.environ.get("db_token")

query = 'type:Article tags.label:"Xbox" language:"en" sentiment<=0 date>"2025-03-03" sortBy:date'

apiCall = f"https://kg.diffbot.com/kg/v3/dql?type=query&token={token}&query={urllib.parse.quote(query)}&size=25"

req = requests.get(apiCall)
results = json.loads(req.content)
print(f"Total results, {results['hits']}")

for result in results["data"]:
    print(result["entity"]["title"])
    print(result["entity"]["date"]["str"])
    print(result["entity"]["summary"])
    if "author" in result["entity"]:
        print(result["entity"]["author"])
    print(result["entity"]["siteName"])
    print(result["entity"]["pageUrl"])
    print(result["entity"]["sentiment"])

    print("------------------------------------")
\
Breaking this down - I began with my query from the visual tool. This then gets url encoded and passed to the API for Knowledge Graph. The only real new item there is the addition of  to keep the result set to a sensible limit.\
I call the API, print the total results found (from the  result) and then iterate over each showing various bit of info from the result. Here's a few of the results:Total results, 68
Xbox will release its first handheld gaming console this year, report claims
d2025-03-10T19:37
Windows Central expects the console to take advantage of the widgets on the Xbox Game Bar to let use...
Jacob Siegal
BGR
https://bgr.com/entertainment/xbox-will-release-its-first-handheld-gaming-console-this-year-report-claims/
0
------------------------------------
Rumour: Next-Gen Xbox a 'PC in Essence' - What Would That Mean for PlayStation?
d2025-03-10T19:00
Recent comments from Windows Central's executive editor Jez Corden have sparked discussion about whe...
Stephen Tailby
Push Square
https://www.pushsquare.com/news/2025/03/rumour-next-gen-xbox-a-pc-in-essence-what-would-that-mean-for-playstation
0
------------------------------------
Xbox handheld out this year and will go up against Nintendo Switch 2 says source
d2025-03-10T18:50
New rumours about Microsoft’s next gen plans suggests that there will be two Xbox handheld consoles ...
GameCentral
Metro
http://metro.co.uk/2025/03/10/xbox-handheld-this-year-will-go-nintendo-switch-2-says-source-22703266/
0
\
This works, but now let's make the date dynamic. I began importing from :from datetime import datetime, timedelta
\
I then generated a formatted date for last week:today = datetime.now()
lastWeek = today + timedelta(days=-7)
fLastWeek = lastWeek.strftime("%Y-%m-%d")
\
And the last bit was to just include that date in my query:query = f'type:Article tags.label:"Xbox" language:"en" sentiment<=0 date>{fLastWeek} sortBy:date'
\
You can see the complete source code for the initial version here and the final version here.Alright, time to automate this. For my automation, I'll be making use of Pipedream, an  flexible workflow system I've used many times in the past. Here's the entire workflow with each part built out:\
I began my workflow with a simple schedule based trigger, ie, when to run. This was somewhat arbitrary, but I picked weekly, on Sunday, at 1PM.\
The next step, , handles the logic I demonstrated earlier, but now in a "Pipedream handler", which is the standard way to write code steps in Pipedream workflow.import os 
import requests 
import json 
from datetime import datetime, timedelta
import urllib.parse

def handler(pd: "pipedream"):

  token = os.environ.get("db_token")

  today = datetime.now()
  lastWeek = today + timedelta(days=-7)
  fLastWeek = lastWeek.strftime("%Y-%m-%d")

  query = f'type:Article tags.label:"Xbox" language:"en" sentiment<=0 date>{fLastWeek} sortBy:date'

  apiCall = f"https://kg.diffbot.com/kg/v3/dql?type=query&token={token}&query={urllib.parse.quote(query)}&size=25"

  req = requests.get(apiCall)
  return json.loads(req.content)
\
The next step is simply a quick code step to end the workflow if no results are found:def handler(pd: "pipedream"):

    if len(pd.steps["getArticles"]["$return_value"]["data"]) == 0:
      pd.flow.exit("No results")
\
Now I want to 'massage' the results a bit. I'm going to eventually email this to myself, so I built a step to format the results in a nice string:from datetime import datetime

def handler(pd: "pipedream"):

  email = f"""
Negative Article Results:

Our search found {pd.steps["getArticles"]["$return_value"]["hits"]} results. Here are the top 25:
  """  

  for result in pd.steps["getArticles"]["$return_value"]["data"]:
    date = datetime.fromtimestamp(result["entity"]["date"]["timestamp"] / 1000)
    date_f = date.strftime("%Y-%m-%d")

    email += f"""
{result["entity"]["title"]}
Sentiment:  {result["entity"]["sentiment"]}
Published:  {date_f}
Link:       {result["entity"]["pageUrl"]}
    """

  return email
\
Again, this is somewhat arbitrary in terms of what I thought important enough to include. You could definitely get more fancier, and even do things like, "on a really bad sentiment, add color, red flags, etc".\
The final step was to simply email myself the results. Pipedream supports a "send an email to the account owner" step that will do just that, email me. If I were building this out for a client, I'd use one of the many Pipedream built-in steps for mail APIs.\
Once run, I get a nice email with a list of articles and their sentiment:This is just one example of using Diffbot's Knowledge Graph API, and as a reminder, articles are only one of the many different types of data you can search. Everything I did here was also done on a , so you can absolutely sign up and try it out yourself. I'm going to be digging into this more so let me know if you've got any questions!]]></content:encoded></item><item><title>Facebook Whistleblower Demands Overturn of Interview Ban - as Her Book Remains a Bestseller</title><link>https://news.slashdot.org/story/25/03/23/0413214/facebook-whistleblower-demands-overturn-of-interview-ban---as-her-book-remains-a-bestseller?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 18:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The latest Facebook whistleblower, a former international lawyer, "cannot grant any of the nearly 100 interview requests she has received from journalists from print and broadcast news outlets in the United States and the United Kingdom," reports the Washington Post (citing "a person familiar with the matter"). 

That's because of an independent arbiter's ruling that "also bars her from talking with lawmakers in the U.S., London and the EU, according to a legal challenge she lodged against the ruling..."

On March 12, an emergency arbiter — a dispute resolution option outside the court system — sided with Meta by ruling that the tech giant might reasonably convince a court that Wynn-Williams broke a non-disparagement agreement she entered as she was being fired by the company in 2017. The arbiter also said that while her publisher Macmillan appeared for the hearing on Meta's motion, Wynn-Williams did not despite having received due notice. The arbiter did not make any assessments about the book's veracity, but Meta spokespeople argued that the ruling meant that "Sarah Wynn Williams' false and defamatory book should never have been published." 


Wynn-Williams this week filed an emergency motion to overturn the ruling, arguing that she didn't receive proper notice of the arbitration proceedings to the email accounts Meta knows she uses, according to a copy of the motion seen by The Post. Wynn-Williams further alleged that her severance agreement including the non-disparagement provisions are unenforceable, arguing that it violates laws that protect whistleblowers from retaliation, among other points. In a statement, legal representatives for Wynn-Williams said they were "confident in the legal arguments and look forward to a swift restoration of Ms. Wynn-Williams' right to tell her story." 
That book — Careless People: A Cautionary Tale of Power, Greed, and Lost Idealism — is currently #1 on the New York Times best-seller list (and #3 on Amazon.com's best-selling books list). And the incident prompted an article by Wired editor at large Steven Levy titled "Meta Tries to Bury a Tell-All Book." ("Please pause for a moment to savor the irony," Levy writes. "Meta, the company that recently announced an end to fact-checking in posts seen by potentially millions of people, is griping that an author didn't fact-check with them?") 

And this led to a heated exchange on X.com between the Wired editor at large and Meta's Chief Technology Officer Andrew Bozworth: 

Steven Levy: Meta probably realizes that all-out war on this book will only help its sales. But they are furious that an insider--who signed an NDA!--is going White Lotus on them, showing what it's like on the inside. 

Meta CTO Bozworth: Except that it is full of lies, Steven. Shame on you. 
Steven Levy: Boz, it would be helpful if Meta called out what it believes are the factual inaccuracies, especially in cases where it calls the book "defamatory." 

Meta CTO Bozworth: Sorry you don't get to make up a bunch of stories and then put the burden on the person you lied about. Read the accounts from former employees who have gone through several of the anecdotes and said flatly they did not happen as written and then extrapolate. 
Steven Levy: I would love for Sheryl, Mark and Joel to speak out on those anecdotes and give their sides of the story. They are the key subjects of those stories and their direct denial of specific incidents would matter. 

Meta CTO Bozworth: Did you read what I wrote? I'm sure you would love to have more fuel for your "nobody wants you to read this" headline, but that's a total bullshit expectation. It isn't unreasonable to expect a journalist like you to do basic diligence. I'm sure you have our comms email! 
Steven Levy: Believe me I was in touch with your comms people...]]></content:encoded></item><item><title>FSF Holds Live Auction of &apos;Historically Important&apos; Free Software Memorabilia</title><link>https://news.slashdot.org/story/25/03/23/1558210/fsf-holds-live-auction-of-historically-important-free-software-memorabilia?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In 30 minutes the Free Software Foundation holds a live auction of memorabilia to celebrate their upcoming 40th anniversary. "By moving out of the FSF office, we got to sort through all the fun and historically important memorabilia and selected the best ones," they announced earlier — and 25 items will up for bids. (To participate in the live auction, you must register in advance.) 

 "This is your chance to get your very own personal souvenir of the FSF," explains an 11-page auction booklet, "from original GNU art to a famous katana and the Internet Hall of Fame medal of the FSF's founder."

That's right... a katana.

Once upon a time, this 41-inch blade turned heads at the FSF's tech team office. Donated by FSF friends and fans of the XKCD webcomic #225, it became a lighthearted "weapon" in the war for user freedom. As RMS himself is anti-violence, he made a silly joke by examining the katana closely instead of brandishing it, symbolizing that software freedom can be defended with wit. In a legendary photo, this was perceived as if he sniffed the blade. Between the etched dragon on the scabbard and the wavy hamon on the blade, it's as flashy as it is symbolic — especially if you like taking on proprietary software with style (and a dash of humor).
 

The auction is intended "to entrust some of the historically important free software memorabilia that were in the FSF's office and archive to the free software community instead of locking them away in a storage unit where no one can enjoy them. 

"Hopefully, this way some of these unique items will be displayed in galleries or on the walls of free software enthusiasts. All auction proceeds will go towards the FSF's mission to promote computer user freedom." 

And speaking of user freedom, here's how they described the Internet Hall of Fame medal:


When Richard M. Stallman, the founder of the FSF, was inducted into the Internet Hall of Fame, it was the ultimate nod to free software's immense impact on the Internet... The medal is shiny, and the frame is fancy, but the real radiance is the recognition that the Internet might look much more locked down and dull without those original free software seeds. Hang it on your wall, and you'll be reminded that hacking for user freedom can change the world.
]]></content:encoded></item><item><title>The Role of Curl Terms in Micromorphic Models</title><link>https://hackernoon.com/the-role-of-curl-terms-in-micromorphic-models?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:31:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[9 Conclusion and perspectives\
In addition to the fitting comparison presented in the present paper, one main result that we present here is the fitting procedure itself that has been automatized to a big extent by asking only the cut-offs and asymptotes to be imposed a priori. This has been done by imposing the exact value of the cut-offs and minimizing the asymptotes’ mean square error compared to the exact numerical values issued via Boch-Floquet analysis. The rest of the curves’ fitting follows directly.\
Based on the findings of this paper, we will briefly present some insight that will give directions to the follow-up research:\
• Further enhance the relaxed micromorphic model via the addition of extra microscopic degrees of freedom to increase its precision at very small wavelengths (approaching the unit cell’s size);\
• Design complex large-scale meta-structures that control elastic energy using the new labyrinthine metamaterial as a basic building block. This design would not be otherwise possible due to the huge number of degrees of freedom resulting from the meshing of all the tiny elements contained in the labyrinthine unit cells;\
• Study negative refraction phenomena in meta-structures including the new labyrinthine metamaterial as a basic building block;\
• Design complex structures for wave control simultaneously including the different metamaterials that were characterized via the relaxed micromorphic model until now.Angela Madeo, Gianluca Rizzi and Jendrik Voss acknowledge support from the European Commission through the funding of the ERC Consolidator Grant META-LEGO, N◦ 101001759. Angela Madeo and Gianluca Rizzi acknowledge funding from the French Research Agency ANR, “METASMART” (ANR-17CE08-0006). Patrizio Neff acknowledges support in the framework of the DFG-Priority Programme 2256 “Variational Methods for Predicting Complex Phenomena in Engineering Structures and Materials”, Neff 902/10-1, Project-No. 440935806.[1] A. Aivaliotis, A. Daouadji, G. Barbagallo, D. Tallarico, P. Neff, and A. Madeo. “Microstructure-related Stoneley waves and their effect on the scattering properties of a 2D Cauchy/relaxed-micromorphic interface”. en. Wave Motion 90 (Aug. 2019). Pp. 99–120. issn: 01652125. doi: 10.1016/j.wavemoti.2019.04.003. url: https://linkinghub.elsevier.com/ retrieve/pii/S0165212518304827 (visited on 04/23/2021).\
[2] A. Aivaliotis, D. Tallarico, M.-V. d’Agostino, A. Daouadji, P. Neff, and A. Madeo. “Frequency- and angle-dependent scattering of a finite-sized meta-structure via the relaxed micromorphic model”. en. Archive of Applied Mechanics 90.5 (May 2020). Pp. 1073–1096. issn: 0939-1533, 1432-0681. doi: 10.1007/s00419-019-01651-9. url: http://link.springer. com/10.1007/s00419-019-01651-9 (visited on 04/23/2021).\
[3] G. Allaire. “Homogenization and two-scale convergence”. SIAM Journal on Mathematical Analysis 23.6 (1992). Pp. 1482– 1518.\
[4] I. Andrianov, V. Bolshakov, V. Danishevs’kyy, and D. Weichert. “Higher order asymptotic homogenization and wave propagation in periodic composite materials”. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 464.2093 (2008). Pp. 1181–1201.\
[5] A. Bacigalupo and L. Gambarotta. “Second-gradient homogenized model for wave propagation in heterogeneous periodic media”. International Journal of Solids and Structures 51.5 (2014). Pp. 1052–1065.\
[6] A. Bensoussan, J. Lions, and G. Papanicolaou. Asymptotic analysis for periodic structures. Vol. 374. American Mathematical Soc., 2011.\
[7] D. S. Bernstein. Matrix Mathematics: Theory, Facts, and Formulas (Second Edition). Princeton reference. Princeton University Press, 2009.\
[8] O. R. Bilal, D. Ballagi, and C. Daraio. “Architected lattices for simultaneous broadband attenuation of airborne sound and mechanical vibrations in all directions”. Physical Review Applied 10.5 (2018). P. 054060.\
[9] G. Bouchitt´e and M. Bellieud. “Homogenization of a soft elastic material reinforced by fibers”. Asymptotic Analysis 32.2 (2002). Pp. 153–183.\
[10] C. Boutin, A. Rallu, and S. Hans. “Large scale modulation of high frequency waves in periodic elastic composites”. Journal of the Mechanics and Physics of Solids 70 (2014). Pp. 362–381.\
[11] T. B¨uckmann, M. Kadic, R. Schittny, and M. Wegener. “Mechanical cloak design by direct lattice transformation”. Proceedings of the National Academy of Sciences 112.16 (2015). Pp. 4930–4934.\
[12] M. Camar-Eddine and P. Seppecher. “Determination of the closure of the set of elasticity functionals”. Archive for Rational Mechanics and Analysis 170.3 (2003). Pp. 211–245.\
[13] P. Celli, B. Yousefzadeh, C. Daraio, and S. Gonella. “Bandgap widening by disorder in rainbow metamaterials”. Applied Physics Letters 114.9 (2019). P. 091903.\
[14] W. Chen and J. Fish. “A dispersive model for wave propagation in periodic heterogeneous media based on homogenization with multiple spatial and temporal scales”. Journal of Applied Mechanics 68.2 (2001). Pp. 153–161.\
[15] R. Craster, J. Kaplunov, and A. Pichugin. “High-frequency homogenization for periodic media”. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 466.2120 (2010). Pp. 2341–2362.\
[16] S. A. Cummer, J. Christensen, and A. Al`u. “Controlling sound with acoustic metamaterials”. Nature Reviews Materials 1.3 (2016). Pp. 1–13.\
[17] M. V. d’Agostino, G. Barbagallo, I.-D. Ghiba, B. Eidel, P. Neff, and A. Madeo. “Effective description of anisotropic wave dispersion in mechanical band-gap metamaterials via the relaxed micromorphic model”. en. Journal of Elasticity 139.2 (May 2020). Pp. 299–329. issn: 0374-3535, 1573-2681. doi: 10.1007/s10659-019-09753-9. url: http://link.springer. com/10.1007/s10659-019-09753-9 (visited on 04/23/2021).\
[18] T. Frenzel, M. Kadic, and M. Wegener. “Three-dimensional mechanical metamaterials with a twist”. Science 358.6366 (2017). Pp. 1072–1074.\
[19] M. Geers, V. Kouznetsova, and M. Brekelmans. “Multi-scale computational homogenization: Trends and challenges”. Journal of Computational and Applied Mathematics 234.7 (2010). Pp. 2175–2182.\
[20] S. Guenneau, A. Movchan, G. P´etursson, and S. A. Ramakrishna. “Acoustic metamaterials for sound focusing and confinement”. New Journal of physics 9.11 (2007). P. 399.\
[21] Z. Hashin and S. Shtrikman. “A variational approach to the theory of the elastic behaviour of multiphase materials”. Journal of the Mechanics and Physics of Solids 11.2 (1963). Pp. 127–140.\
[22] R. Hill. “Elastic properties of reinforced solids: some theoretical principles”. Journal of the Mechanics and Physics of Solids 11.5 (1963). Pp. 357–372.\
[23] R. Hu and C. Oskay. “Nonlocal homogenization model for wave dispersion and attenuation in elastic and viscoelastic periodic layered media”. Journal of Applied Mechanics 84.3 (2017).\
[24] N. Kaina, A. Causier, Y. Bourlier, M. Fink, T. Berthelot, and G. Lerosey. “Slow waves in locally resonant metamaterials line defect waveguides”. Scientific reports 7.1 (2017). Pp. 1–11.\
[25] N. Kaina, F. Lemoult, M. Fink, and G. Lerosey. “Negative refractive index and acoustic superlens from multiple scattering in single negative metamaterials”. en. Nature 525.7567 (Sept. 2015). Pp. 77–81. issn: 0028-0836, 1476-4687. doi: 10.1038/ nature14678. url: http://www.nature.com/articles/nature14678 (visited on 04/23/2021).\
[26] A. O. Krushynska, M. Miniaci, F. Bosia, and N. M. Pugno. “Coupling local resonance with Bragg band gaps in single-phase mechanical metamaterials”. Extreme Mechanics Letters 12 (2017). Pp. 30–36.\
[27] R. Lakes. “Foam structures with a negative Poisson’s ratio”. Science 235.4792 (1987). Pp. 1038–1040. [28] Z. Liu, X. Zhang, Y. Mao, Y. Zhu, Z. Yang, C. T. Chan, and P. Sheng. “Locally resonant sonic materials”. science 289.5485 (2000). Pp. 1734–1736.\
[29] C. Miehe, J. Schr¨oder, and J. Schotte. “Computational homogenization analysis in finite plasticity simulation of texture development in polycrystalline materials”. Computer Methods in Applied Mechanics and Engineering 171.3-4 (1999). Pp. 387–418.\
[30] G. Milton. “The Theory of Composites. 2002”. Cambridge Monographs on Applied and Computational Mathematics (2002).\
[31] D. Misseroni, D. J. Colquitt, A. B. Movchan, N. V. Movchan, and I. S. Jones. “Cymatics for the cloaking of flexural vibrations in a structured plate”. Scientific reports 6.1 (2016). Pp. 1–11.\
[32] P. Neff, I. D. Ghiba, M. Lazar, and A. Madeo. “The relaxed linear micromorphic continuum: well-posedness of the static problem and relations to the gauge theory of dislocations”. en. The Quarterly Journal of Mechanics and Applied Mathematics 68.1 (Feb. 2015). Pp. 53–84. issn: 0033-5614, 1464-3855. doi: 10 . 1093 / qjmam / hbu027. url: https : / / academic.oup.com/qjmam/article-lookup/doi/10.1093/qjmam/hbu027 (visited on 04/25/2021).\
[33] P. Neff, B. Eidel, M. V. d’Agostino, and A. Madeo. “Identification of scale-independent material parameters in the relaxed micromorphic model through model-adapted first order homogenization”. en. Journal of Elasticity 139.2 (May 2020). Pp. 269–298. issn: 0374-3535, 1573-2681. doi: 10.1007/s10659-019-09752-w. url: http://link.springer.com/10.1007/ s10659-019-09752-w (visited on 04/23/2021).\
[34] P. Neff, I.-D. Ghiba, A. Madeo, L. Placidi, and G. Rosi. “A unifying perspective: the relaxed linear micromorphic continuum”. en. Continuum Mechanics and Thermodynamics 26.5 (Sept. 2014). Pp. 639–681. issn: 0935-1175, 1432-0959. doi: 10.1007/s00161-013-0322-9. url: http://link.springer.com/10.1007/s00161-013-0322-9 (visited on 04/23/2021).\
[35] C. Pideri and P. Seppecher. “A second gradient material resulting from the homogenization of an heterogeneous linear elastic medium”. Continuum Mechanics and Thermodynamics 9.5 (1997). Pp. 241–257.\
[36] G Rizzi, F Dal Corso, D Veber, and D Bigoni. “Identification of second-gradient elastic materials from planar hexagonal lattices. Part II: Mechanical characteristics and model validation”. International Journal of Solids and Structures 176 (2019). Pp. 19–35.\
[37] G. Rizzi, M. Collet, F. Demore, B. Eidel, P. Neff, and A. Madeo. “Exploring metamaterials’ structures through the relaxed micromorphic model: switching an acoustic screen into an acoustic absorber”. Frontiers in Materials 7 (Mar. 2021). P. 589701. issn: 2296-8016. doi: 10.3389/fmats.2020.589701. url: https://www.frontiersin.org/articles/10. 3389/fmats.2020.589701/full (visited on 04/23/2021).\
[38] G. Rizzi, M. V. d’Agostino, P. Neff, and A. Madeo. “Boundary and interface conditions in the relaxed micromorphic model: exploring finite-size metastructures for elastic wave control”. (arXiv:2105.00963) to appear in Mathematics and Mechanics of Solids (2021).\
[39] G. Rizzi, P. Neff, and A. Madeo. “Metamaterial shields for inner protection and outer tuning through a relaxed micromorphic approach”. arXiv preprint arXiv:2111.12001 (2021).\
[40] E. S´anchez-Palencia. “Non-homogeneous media and vibration theory”. Lecture Notes in Physics 127 (1980).\
[41] A. Sridhar, V. Kouznetsova, and M. Geers. “A general multiscale framework for the emergent effective elastodynamics of metamaterials”. Journal of the Mechanics and Physics of Solids 111 (2018). Pp. 414–433. [42] A. Srivastava and S. Nemat-Nasser. “On the limit and applicability of dynamic homogenization”. Wave Motion 51.7 (2014). Pp. 1045–1054.\
[43] A. Srivastava and J. R. Willis. “Evanescent wave boundary layers in metamaterials and sidestepping them through a variational approach”. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 473.2200 (2017). P. 20160765.\
[44] P. Suquet. “Elements of Homogenization for Inelastic Solid Mechanics, Homogenization Techniques for Composite Media”. Lecture Notes in Physics 272 (1985). P. 193.\
[45] D. Tallarico, A. Trevisan, N. V. Movchan, and A. B. Movchan. “Edge waves and localization in lattices containing tilted resonators”. Frontiers in Materials 4 (2017). P. 16.\
[46] P. Wang, F. Casadei, S. Shan, J. C. Weaver, and K. Bertoldi. “Harnessing buckling to design tunable locally resonant acoustic metamaterials”. Physical review letters 113.1 (2014). P. 014301.\
[47] J. R. Willis. “Negative refraction in a laminate”. en. Journal of the Mechanics and Physics of Solids 97 (Dec. 2016). Pp. 10–18. issn: 00225096. doi: 10.1016/j.jmps.2015.11.004. url: https://linkinghub.elsevier.com/retrieve/pii/ S0022509615302623 (visited on 04/23/2021).\
[48] J. Willis. “Bounds and self-consistent estimates for the overall properties of anisotropic composites”. Journal of the Mechanics and Physics of Solids 25.3 (1977). Pp. 185–202.\
[49] J. Willis. “Exact effective relations for dynamics of a laminated body”. Mechanics of Materials 41.4 (2009). Pp. 385–393.\
[50] J. Willis. “Effective constitutive relations for waves in composites and metamaterials”. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 467.2131 (2011). Pp. 1865–1879.\
[51] J. Willis. “The construction of effective relations for waves in a composite”. Comptes Rendus M´ecanique 340.4-5 (2012). Pp. 181–192.\
[52] R. Zhu, X. N. Liu, G. K. Hu, C. T. Sun, and G. L. Huang. “Negative refraction of elastic waves at the deep-subwavelength scale in a single-phase metamaterial”. en. Nature Communications 5.1 (Dec. 2014). P. 5510. issn: 2041-1723. doi: 10. 1038/ncomms6510. url: http://www.nature.com/articles/ncomms6510 (visited on 04/23/2021).(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.]]></content:encoded></item><item><title>How Symmetric and Skew-Symmetric Tensors Interact</title><link>https://hackernoon.com/how-symmetric-and-skew-symmetric-tensors-interact?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:31:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A Most general 4th order tensor belonging to the tetragonal symmetry classConsidering the following quadratic form\
where L is a 4th order tensor and D is a 2nd order one, the most general form of L if it belongs to the tetragonal symmetry class written in Voigt notation is\
where the order of the element of the vector associated with the quadratic form A.1 isIf we now split the tensor D in its symmetric and skew-symmetric part, the corresponding vector in Voigt notation are\
Because of the class of symmetry considered, it is necessary to take into account a mixed constitutive matrix that couples the symmetric and skew-symmetric part of D in order to build back the quadratic form YB Coefficients for the dispersion curves without Curl PC Coefficients for the dispersion curves with Curl P(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.]]></content:encoded></item><item><title>Semi-Analytical Algorithm for Metamaterial Parameter Fitting</title><link>https://hackernoon.com/semi-analytical-algorithm-for-metamaterial-parameter-fitting?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:30:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[8 Summary of the obtained results\
We want to emphasize that the main focus of this work is not the result of the fitting of the three different approaches per se, but the semi-analytical fitting algorithm itself and the underlying consistency of the relaxed micromorphic model with respect to the material properties and some of the geometrical characteristic of the metamaterial that it represents. Using the complex but analytically defined expressions of the asymptotes, we can find a numerical fit of all material parameters by only giving the numerical values computed with Comsol Multiphysics® and the apparent mass density ρ of the unit cell. Note that we only use the cut-offs k = 0 and asymptotes k → ∞ for calculating the material parameters while the shape of the curves for intermediate values of k comes automatically.\
The routine is completely written with Mathematica allowing us to use symbolic calculations. The essential part of the fitting procedure uses the inbuilt algorithm NMinimize (with the Method RandomSearch) to minimize the mean square error of the asymptotes between the relaxed micromorphic model and the numerical values of the finite element approach in Comsol Multiphysics®. Therefore, in general, if a local minimum is found, it is not guaranteed that it corresponds to a global optimum as well.(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.]]></content:encoded></item><item><title>The Science Behind Stronger, Smarter Materials</title><link>https://hackernoon.com/the-science-behind-stronger-smarter-materials?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:30:50 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Again, the cut-offs are independent on the coefficients with higher order of k and thus they do not change with respect to the two previous cases. For the asymptotes we only consider the terms with the highest order of k available and compute\
We have again three asymptotes (the roots of a third order polynomial) which in general causes the analytical expressions to be impractical rather quickly. However, in this case it is possible to find one root by hand(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.]]></content:encoded></item><item><title>A New Method for Predicting How Waves Move Through Materials</title><link>https://hackernoon.com/a-new-method-for-predicting-how-waves-move-through-materials?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:30:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[6 Fitting of the relaxed micromorphic parameters with curvature (with Curl P)Because the cut-offs are independent of the coefficients with higher order of k, they do not change with the addition of Curl P. Instead, the expressions of the asymptotes hugely differ compared to the expression without Curl P discussed before. We only include the terms with the highest order of k available and compute\
Surprisingly, the asymptotes with Curl P are significantly simpler because we must only solve a second-order polynomial instead of a third-order polynomial needed for the cut-offs and the asymptotes without Curl P. We now only have four distinct horizontal asymptotes (two shear and two pressure) in contrast to six before, which means that we must allow that the two curves (one shear and one pressure) will tend to infinity for high values of k, and our choice falls on the two highest optic curves. The same reasoning about the use of the asymptote in Section 5.1 is applied here besides for the two highest optic curves that do not have a horizontal asymptote.\
Although the expressions of the asymptotes are different from the ones without the Curl P, we still have the same split between the parameters, resulting in 4 independent parameters for every group of asymptotes, cf. Table 7.\
We list the numerical values of all parameters used for the fitting of the micromorphic model in Table 8.\
\
Most other values remain at the same magnitude but are slightly higher, cf, Table 9. In future works, we will consider an enhanced relaxed micromorphic model to better describe these effects.(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.]]></content:encoded></item><item><title>Fine-Tuning Acoustic and Optical Waves in Metamaterials</title><link>https://hackernoon.com/fine-tuning-acoustic-and-optical-waves-in-metamaterials?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:30:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Instead of fitting dispersion curves pointwise, we focus on using the analytical expression of the cut-offs (k = 0) and of the asymptotes (k → ∞). The explicit expression of the cut-offs is already discussed in Section 4.3. We use a similar approach to calculate the asymptotes as well by considering the limit k → ∞ where only the terms with the highest order of k are important. Thus, we arrive at\
\
In contrast to the analytical expression of the cut-offs, we are not able to simplify the asymptotes’ expression in a feasible way. This is mainly due to the fact that we must solve a third order polynomial while we only had two non-zero cut-offs each before.\
Since the dispersion curves of the unit cell obtained via Bloch-Floquet analysis are by nature periodic, the limit for k → ∞ is per se meaningless when considering the Bloch-Floquet approach. The value for k = 1/Lc (with Lc the size of the unit cell) is the periodicity limit. On the other hand, this limit has of course meaning for a continuum model like the relaxed micromorphic model. To reconcile these two limits in the fitting procedure, we will impose that the limit k → ∞ for our continuum model will coincide with the periodicity limit of the Bloch-Floquet curves k = 1/Lc. This strategy allows us to preserve the width of the band-gap, cf. Figure 2.We start the fitting with the macroscopic apparent density ρ and values of the macro parameters κM, µM, µ∗ M, i.e. the material constants necessary for the classical homogenization of an infinite large micromorphic material. For an anisotropic Cauchy material, the speed of the acoustic waves is\
\
As a second step, we use the analytical expressions for the cut-offs (4.8) and calculateThe fitting shown in Figure 3 behaves well for all frequencies ω and wavenumber k for zero degrees of incidence but looses some precision for an incidence angle of 45◦ especially for higher values of k. This calls for a further generalization of the relaxed micromorphic model which will be object of following papers. In any case, the achieved overall precision already allows us to explore the dispersive metamaterial’s characteristics at a satisfactory level.\
The absence of higher-order terms (Curl P and Curl P˙) caused the reduction to a single expression k(ω) describing all three dispersion curves in one, cf. equation (5.1). Thus for every frequency ω, there is exactly\
\
one wavenumber k which may be imaginary if the term inside the root is negative. For the plots here, we only show k(ω) where the expression is real-valued and ignore imaginary k(ω) which arise in the band gap and for higher frequencies. Moreover, we cannot have two distinct wavenumbers with the same frequency which implies that all curves are monotonic. For every group of dispersion curves, e.g. the three pressure waves for 45◦ incidence, each individual curve is bounded by the others. Starting with the acoustic curves, their asymptote must be below the cut-off of the lower optic curve of the same type (pressure or shear) while the asymptote of the lower optic curves is bounded from above by the cut-off of the highest optic curve. In particular, self-intersection between two pressure or two shear curves is not possible with this simplified version of the relaxed micromorphic model.\
On the other hand, we observe that for the numerical values from Comsol Multiphysics® the asymptote of the acoustic shear wave at 45◦ should be slightly higher than the cut-off of the lower optic curve with 581.95 Hz and 554.61 Hz, respectively. In addition, assuming that all micro parameters κm, µm, µ∗ m (see Table 6) are larger than their corresponding macro counterparts κM, µM, µ∗ M, we did not manage to generate decreasing dispersion curves as observed for the lower optic pressure wave for an angle of incidence of 45◦ . This effect can instead be achieved when considering mixed space-time derivatives on the micro-distortion tensor P, cf. Section 7.\
When the relative positions of the curves allow to fit the cut-offs and the asymptotes of each curve separately (e.g. for an incidence angle of zero degrees shown here) the simplified version of the relaxed micromorphic model used does indeed shows very good results. We want to emphasize that we only used the limit cases k = 0 (cut-offs) and k → ∞ (asymptotes) for the fitting procedure but have an appreciable approximation for all values of k, ω.(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.]]></content:encoded></item><item><title>How Unit Cell Size Affects Dispersion in Metamaterials</title><link>https://hackernoon.com/how-unit-cell-size-affects-dispersion-in-metamaterials?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:30:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We assume a plane strain[8] time harmonic ansatz for the displacement u and the micro-distortion tensor P\
Substituting the ansatz (3.1) in the equilibrium equations (2.6), we obtain the homogeneous algebraic linear system4 New considerations on the relaxed micromorphic parametersIn this section, we draw some useful considerations about the consistency of the relaxed micromporpic model with respect to a change of unit cell’s size and of the material properties of the base material. The model’s consistency is checked against a standard Bloch-Floquet analysis of the wave propagation performed using the unit cell described in Section 1.1 with built in periodic Bloch-Floquet boundary conditions from Comsol Multiphysics®.\
The following two connections between the properties of the unit cell and the behaviour of the dispersion curves can be drawn:\
• The dispersion curves scale proportionally in ω with respect to the speed of the wave of the bulk material composing the unit cell;\
• The dispersion curves scale inversely in both ω and k with respect to the size of the unit cell.\
Both results are useful to avoid repeating the time-consuming fitting procedure when changing the size of the cell and the base material’s properties while keeping the unit cell’s geometry unchanged.4.1 Consistency of the relaxed micromorphic model with respect to a change in the unit cell’s bulk material properties4.2 Consistency of the relaxed micromorphic model with respect to a change in the unit cell’s sizeWhile keeping the geometry and the material unaltered, the dispersion properties of a microstructured isotropic Cauchy material are inversely proportional to the size of its unit cell, meaning that halving the size of the unit cell will double the frequency response for each value of the length k of the wavevector, which also changes with the same inverse proportionality since it represents the spatial periodicity of the structure. This can be easily retrieved by performing standard Bloch-Floquet analysis.4.3 Relaxed micromorphic cut-offsThe cut-offs of the dispersion curves play an important role in fitting the material parameters of the relaxed micromorphic model [33, 17, 37]. For the convenience of the reader, we show the calculations of the analytic expressions again. In the case k = 0, the dispersion relation (3.6) simplifies into\
\
Equations (4.7) can be simplified as in Table 2 with\
\
The values of theses cut-offs have been fixed according to ®simulations as\
\
and the values of last points from ®are used to fix the asymptotes, cf. Table 3.(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.]]></content:encoded></item><item><title>Modeling Finite-Size Metamaterials with Relaxed Micromorphic Theory</title><link>https://hackernoon.com/modeling-finite-size-metamaterials-with-relaxed-micromorphic-theory?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:30:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
for the classical Cauchy model, and\
for the relaxed micromorphic model, where we set\
The Neumann boundary condition for the classical Cauchy model are2.1 Tetragonal Symmetry / Shape of elastic tensors (in Voigt notation)[7] We write “m” for “micro” and “M” for “macro” for the corresponding elastic parameters to shorten the following expressions.(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.]]></content:encoded></item><item><title>New Material Design Approach Could Change How Sound Travels Through Space</title><link>https://hackernoon.com/new-material-design-approach-could-change-how-sound-moves-through-space?source=rss</link><author>Labyrinthine</author><category>tech</category><pubDate>Sun, 23 Mar 2025 17:30:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Jendrik Voss, Institute for Structural Mechanics and Dynamics, Technical University Dortmund and a Corresponding Author (jendrik.voss@tu-dortmund.de);(2) Gianluca Rizzi, Institute for Structural Mechanics and Dynamics, Technical University Dortmund;(3) Patrizio Neff, Chair for Nonlinear Analysis and Modeling, Faculty of Mathematics, University of Duisburg-Essen;(4) Angela Madeo, Institute for Structural Mechanics and Dynamics, Technical University Dortmund.We present an inertia-augmented relaxed micromorphic model that enriches the relaxed micromorphic model previously introduced by the authors via a term Curl P˙ in the kinetic energy density. This enriched model allows us to obtain a good overall fitting of the dispersion curves while introducing the new possibility of describing modes with negative group velocity that are known to trigger negative refraction effects. The inertia-augmented model also allows for more freedom on the values of the asymptotes corresponding to the cut-offs. In the previous version of the relaxed micromorphic model, the asymptote of one curve (pressure or shear) is always bounded by the cut-off of the following curve of the same type. This constraint does not hold anymore in the enhanced version of the model. While the obtained curves’ fitting is of good quality overall, a perfect quantitative agreement must still be reached for very small wavelengths that are close to the size of the unit cell.Metamaterials are materials whose mechanical properties go beyond those of classical materials thanks to their heterogeneous microstructure. They can show unusual static/dynamic responses such as negative Poisson’s ratio [27], twist or bend in response to being pushed or pulled [18, 36], band-gaps [28, 46, 8, 13], cloaking [11, 31], focusing [20, 16], channeling [24, 45], negative refraction [52, 25, 47], etc. The working frequency of each metamaterial strongly depends on the characteristic size and the geometry of the underlying unit cell, as well as on the choice of the base material. In this paper, we present a labyrinthine metamaterial that, thanks to the use of a polymeric based material and an optimized distribution of mass inside the unit cell (see Figure 1), gives rise to a wide acoustic band-gap with characteristic unit cell’s size of the order of centimeters.\
The direct finite element modeling of structures build up with this labyrinthine metamaterial is unfeasible due to the extremely tight meshing that would be needed to correctly cover the narrow strips of material inside each unit cell. It is thus apparent the need for a homogenized model to use this type of very promising metamaterials in actual engineering designs. Various homogenization techniques have been developed with the purpose of providing rigorous predictions of the macroscopic metamaterial’s mechanical response when the properties of the base materials and their spatial distribution are known. These homogenization approaches have been shown to be useful in describing the overall behavior of metamaterials in the static and quasi-static regimes [6, 40, 3, 30, 21, 48, 35, 9, 12, 44, 29, 19, 22] as well as, more recently, in the dynamic regime [5, 14, 10, 15, 4, 23, 49, 50, 51, 42, 41, 43]. However, these models are often unsuited to deal with finitesize metamaterials, because they are based on upscaling techniques valid for unbounded media. Because of that, finite-size metamaterials’ structures are mostly investigated via Finite Element simulations which are performed using directly the microstructured material, e.g. [26]. The downside of this approach is that the computational cost quickly becomes unsustainable (especially for unit cells as the one presented in this paper), although the propagation patterns obtained are very accurate. This heavily limits the possibility of exploring large-scale or very convoluted geometric meta-structures.\
To overcome this problem and open up the possibility of designing complex meta-structures using the metamaterial presented in this paper as a basic building block, we propose to use an inertia-augmented relaxed micromorphic model. This model is based on the relaxed micromorphic model that we previously established [34, 32, 17, 1, 2] and has been augmented with a new inertia term accounting for coupled space-time derivatives of the micro-distortion tensor. The relaxed micromorphic model has extensively proven its efficacy in describing the broadband behavior of many infinite and finite-size metamaterials [1, 2, 37, 38, 39] and is extended in this paper so as to be able to account for negative group velocity which was not the case before. We will show that the proposed model is able to describe well the labyrinthine metamaterial’s response for a large range of frequencies (going beyond the first band-gap) and wave numbers (approaching the size of the unit cell) and for all directions of propagation with a limited number of frequency- and scale-independent constitutive parameters. The new inertia-augmented term will be shown to trigger modes with negative group velocities that are known to be associated with negative refraction phenomena. The results presented in this paper will allow us to shortly present new designs of finite-size labyrinthine metamaterials’ structures that can control elastic energy in the acoustic regime for eventual subsequent re-use.In this section, we present a new unit cell’s design that gives rise to a metamaterial for acoustic control. This unit cell is designed to achieve a band-gap at relatively low frequencies (600−2000 Hz) so that application for acoustic control can be targeted. The unit cell considered is made out of polyethylene, cf. Table 1. Compared to Aluminium or Titanium, that we used for the metamaterials studied in [37, 38, 39], Polyethylene gives rise to lower wave speeds, thus allowing band-gap phenomena to appear at lower frequencies.\
\
A further lowering of the band-gap is obtained through the adoption of a labyrinth-type geometry, cf. Figure 1. This structure presents a tetragonal symmetry and thus features a reduced number of parameters with respect to a fully anisotropic system. The circular center of the unit cell is connected by thin bars allowing the heavier center to move easily, thus giving rise to local resonance phenomena of relatively low frequencies while additionally providing a very soft macro-material behaviour.]]></content:encoded></item><item><title>US Security Agencies Halt Coordinated Effort to Counter Russian Sabotage and Cyberattacks</title><link>https://yro.slashdot.org/story/25/03/22/1824242/us-security-agencies-halt-coordinated-effort-to-counter-russian-sabotage-and-cyberattacks?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Reuters reported this week that several U.S. national security agencies "have halted work on a coordinated effort to counter Russian sabotage, disinformation and cyberattacks..."


The plan was led by the president's National Security Council (NSC) and involved at least seven national security agencies working with European allies to disrupt plots targeting Europe and the United States, seven former officials who participated in the working groups told Reuters... [S]ince Trump took office on January 20 much of the work has come to a standstill, according to eleven current and former officials, all of whom requested anonymity to discuss classified matters... Regular meetings between the National Security Council and European national security officials have gone unscheduled, and the NSC has also stopped formally coordinating efforts across U.S. agencies... 

The FBI last month ended an effort to counter interference in U.S. elections by foreign adversaries including Russia and put on leave staff working on the issue at the Department of Homeland Security. The Department of Justice also disbanded a team that seized the assets of Russian oligarchs... Department of Homeland Security Assistant Secretary Tricia McLaughlin told Reuters the agency had placed on administrative leave personnel working on misinformation and disinformation on its election security team, without elaborating further.
]]></content:encoded></item><item><title>Decentralized: Bob Wazneh’s Vision for AI, Blockchain, and Web3</title><link>https://hackernoon.com/decentralized-bob-waznehs-vision-for-ai-blockchain-and-web3?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Sun, 23 Mar 2025 16:30:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Many people find it challenging to keep up with the rapid pace of technological advancements, especially in fields such as AI, blockchain, and Web3.  not only understands these emerging technologies, but he has consistently leveraged them to drive success.\
From his early days in IT and marketing to his current leadership role in the Web3 space, the Lebanese Wazneh has built a career focused on staying ahead of the curve. His ability to adapt and guide companies through the ever-evolving tech landscape reveals his understanding of how technology can shape the future.Wazneh's career trajectory significantly turned in 2019 when he co-founded a growth hacking agency. Within six months, it scaled into a seven-figure business. However, his true impact would come when he transitioned into the Web3 and AI spaces. By 2021, Wazneh had founded , which incubates and accelerates over 500 ventures. IBC Ventures has been valued at over $500 million.\
Since then,  has established himself as a leading voice in the blockchain and Web3 space. He has invested in and advised several companies. His leadership role at CJN (Citizen Journalism Network) is a prime example of his forward-looking mindset. Under Wazneh’s guidance, the network grew from zero to 4 billion monthly views on X. \
"Seeing how much influence decentralized media has on global conversations, I've come to believe that Web3 and blockchain can empower individuals and provide financial freedom in ways traditional systems never could,"  explains.Philanthropy: Giving Back and Offering WisdomBeyond his business ventures, Wazneh is deeply committed to philanthropy. He actively supports startups and individuals in need—particularly those in financially unstable regions.\
"I've always believed that my success should be leveraged to uplift others," he says.\
From mentoring young entrepreneurs to paying off debts for struggling families in Lebanon,  ensures that his success serves a higher purpose.\
His contributions also include much-needed advice for aspiring entrepreneurs. Wazneh insists they'll succeed if they:: Read and learn from those who have successfully navigated the path you're considering.Embrace failure as a lesson: Even failure provides invaluable lessons that will guide your next venture.: Ensure there's a demand for your idea by checking its trends, search volume, and potential market before taking action. Stay open to change, and avoid letting your ego prevent you from adjusting when necessary.Resilience in the Face of AdversityThe resilient  faced significant setbacks in his early entrepreneurial years, including failed affiliate marketing and real estate ventures. However, He views failure as a stepping stone to success: "Failure taught me invaluable lessons about resilience, adaptability, and innovation." His ability to work through adversity—particularly in volatile markets like crypto—has shaped his leadership style.\
 personal experience with a life-threatening accident also shaped his approach to life and business.\
"The recovery process was arduous, but it taught me the importance of perseverance and maintaining a long-term perspective," he insists.\
This experience fueled his dedication to helping others facing similar struggles.The Road Ahead: Empowering the FutureAs  continues leading in Web3, AI, and decentralized technologies, his vision includes expanding his impact by helping traditional businesses transition into the digital age: "I want to assist businesses in integrating Web3 and blockchain technologies, helping them scale and grow in a sustainable way."\
With a focus on creating opportunities and advocating for decentralized freedom,  future is as ambitious as it is transformative. His story proves that with resilience, integrity, and a passion for inventiveness, people can change industries—and positively impact society.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Can AI Really Code? I Put DeepSeek to the Test (3/23/2025)</title><link>https://hackernoon.com/3-23-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Sun, 23 Mar 2025 16:05:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, March 23, 2025?By @filestack [ 7 Min read ] I remember the day I first started experimenting with DeepSeek for coding. It felt like stepping into a new dimension where code could almost write itself. Read More.By @hackernooncontests [ 3 Min read ] Join the Web3 Development Writing Contest by GetBlock  HackerNoon! Write about blockchain APIs, dApp development  more for a chance to win from $5,000. Read More.By @dineshbesiahgari [ 17 Min read ] AutoResponder AI automates email replies using AWS  AI. Streamline inbox management, boost productivity, and stay responsive with this serverless Gmail assista Read More.By @dineshbesiahgari [ 4 Min read ] Explore the AI Knowledge Ark, a visionary initiative to safeguard humanity’s intellectual and cultural legacy. Learn how AI ensures knowledge survival, inspired Read More.By @moonlock [ 5 Min read ] Is Netflix’s Zero Day realistic? A malware researcher breaks down 3 real cyber threats and 3 myths, separating cybersecurity fact from fiction. Read More.By @vitaliikuzmenko [ 3 Min read ] Meet Leadige LLC, Startups of the Year 2024 Nominee—Where Strategy, Creativity  Data Drive Revenue. Read More.By @techietales [ 4 Min read ] The upcoming launch of $SESH represents a significant milestone for Session, as it seeks to create a sustainable and decentralized messaging ecosystem. Read More.By @leonadato [ 12 Min read ] No matter how inexpensive a monitoring vendors prices seem, if you dont have a plan for your data, any cost can seem like its too much. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>How Fraction AI is Democratizing AI Ownership for the Masses</title><link>https://hackernoon.com/how-fraction-ai-is-democratizing-ai-ownership-for-the-masses?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Sun, 23 Mar 2025 15:35:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In this exclusive interview for HackerNoon’s "Behind the Startup" series, we sit down with Shashank Yadav, the founder and CEO of Fraction AI, a platform that empowers users to train and own their own AI models. With a background in AI and experience in core machine learning teams at Goldman Sachs and Microsoft, Shashank shares his journey, insights on the challenges of scaling AI, and how Fraction AI is tackling the industry's biggest bottleneck – reliable, high-quality data. Get an inside look into how Fraction AI is disrupting the AI landscape and democratizing AI ownership.: Hey Ishan, great to be here. I’m Shashank, founder of Fraction AI. My background is in AI. I studied computer science at IIT Delhi with a focus on AI research. After that, I worked on the core ML team at Goldman Sachs, then joined an early-stage startup as an AI researcher, and later moved to a hedge fund applying AI to quantitative trading.\
The problem I kept running into was that AI was becoming centralized. A few companies controlled the most powerful models, and everyone else was stuck using off-the-shelf versions that weren’t tailored to their needs. But AI isn’t one-size-fits-all. A lawyer needs a different model than a trader or a developer. The best AI is specialized, yet training your own was either too expensive or too complex.\
That’s why I started Fraction AI. It’s a platform where anyone can own and train their own AI models. Users create AI agents that compete in sessions. Each agent pays a small entry fee, generates the best possible output for a task, and gets judged by an LLM. Winners earn rewards, and their models improve based on their best outputs. Over time, users build highly specialized AIs that keep getting better.\
Instead of relying on a few big models, we’re creating an ecosystem where thousands of smaller, specialized models compete, learn, and grow. AI shouldn’t just be something you use. It should be something you own and improve. That’s what we’re building.Ishan Pandey: You’ve worked in core ML teams at Microsoft and Goldman Sachs. How did those experiences shape your approach to building Fraction AI?: Yeah, during college, I interned at Microsoft on the Bing team, working on machine learning for search ranking. That was my first real exposure to large-scale AI systems. Search isn’t just about finding information, but understanding what users really want and ranking results effectively. It taught me that AI isn’t just about smart models, it’s about making them work in the real world.\
At Goldman Sachs, I was on the core ML team, building models for financial predictions. In finance, even small improvements matter, and models are constantly tested in real-world conditions where mistakes are costly. That experience taught me how to build AI that is reliable, adaptable, and improves over time instead of just performing well in a controlled setting.\
Later, at a hedge fund, I worked on AI for quant trading. That’s where I saw how powerful competition can be. Models that continuously adapt and learn from competing strategies tend to perform better than those that stay static.\
All of that shaped Fraction AI. Instead of building one perfect AI, we created a system where AI agents compete, learn, and improve based on real-world feedback. The best AI isn’t designed in isolation - it evolves by constantly testing itself against others. That’s the idea behind Fraction AI.: Yeah I stand strongly by that statement. Current AI models have already seen most of the internet. More compute won’t help if there’s nothing new to learn from. The real challenge is getting fresh, high-quality data. DeepSeek figured this out and trained a model using pure reinforcement learning instead of traditional datasets. They realized you can’t just keep fine-tuning on the same old data, you need a system that generates new, useful information.\
We’re taking that idea further with Fraction AI. Instead of relying on static datasets, we let AI agents compete in real-world tasks. The best outputs get judged, refined, and used to improve the next generation of models. It’s decentralized and constantly evolving. AI should belong to everyone, not just a few companies. The best way to make that happen is to create a system where people train and improve their own models by generating new, high-quality data. Instead of AI being locked away, it keeps evolving through real-world use. The biggest misconception is that scaling AI is just about throwing more compute at bigger models. That worked in the past, but we’ve hit a wall, more parameters don’t automatically mean better results. The real bottleneck now is data, not compute. Another mistake is thinking AI is static. Many companies fine-tune a model once and assume it’s “done.” But AI isn’t like software, it needs to keep learning from new data to stay relevant. If your AI isn’t continuously improving, it’s falling behind.\
Fraction AI fixes this by making AI self-improving. Instead of training a model once and hoping it works forever, we create a system where AI agents constantly compete, learn from their best outputs, and evolve in real time. It’s not just about scaling models, it’s about scaling learning. The future of AI isn’t about building the biggest model. It’s about creating systems that can grow on their own. That’s what we’re building. The biggest challenge was shifting from solving technical problems to running an actual company. In big tech, you focus on building models, but as a founder, you have to think about everything - product, users, funding, and making sure what you build actually matters.\
I spent a lot of time watching Y Combinator courses to understand how to build and scale a startup. IIT Delhi has a huge entrepreneurship culture, so I had a lot of people to look up to who had already taken the leap. That gave me confidence that it was possible. Becoming a Nailwal Fellow was also a game-changer. Sandeep Nailwal, co-founder of Polygon, is one of the most respected guys in Web3, and getting his guidance was incredibly valuable. He understands how to build in an open, decentralized way while still making things work at scale.The hardest part of starting a company isn’t the tech, it’s figuring out how to turn your vision into something real, something people actually use. Learning from others who’ve done it before made a huge difference.: Fraction AI is built around the idea that AI should improve itself through competition and real-world use. Instead of relying on static datasets, we create a system where AI agents generate, refine, and improve data at scale. Here’s how it works: Users create AI agents, each with its own system prompt and tuning. These agents compete in sessions where they generate outputs for a given task. Their responses are scored by an LLM judge, and the best-performing agents earn rewards. This process repeats continuously, creating a feedback loop where AI models improve over time.\
But we don’t just collect data - we fine-tune the models too. The best outputs from these competitions are fed back into the training process, helping agents evolve and specialize. Over multiple sessions, users can upgrade their models, making them smarter and better suited to their specific tasks.\
This creates a scalable system for high-quality data collection and model improvement. Instead of relying on pre-existing datasets, AI agents generate fresh, relevant data that’s validated in real-time. The result is an ecosystem where AI isn’t static - it’s always learning, always improving.Ishan Pandey: What advice would you give to AI startups trying to navigate the balance between innovation, sales, and funding? The key is timing. In the early days, focus on innovation and sales at the same time - you need just enough product to prove people want it, but you also have to start selling early. Don’t wait for perfection. If you can’t get someone to pay for it, it’s probably not solving a real problem.\
Once you have even a small proof of demand, raise funds as soon as possible. You need to survive long enough to build something great. A lot of startups fail because they focus too much on the product without securing enough runway. Don’t focus too much on dilution at this point, startups are a zero or one game anyways.\
After fundraising, it becomes all about sales and continuous innovation. Keep improving the product while scaling up revenue. If you can keep selling and keep pushing the tech forward, you’ll stay ahead.\
In short: Prove demand → Raise fast → Scale sales while improving the product.:::tip
Vested Interest Disclosure: This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Raspberry Pi Announces New Tool for Customized Software Images</title><link>https://build.slashdot.org/story/25/03/23/0012253/raspberry-pi-announces-new-tool-for-customized-software-images?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["For developers and organisations that require a custom software image, a flexible and transparent build system is essential," according to an announcement Friday at Raspberry Pi.com. 

"[T]o support these customers, we have created rpi-image-gen, a powerful new tool designed to put you in complete control of your Raspberry Pi images."

If you're building an embedded system or an industrial controller, you'll need complete control over the software resident on the device, and home users may wish to build their own OS and have it pre-configured exactly the way they want... rpi-image-gen is an alternative to pi-gen, which is the tool we use to create and deploy the Raspberry Pi OS distribution. rpi-image-gen... offers a very granular level of control over file system construction and software image creation... [B]eing able to help reduce software build time, provide guaranteed ownership of support, and reuse standard methodologies to ensure authenticity of software were all of paramount importance, and among the reasons why we created a new home-grown build tool for Raspberry Pi devices... 


There is a small number of examples in the tree which demonstrate different use cases of rpi-image-gen [including the lightweight image slim and webkiosk for booting into browser kiosk mode]. All create bootable disk images and serve to illustrate how one might use rpi-image-gen to create a bespoke image for a particular purpose. The number of examples will grow over time and we welcome suggestions for new ones... Visit the rpi-image-gen GitHub repository to get started. There, you'll find documentation and examples to guide you through creating custom Raspberry Pi images.
 
Some technical details from the announcement.

"Similar to pi-gen, rpi-image-gen leverages the power, reliability, and trust of installing a Debian Linux system for the device. However, unlike pi-gen, rpi-image-gen introduces some new concepts [profiles, image layouts, and config] which serve to dictate the build footprint and installation."

The tool also lets you exclude from your package "things that would otherwise be installed as part of the profile."
The tool's GitHub repository notes that it also allows you output your software bill of materials (SBOM) "to list the exact set of packages that were used to create the image." And it can even generate a list of CVEs identified from the SBOM to "give consumers of your image confidence that your image does not contain any known vulnerabilities."
]]></content:encoded></item><item><title>Barack Obama joins Bluesky</title><link>https://techcrunch.com/2025/03/23/barack-obama-joins-bluesky/</link><author>Sarah Perez</author><category>tech</category><pubDate>Sun, 23 Mar 2025 15:20:11 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Social network Bluesky, an alternative to X built on open source technologies, has scored a big win in terms of attracting notable users to its platform. The company on Sunday confirmed that former president Barack Obama has joined its service. Bluesky COO Rose Wang replied to a post where someone wondered if the account posting […]]]></content:encoded></item><item><title>&apos;This Is the Sharpest Image Yet of Our Universe As a Baby&apos;</title><link>https://science.slashdot.org/story/25/03/22/0542234/this-is-the-sharpest-image-yet-of-our-universe-as-a-baby?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 14:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[ Science magazine reports:
A strange-looking telescope that scanned the skies from a perch in northern Chile for 15 years has released its final data set: detailed maps of the infant universe showing the roiling clouds of hydrogen and helium gas that would one day coalesce into the stars and galaxies we see today. 


The Atacama Cosmology Telescope is not the first to survey the cosmic microwave background (CMB), the light released 380,000 years after the Big Bang when the early universe's soup of particles formed atoms and space became transparent. But the data — posted as preprints online today — give researchers a new level of detail on the density of the gas clouds and how they were moving. 
At the top of the page for Science's article is an image where different colors "show areas where the polarization of the CMB light — its direction of vibration — differ, revealing how gases first move tangentially around areas of higher density (orange) and later fall straight in (blue) under the influence of gravity." 

Long-time Slashdot reader sciencehabit writes:
Using the data, researchers tested how well the standard cosmological theory, known as lambda cold dark matter, described the universe at that time 13.8 billion years ago; it's a remarkably good fit, they conclude. 
The article notes that "back in the Chilean desert," the Atacama Cosmology Telescope's successor, the Simons Observatory, has already taken its first image, and "will begin its even more detailed examination of the CMB in the coming months."]]></content:encoded></item><item><title>Ripple in Time: Is XRP About to Go Parabolic in 2025?</title><link>https://hackernoon.com/ripple-in-time-is-xrp-about-to-go-parabolic-in-2025?source=rss</link><author>Dmytro Spilka</author><category>tech</category><pubDate>Sun, 23 Mar 2025 14:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[While the wider cryptocurrency market is having to contend with high volatility and widespread uncertainty, XRP appears to be thriving. With the SEC dropping its long-standing appeal against Ripple, could it finally live up to its potential?\
The Securities and Exchange Commission first opened its lawsuit against Ripple in December 2020 before XRP reached a low of $0.16 on January 2, 2021. By January 18, 2025, XRP was trading at $2.63, and with the SEC ending its interest in the coin, there’s plenty of optimism for Ripple to rally higher.\
“Today marks a historic victory—for Ripple, our employees and customers, and for the entire crypto industry,” , Ripple CEO in a statement. “After more than four years of litigation, the SEC will drop its appeal against Ripple, concluding the SEC’s 4+ year long lawsuit. This is subject to Commission vote and approval.”\
Much has been made of Ripple’s impressive technical attributes. Notably, its Ripple Protocol Consensus Algorithm, which validates transactions through designated servers until they reach a supermajority agreement.\
It’s this algorithm that’s helped to make Ripple far more functional than other major cryptocurrencies like Bitcoin and has paved the way for transactions to be processed in just three to five seconds while handling up to 1,500 transactions per second.\
Crucially, it’s this high-speed blockchain network that gives Ripple plenty of potential use cases as cryptocurrency continues its emergence into mainstream adoption.\
But could removing the shackles of SEC scrutiny just be the beginning of Ripple’s journey? Let’s take a deeper look at the coin’s prospects for 2025 and beyond:Functionality is Ripple’s USPNot many cryptocurrencies that were launched in 2012 possess blockchains that are flexible enough to transform the fintech landscape, but Ripple is an excellent asset when it comes to fast cross-border payments.\
When institutions use Ripple to make a payment overseas, XRP acts as a  to optimize currency transfers in a fast and low-cost manner. Whether this means performing a direct currency exchange or using XRP as an intermediary, the flexibility Ripple provides can actively help reduce transaction costs without the need for appointing middlemen to handle the task.\
Many forecasters suggest that cryptocurrency will become  into mainstream usage thanks to more regulatory clarity from the SEC’s new acting director Mark Uyeda, Ripple could be one of the key beneficiaries of the many functional cryptocurrencies throughout the ecosystem.\
In fact, LMAX Group recently incorporated XRP into its institutional trading infrastructure, intending to offer institutions  to highly liquid assets throughout markets.\
LMAX, which provides institutional execution venues for forex and digital assets trading, also acquired FX HedgePool, an institutional swaps matching service, and appears to have highlighted Ripple as an effective resource for boosting efficiency.\
“Ripple’s utility in lowering transaction costs for international payments makes it a potential leader in financial systems and fintech,” explains Maxim Manturov, head of investment advice at Freedom24. “If Ripple's global partnerships continue to expand, the price of XRP could reach $5-$8.”Despite a strong start to 2025 and a flow of good news, XRP is showing signs of strain amid more widespread bearish market sentiment.\
Worryingly, some  believe that market pressures could push Ripple below $2 if support barriers are retested amid an increasingly volatile outlook for crypto.\
One of the challenges of crypto’s emergence into mainstream adoption is that the landscape is more vulnerable to Wall Street price movements, and wider macroeconomic pressures could stunt XRP’s growth in 2025 as investment banks like Goldman Sachs  for the year ahead.\
It’s also worth looking at a decline in XRP funding rates, which have been  in recent weeks as traders open short positions following more widespread crypto market challenges.\
Funding rates refer to the periodic payments between traders holding long and short positions to keep the price of perpetual futures contracts linked to the underlying asset they’re tracking.\
However, a cause for optimism is that XRP’s bearish sentiment of late is far less pronounced than the likes of other popular cryptocurrencies like Bitcoin (BTC) and Ethereum (ETH), indicating that an upturn in sentiment could see Ripple outperform the wider market.Whether Ripple goes parabolic in 2025 or not will likely depend more on the state of the wider economic outlook in the United States and beyond. However, evidence of the incorporation of cryptocurrencies into the mainstream financial sector is likely to be a catalyst for growth for functional assets like XRP.\
Crypto’s push towards the mainstream will transform the industry and can help to provide more exposure to its brightest projects. In terms of payment efficiency, an emboldened Ripple that’s free from the SEC’s shackles could form a formidable force.\
The future is difficult to predict amid wider crypto market volatility, but it’s difficult to think of a more exciting prospect in the cryptocurrency space right now than a resurgent Ripple.]]></content:encoded></item><item><title>Browser Use, the tool making it easier for AI ‘agents’ to navigate websites, raises $17M</title><link>https://techcrunch.com/2025/03/23/browser-use-the-tool-making-it-easier-for-ai-agents-to-navigate-websites-raises-17m/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Sun, 23 Mar 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[We may not have an agreed-upon definition of AI “agent” yet, but a multitude of startups want to create “agentic” tools to automate various tasks online. One such firm, Browser Use, has attracted a ton of interest from developers and investors thanks to its solution that makes websites more “readable” for AI agents. Browser Use […]]]></content:encoded></item><item><title>&apos;Wired&apos; Drops Paywalls for Articles Based on Public Records Requests, Urges Other Sites to Follow</title><link>https://news.slashdot.org/story/25/03/23/0324221/wired-drops-paywalls-for-articles-based-on-public-records-requests-urges-other-sites-to-follow?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 11:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Wired's web site "is going to stop paywalling articles that are primarily based on public records obtained through the Freedom of Information Act," their global editorial director announced this week:

They're called public records for a reason, after all. And access to public documents is more important than ever at this moment, with government websites and records disappearing... [S]ome may argue that, from a business standpoint, not charging for stories primarily relying on public records automatically means fewer subscriptions and therefore less revenue. We disagree. 

Sure, the FOIA process is time- and labor-intensive. Reporters face stonewalling, baseless denials, lengthy appeals processes, and countless other obstacles and delays. Investigative reports based on public records are among the most expensive stories to produce and share with the public... But while some readers might not subscribe to outlets that give away some of their best journalism for free, it's just as possible that readers will recognize this sacrifice and reward these outlets with more traffic and subscriptions in the long run... 

We hope others will follow Wired's lead (and shoutout to outlets like 404 Media that also make their FOIA-based reporting available for free). We also hope those who stand to benefit from these outlets' leadership (that's you, reader) will do their part and subscribe if you can afford it. They're not asking for an arm and a leg... The Fourth Estate needs to step up and invest in serving the public during these unprecedented times. And the public needs to return the favor and support quality journalism, so that hopefully one day we can do away with those annoying paywalls altogether.
]]></content:encoded></item><item><title>You Can Help Us Investigate Surveillance Marketing Using Facebook Data: Here&apos;s How</title><link>https://hackernoon.com/you-can-help-us-investigate-surveillance-marketing-using-facebook-data-heres-how?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sun, 23 Mar 2025 09:00:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Surveillance marketers are changing how they track you online, and The Markup is joining forces with Consumer Reports to investigate. But we need your help.\
Now, we need your help again. Instead of relying on tracking pixels—which is web traffic that The Markup, Consumer Reports, and others can detect using tools in the browser—companies may now be tracking you in a way that’s completely undetectable by users and their devices.\
It’s called “server-to-server tracking,” which essentially means that when companies get some information about you, their servers send it directly to another company’s server. Privacy research tools often rely on certain signals from your computer, mobile device, or browser to detect tracking. But “server-to-server tracking” doesn’t emit any of these sorts of signals.\
We need a new way to see what companies are up to with your data, and we have reason to believe that starting with what they’re sharing with Facebook will get us there. And that’s where you come in.Share Your Facebook Data with UsWe need your help seeing what companies have sent to Facebook about you, including what they’ve sent using server-to-server tracking. (We’ll likely also see data that was sent through the Meta Pixel.) We’ll walk you through how to download your data from Facebook and share it with us. Then, Consumer Reports will use two specific parts of your data file: These include “events” reported to Facebook by other companies, including companies that have their servers tell Facebook’s servers about something that you did—for example, if you tapped on a button on a company’s mobile app, added an item to your cart or wishlist, or bought something in its physical store. Companies that use this feature will recognize it as Facebook’s Conversions API.Facebook custom audiences that include you. These are lists of email addresses or phone numbers that companies upload to Facebook. Facebook advertises this as a way for companies to target their ads to people who are already in their “existing audiences” or if Facebook thinks you are similar to those people. For example, a company could upload its newsletter subscriber list.The entire process should only take you 5 to 10 minutes. It involves downloading some data from Facebook.Step 1: Sign up as a volunteerVisit Consumer Reports’ Facebook tracking volunteer sign-up page.Use the form on the page to sign up as a volunteer.Follow the full instructions, including how to download your Facebook data, in the Google Form you’ll see after signing up. These instructions will also be emailed to you.Step 2: Fill out the Google Form and download your Facebook dataThe Google Form will ask for your email address and for you to read through the consent form. You’ll be consenting to sharing the data that Facebook has about you but not your personal content, photos, or messages. Any personally identifying information will be kept confidential. We dig into exactly what data is being collected below.Then, it will guide you through downloading a subset of your data from Facebook and uploading it to the form.It will ask you some optional survey questions and give you space to share any feedback.Finally, submit your data so researchers and journalists at The Markup and Consumer Reports can use it to investigate the new ways companies are tracking users like you. That’s it! The more people who help, the more we’ll understand about this new tracking technique. Even if you haven’t logged in to Facebook in years, you can help.\
While we won’t know demographic information about you, we especially encourage you to contribute your data if you fall into one of the categories below, because digital advertisers heavily target members of these categories (so we’re likely to get high-quality data) and because we’re really interested in seeing the names of companies targeting ads to these groups.Parents with kids who are 18 or youngerAnyone over the age of 65Women between the ages of 18 and 35People who have linked their Facebook account to a financial service or health care service (for example, if you use your Facebook account to log in)What If I’ve Never Had a Facebook Account?We already know that the Meta Pixel collects information about you even if you don’t have a Facebook account. The same applies here. Even if you don’t have a Facebook account, Facebook probably still has data about you—but you won’t be able to download it using our instructions for this investigation. However, we’re really interested in looking into this in the future.\
If you’ve never made a Facebook account before and want to help us investigate what data Facebook has about you, send a note to hello@themarkup.org or community@cr.consumer.org, and we’ll contact you if we do a follow-up investigation.What Data Is Being CollectedConsumer Reports collects:The name of the company that sent shared information about you to FacebookDates and times that information was sharedIdentifiers for type of data shared, such as when you ran an app or made a purchaseYour zip code, from the optional surveyWhether you use certain privacy tools while browsing online, from the optional surveyThe name and photo associated with your Google account. Google Forms collects this data in order to allow users to upload their data securely, but Consumer Reports will not use this information and will not associate it with the Facebook data you upload in any way. You can also avoid sharing this information by creating a new Google account for the purpose of uploading your data.Consumer Reports will anonymize the data so it can’t extract any specific data about you, and it will store and process your Facebook data separately from any of your personal information.\
If you’re a Facebook user, Consumer Reports will not collect your posts, including messages or photos—the download guide explains how to get the data Facebook has on you without downloading your personal content in the process. Additionally, even if you ultimately upload your personal content, Consumer Reports will only pull the data listed above for this study.\
You’ll be able to contribute data going forward, but Consumer Reports will start analyzing the data on Aug. 14, and we’ll be looking to publish stories in the coming months.\
We hope you can join us. We’re excited to see what we can find.]]></content:encoded></item><item><title>NASA Considers Eliminating Its Headquarters in Washington D.C.</title><link>https://science.slashdot.org/story/25/03/22/223207/nasa-considers-eliminating-its-headquarters-in-washington-dc?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 07:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[NASA is considering "closing its headquarters and scattering responsibilities among the states," reports Politico, citing two people familiar with the plan.

"The proposal could affect up to 2,500 jobs and redistribute critical functions, including who manages space exploration and organizes major science missions."
While much of the day-to-day work occurs at NASA's 10 centers, the Washington office plays a strategic role in lobbying for the agency's priorities in Congress, ensuring the White House supports its agenda and partnering with foreign countries on critical space projects. Some of the headquarter's offices might remain in Washington, the people said, but it's not clear which ones those would be or who would keep their jobs... 

One of the biggest fallouts is the damage it could do to coordination among NASA leadership on pressing issues... It would also limit cooperation with international partners on space, which is often done through embassies in Washington. NASA works with foreign partners on a range of projects, including the International Space Station and returning to the moon. The European Space Agency, for example, plans to provide modules for Gateway, a lunar space station that is central to NASA's Artemis program to land American astronauts back on the moon... The agency also helps coordinate support from foreign nations for the Artemis accords, which set goals for transparency and data sharing — and help create a level of trust in an unregulated part of the universe. 
But the reallocation could have some benefits. Such a move would bring headquarters employees closer to the processes they manage. And it would give legislative liaison staff a chance to interact with lawmakers in their districts. "You're probably getting a lot more time with [lawmakers] at the local center or hosting events in the state or district," said Tom Culligan, a longtime space lobbyist,, the space industry lobbyist.
]]></content:encoded></item><item><title>The TechBeat: House Of Doge And Dogecoin Foundation Unveil Board-Elect, Advisors And Global Dogecoin Adoption Plan (3/23/2025)</title><link>https://hackernoon.com/3-23-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sun, 23 Mar 2025 06:11:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @darragh [ 8 Min read ] 
 Explore Trump's bold Mar‑a‑Lago Accord—a daring plan using century bonds, currency moves, and digital assets to reshape America's economy. Read More.By @dineshbesiahgari [ 17 Min read ] 
 AutoResponder AI automates email replies using AWS & AI. Streamline inbox management, boost productivity, and stay responsive with this serverless Gmail assista Read More.By @dineshbesiahgari [ 4 Min read ] 
 Explore the AI Knowledge Ark, a visionary initiative to safeguard humanity’s intellectual and cultural legacy. Learn how AI ensures knowledge survival, inspired Read More.By @dineshbesiahgari [ 5 Min read ] 
 Can AI optimize itself for sustainability? Explore how LLMs can reduce energy use through pruning, quantization, and hardware co-design for a greener future. Read More.By @moonlock [ 5 Min read ] 
 Is Netflix’s 'Zero Day' realistic? A malware researcher breaks down 3 real cyber threats and 3 myths, separating cybersecurity fact from fiction. Read More.By @noda [ 3 Min read ] 
 Discover how Open Banking is evolving from Pay-by-Bank to intelligent, automated payments with VRP.  Read More.By @kfamyn [ 20 Min read ] 
 Master Swift Grand Central Dispatch in iOS: learn thread/queue interplay, sync vs async execution, QoS, and deadlock pitfalls via practical exercises. Read More.By @jessidg [ 4 Min read ] 
 The U.S. suspends TechWomen, a vital STEM exchange program, leaving global scholars in limbo. Advocates rally to restore funding and save the initiative. Read More.By @rex12543 [ 6 Min read ] 
 From designer to developer—discover how I built the Pixel Icon Library website with Cursor AI, Tailwind CSS, and GitHub Pages, all with zero coding experience! Read More.By @thomascherickal [ 7 Min read ] 
 Vibe coding is not coming for developers. Its helping developers become superheroes by turbocharging them - provided that they embrace AI into their workflow! Read More.By @tristanbietsch [ 6 Min read ] 
 Follow my solo journey building a Bitcoin wallet from scratch. Technical challenges, architecture decisions, and lessons learned over two months of development. Read More.By @badmonster0 [ 8 Min read ] 
 Tutorial on indexing codebase for RAG with CocoIndex and Tree-sitter: chunking, embedding, semantic search, and build vector index for efficient retrieval. Read More.By @danielcrouch [ 7 Min read ] 
 AI-generated code is leading cause for top 10 vulnerabilities and nearly 40% of code has security bugs. Read More.By @chainwire [ 5 Min read ] 
 This milestone partnership, aimed at advancing Dogecoin ($DOGE) as a widely accepted global currency, establishes House of Doge as the official and exclusive pa Read More.By @sergej.kostenko [ 6 Min read ] 
 Clone traps automate threat response by deceiving the deceivers and fighting AI with AI. Read More.By @z3nch4n [ 5 Min read ] 
 Cloud security booms to $156.25B by 2032 despite on-prem shifts. Hybrid & multi-cloud demand soars, quantum threats loom—your expertise is still key! Read More.By @sharkroman [ 7 Min read ] 
 How AI threatens global jobs: 44% of AI apps can replace workers, creating risks of mass unemployment. Learn about the "Turing Trap" and potential solutions. Read More.By @obyte [ 8 Min read ] 
 Crypto crime continues to rise, reminding investors of the importance of caution. That’s why we’ll dive here into some of the worst crypto scams ever! Read More.]]></content:encoded></item><item><title>Hungary To Use Facial Recognition to Suppress Pride March</title><link>https://yro.slashdot.org/story/25/03/22/2333213/hungary-to-use-facial-recognition-to-suppress-pride-march?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 03:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Hungary's Parliament not only voted to ban Pride events. They also voted to "allow authorities to use facial recognition software to identify attenders and potentially fine them," reports the Guardian.

[The nationwide legislation] amends the country's law on assembly to make it an offence to hold or attend events that violate Hungary's contentious "child protection" legislation, which bars any "depiction or promotion" of homosexuality to minors under the age of 18. The legislation was condemned by Amnesty International, which described it as the latest in a series of discriminatory measures the Hungarian authorities have taken against LGBTQ+ people... 
Organisers said they planned to go ahead with the march in Budapest, despite the law's stipulation that those who attend a prohibited event could face fines of up to 200,000 Hungarian forints [£425 or $549 U.S. dollars].
]]></content:encoded></item><item><title>Italy Demands Google Poison Its Public DNS Under Strict Piracy Shield Law</title><link>https://yro.slashdot.org/story/25/03/23/0043217/italy-demands-google-poison-its-public-dns-under-strict-piracy-shield-law?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Mar 2025 01:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Italy is using its Piracy Shield law to go after Google," reports Ars Technica, "with a court ordering the Internet giant to immediately begin poisoning its public DNS servers" to prevent people from reaching pirate streams of football games. 

"Italy's communication regulator praises the ruling and hopes to continue sticking it to international tech firms."
Spotted by TorrentFreak, AGCOM Commissioner Massimiliano Capitanio took to LinkedIn to celebrate the ruling, as well as the existence of the Italian Piracy Shield. "The Judge confirmed the value of AGCOM's investigations, once again giving legitimacy to a system for the protection of copyright that is unique in the world," said Capitanio. Capitanio went on to complain that Google has routinely ignored AGCOM's listing of pirate sites, which are supposed to be blocked in 30 minutes or less under the law. He noted the violation was so clear-cut that the order was issued without giving Google a chance to respond, known as inaudita altera parte in Italian courts.
]]></content:encoded></item><item><title>Jonah Peretti helped shaped digital media — can he do it again?</title><link>https://techcrunch.com/2025/03/22/jonah-peretti-helped-shaped-digital-media-can-he-do-it-again/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sun, 23 Mar 2025 01:00:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Jonah Peretti’s career to date has been defined by constant reinvention. As the founder and CEO of BuzzFeed, Peretti has been at the forefront of digital media for almost two decades, navigating changes as the once small startup transformed into, at one point, a multimedia powerhouse. But as the company grows older, one question lingers: […]]]></content:encoded></item></channel></rss>